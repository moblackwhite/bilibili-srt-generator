1
00:00:00,000 --> 00:00:01,520
我们的问题10

2
00:00:01,520 --> 00:00:05,560
如果买的话是显存越大越好吗

3
00:00:05,560 --> 00:00:09,080
还有什么价值指评价值标

4
00:00:09,080 --> 00:00:14,759
当然是越大越好了

5
00:00:14,759 --> 00:00:15,720
没什么

6
00:00:15,720 --> 00:00:16,480
就是说

7
00:00:16,480 --> 00:00:18,960
但是显存是很贵的事情

8
00:00:19,640 --> 00:00:22,400
显存比CPU内存要贵

9
00:00:23,719 --> 00:00:26,400
我们没有讲说GPU是怎么工作的

10
00:00:26,440 --> 00:00:28,199
我们会在之后的一节

11
00:00:28,199 --> 00:00:30,320
给大家介绍GPU的工作原理

12
00:00:30,320 --> 00:00:32,119
就说真的去给大家讲

13
00:00:32,119 --> 00:00:33,839
CPUGPU的区别什么样子

14
00:00:33,879 --> 00:00:37,399
但是说从简单来讲

15
00:00:37,840 --> 00:00:41,240
GPU的显存是越大越好

16
00:00:41,879 --> 00:00:44,159
但是越大越贵

17
00:00:44,159 --> 00:00:47,679
所以你要在你的承受范围之内买

18
00:00:48,719 --> 00:00:49,960
我们还有一点问题

19
00:00:50,159 --> 00:00:54,920
我们干脆来一次过一下

20
00:00:54,920 --> 00:00:59,359
GPU会不会存在性能上限

21
00:00:59,359 --> 00:01:00,280
当然会的了

22
00:01:00,359 --> 00:01:02,719
就是说现在还GPU快

23
00:01:02,719 --> 00:01:05,599
GPU是通过不断的加核的个数

24
00:01:05,599 --> 00:01:07,480
来突破摩尔定律的

25
00:01:07,680 --> 00:01:09,599
所以GPU确实会有上限

26
00:01:11,200 --> 00:01:12,799
不会是无限的往下走的

27
00:01:12,799 --> 00:01:15,120
现在已经7纳米到4纳米

28
00:01:15,120 --> 00:01:16,760
4纳米已经在4纳米

29
00:01:16,760 --> 00:01:18,640
可能是两三年之后到4纳米

30
00:01:18,640 --> 00:01:19,640
已经是到了

31
00:01:20,719 --> 00:01:22,520
电的极限了

32
00:01:22,560 --> 00:01:24,280
然后加核的话

33
00:01:24,280 --> 00:01:25,400
你可以一直往下加

34
00:01:25,400 --> 00:01:26,519
但是也加到一定程度

35
00:01:26,519 --> 00:01:27,799
就没有太大的

36
00:01:29,599 --> 00:01:32,159
也没有太大的加的

37
00:01:33,680 --> 00:01:34,799
就那么大芯片

38
00:01:34,799 --> 00:01:36,120
你得把芯片做大

39
00:01:36,200 --> 00:01:38,240
所以性能会有上限

40
00:01:38,280 --> 00:01:39,840
但很有可能还是可以

41
00:01:39,840 --> 00:01:40,760
就单卡的性能

42
00:01:40,760 --> 00:01:42,319
可能还是能够撑几年

43
00:01:42,599 --> 00:01:44,319
就可能还撑个五六年没关系

44
00:01:45,960 --> 00:01:48,480
显存不够没怎么办

45
00:01:48,719 --> 00:01:49,960
第12问题

46
00:01:50,079 --> 00:01:51,640
显存不够的话

47
00:01:51,920 --> 00:01:53,960
只能把batch size调小

48
00:01:54,840 --> 00:01:55,840
batch size但是

49
00:01:55,840 --> 00:01:57,079
CUDA占用率很低

50
00:01:57,079 --> 00:01:58,079
怎么办呢

51
00:01:58,120 --> 00:01:59,400
对

52
00:01:59,439 --> 00:02:01,159
就是说你batch size一小

53
00:02:01,400 --> 00:02:03,280
你的性能就往下降

54
00:02:03,359 --> 00:02:04,200
所以

55
00:02:05,120 --> 00:02:06,599
那你就只能我的建议是

56
00:02:06,599 --> 00:02:08,240
你把你的模型变小一点

57
00:02:09,159 --> 00:02:10,879
就是把模型变小一点

58
00:02:11,319 --> 00:02:13,159
就你用resnet50的话

59
00:02:13,159 --> 00:02:15,159
如果你说resnetU2的话

60
00:02:15,159 --> 00:02:15,960
你就改成50

61
00:02:15,960 --> 00:02:17,280
或者改成更小的网络

62
00:02:19,960 --> 00:02:21,960
GPU使用率是不是越高越好

63
00:02:21,960 --> 00:02:23,000
长期满负荷

64
00:02:23,000 --> 00:02:24,400
是不是对显卡不太好

65
00:02:24,560 --> 00:02:26,800
满负荷对显卡是没问题的

66
00:02:27,039 --> 00:02:29,120
唯一的问题是说你会过热

67
00:02:30,159 --> 00:02:31,759
就你得去看你的温度

68
00:02:32,120 --> 00:02:34,360
你的自己买的卡的话

69
00:02:34,360 --> 00:02:36,599
你温度最好不要超过80太久

70
00:02:36,599 --> 00:02:37,560
你去看你的温度

71
00:02:37,560 --> 00:02:38,840
就你能看到你的温度

72
00:02:38,840 --> 00:02:41,080
就是说你不要超过80

73
00:02:41,080 --> 00:02:42,280
或90度以上

74
00:02:42,280 --> 00:02:42,759
就

75
00:02:43,039 --> 00:02:43,920
到90度以上

76
00:02:43,920 --> 00:02:46,319
CPU就会说温度太高

77
00:02:46,319 --> 00:02:47,199
它就会降频

78
00:02:47,199 --> 00:02:48,560
它会自动做保护

79
00:02:48,599 --> 00:02:49,840
但是是可能会少的

80
00:02:49,840 --> 00:02:51,039
反正我是少过很多块卡

81
00:02:51,039 --> 00:02:52,000
我还把我们整

82
00:02:52,000 --> 00:02:52,960
我在CMU的时候

83
00:02:52,960 --> 00:02:55,199
把整栋楼的电给烧断过了

84
00:02:55,319 --> 00:02:58,199
就是说大家就是说满负荷是没问题

85
00:02:58,199 --> 00:02:59,719
但是一定要注意降温

86
00:02:59,719 --> 00:03:00,360
降温的话

87
00:03:00,360 --> 00:03:00,840
你就是

88
00:03:01,199 --> 00:03:02,639
买风扇大一点

89
00:03:02,680 --> 00:03:03,680
或者用水冷

90
00:03:04,960 --> 00:03:05,639
都可以

91
00:03:09,560 --> 00:03:11,800
就你的系统是windows显卡

92
00:03:11,800 --> 00:03:13,680
是1080ti显卡挺好的

93
00:03:13,719 --> 00:03:15,840
然后你用convert device等于0

94
00:03:15,879 --> 00:03:16,879
通常两个问题

95
00:03:16,879 --> 00:03:18,719
一个是说你cuda没装好

96
00:03:19,359 --> 00:03:22,159
你用NVIDIA-SMA看一下

97
00:03:22,159 --> 00:03:22,840
你有没有

98
00:03:22,879 --> 00:03:24,879
第二个是你可能是装的CPU版的

99
00:03:24,879 --> 00:03:25,400
PyTorch

100
00:03:25,400 --> 00:03:27,039
你要装GPU版PyTorch

101
00:03:29,759 --> 00:03:31,280
一般使用GPU训练

102
00:03:31,280 --> 00:03:32,240
data在哪一步

103
00:03:32,240 --> 00:03:33,079
to GPU比较好

104
00:03:33,079 --> 00:03:34,360
一般你是到最后

105
00:03:34,560 --> 00:03:36,120
就是说在network之前

106
00:03:36,120 --> 00:03:37,479
to GPU一般是这样子

107
00:03:37,520 --> 00:03:39,879
因为你的很多数据的变化

108
00:03:39,879 --> 00:03:41,599
就是说做data pre-processing

109
00:03:41,599 --> 00:03:42,280
在GPU上

110
00:03:42,280 --> 00:03:43,840
不一定支持的比较好

111
00:03:44,000 --> 00:03:45,479
就是说假设你的data

112
00:03:45,479 --> 00:03:47,479
在GPU上能做得比较好的话

113
00:03:47,480 --> 00:03:49,040
你可以往前走

114
00:03:49,040 --> 00:03:50,920
尽量在GPU上做运算

115
00:03:50,960 --> 00:03:52,600
很多时候做图片的话

116
00:03:52,600 --> 00:03:54,160
你可以在GPU上

117
00:03:54,160 --> 00:03:55,760
做一些图片的预处理

118
00:03:55,960 --> 00:03:57,080
但反过来讲

119
00:03:57,080 --> 00:03:59,040
是说你也得占用GPU的资源

120
00:03:59,040 --> 00:03:59,760
就是说

121
00:03:59,880 --> 00:04:00,920
你算一下

122
00:04:00,920 --> 00:04:02,520
就是说假设你的数据

123
00:04:02,520 --> 00:04:04,120
读的比GPU计算快的话

124
00:04:04,120 --> 00:04:05,360
你就尽量在CPU做

125
00:04:05,360 --> 00:04:07,760
这样子把你的计算

126
00:04:07,760 --> 00:04:09,280
留给你的网络

127
00:04:09,280 --> 00:04:11,000
就是说留给你的network

128
00:04:11,000 --> 00:04:12,400
就是说你的GPU的计算

129
00:04:12,400 --> 00:04:13,240
留给你的

130
00:04:13,280 --> 00:04:15,560
做真的做前向反向运算

131
00:04:17,480 --> 00:04:19,640
Tensor.cuda和2Device

132
00:04:19,640 --> 00:04:20,879
不一样的是

133
00:04:21,280 --> 00:04:22,640
2是你的network

134
00:04:22,640 --> 00:04:23,840
的module的东西

135
00:04:23,840 --> 00:04:25,520
module有个2Device

136
00:04:25,560 --> 00:04:27,280
module只能用2Device

137
00:04:27,280 --> 00:04:29,240
这个就是copy的某一个GPU

138
00:04:31,600 --> 00:04:33,160
GPU上推理是什么意思

139
00:04:33,160 --> 00:04:34,120
就是做inference

140
00:04:34,120 --> 00:04:34,879
就不做training

141
00:04:34,879 --> 00:04:36,520
就是在GPU上forward

142
00:04:36,840 --> 00:04:38,600
就我们刚刚就是在GPU上

143
00:04:38,600 --> 00:04:39,240
点forward

144
00:04:39,240 --> 00:04:40,400
就call那个函数

145
00:04:40,439 --> 00:04:41,560
他就是做推理

146
00:04:41,600 --> 00:04:43,600
就是训练就是算梯度

147
00:04:44,600 --> 00:04:48,200
使用GPU后

148
00:04:48,200 --> 00:04:49,080
加速不明显

149
00:04:49,080 --> 00:04:49,720
有哪些可能

150
00:04:49,720 --> 00:04:51,080
运行是GPU使用力

151
00:04:51,080 --> 00:04:51,920
在60到70

152
00:04:51,920 --> 00:04:53,600
60到70不低了

153
00:04:54,879 --> 00:04:55,560
不低了

154
00:04:55,560 --> 00:04:57,879
但是不明显的话

155
00:04:58,800 --> 00:05:00,480
你可能GPU不够快

156
00:05:00,480 --> 00:05:01,720
GPU也没有说

157
00:05:01,720 --> 00:05:03,240
一定要比CPU快多少

158
00:05:03,280 --> 00:05:04,760
说不定你CPU挺快的

159
00:05:05,920 --> 00:05:07,360
或者是说你的

160
00:05:11,520 --> 00:05:12,480
你可以去看一下

161
00:05:12,480 --> 00:05:14,280
你的GPU的计算

162
00:05:14,319 --> 00:05:16,240
就是看你GPU的形况怎么样

163
00:05:16,879 --> 00:05:19,600
而且你的网络可能60到70

164
00:05:19,600 --> 00:05:20,560
我觉得还行

165
00:05:21,160 --> 00:05:24,480
就是说你可以尝试优化

166
00:05:24,480 --> 00:05:25,040
优化看看

167
00:05:25,040 --> 00:05:26,080
能不能到80%

168
00:05:26,080 --> 00:05:26,680
90%

169
00:05:26,720 --> 00:05:27,759
对CN的话

170
00:05:27,759 --> 00:05:29,120
很容易到80%

171
00:05:29,120 --> 00:05:30,600
90%或95%以上

172
00:05:32,640 --> 00:05:34,120
但media上训练模型

173
00:05:34,120 --> 00:05:35,680
如果在其他GPU跑的话

174
00:05:35,680 --> 00:05:36,759
也支持吗

175
00:05:36,960 --> 00:05:38,319
也能 不好

176
00:05:38,319 --> 00:05:39,040
支持不好

177
00:05:39,040 --> 00:05:39,759
就是这样

178
00:05:40,160 --> 00:05:41,000
就比较难

179
00:05:41,000 --> 00:05:42,400
就AMD的话

180
00:05:42,400 --> 00:05:44,400
最近开始慢慢的好了

181
00:05:45,240 --> 00:05:46,120
比较难搞

182
00:05:46,760 --> 00:05:48,400
我不建议大家去试

183
00:05:48,400 --> 00:05:49,520
反正我是

184
00:05:51,040 --> 00:05:52,440
指公司关系

185
00:05:52,440 --> 00:05:53,760
所以反正试过挺多的

186
00:05:53,800 --> 00:05:55,960
就都挺难用的

187
00:05:57,080 --> 00:06:00,720
怎么评价Apple M1GPU和CPU共用内存

188
00:06:04,120 --> 00:06:04,920
就是说

189
00:06:06,400 --> 00:06:08,120
共不共用内存不是关键

190
00:06:08,120 --> 00:06:09,800
主要是看内存的带宽

191
00:06:09,800 --> 00:06:10,840
他的M1

192
00:06:10,840 --> 00:06:11,800
他的内存带宽

193
00:06:11,800 --> 00:06:12,680
我记得还行

194
00:06:12,680 --> 00:06:13,319
就是说

195
00:06:14,360 --> 00:06:16,520
就是因为他在一个芯片里面

196
00:06:16,639 --> 00:06:18,480
就是说你基本上就是集成显卡

197
00:06:18,480 --> 00:06:20,199
都是跟CPU共用内存的

198
00:06:20,240 --> 00:06:22,160
因为他的内存作用在芯片里面

199
00:06:22,199 --> 00:06:23,240
他还可以

200
00:06:23,240 --> 00:06:24,560
其实M1的内存

201
00:06:24,560 --> 00:06:25,960
我没实际测过

202
00:06:26,360 --> 00:06:28,639
我觉得M1的GPU内存还不错

203
00:06:29,160 --> 00:06:31,040
因为他跟他都做到芯片里面

204
00:06:31,040 --> 00:06:32,199
所以相对来说比较快

205
00:06:33,120 --> 00:06:34,600
Huda和GPU什么关系

206
00:06:34,639 --> 00:06:35,720
GPU是硬件

207
00:06:35,720 --> 00:06:38,759
Huda是可以认为是你的开发的SDK

208
00:06:38,800 --> 00:06:39,680
就跟你

209
00:06:39,680 --> 00:06:42,840
理论上你的CPU和你的C++编译器

210
00:06:42,840 --> 00:06:44,040
有什么关系的区别

211
00:06:44,079 --> 00:06:45,079
就你的Huda认为

212
00:06:45,079 --> 00:06:47,240
就是编译Huda的编译器

213
00:06:49,639 --> 00:06:52,199
TensorFlow对M1作用

214
00:06:52,199 --> 00:06:55,160
划了性能比TPUCPU提升不少

215
00:06:55,199 --> 00:06:57,519
看好M1做DL的未来吗

216
00:06:58,480 --> 00:07:00,840
这种新除非你在苹果上做

217
00:07:00,840 --> 00:07:02,319
不然的话你做出来干嘛

218
00:07:02,360 --> 00:07:03,840
M1只有苹果上有

219
00:07:03,879 --> 00:07:05,439
M1是很好的芯片

220
00:07:05,480 --> 00:07:06,560
肯定的

221
00:07:06,560 --> 00:07:08,360
反正我肯定会买新的iMac

222
00:07:08,560 --> 00:07:10,000
用来剪视频

223
00:07:10,040 --> 00:07:10,879
但是

224
00:07:12,040 --> 00:07:13,000
你得想好

225
00:07:13,000 --> 00:07:16,800
你是不是要在苹果的硬件上做芯片

226
00:07:16,879 --> 00:07:17,280
对吧

227
00:07:17,280 --> 00:07:18,759
你又买不到M1芯片

228
00:07:18,759 --> 00:07:19,160
对吧

229
00:07:21,160 --> 00:07:22,120
GPU上推理

230
00:07:22,120 --> 00:07:24,720
是不是可以帮助提升性能的上限

231
00:07:24,840 --> 00:07:28,080
推理会通常会比training会

232
00:07:28,280 --> 00:07:30,199
可能利用率会好一点的

233
00:07:30,199 --> 00:07:32,759
因为内存推理内存不是关键

234
00:07:32,960 --> 00:07:34,680
会把batch size大一点

235
00:07:38,360 --> 00:07:40,960
自定义的block被放在同一个sequencer的

236
00:07:40,960 --> 00:07:42,000
不同层

237
00:07:42,040 --> 00:07:44,280
但是不想share参数怎么办

238
00:07:44,720 --> 00:07:46,080
它不是share参数了

239
00:07:46,080 --> 00:07:48,920
就是说你每一次创建一个新的实例

240
00:07:48,920 --> 00:07:50,560
它就会有自己的参数

241
00:07:50,920 --> 00:07:53,480
只有当你把一个创建的实例

242
00:07:53,480 --> 00:07:54,680
把它放在不同地方

243
00:07:54,680 --> 00:07:55,639
它才会share

244
00:07:55,680 --> 00:07:57,600
每一次你call MLP

245
00:07:57,600 --> 00:07:57,960
括号

246
00:07:57,960 --> 00:07:59,480
它就会创建新的参数

247
00:07:59,480 --> 00:08:00,360
它不会share

248
00:08:00,360 --> 00:08:01,960
所以你就不用担心这个事情

249
00:08:01,960 --> 00:08:03,040
默认是不share

