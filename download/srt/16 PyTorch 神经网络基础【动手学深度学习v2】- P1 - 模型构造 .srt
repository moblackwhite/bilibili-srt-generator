1
00:00:00,000 --> 00:00:02,439
第一个问题是说

2
00:00:02,439 --> 00:00:07,000
将类别变量转成伪变量的时候

3
00:00:07,000 --> 00:00:09,439
类存炸掉了怎么办

4
00:00:09,439 --> 00:00:13,679
应该是这个问题应该是我们之前竞赛的问题

5
00:00:13,679 --> 00:00:14,679
就是说

6
00:00:15,880 --> 00:00:18,400
这个问题是意思给大家解释一下

7
00:00:19,120 --> 00:00:20,080
就是说

8
00:00:20,519 --> 00:00:21,679
我们之前有讲过

9
00:00:21,679 --> 00:00:24,760
假设你特征是一些字符串的话

10
00:00:24,760 --> 00:00:26,240
是一些离散值的话

11
00:00:26,519 --> 00:00:29,080
你怎么样把它变成一个tensor呢

12
00:00:29,120 --> 00:00:31,320
那我们的做法是用one-hot encoding

13
00:00:31,320 --> 00:00:33,200
就是把所有的值拿出来

14
00:00:33,200 --> 00:00:35,679
然后生成一个很长的列表

15
00:00:35,719 --> 00:00:38,840
假设你的值里面有一百万个

16
00:00:38,880 --> 00:00:41,679
有一百万个不一样的元素的话

17
00:00:41,719 --> 00:00:42,880
那么你新的矩阵

18
00:00:42,880 --> 00:00:45,480
就会变成至少有一百万个列

19
00:00:46,000 --> 00:00:48,120
那么很有可能你的类存就炸掉了

20
00:00:49,000 --> 00:00:51,600
所以这个地方有两个办法

21
00:00:52,280 --> 00:00:55,320
一个办法是你使用西数矩阵来存

22
00:00:56,000 --> 00:00:58,560
因为就是说你每一行里面

23
00:00:58,560 --> 00:01:00,600
其实就一个一剩下都是零

24
00:01:00,600 --> 00:01:02,440
那你就不应该把它存成一个

25
00:01:02,440 --> 00:01:04,560
使全部把零给存下来

26
00:01:04,600 --> 00:01:05,879
可以存成西数矩阵

27
00:01:05,879 --> 00:01:07,960
这个是常用的做法

28
00:01:08,799 --> 00:01:10,000
第二个是说

29
00:01:10,040 --> 00:01:13,439
假设你真的遇到这样子情况的话

30
00:01:13,879 --> 00:01:14,920
那应该是

31
00:01:15,640 --> 00:01:17,240
你就不应该把这个

32
00:01:18,079 --> 00:01:20,439
列变成一个伪变量

33
00:01:20,480 --> 00:01:21,840
就不应该用one-hot encoding

34
00:01:21,879 --> 00:01:23,200
这个太低效了

35
00:01:23,560 --> 00:01:26,000
就是说在如果你是知道

36
00:01:26,000 --> 00:01:28,320
你应该用one-hot encoding的时候

37
00:01:28,320 --> 00:01:29,280
你是可以用的

38
00:01:29,320 --> 00:01:31,040
但是目前来说

39
00:01:31,040 --> 00:01:32,720
我们其实one-hot encoding

40
00:01:32,760 --> 00:01:35,800
当你真的是有很多很多变量的时候

41
00:01:35,800 --> 00:01:37,200
你应该通过别的方法

42
00:01:37,240 --> 00:01:40,040
比如说我觉得你如果是讲竞赛的话

43
00:01:40,040 --> 00:01:41,520
里面有比如说地址

44
00:01:41,880 --> 00:01:43,240
每个人的地址是不一样的

45
00:01:43,240 --> 00:01:44,320
所以你有多少个房子

46
00:01:44,320 --> 00:01:45,200
就多少个地址

47
00:01:45,320 --> 00:01:46,840
然后还有一个summary

48
00:01:46,960 --> 00:01:47,960
就每个人的summary

49
00:01:47,960 --> 00:01:49,480
也就是那个介绍肯定是不一样的

50
00:01:50,040 --> 00:01:51,920
所以你的一个做法是说

51
00:01:52,160 --> 00:01:53,480
你可以做back of words

52
00:01:53,480 --> 00:01:56,240
就是说你可以把每一个词给拿出来

53
00:01:56,640 --> 00:01:58,160
就你不要把整个句子拿出来

54
00:01:58,280 --> 00:01:59,080
就每个词

55
00:01:59,640 --> 00:02:01,240
就比如说有三个词的话

56
00:02:01,240 --> 00:02:03,160
那么我这一行就有三个

57
00:02:03,160 --> 00:02:04,960
每一个对应的是那个词

58
00:02:05,280 --> 00:02:05,560
对吧

59
00:02:05,560 --> 00:02:06,680
这是最简单的做法

60
00:02:06,840 --> 00:02:09,280
然后我们在之后再讲

61
00:02:09,280 --> 00:02:10,879
载元处理的话

62
00:02:10,920 --> 00:02:13,960
会给大家介绍更多的课程表达的方法

63
00:02:14,120 --> 00:02:15,760
大家也可以往后看一下

64
00:02:15,920 --> 00:02:17,719
最简单的办法是说在这里

65
00:02:17,840 --> 00:02:19,160
如果你没有往后看的话

66
00:02:19,160 --> 00:02:21,360
那你就是对这样子类就不做

67
00:02:21,879 --> 00:02:22,840
往后的encoding了

68
00:02:22,840 --> 00:02:23,879
就把它忽略掉

69
00:02:27,240 --> 00:02:28,240
对就是说同样的

70
00:02:28,240 --> 00:02:30,320
我觉得第二个问题是跟之前是一样的

71
00:02:30,320 --> 00:02:32,360
就是object怎么处理

72
00:02:32,520 --> 00:02:34,960
就是你如果你不知道的话

73
00:02:34,960 --> 00:02:36,120
就不要处理就丢掉它

74
00:02:36,120 --> 00:02:37,360
就不要做到往后

75
00:02:37,360 --> 00:02:38,760
就不要理散化

76
00:02:38,920 --> 00:02:40,480
不然的话你就往后看

77
00:02:40,480 --> 00:02:43,080
就看到NLP那一张就讲怎么样

78
00:02:43,080 --> 00:02:43,680
back of words

79
00:02:43,680 --> 00:02:44,600
怎么样做

80
00:02:44,719 --> 00:02:49,360
甚至你可以做那种切词类的做法

81
00:02:49,360 --> 00:02:54,680
第三个问题是说

82
00:02:54,680 --> 00:02:57,160
NLP的乘数

83
00:02:57,280 --> 00:03:01,360
就是说有大概规律吗

84
00:03:01,560 --> 00:03:06,160
还是就是说比如说你input size的除2除4

85
00:03:06,200 --> 00:03:09,520
还是靠cross validation来寻餐

86
00:03:09,960 --> 00:03:12,200
我们有讲过

87
00:03:12,240 --> 00:03:13,800
大家可以往回看一下

88
00:03:13,800 --> 00:03:16,120
我们有一次QA应该是讲过

89
00:03:16,200 --> 00:03:19,000
怎么样一些经验上的东西

90
00:03:19,000 --> 00:03:21,120
不过我觉得你说的也挺好的

91
00:03:21,120 --> 00:03:23,520
就是说你先把它除2

92
00:03:23,520 --> 00:03:24,319
然后再除2

93
00:03:24,319 --> 00:03:25,360
再除2也挺好的

94
00:03:25,360 --> 00:03:28,319
我觉得但要么就回去看一下我们的录像

95
00:03:28,319 --> 00:03:30,520
应该是讲NLP那一张的

96
00:03:30,560 --> 00:03:33,280
QA里面有讲过一些经验上的东西

97
00:03:33,319 --> 00:03:35,800
另外一个他确实没有特别

98
00:03:37,120 --> 00:03:38,360
特别靠谱的方法

99
00:03:38,360 --> 00:03:40,560
主要是靠自己搜餐

100
00:03:40,599 --> 00:03:41,400
这就是挑餐

101
00:03:49,039 --> 00:03:50,439
就是说实例化后

102
00:03:50,439 --> 00:03:52,000
不用调用实例方法

103
00:03:52,000 --> 00:03:54,199
又不用调用forward函数

104
00:03:54,360 --> 00:03:56,240
可以调用net.x

105
00:03:56,240 --> 00:03:59,919
是因为傅瑞实现了模式方法吗

106
00:03:59,919 --> 00:04:01,719
我觉得这个问题挺好的

107
00:04:01,719 --> 00:04:02,840
因为我之前忘讲了

108
00:04:02,840 --> 00:04:05,680
就是说我们实现的是两个类对吧

109
00:04:06,159 --> 00:04:07,240
两个函数

110
00:04:07,240 --> 00:04:10,479
就init method和forward method

111
00:04:10,759 --> 00:04:13,439
但我们从来没实现过这个函数

112
00:04:13,439 --> 00:04:16,959
这个函数他调其实是net.x

113
00:04:16,959 --> 00:04:18,519
下滑线x的函数

114
00:04:18,519 --> 00:04:20,799
这是python自己的一个build in的

115
00:04:20,799 --> 00:04:21,479
一个method

116
00:04:21,759 --> 00:04:24,680
就是说其实是因为他用的是nmodule

117
00:04:24,680 --> 00:04:27,359
nmodule把调用函数

118
00:04:27,359 --> 00:04:28,959
调用下滑线下滑线

119
00:04:28,959 --> 00:04:30,359
扩下滑线下滑线函数

120
00:04:30,399 --> 00:04:31,959
等价于forward函数了

121
00:04:32,759 --> 00:04:34,159
然后就是做成等价的

122
00:04:34,159 --> 00:04:37,560
所以导致说你可以这里

123
00:04:37,560 --> 00:04:38,519
当然可以点

124
00:04:38,759 --> 00:04:41,279
点forward可以这么调

125
00:04:43,079 --> 00:04:44,039
可以这么调对吧

126
00:04:44,039 --> 00:04:45,159
这个当然是没错的

127
00:04:45,159 --> 00:04:46,279
但是为了简洁起见

128
00:04:46,279 --> 00:04:47,959
这个函数已经被做一次映射

129
00:04:47,959 --> 00:04:49,759
因为在module里面做映射了

130
00:04:49,759 --> 00:04:50,519
所以是可以的

131
00:04:54,519 --> 00:04:55,479
第5个问题

132
00:04:55,599 --> 00:04:59,079
forward函数的函数怎么得出来的

133
00:04:59,239 --> 00:05:01,599
这个是你的神经网络定义出来的

134
00:05:01,599 --> 00:05:03,159
就是神经网络是怎么定义的

135
00:05:03,159 --> 00:05:05,039
那么他的forward函数怎么实现

136
00:05:05,279 --> 00:05:06,919
就是说你的forward函数是

137
00:05:06,959 --> 00:05:09,519
被这paper里面的公式敲出来的

138
00:05:09,519 --> 00:05:18,079
凯明初始化是按什么样规则初始化的

139
00:05:18,079 --> 00:05:19,959
我们没有讲凯明初始化

140
00:05:20,079 --> 00:05:21,799
就是凯明就是何凯明

141
00:05:21,799 --> 00:05:23,199
就是一个人名

142
00:05:23,199 --> 00:05:26,959
所以我们常他论文里面提出来的方法

143
00:05:27,000 --> 00:05:29,719
我们一般叫做凯明初始化

144
00:05:29,759 --> 00:05:32,279
凯明初始化跟你的XV初始化是一样

145
00:05:32,279 --> 00:05:33,279
XV也是个人名

146
00:05:34,120 --> 00:05:36,079
很像我们就没讲了

147
00:05:36,079 --> 00:05:37,319
大家可以去看一下实现

148
00:05:37,319 --> 00:05:38,879
我们这里就不仔细讲了

149
00:05:38,920 --> 00:05:40,839
它是很类似的一个东西

150
00:05:40,879 --> 00:05:44,360
就是根据你的输出出来做一些初始化

151
00:05:44,399 --> 00:05:46,680
在实际上来说

152
00:05:46,719 --> 00:05:48,319
实际效果来讲

153
00:05:48,399 --> 00:05:49,759
我觉得都差不多

154
00:05:49,759 --> 00:05:50,959
就是说凯明也好

155
00:05:50,959 --> 00:05:51,879
XV也好

156
00:05:51,959 --> 00:05:53,600
就是说初始化这个东西

157
00:05:53,600 --> 00:05:55,040
大家不要太迷信了

158
00:05:55,159 --> 00:05:57,079
初始化的主要的方法

159
00:05:57,079 --> 00:05:59,680
是让你在模型一开始迭代的时候

160
00:05:59,680 --> 00:06:01,159
不要炸掉

161
00:06:01,920 --> 00:06:03,560
就不让大家训练有问题

162
00:06:04,399 --> 00:06:06,319
就是说具体你放在哪里

163
00:06:06,319 --> 00:06:06,759
放在这里

164
00:06:06,759 --> 00:06:07,839
放在那里其实差不多

165
00:06:08,440 --> 00:06:09,720
就是说初始化

166
00:06:09,720 --> 00:06:10,600
主要的是说

167
00:06:10,600 --> 00:06:13,320
让你的整个我们有讲过之前

168
00:06:13,320 --> 00:06:14,440
就是说主要方法

169
00:06:14,440 --> 00:06:16,880
是说让你在模型一开始的时候

170
00:06:16,880 --> 00:06:20,400
使得每一层它的输入输出的大小

171
00:06:20,400 --> 00:06:21,880
在一个尺度上面

172
00:06:21,920 --> 00:06:24,280
不要说越往后面就越大

173
00:06:24,280 --> 00:06:25,400
或者越往后面越小

174
00:06:25,400 --> 00:06:26,280
就炸掉了

175
00:06:26,920 --> 00:06:28,120
要么就没了

176
00:06:28,160 --> 00:06:29,760
所以初始化主要干这个事情

177
00:06:29,920 --> 00:06:32,680
所以你稍微大一点

178
00:06:32,680 --> 00:06:33,680
稍微小一点没关系

179
00:06:33,680 --> 00:06:35,560
只要在合理范围里面就行了

180
00:06:35,680 --> 00:06:37,480
以及初始化很难说

181
00:06:37,480 --> 00:06:40,000
初始化会给最终模型带来

182
00:06:40,639 --> 00:06:42,240
不一样也不一定

183
00:06:42,240 --> 00:06:44,160
就是说几乎我觉得差不多

184
00:06:44,160 --> 00:06:46,360
就是说只要你一开始没有什么问题

185
00:06:46,360 --> 00:06:48,800
初始化是不会太影响后面的进度

186
00:06:48,800 --> 00:06:50,560
所以就是为什么说

187
00:06:50,600 --> 00:06:52,560
你的初始化行了就OK了

188
00:06:55,720 --> 00:06:56,480
第问题7

189
00:06:56,480 --> 00:06:58,800
我们创建后的网络之后

190
00:06:58,800 --> 00:07:00,399
Torch是按什么规则

191
00:07:00,399 --> 00:07:02,120
给参数初始化的

192
00:07:03,120 --> 00:07:04,000
大家可以去看一下

193
00:07:04,000 --> 00:07:05,839
我记得我还真忘了

194
00:07:05,839 --> 00:07:07,759
Torch的默认的初始化了

195
00:07:08,160 --> 00:07:11,079
它应该可能是按照均匀

196
00:07:11,800 --> 00:07:14,839
在0.0几之间做均匀初始化

197
00:07:14,839 --> 00:07:16,160
就是说位置上

198
00:07:16,279 --> 00:07:18,759
均匀初始化在某一个区间里面

199
00:07:18,800 --> 00:07:22,399
然后它的偏移上0做的

200
00:07:22,399 --> 00:07:23,360
大家可以去搜一下

201
00:07:23,720 --> 00:07:25,040
就是我记得不一定对

202
00:07:25,040 --> 00:07:26,399
我只记得M3

203
00:07:26,399 --> 00:07:28,360
是按照一个奇怪的

204
00:07:28,560 --> 00:07:29,680
敲进去的

205
00:07:29,720 --> 00:07:32,240
-0.07和正0.07之间的

206
00:07:32,240 --> 00:07:33,399
均匀做初始化

207
00:07:33,720 --> 00:07:35,680
PyTorch TensorFlow

208
00:07:35,680 --> 00:07:37,080
你带着给他去搜一下

209
00:07:41,320 --> 00:07:42,560
自定义第8

210
00:07:42,680 --> 00:07:44,120
自定义的激活函数

211
00:07:44,120 --> 00:07:46,240
如果是非可导的话

212
00:07:46,240 --> 00:07:48,520
AutoGrad是否可以抽出导数

213
00:07:48,680 --> 00:07:51,400
还是必须先自定义导数函数

214
00:07:51,400 --> 00:07:52,000
嗯

215
00:07:53,160 --> 00:07:54,240
其实啊

216
00:07:55,880 --> 00:08:00,000
其实没有不能导的函数

217
00:08:00,040 --> 00:08:04,399
你几乎找不出来不能导的函数

218
00:08:04,519 --> 00:08:05,600
但是就是说

219
00:08:06,600 --> 00:08:08,439
你只是说你很多函数

220
00:08:08,439 --> 00:08:10,439
是不是处处可导

221
00:08:12,680 --> 00:08:14,160
就是说可能会断点

222
00:08:14,639 --> 00:08:16,879
但是在数值运算来讲

223
00:08:16,879 --> 00:08:18,519
我们这都是数值运算

224
00:08:18,639 --> 00:08:21,439
你碰到断点的概率是非常低的

225
00:08:21,439 --> 00:08:22,959
而且你碰到它那就无所谓

226
00:08:22,959 --> 00:08:25,000
随便给个0也行

227
00:08:25,000 --> 00:08:25,879
给个什么都行

228
00:08:25,920 --> 00:08:26,759
没关系

229
00:08:26,839 --> 00:08:27,879
所以是说

230
00:08:27,879 --> 00:08:33,159
几乎你想象不出不能导的一个函数

231
00:08:33,159 --> 00:08:35,759
我不知道有什么函数是可以

232
00:08:35,759 --> 00:08:36,840
就求不出导数

233
00:08:36,840 --> 00:08:39,720
只是说有些导数的中间有些点是

234
00:08:39,919 --> 00:08:41,519
没有定义好

235
00:08:41,919 --> 00:08:44,200
但是这些点通常是一些很零碎点

236
00:08:44,200 --> 00:08:45,039
就那么几个点

237
00:08:45,120 --> 00:08:48,720
所以你这个点要么就是取左边的导数

238
00:08:48,720 --> 00:08:50,080
要么取右边导数都没关系

239
00:08:50,080 --> 00:08:51,320
导数会跳没关系

240
00:08:51,360 --> 00:08:52,320
所以是说

241
00:08:52,360 --> 00:08:54,240
大家可以想一想

242
00:08:54,240 --> 00:08:56,039
就是说什么样的

243
00:08:56,039 --> 00:08:57,240
还是真的是不能导的

244
00:08:57,240 --> 00:09:00,279
但就数学上大家学数分的话

245
00:09:00,919 --> 00:09:03,639
我还是十二十年前十年前学过数分

246
00:09:03,680 --> 00:09:06,159
数学上我是能构造出

247
00:09:06,480 --> 00:09:07,720
处处不能导的函数

248
00:09:07,720 --> 00:09:08,240
对吧

249
00:09:08,919 --> 00:09:10,080
但实际上没有

250
00:09:10,080 --> 00:09:11,080
实际上我实现了

251
00:09:11,080 --> 00:09:12,360
我就实现那么多东西

252
00:09:12,360 --> 00:09:13,960
数值运算里面没有这东西

253
00:09:14,039 --> 00:09:15,159
所以大家不用

254
00:09:16,080 --> 00:09:16,720
对

255
00:09:16,720 --> 00:09:17,360
当然是说

256
00:09:17,360 --> 00:09:19,799
如果你做了一些SVD这种操作的话

257
00:09:19,799 --> 00:09:20,720
那就比较麻烦一点

258
00:09:20,720 --> 00:09:21,879
SVD也是可以导的

259
00:09:21,879 --> 00:09:22,680
实际上

260
00:09:22,919 --> 00:09:24,759
但是如果你做了一些

261
00:09:25,080 --> 00:09:27,600
比较很复杂的操作的话

262
00:09:27,639 --> 00:09:28,439
可能

263
00:09:29,399 --> 00:09:30,480
求不起来比较麻烦

264
00:09:30,480 --> 00:09:31,840
但是实际上都是可以导的

265
00:09:35,360 --> 00:09:36,120
问题9

266
00:09:36,120 --> 00:09:37,319
模型是什么

267
00:09:37,480 --> 00:09:39,960
是类似于多元函数吗

268
00:09:40,279 --> 00:09:42,039
另外每一层是怎么定义的

269
00:09:42,039 --> 00:09:43,120
有什么依据吗

270
00:09:43,399 --> 00:09:45,000
模型它模型

271
00:09:45,000 --> 00:09:46,519
你可以认为是个多元函数

272
00:09:46,519 --> 00:09:47,360
这个没问题

273
00:09:47,480 --> 00:09:48,960
具体每一层是怎么定义的

274
00:09:48,960 --> 00:09:50,840
我们整个课就是讲这件事情的

275
00:09:50,840 --> 00:09:52,960
就是建议这位同学

276
00:09:52,960 --> 00:09:54,960
可以往后往前学一学

277
00:09:54,960 --> 00:09:56,160
就是每一层

278
00:09:56,160 --> 00:09:58,680
我们其实我们现在就讲了几个简单层

279
00:09:58,840 --> 00:09:59,800
全连接层

280
00:09:59,920 --> 00:10:02,120
以及最简单的一些机关函数

281
00:10:02,320 --> 00:10:04,240
然后以后面我们会讲更多的层

282
00:10:04,280 --> 00:10:06,280
就每一层我们会介绍

283
00:10:06,320 --> 00:10:07,200
怎么样定义的

284
00:10:07,200 --> 00:10:08,360
怎么把它构造起来

285
00:10:08,360 --> 00:10:09,639
得到比较好的模型

286
00:10:09,759 --> 00:10:12,440
就是我们整个课基本上讲这个事情

287
00:10:12,879 --> 00:10:13,519
OK

288
00:10:13,680 --> 00:10:15,320
所以这应该是我们所有的问题

289
00:10:15,320 --> 00:10:17,400
我来刷新一下

290
00:10:17,400 --> 00:10:21,080
OK

291
00:10:22,080 --> 00:10:22,920
OK

292
00:10:22,920 --> 00:10:23,360
我看一下

293
00:10:24,720 --> 00:10:25,360
应该没问题

294
00:10:25,360 --> 00:10:27,240
而且大家说不卡

295
00:10:27,240 --> 00:10:28,759
其实不卡这个东西看运气

296
00:10:28,759 --> 00:10:30,160
今天确实不卡

297
00:10:30,160 --> 00:10:33,639
但是你不能保证明天或者以后不卡

298
00:10:33,840 --> 00:10:35,720
我确实把码率往低调了一点

299
00:10:35,720 --> 00:10:38,280
但是卡好像是因为我觉得这个网络

300
00:10:38,280 --> 00:10:40,120
就是偶尔有时候会有问题

301
00:10:40,560 --> 00:10:42,080
我们这里就是

302
00:10:42,080 --> 00:10:42,840
美国

303
00:10:42,840 --> 00:10:45,520
美国就是所有东西都是50年代

304
00:10:45,520 --> 00:10:46,360
60年代建的

305
00:10:46,360 --> 00:10:48,160
就是小破房

306
00:10:48,160 --> 00:10:50,280
所以技术都是不行

