1
00:00:00,000 --> 00:00:03,399
第二个概念是我们常见的一个现象

2
00:00:03,399 --> 00:00:05,719
叫做过拟核和欠拟核

3
00:00:07,200 --> 00:00:10,000
英语上来就是under fitting和over fitting

4
00:00:13,000 --> 00:00:16,160
就是说我们这里很简单的总结一下

5
00:00:16,160 --> 00:00:18,280
什么叫做过拟核和欠拟核

6
00:00:18,679 --> 00:00:20,160
就我们画了一个表

7
00:00:20,160 --> 00:00:22,640
第一个就是说我们的

8
00:00:24,199 --> 00:00:26,400
第一个是我们的模型的容量

9
00:00:26,440 --> 00:00:28,640
就是我们有讲过模型容量

10
00:00:28,760 --> 00:00:31,160
昨天就是你这个模型的复杂度

11
00:00:31,519 --> 00:00:32,719
你复杂的模型

12
00:00:32,759 --> 00:00:34,399
可以学习更复杂的函数

13
00:00:34,439 --> 00:00:37,399
你简单的模型可以你就不那么好了

14
00:00:37,439 --> 00:00:39,439
就是说线性模型它就比较简单

15
00:00:39,560 --> 00:00:41,679
你多层感知机就可以比较复杂

16
00:00:42,359 --> 00:00:44,840
所以你的容量有一个低和一个高

17
00:00:45,159 --> 00:00:46,280
低的话就简单模型

18
00:00:46,480 --> 00:00:47,719
高就是复杂模型

19
00:00:48,519 --> 00:00:49,960
另外一个是你的数据

20
00:00:51,120 --> 00:00:52,400
你可以有简单数据

21
00:00:52,760 --> 00:00:54,000
你可以有复杂数据

22
00:00:54,439 --> 00:00:55,120
简单数据

23
00:00:55,120 --> 00:00:57,240
比如说我们其实一直在使用的

24
00:00:57,240 --> 00:00:59,280
我们最简单的是人工数据集

25
00:00:59,320 --> 00:01:00,600
我们还记不记得

26
00:01:00,600 --> 00:01:02,079
就是几个星期前

27
00:01:02,079 --> 00:01:03,920
我们讲过一个人人工数据集

28
00:01:04,600 --> 00:01:07,480
我们的现在常用的是FastMList

29
00:01:07,719 --> 00:01:10,240
它在于简单和复杂之间

30
00:01:10,280 --> 00:01:14,840
算还简单里面比较还不错的数据集了

31
00:01:15,520 --> 00:01:16,439
然后复杂的话

32
00:01:16,439 --> 00:01:19,760
你当然可以说ImageNet或者实际的

33
00:01:19,960 --> 00:01:22,280
数据集很有可能都是比较复杂的

34
00:01:23,480 --> 00:01:26,320
所以是说根据你的数据集的

35
00:01:26,320 --> 00:01:27,440
简单和复杂

36
00:01:27,440 --> 00:01:30,400
我们应该来选择对应的模型的容量

37
00:01:31,680 --> 00:01:32,760
最简单是说

38
00:01:32,800 --> 00:01:34,680
假设你的数据是简单的话

39
00:01:35,160 --> 00:01:38,040
那么你应该选择比较低的模型容量

40
00:01:39,160 --> 00:01:41,000
这会得到一个比较正常的结果

41
00:01:41,880 --> 00:01:43,360
但如果你简单数据

42
00:01:43,360 --> 00:01:46,040
使用了很复杂的模型容量的话

43
00:01:46,200 --> 00:01:47,880
那就会出现过拟核

44
00:01:49,840 --> 00:01:51,360
过拟核的意思是说

45
00:01:51,400 --> 00:01:53,160
你在模型里

46
00:01:53,959 --> 00:01:57,039
比如说我在一个很简单数据集上

47
00:01:57,039 --> 00:01:59,039
用一个特别深的一个生存网络

48
00:01:59,479 --> 00:02:00,399
你过拟核就是说

49
00:02:00,399 --> 00:02:03,560
你的模型够复杂

50
00:02:03,599 --> 00:02:06,840
我直接帮你每一个样本全部记住

51
00:02:08,680 --> 00:02:10,719
我记住样本很有可能看到新样本

52
00:02:10,719 --> 00:02:11,960
我并没有犯坏性

53
00:02:14,479 --> 00:02:15,400
第二个情况是说

54
00:02:15,400 --> 00:02:17,319
当你的数据是复杂的情况下

55
00:02:17,759 --> 00:02:19,960
假设你模型容量比较低的话

56
00:02:20,479 --> 00:02:21,319
你就比较麻烦

57
00:02:21,319 --> 00:02:22,960
你训练不好那个模型

58
00:02:23,319 --> 00:02:25,520
我们有讲过我们的异或函数

59
00:02:26,240 --> 00:02:27,560
它不算复杂的数据

60
00:02:27,759 --> 00:02:29,319
但是你的模型过于简单

61
00:02:29,319 --> 00:02:31,560
所以无法拟核异或函数

62
00:02:32,479 --> 00:02:34,199
或者说我们做复杂一点

63
00:02:34,199 --> 00:02:36,800
我们做一个就算是fashion list

64
00:02:36,960 --> 00:02:38,639
假设你用一个线性模型的话

65
00:02:38,680 --> 00:02:40,479
很有可能你精度不那么好

66
00:02:42,560 --> 00:02:45,519
所以当你的数据比较复杂的时候

67
00:02:45,560 --> 00:02:48,840
你应该选用比较高一点的模型容量

68
00:02:49,039 --> 00:02:51,719
这样子你可以得到比较正常的一个模型

69
00:02:53,960 --> 00:02:55,159
所以我们先来看一下

70
00:02:55,159 --> 00:02:57,359
模型容量的一个具体的定义

71
00:02:59,240 --> 00:03:00,919
模型的容量是指

72
00:03:02,159 --> 00:03:04,719
拟核各种函数的能力

73
00:03:05,639 --> 00:03:07,319
就低容量的模型

74
00:03:07,319 --> 00:03:08,960
来拟核训练数据

75
00:03:09,719 --> 00:03:10,759
高容量的模型

76
00:03:10,759 --> 00:03:12,599
可以记住所有的训练数据

77
00:03:13,199 --> 00:03:14,359
我们这里举了一个例子

78
00:03:15,120 --> 00:03:16,919
就可以看到这里有些点

79
00:03:17,599 --> 00:03:18,280
就有些点

80
00:03:18,919 --> 00:03:19,840
这是我们的数据

81
00:03:19,960 --> 00:03:23,599
你可以认为我的轴是

82
00:03:24,159 --> 00:03:25,319
这个轴是x

83
00:03:25,680 --> 00:03:27,759
这个轴是我的label

84
00:03:30,240 --> 00:03:31,400
我要拟核这个函数的话

85
00:03:31,400 --> 00:03:33,280
它其实就是一个很简单的线性回归

86
00:03:33,640 --> 00:03:34,319
不是线性回归

87
00:03:34,319 --> 00:03:35,039
就是一个回归

88
00:03:35,039 --> 00:03:36,719
一元函数的回归

89
00:03:37,240 --> 00:03:39,039
那么假设你的模型就是一个

90
00:03:39,480 --> 00:03:40,120
直线的话

91
00:03:40,120 --> 00:03:41,120
一个线性的话

92
00:03:41,400 --> 00:03:43,000
那么你是无法拟核这个函数

93
00:03:43,039 --> 00:03:46,360
你就是只能算出这一根线的样子

94
00:03:46,360 --> 00:03:48,880
但反过来讲

95
00:03:48,880 --> 00:03:50,640
你的模型够复杂

96
00:03:50,880 --> 00:03:53,760
你可以做一个非常复杂的一个曲线的话

97
00:03:53,760 --> 00:03:55,320
那么我的模型会

98
00:03:55,800 --> 00:03:59,680
会把整个数据给你完全的拟核住

99
00:04:01,440 --> 00:04:02,280
大家可以看到说

100
00:04:02,280 --> 00:04:03,640
这两种情况都不是很好

101
00:04:03,640 --> 00:04:05,240
就第一种情况过于简单

102
00:04:05,440 --> 00:04:06,360
第二种情况

103
00:04:06,360 --> 00:04:08,280
大家知道应该是比较合理的范围

104
00:04:08,280 --> 00:04:09,920
应该是一根这样子的二次曲线

105
00:04:09,920 --> 00:04:10,320
对吧

106
00:04:11,240 --> 00:04:13,040
所以但是你在函数过于的

107
00:04:13,040 --> 00:04:14,440
去拟核这个函数

108
00:04:14,759 --> 00:04:16,759
把我的噪音全部拟核住了

109
00:04:20,199 --> 00:04:20,680
OK

110
00:04:22,240 --> 00:04:25,560
所以以比较好的一个是这样子的一个东西

111
00:04:25,600 --> 00:04:26,399
就是说

112
00:04:27,680 --> 00:04:29,800
我的x轴是我的模型的容量

113
00:04:30,279 --> 00:04:31,240
这里是

114
00:04:32,279 --> 00:04:33,399
这里是d

115
00:04:34,720 --> 00:04:35,639
这里是高

116
00:04:37,560 --> 00:04:39,279
那我的y轴是我的误差

117
00:04:39,279 --> 00:04:41,000
就是我在我的这个模型

118
00:04:41,000 --> 00:04:43,199
在数据上面的一个训练

119
00:04:43,279 --> 00:04:45,759
就假设我们的数据级是固定的情况下

120
00:04:46,120 --> 00:04:47,279
是一个中等数据级

121
00:04:47,639 --> 00:04:49,159
那么我们可以从模型

122
00:04:49,159 --> 00:04:50,120
从低开始

123
00:04:50,120 --> 00:04:51,159
从最简单的模型

124
00:04:51,159 --> 00:04:52,599
一直做到最高的模型

125
00:04:52,639 --> 00:04:55,279
这个其实也是我们常见的调参的一个策略

126
00:04:56,639 --> 00:04:57,560
看到会怎么样

127
00:04:57,839 --> 00:04:58,680
首先是说

128
00:04:58,719 --> 00:05:00,719
假设你的模型比较低的时候

129
00:05:00,919 --> 00:05:02,839
那么你的训练误差

130
00:05:03,000 --> 00:05:04,279
就会比较高

131
00:05:04,439 --> 00:05:06,199
这是因为你这个模型过于简单

132
00:05:06,319 --> 00:05:07,959
你无法拟核我这一个

133
00:05:07,959 --> 00:05:09,480
也还算复杂的数据

134
00:05:10,159 --> 00:05:11,800
你的范化误差一样的是高

135
00:05:12,480 --> 00:05:15,280
然后随着你的模型的容量增加

136
00:05:15,840 --> 00:05:18,199
那么你的训练的误差

137
00:05:18,199 --> 00:05:20,079
就是我对数据的拟核能力

138
00:05:20,079 --> 00:05:21,400
是开始下降的

139
00:05:21,960 --> 00:05:22,960
就是说你降到最后

140
00:05:22,960 --> 00:05:23,920
你甚至可以降到0

141
00:05:23,920 --> 00:05:25,080
这个地方可以到0

142
00:05:26,040 --> 00:05:27,480
就是说你神经网络理论上

143
00:05:27,480 --> 00:05:28,600
可以你可以

144
00:05:28,600 --> 00:05:29,680
你不管你数据有多大

145
00:05:29,680 --> 00:05:31,000
我都可以帮你记住

146
00:05:33,480 --> 00:05:34,639
那么但是你

147
00:05:34,759 --> 00:05:37,199
不是永远的记住所有的数据是好的

148
00:05:37,199 --> 00:05:38,519
数据里面大量的噪音

149
00:05:38,519 --> 00:05:40,319
你记住数据可能没什么用

150
00:05:40,959 --> 00:05:43,599
所以你会发现你的范化误差

151
00:05:43,599 --> 00:05:45,439
就是我们真正关心的误差

152
00:05:45,439 --> 00:05:47,120
是会随着

153
00:05:47,120 --> 00:05:48,879
一开始我会随着往下降

154
00:05:49,000 --> 00:05:50,920
但是降到某一个点之后

155
00:05:51,240 --> 00:05:53,759
它开始缓慢的往上升

156
00:05:54,279 --> 00:05:55,439
这是因为你的模型

157
00:05:55,439 --> 00:05:56,800
过于的去关注细节

158
00:05:56,800 --> 00:05:58,680
导致你真的拿一个新的数据

159
00:05:58,680 --> 00:06:01,759
你被一些无关的细节所困扰住了

160
00:06:02,399 --> 00:06:04,159
所以你的范化误差

161
00:06:04,159 --> 00:06:06,279
其实是有一个比较大的一个gap

162
00:06:07,600 --> 00:06:09,879
那么你会发现一个什么样的问题

163
00:06:09,879 --> 00:06:10,600
是说

164
00:06:10,600 --> 00:06:12,159
不好意思

165
00:06:13,040 --> 00:06:16,000
就是说你最优的点会在这个地方

166
00:06:16,040 --> 00:06:18,920
就是你的范化误差在往上升的时候

167
00:06:20,879 --> 00:06:22,719
然后你在这里的话

168
00:06:22,719 --> 00:06:24,199
你那就是嵌拟核

169
00:06:24,719 --> 00:06:26,319
这里的话就是过拟核

170
00:06:26,439 --> 00:06:27,879
它中间这个gap

171
00:06:30,040 --> 00:06:32,159
我们通常用来衡量的一个模型的

172
00:06:32,159 --> 00:06:34,199
过拟核和嵌拟核的一个

173
00:06:34,279 --> 00:06:35,079
一个去

174
00:06:35,920 --> 00:06:36,839
一个程度

175
00:06:37,840 --> 00:06:39,960
所以是说我们最优在这个情况

176
00:06:39,960 --> 00:06:40,880
是在这个地方

177
00:06:41,760 --> 00:06:43,120
当然是说

178
00:06:43,480 --> 00:06:44,800
这是这一类模型

179
00:06:44,800 --> 00:06:46,080
你可以通过不同的模型

180
00:06:46,080 --> 00:06:46,880
你的核心

181
00:06:46,880 --> 00:06:49,160
你是要把我们的核心任务

182
00:06:49,160 --> 00:06:50,280
第一个是说

183
00:06:50,480 --> 00:06:51,560
其实我们的核心任务

184
00:06:51,560 --> 00:06:52,800
是要这个点往下拉

185
00:06:52,800 --> 00:06:53,320
对吧

186
00:06:55,640 --> 00:06:56,520
第二个任务是说

187
00:06:56,520 --> 00:06:58,600
我们要尽量把中间这一块

188
00:06:58,600 --> 00:06:59,480
把它

189
00:06:59,880 --> 00:07:01,800
不要弄特别大

190
00:07:02,320 --> 00:07:03,560
所以你会发现

191
00:07:04,160 --> 00:07:06,839
有时候我为了把这个模型

192
00:07:06,839 --> 00:07:09,040
把我的范化误差往下降

193
00:07:09,120 --> 00:07:12,280
我不得不承受一定程度的过拟核

194
00:07:12,280 --> 00:07:14,040
这个也是深度学习所要的

195
00:07:14,079 --> 00:07:14,839
就过拟核

196
00:07:14,839 --> 00:07:16,759
本质上不是一个很坏的事情

197
00:07:16,959 --> 00:07:18,199
就是说如果你的

198
00:07:18,199 --> 00:07:18,720
就是说你

199
00:07:18,720 --> 00:07:21,199
首先你的模型容量得够

200
00:07:21,639 --> 00:07:24,680
然后我们再去控制它的容量

201
00:07:24,680 --> 00:07:26,720
这个是我们整个深度学习

202
00:07:26,720 --> 00:07:27,959
最核心的一个进度

203
00:07:27,959 --> 00:07:30,399
就是说你的模型先足够大

204
00:07:30,879 --> 00:07:31,279
不大

205
00:07:31,279 --> 00:07:32,560
你根本就没什么前途

206
00:07:32,560 --> 00:07:33,000
对吧

207
00:07:33,000 --> 00:07:34,439
在足够大的情况下

208
00:07:34,439 --> 00:07:37,720
我通过各种手段来控制你的模型容量

209
00:07:37,720 --> 00:07:40,399
使得我最后能得到范化误差的

210
00:07:40,399 --> 00:07:41,120
往下降

211
00:07:41,360 --> 00:07:43,399
这是我们整个深度学习的一个核心

212
00:07:44,519 --> 00:07:45,040
OK

213
00:07:46,720 --> 00:07:48,319
所以我们来可以是说

214
00:07:48,319 --> 00:07:48,879
我们可以

215
00:07:48,879 --> 00:07:51,040
其实我们模型容量是可以估计的

216
00:07:51,560 --> 00:07:52,279
就是说

217
00:07:52,519 --> 00:07:54,439
但是我们比较难以比较

218
00:07:54,439 --> 00:07:56,680
在不同种类算法之间

219
00:07:57,240 --> 00:07:58,360
的模型容量

220
00:07:58,920 --> 00:08:01,720
比如说我们的数模型

221
00:08:01,880 --> 00:08:03,280
这个是一个数

222
00:08:04,120 --> 00:08:05,160
是个数

223
00:08:06,400 --> 00:08:07,880
听说tree algorithm

224
00:08:07,880 --> 00:08:09,720
就是说一些随机三

225
00:08:09,720 --> 00:08:11,800
三菱和神经网络之间

226
00:08:11,800 --> 00:08:13,400
这两种模型特别不一样

227
00:08:13,400 --> 00:08:15,320
所以不好直接比较

228
00:08:16,160 --> 00:08:19,040
但是说我给定一个模型的种类

229
00:08:19,440 --> 00:08:20,080
一般来说

230
00:08:20,080 --> 00:08:21,160
我们可以认为

231
00:08:21,400 --> 00:08:22,840
有两个主要的因素

232
00:08:23,760 --> 00:08:25,960
第一个是说你参数的个数

233
00:08:25,960 --> 00:08:28,160
就是说你可以学习的参数的个数

234
00:08:29,200 --> 00:08:30,240
就是说你可以看到

235
00:08:30,240 --> 00:08:31,560
我们这个线性模型

236
00:08:32,040 --> 00:08:34,360
假设我们是有第一个数据的话

237
00:08:34,360 --> 00:08:36,440
那我们会有d加1个可以学习的

238
00:08:36,440 --> 00:08:38,840
就1那个是偏一

239
00:08:39,440 --> 00:08:41,320
所以线性模型的

240
00:08:41,360 --> 00:08:43,040
参数的个数是d加1

241
00:08:43,680 --> 00:08:45,920
如果我做了一个单层的

242
00:08:45,920 --> 00:08:47,000
隐含层的话

243
00:08:47,000 --> 00:08:48,879
假设隐含层是m的话

244
00:08:48,879 --> 00:08:50,480
那么我是d加1乘以m

245
00:08:50,480 --> 00:08:51,879
加上m加1乘以k

246
00:08:51,879 --> 00:08:54,040
假设k是我的最后的

247
00:08:54,279 --> 00:08:56,120
分类的类别数

248
00:08:57,000 --> 00:08:58,040
那么当然可以看到

249
00:08:58,040 --> 00:08:58,800
只要m

250
00:08:59,680 --> 00:09:00,759
也很大的话

251
00:09:01,080 --> 00:09:02,679
那么单层的

252
00:09:02,840 --> 00:09:04,919
单隐藏层的感知器

253
00:09:04,919 --> 00:09:07,559
肯定是要大于我的线性模型的

254
00:09:08,319 --> 00:09:10,279
就是说你可以简单通过我的

255
00:09:10,480 --> 00:09:11,840
模型的参数的个数

256
00:09:11,840 --> 00:09:13,279
来判断两个模型

257
00:09:13,279 --> 00:09:16,679
是不是哪一个比哪一个容量要高

258
00:09:17,319 --> 00:09:18,759
就第二个是

259
00:09:18,919 --> 00:09:21,080
参数值的选择范围

260
00:09:22,480 --> 00:09:23,240
就是说

261
00:09:23,279 --> 00:09:24,519
假设我一个参数

262
00:09:24,519 --> 00:09:25,879
可以选择

263
00:09:25,919 --> 00:09:27,319
在一个很大的区域里面

264
00:09:27,319 --> 00:09:28,319
选择值的话

265
00:09:28,360 --> 00:09:30,319
那么我的模型复杂度会比较高

266
00:09:31,439 --> 00:09:33,639
假设我的参数

267
00:09:33,639 --> 00:09:36,039
只能在一个很小的范围里面选值的话

268
00:09:36,039 --> 00:09:37,319
那么的模型容量

269
00:09:37,319 --> 00:09:38,720
可以认为是比较低的

270
00:09:39,720 --> 00:09:40,159
OK

271
00:09:40,159 --> 00:09:42,519
所以就是说两个核心的东西

272
00:09:42,600 --> 00:09:43,600
参数的个数

273
00:09:43,879 --> 00:09:45,360
参数值的选择范围

274
00:09:45,399 --> 00:09:47,799
我们今后会不断的看到这两个

275
00:09:47,799 --> 00:09:50,960
我们怎么通过去调整这两个属性

276
00:09:50,960 --> 00:09:52,919
来控制我们模型的复杂度

277
00:09:55,679 --> 00:09:56,919
当然我们有理论上

278
00:09:57,279 --> 00:09:58,840
我们是有一些理论的

279
00:09:58,960 --> 00:10:01,399
我们有简单的提到过

280
00:10:01,399 --> 00:10:02,680
统计学习理论

281
00:10:02,879 --> 00:10:05,200
就是统计学习理论的一个核心思想

282
00:10:05,200 --> 00:10:06,240
叫做VC

283
00:10:06,240 --> 00:10:08,120
VC dimension

284
00:10:08,120 --> 00:10:10,800
就是两个大佬的

285
00:10:11,040 --> 00:10:12,879
首字母的缩写

286
00:10:13,280 --> 00:10:14,680
第一个是瓦普利克

287
00:10:15,040 --> 00:10:15,840
就是说

288
00:10:16,560 --> 00:10:17,960
对一个分类模型来讲

289
00:10:17,960 --> 00:10:19,360
我们简单介绍一下VC

290
00:10:19,399 --> 00:10:20,920
但我们还是说整个课

291
00:10:20,920 --> 00:10:22,519
我们不会涉及到太多理论

292
00:10:23,480 --> 00:10:25,160
对一个分类模型来讲

293
00:10:25,879 --> 00:10:28,040
VC为等加余

294
00:10:29,000 --> 00:10:31,840
一个最大的数据集的大小

295
00:10:32,960 --> 00:10:33,639
这个数据集

296
00:10:34,480 --> 00:10:36,920
不管如何我们给定它的标号

297
00:10:38,080 --> 00:10:39,639
都存在一个模型

298
00:10:39,639 --> 00:10:40,680
就是存在一个模型

299
00:10:40,680 --> 00:10:42,560
说我们给定这个模型的参数

300
00:10:42,879 --> 00:10:44,759
能对它进行完美的分类

301
00:10:46,639 --> 00:10:47,720
这直观上也很容易

302
00:10:47,720 --> 00:10:48,200
对吧

303
00:10:48,399 --> 00:10:49,759
假设我的模型

304
00:10:50,080 --> 00:10:52,160
可以做一个很复杂的数据集

305
00:10:52,720 --> 00:10:54,120
就是说比如说我可以有100

306
00:10:54,120 --> 00:10:55,400
给我100张图片

307
00:10:56,240 --> 00:10:57,000
那么每个图片

308
00:10:57,000 --> 00:10:58,879
我不管它的标号怎么变化

309
00:10:59,240 --> 00:11:01,559
不管我的图片里面的值怎么选

310
00:11:02,679 --> 00:11:04,480
我都可以通过一个模型

311
00:11:04,480 --> 00:11:06,000
来对它进行分类的话

312
00:11:07,200 --> 00:11:09,559
那么其实它的模型复杂度比一个

313
00:11:09,559 --> 00:11:12,200
我只能对一个10张图片的数据集

314
00:11:12,240 --> 00:11:13,639
不管怎么标号都能分类

315
00:11:13,720 --> 00:11:14,559
当然来得快

316
00:11:15,279 --> 00:11:16,879
或者是说你可以认为是

317
00:11:17,879 --> 00:11:19,399
我的模型复杂度等

318
00:11:19,840 --> 00:11:24,039
复杂度等价于我能够完美的记住

319
00:11:24,720 --> 00:11:25,679
一个数据集

320
00:11:26,160 --> 00:11:27,720
这个数据集最大有多大

321
00:11:30,680 --> 00:11:32,520
我们可以举几个简单的例子

322
00:11:34,280 --> 00:11:37,320
就是说二维输入的感知机

323
00:11:37,320 --> 00:11:38,920
还回忆一下感知机

324
00:11:39,120 --> 00:11:41,000
就是两个二维的输入

325
00:11:41,000 --> 00:11:42,560
就是输入的特征是2

326
00:11:42,880 --> 00:11:44,000
然后你的输出是1

327
00:11:44,560 --> 00:11:46,240
它的VC位等于3

328
00:11:48,360 --> 00:11:50,480
就是它可以任意分类两个点

329
00:11:50,480 --> 00:11:52,040
就二维输入是平面上点

330
00:11:53,360 --> 00:11:53,760
可以看到

331
00:11:54,720 --> 00:11:56,240
就假设全振

332
00:11:56,679 --> 00:11:57,799
两个振一个负

333
00:11:57,799 --> 00:11:59,559
还有两个负一个振

334
00:11:59,559 --> 00:12:00,600
你不管怎么样

335
00:12:00,600 --> 00:12:01,279
三个点

336
00:12:01,279 --> 00:12:02,559
不管你怎么做标号

337
00:12:02,559 --> 00:12:04,919
或者你一个点怎么移动位置

338
00:12:05,360 --> 00:12:07,279
我都可以画一根线把你圈出来

339
00:12:08,840 --> 00:12:10,039
但是你不能做异祸

340
00:12:10,759 --> 00:12:11,919
异祸就是在这里

341
00:12:12,639 --> 00:12:13,399
我们有讲过

342
00:12:13,399 --> 00:12:14,439
昨天有讲过异祸

343
00:12:15,120 --> 00:12:15,600
就是说你

344
00:12:16,360 --> 00:12:17,600
一根线就不够了

345
00:12:17,600 --> 00:12:18,960
一根平面线是不够的

346
00:12:18,960 --> 00:12:19,960
你必须要一根曲线

347
00:12:20,960 --> 00:12:21,600
所以就是说

348
00:12:22,000 --> 00:12:22,879
二维的感知机

349
00:12:23,320 --> 00:12:24,439
它的VC位是3

350
00:12:24,439 --> 00:12:25,080
但不是4

351
00:12:25,679 --> 00:12:27,279
所以对于4这种异祸

352
00:12:27,759 --> 00:12:29,960
异祸是一个4个点的时候的

353
00:12:29,960 --> 00:12:31,279
一个具体的一个样例

354
00:12:31,279 --> 00:12:32,159
它不能做分类

355
00:12:33,759 --> 00:12:35,439
我们可以泛化到一点来说

356
00:12:35,879 --> 00:12:37,639
支持n维输入的感知机的

357
00:12:37,639 --> 00:12:38,799
VC位是n加1

358
00:12:40,679 --> 00:12:42,919
一些多层感知机的VC位

359
00:12:42,919 --> 00:12:45,320
是n乘以log n

360
00:12:46,720 --> 00:12:47,240
你可以认为

361
00:12:48,279 --> 00:12:50,639
那就是比你的线性的

362
00:12:50,639 --> 00:12:52,960
要多一个log n的scale在里面

363
00:12:59,240 --> 00:13:00,840
叫VC位的好处是说

364
00:13:00,840 --> 00:13:03,000
它提供了一些理论的依据

365
00:13:03,000 --> 00:13:05,960
说我们就知道一个模型是好还是坏

366
00:13:07,799 --> 00:13:09,439
特别是说它其实衡量的

367
00:13:09,439 --> 00:13:13,120
就是你训练误差和犯好误差之间

368
00:13:13,120 --> 00:13:13,799
那个间隔

369
00:13:14,639 --> 00:13:16,319
但是我们也提过

370
00:13:16,319 --> 00:13:17,600
就统计学习理论

371
00:13:17,600 --> 00:13:19,879
我们在深度学习中很少用

372
00:13:19,879 --> 00:13:22,840
因为我们还没有把VC位这一块

373
00:13:22,840 --> 00:13:25,039
能够很好的应用到深度学习上

374
00:13:25,720 --> 00:13:28,200
因为它的衡量不是很准确

375
00:13:28,200 --> 00:13:30,200
它就是一个一个lower bound

376
00:13:30,840 --> 00:13:34,080
另外一个是深度学习模型的VC位

377
00:13:34,080 --> 00:13:35,279
算起来特别难

378
00:13:35,600 --> 00:13:37,360
对一些很简单的感知机

379
00:13:37,360 --> 00:13:40,279
就是假设没有激活函数比较简单

380
00:13:40,279 --> 00:13:41,799
所有东西都比较简单情况下

381
00:13:41,799 --> 00:13:42,679
我们能算

382
00:13:42,679 --> 00:13:45,080
但是对于绝大部分常用的模型

383
00:13:45,080 --> 00:13:46,440
我们其实算不出来的

384
00:13:46,840 --> 00:13:47,720
所以就是说

385
00:13:48,280 --> 00:13:50,120
就是有一个这样子的东西

386
00:13:50,120 --> 00:13:51,320
大家以后遇到了

387
00:13:51,320 --> 00:13:52,600
知道是什么概念

388
00:13:52,760 --> 00:13:53,680
就行了

389
00:13:53,840 --> 00:13:55,160
这是我们这个课

390
00:13:55,160 --> 00:13:57,720
对统计学习的一个要求

391
00:13:59,160 --> 00:14:01,440
就另外一个是说数据的复杂度

392
00:14:02,840 --> 00:14:04,200
就我们讲了模型复杂度

393
00:14:04,200 --> 00:14:06,280
数据复杂度那更不好衡量了

394
00:14:06,280 --> 00:14:08,440
就是说只有一些直观上的一些理解

395
00:14:08,920 --> 00:14:11,760
就数据复杂度有多个比较重要的因素

396
00:14:12,720 --> 00:14:14,120
第一个是你样本的个数

397
00:14:14,799 --> 00:14:15,639
你样本很少

398
00:14:15,639 --> 00:14:17,720
100个样本和我100万个样本

399
00:14:17,720 --> 00:14:18,679
当然就不一样了

400
00:14:19,639 --> 00:14:21,879
第二个是说每个样本的元素个数

401
00:14:22,440 --> 00:14:23,759
我就是个二维的向量

402
00:14:23,759 --> 00:14:25,159
还是一个比较大的图片

403
00:14:25,200 --> 00:14:26,720
图片到底是一个横向图片

404
00:14:27,080 --> 00:14:28,440
我们用的是28×28

405
00:14:28,639 --> 00:14:31,240
现在在整个测试里面

406
00:14:31,320 --> 00:14:33,399
但是imagenet是更大的

407
00:14:33,399 --> 00:14:35,120
它应该是256×256

408
00:14:35,120 --> 00:14:36,159
至少是这样子

409
00:14:36,320 --> 00:14:39,039
所以imagenet它的图片会大很多

410
00:14:39,759 --> 00:14:41,639
第三个是说我这个图片里面

411
00:14:41,639 --> 00:14:45,600
是不是有一些复杂的时间的空间的结构

412
00:14:46,039 --> 00:14:48,399
空间就是说图片都有空间结构

413
00:14:48,720 --> 00:14:51,279
时间结构就是比如说股票预测了

414
00:14:51,279 --> 00:14:52,639
当然有个时间的结构

415
00:14:53,480 --> 00:14:56,039
还有很多数据是有时空都有的

416
00:14:56,159 --> 00:14:57,360
比如说视频

417
00:14:57,799 --> 00:14:58,120
对吧

418
00:14:58,120 --> 00:14:59,759
有时间轴有空间轴

419
00:15:00,919 --> 00:15:03,840
另外一个是说我这个数据的多样性有多大

420
00:15:04,360 --> 00:15:05,199
就我这个类比

421
00:15:05,240 --> 00:15:07,039
比如说我是做一个10类的分类

422
00:15:07,039 --> 00:15:08,039
还是100类的分类

423
00:15:08,039 --> 00:15:09,200
还是1000类的分类

424
00:15:09,639 --> 00:15:10,559
就是多样性

425
00:15:10,720 --> 00:15:14,160
所以这些都是一些比较重要的数据的复杂度

426
00:15:14,360 --> 00:15:16,320
大家去衡量一个数据的时候

427
00:15:16,320 --> 00:15:18,760
可以通过这几块去大概衡量

428
00:15:18,920 --> 00:15:20,400
但更重要的是说

429
00:15:20,600 --> 00:15:22,040
这都是一个相对概念

430
00:15:22,360 --> 00:15:26,280
就是说你大概要通过不断的去对真实数据

431
00:15:26,280 --> 00:15:27,880
去做一些训练

432
00:15:27,920 --> 00:15:29,400
然后下一次碰到一个数据的时候

433
00:15:29,400 --> 00:15:30,840
我大概有个直观的理解

434
00:15:30,840 --> 00:15:33,080
说跟我之前遇到的数据集

435
00:15:33,280 --> 00:15:35,160
复杂一点还是难一点

436
00:15:35,200 --> 00:15:37,840
这样子我能够选择合适的模型的

437
00:15:38,040 --> 00:15:40,400
容量的模型去进行理合

438
00:15:40,840 --> 00:15:44,520
所以就是说这个东西很多是一个直观的感觉

439
00:15:45,320 --> 00:15:48,360
但是通过大家要不断的对大量的数据

440
00:15:48,360 --> 00:15:51,560
做一些挑战

441
00:15:51,560 --> 00:15:53,920
大家会得到一些直观上的感受

442
00:15:54,200 --> 00:15:56,680
但是现在我们目前阶段只能这样子

443
00:15:56,680 --> 00:15:58,320
希望未来5年之后

444
00:15:58,320 --> 00:15:59,520
我们自动局局学习

445
00:15:59,520 --> 00:16:00,960
能够很好的解决这个问题

446
00:16:01,120 --> 00:16:02,440
但现在我们还在路上

447
00:16:02,440 --> 00:16:04,400
OK

448
00:16:05,560 --> 00:16:06,840
最后我们总结一下

449
00:16:07,800 --> 00:16:11,720
模型容量需要匹配我们的数据复杂度

450
00:16:12,399 --> 00:16:13,720
就你不匹配的话

451
00:16:13,720 --> 00:16:16,360
容会导致嵌离合和过离合

452
00:16:17,639 --> 00:16:20,399
统计机学习提供了一些数学工具

453
00:16:20,399 --> 00:16:22,480
来衡量模型的复杂度

454
00:16:22,800 --> 00:16:26,320
但实际中我们其实主要是观察

455
00:16:26,320 --> 00:16:29,879
我们的训练误差和验证误差的之间的区别

456
00:16:29,879 --> 00:16:32,600
来做一些实际上的一些感觉

457
00:16:33,519 --> 00:16:33,840
OK

458
00:16:33,840 --> 00:16:36,240
我们等会会通过一个代码的样例

459
00:16:36,240 --> 00:16:37,399
给大家看一下

460
00:16:37,440 --> 00:16:39,840
到底什么样是过离合

461
00:16:39,840 --> 00:16:40,879
什么样是嵌离合

462
00:16:40,879 --> 00:16:43,639
可以通过很简单一些函数的样子

463
00:16:43,639 --> 00:16:45,240
来进行判断

