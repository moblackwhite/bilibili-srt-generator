1
00:00:00,000 --> 00:00:03,000
好 我们来看一下Nullet的实现

2
00:00:03,000 --> 00:00:08,000
它实现就是全在这里了

3
00:00:08,000 --> 00:00:11,160
就是先给大家看一下

4
00:00:11,160 --> 00:00:15,359
首先我们有个reshape这个函数

5
00:00:15,359 --> 00:00:17,359
我们之前有定义过这个东西

6
00:00:17,359 --> 00:00:20,960
我们想用Nsequential

7
00:00:20,960 --> 00:00:23,839
所以我们就直接定了一个自定义的类

8
00:00:23,839 --> 00:00:24,960
就是reshape

9
00:00:24,960 --> 00:00:26,120
它就是干嘛呢

10
00:00:26,120 --> 00:00:30,359
就是把你的X放成一个p量数不变

11
00:00:30,359 --> 00:00:33,880
通道数变成1

12
00:00:33,880 --> 00:00:35,920
然后28×28

13
00:00:35,920 --> 00:00:41,640
这就是给我的输入用的

14
00:00:41,640 --> 00:00:43,600
然后呢

15
00:00:43,600 --> 00:00:47,240
接下来是

16
00:00:47,240 --> 00:00:51,320
我把我的摄像头关一下

17
00:00:51,320 --> 00:00:55,960
接下来我们就是把你那个1×28×28的图片

18
00:00:55,960 --> 00:01:00,120
放进我们的第一个卷迹层里面

19
00:01:00,120 --> 00:01:00,840
可以看到啊

20
00:01:00,840 --> 00:01:02,359
输入通道是1

21
00:01:02,359 --> 00:01:04,400
输出是6

22
00:01:04,400 --> 00:01:07,079
然后我们的kernel的size就是那个windows

23
00:01:07,079 --> 00:01:08,519
就是5×5

24
00:01:08,519 --> 00:01:09,400
Padding的元2

25
00:01:09,400 --> 00:01:11,640
Padding的2就是

26
00:01:11,640 --> 00:01:12,560
因为我们的

27
00:01:12,560 --> 00:01:14,159
我们这是28×28

28
00:01:14,159 --> 00:01:16,840
因为它的原始输入是32×32

29
00:01:16,840 --> 00:01:18,640
它原始的图片

30
00:01:18,640 --> 00:01:20,480
就是说最早的Nullet那个数据机

31
00:01:20,480 --> 00:01:25,079
它给你在两边各Pad了两列和两行

32
00:01:25,079 --> 00:01:27,799
所以就是它给你Pad好了

33
00:01:27,799 --> 00:01:29,359
这个数据机我们上18×28

34
00:01:29,359 --> 00:01:31,239
就把那个边缘给你删掉了

35
00:01:31,239 --> 00:01:31,599
所以呢

36
00:01:31,599 --> 00:01:34,239
我们就在额外的加了一个Padding的元2在这个地方

37
00:01:35,879 --> 00:01:36,359
然后呢

38
00:01:36,359 --> 00:01:37,000
我们知道啊

39
00:01:37,000 --> 00:01:39,280
我们要为了得到非先兴性

40
00:01:39,280 --> 00:01:43,799
我们在卷迹后面加入一个Sigma的激活函数

41
00:01:46,319 --> 00:01:47,039
然后呢

42
00:01:47,039 --> 00:01:50,439
然后我们用均值持话层

43
00:01:50,439 --> 00:01:51,640
就Average pooling

44
00:01:52,640 --> 00:01:56,640
我的windows我的kernel size是等于2的

45
00:01:56,640 --> 00:01:58,400
就是你的kernel size不要写

46
00:01:58,400 --> 00:01:59,799
其实不用写没关系啊

47
00:01:59,799 --> 00:02:00,879
因为它是第一个参数

48
00:02:02,400 --> 00:02:03,960
删掉啊

49
00:02:03,960 --> 00:02:05,159
stride等于2

50
00:02:05,159 --> 00:02:08,039
就是就是你那个二长二的长可是不会

51
00:02:09,560 --> 00:02:11,199
被重叠在一起

52
00:02:13,520 --> 00:02:14,159
然后呢

53
00:02:14,159 --> 00:02:17,080
你的卷迹层啊

54
00:02:17,080 --> 00:02:18,560
输入是6

55
00:02:18,560 --> 00:02:21,599
输出通道数从6变成了16

56
00:02:22,640 --> 00:02:24,560
就是说变得更多了

57
00:02:24,560 --> 00:02:27,120
然后你的kernel size还是5

58
00:02:27,120 --> 00:02:29,040
就是那么啊

59
00:02:29,040 --> 00:02:30,360
这个地方我就不做Padding了

60
00:02:31,400 --> 00:02:33,160
然后卷迹后面还是一样的啊

61
00:02:33,160 --> 00:02:35,560
跟一个Sigma的激活函数

62
00:02:37,560 --> 00:02:44,360
然后再加一个均值的持话层超参数跟前面是一样的

63
00:02:44,360 --> 00:02:45,320
二长二啊

64
00:02:45,320 --> 00:02:46,880
然后也是不重叠的窗口

65
00:02:48,320 --> 00:02:51,280
最后因为你卷迹层出来是一个4D的东西

66
00:02:51,319 --> 00:02:53,199
我要把最后的通道数啊

67
00:02:53,199 --> 00:02:56,640
你的高和宽把你变成一个一维的向量

68
00:02:56,640 --> 00:02:59,560
输入到我的多层感知机

69
00:02:59,560 --> 00:03:01,000
所以我用了一个flatten

70
00:03:01,000 --> 00:03:03,879
flatten就是低维保持住

71
00:03:03,879 --> 00:03:07,719
然后低维p量那个维度保持住

72
00:03:07,719 --> 00:03:10,280
后面全部拉成一个维度就拉成一个向量

73
00:03:11,800 --> 00:03:13,120
然后就是啊

74
00:03:13,120 --> 00:03:14,879
因为你那个是啊

75
00:03:14,879 --> 00:03:15,639
刚刚讲错了

76
00:03:15,639 --> 00:03:16,360
那128

77
00:03:16,360 --> 00:03:17,280
128是输入

78
00:03:17,280 --> 00:03:22,159
因为你是一时应该是16乘以啊

79
00:03:22,159 --> 00:03:24,439
5乘以5啊

80
00:03:24,439 --> 00:03:27,039
那就是等于120

81
00:03:28,199 --> 00:03:31,680
然后因为你的输出是16嘛

82
00:03:31,680 --> 00:03:36,319
然后你的应该你的那个高和宽在此话之后应该就变成5乘5

83
00:03:37,840 --> 00:03:39,080
所以那是120

84
00:03:41,560 --> 00:03:42,319
不不不不不不

85
00:03:42,319 --> 00:03:43,159
你写出来吧

86
00:03:43,159 --> 00:03:44,240
在这里有一层啊

87
00:03:44,240 --> 00:03:45,680
就是这个地方啊

88
00:03:45,680 --> 00:03:47,280
这还是低层还是120啊

89
00:03:47,280 --> 00:03:48,599
输出是120

90
00:03:48,599 --> 00:03:50,879
那么你的输入就是你得算一下

91
00:03:50,879 --> 00:03:52,680
就这是PyTorch不那么好的地方

92
00:03:52,680 --> 00:03:56,240
就是你想KLS和MSNet

93
00:03:56,240 --> 00:03:57,319
他们那种Symbolic

94
00:03:57,319 --> 00:03:58,400
他是不用算了

95
00:03:58,400 --> 00:04:00,640
就是说你这你得算一下啊

96
00:04:00,640 --> 00:04:01,280
算一下就是说

97
00:04:01,280 --> 00:04:04,640
你要知道你最后一层的输出是多少

98
00:04:04,640 --> 00:04:06,719
那就是16乘以5乘以5

99
00:04:06,719 --> 00:04:07,520
所以他啊

100
00:04:07,520 --> 00:04:08,280
然后呢

101
00:04:08,280 --> 00:04:09,439
输出是120

102
00:04:10,520 --> 00:04:11,920
然后再做一个skim mode

103
00:04:11,920 --> 00:04:13,400
你激活一下

104
00:04:13,439 --> 00:04:17,000
然后再把120降到84

105
00:04:17,000 --> 00:04:18,399
同样一个激活程

106
00:04:18,399 --> 00:04:19,840
最后的最后啊

107
00:04:19,840 --> 00:04:20,680
降到10

108
00:04:20,680 --> 00:04:21,759
因为10是我的类别

109
00:04:23,040 --> 00:04:25,040
所以基本上可以看到是说啊

110
00:04:25,040 --> 00:04:29,439
你后面就是一个三层的就有两个隐藏层的啊

111
00:04:29,439 --> 00:04:30,840
多层感知机

112
00:04:30,840 --> 00:04:33,120
那么前面就是两个卷积层

113
00:04:33,139 --> 00:04:37,199
每个卷积层后面有一个激活层和一个迟化层

114
00:04:37,199 --> 00:04:40,240
这就是Nernet这个函数啊

115
00:04:40,259 --> 00:04:41,840
当年啊

116
00:04:41,840 --> 00:04:43,400
这篇文章还有这篇文章

117
00:04:43,400 --> 00:04:44,760
大家有兴趣可以去看一下

118
00:04:44,780 --> 00:04:45,280
就是说

119
00:04:46,840 --> 00:04:47,640
整的挺多了

120
00:04:47,640 --> 00:04:49,000
讲了写了三十页

121
00:04:49,020 --> 00:04:51,600
然后里面其实还是有挺多有意思的东西

122
00:04:51,620 --> 00:04:53,040
但是这些那些有意思的东西

123
00:04:53,040 --> 00:04:55,160
现在的现在都没有被实现

124
00:04:55,160 --> 00:04:56,040
就太抄袭了

125
00:04:56,040 --> 00:04:58,840
里面有一点很多什么graph matching的东西

126
00:04:58,860 --> 00:05:00,960
现在都没有实现啊

127
00:05:00,960 --> 00:05:03,720
但是他的核心的核心就是这么几行代码

128
00:05:05,160 --> 00:05:05,480
OK

129
00:05:08,160 --> 00:05:09,000
那定义好之后

130
00:05:09,000 --> 00:05:11,720
我们可以看到是说啊

131
00:05:11,920 --> 00:05:15,440
比如说我就随机给你一个random的一个输入啊

132
00:05:15,440 --> 00:05:16,720
然后我们在Net

133
00:05:16,740 --> 00:05:18,720
因为我们是NN sequential 构造的

134
00:05:18,740 --> 00:05:22,440
所以我们能够把我就是4 layer in net

135
00:05:22,440 --> 00:05:25,440
就是对里面每一层做一次迭代

136
00:05:25,460 --> 00:05:26,760
就把它拿出来

137
00:05:26,760 --> 00:05:27,960
拿出来之后呢

138
00:05:27,960 --> 00:05:30,840
我们就把它的输入算一下

139
00:05:30,860 --> 00:05:33,200
然后还是give x

140
00:05:33,200 --> 00:05:37,120
然后我们就可以把你那个x给你输出一下啊

141
00:05:37,120 --> 00:05:38,720
就可以让你看到啊

142
00:05:38,740 --> 00:05:41,079
每一层他的那

143
00:05:42,240 --> 00:05:43,560
输出的大就是什么

144
00:05:43,560 --> 00:05:46,639
当然你可以用这个一层的名字在这个地方

145
00:05:47,879 --> 00:05:49,040
当然你pyTorch

146
00:05:49,060 --> 00:05:51,000
你可以有summary这个函数

147
00:05:51,000 --> 00:05:53,879
你就说你可以说他会自动给你summarize一下

148
00:05:53,879 --> 00:05:56,519
每一层大概是有多少个啊

149
00:05:56,519 --> 00:05:57,280
参数啊

150
00:05:57,280 --> 00:05:58,439
输出是多少

151
00:05:58,459 --> 00:06:01,079
但我们后面你这个summary就比较难了

152
00:06:01,079 --> 00:06:03,439
后面就是你要几百层的话就不好写

153
00:06:03,439 --> 00:06:05,720
所以我们手动的时候更灵活一点

154
00:06:06,720 --> 00:06:07,120
好

155
00:06:07,120 --> 00:06:08,840
我们可以看到就说啊

156
00:06:08,840 --> 00:06:10,040
这里面比较关键的时候

157
00:06:10,040 --> 00:06:12,400
你要去看每一个输入啊

158
00:06:12,400 --> 00:06:15,160
他的在里面的变化是什么样子的

159
00:06:15,180 --> 00:06:18,960
然后同样道理是说有时候你比较深的时候

160
00:06:18,980 --> 00:06:22,120
你也不知道这个输出是什么样子

161
00:06:22,120 --> 00:06:23,640
然后你就是run一下

162
00:06:23,640 --> 00:06:28,800
就跑一下就可以知道哪一个具体哪一个层的输出的尺寸是什么样子

163
00:06:28,820 --> 00:06:30,320
让你把它填进去啊

164
00:06:30,320 --> 00:06:33,880
这个是pyTorch一开始没有做特别好的地方

165
00:06:34,519 --> 00:06:35,360
就你得去填

166
00:06:36,639 --> 00:06:37,639
OK

167
00:06:37,639 --> 00:06:39,480
那我们来看一下这个地方啊

168
00:06:39,480 --> 00:06:41,480
可以看到是说首先你的输入对吧

169
00:06:41,480 --> 00:06:43,879
输入是第一个都是批量大小

170
00:06:43,899 --> 00:06:47,279
你就不用管那么第一个就是你12828

171
00:06:48,639 --> 00:06:50,920
那么看到说第一个卷积层干的事情

172
00:06:50,920 --> 00:06:52,240
就是说啊

173
00:06:52,240 --> 00:06:54,040
把你的通道数加六了

174
00:06:55,199 --> 00:06:56,759
然后你的高快门变

175
00:06:58,159 --> 00:07:00,159
那是因为确实你那个图片

176
00:07:00,159 --> 00:07:00,719
你的图片

177
00:07:01,720 --> 00:07:05,680
因为2828确实已经卡到了整个它的边框

178
00:07:05,680 --> 00:07:09,680
就是说你的图片的那个字已经在那个边框边边住了

179
00:07:09,700 --> 00:07:10,840
所以你再往下减

180
00:07:10,860 --> 00:07:12,000
所以你需要py了一点点

181
00:07:12,000 --> 00:07:12,440
不然的话

182
00:07:12,440 --> 00:07:14,960
你就是你就如果你那个边框就在边框

183
00:07:14,960 --> 00:07:17,800
我那个窗口一看一下就没了

184
00:07:17,820 --> 00:07:18,320
对吧

185
00:07:18,320 --> 00:07:19,640
所以你再py了一点点的话

186
00:07:19,640 --> 00:07:22,160
你多两年多一两下还在那个框里面

187
00:07:24,040 --> 00:07:25,280
所以这就是说一般来说

188
00:07:25,280 --> 00:07:26,240
你看到啊

189
00:07:26,260 --> 00:07:27,800
第一个卷积层会有所减少

190
00:07:27,820 --> 00:07:29,240
但是我们加了个pyting在这个地方

191
00:07:31,720 --> 00:07:33,800
然后你的当然你计划很凶

192
00:07:33,800 --> 00:07:35,000
你不会变了

193
00:07:35,000 --> 00:07:37,160
而卷你的持话层呢

194
00:07:37,180 --> 00:07:38,600
说是你的通道数不变

195
00:07:38,600 --> 00:07:40,480
你的高宽变了

196
00:07:40,500 --> 00:07:41,840
所以说第一个模块啊

197
00:07:41,840 --> 00:07:45,280
就是说卷积加激活加持话

198
00:07:45,280 --> 00:07:49,640
等于是把你的一成2828变成了六成1414

199
00:07:49,660 --> 00:07:51,120
就是你高宽减半

200
00:07:51,120 --> 00:07:53,960
但是你的通道数增加了六倍

201
00:07:53,960 --> 00:07:55,800
所以其实你是信息变多了

202
00:07:55,800 --> 00:07:56,080
对吧

203
00:07:58,080 --> 00:07:58,840
那接下来啊

204
00:07:58,840 --> 00:08:03,200
第二组也是卷积加激活加持话是一个组啊

205
00:08:03,220 --> 00:08:04,680
那就是从

206
00:08:06,280 --> 00:08:06,760
啊

207
00:08:10,600 --> 00:08:13,320
他应该输入是六成14成14

208
00:08:13,340 --> 00:08:14,600
因为这是他输出啊

209
00:08:14,600 --> 00:08:16,320
从六成14到14

210
00:08:16,320 --> 00:08:18,600
他的输出就是16成5成5

211
00:08:19,800 --> 00:08:24,920
也就是说你的高宽仍然被减了大概三倍的样子

212
00:08:24,940 --> 00:08:26,960
然后你的通道数呢

213
00:08:26,980 --> 00:08:28,160
你从六变成了16

214
00:08:29,840 --> 00:08:30,360
啊

215
00:08:30,360 --> 00:08:33,680
最后就是你因为你拉直之后就是一个全面MLP

216
00:08:33,680 --> 00:08:35,560
MLP的核心思想是说

217
00:08:35,560 --> 00:08:41,600
你通过逐渐的把那个很长的一个项链慢慢的往下压压压

218
00:08:41,600 --> 00:08:43,840
就是说通过啊

219
00:08:43,840 --> 00:08:47,800
Hindsight就是你的隐藏层的输出逐渐的往下降

220
00:08:47,800 --> 00:08:50,040
最后能够比较

221
00:08:51,240 --> 00:08:53,360
smooth的就是降到啊

222
00:08:53,360 --> 00:08:55,759
就你开始400降到120

223
00:08:55,759 --> 00:08:56,800
然后再降到84

224
00:08:56,800 --> 00:08:58,400
再降到10

225
00:08:58,720 --> 00:09:00,879
这就是他的Nernet

226
00:09:00,899 --> 00:09:06,120
他的一个整个输入在整个模型里面的一个变化的过程

227
00:09:06,139 --> 00:09:08,039
基本上可以看到是在卷积

228
00:09:08,059 --> 00:09:11,240
就是把把那个层把它变小变小变小

229
00:09:11,240 --> 00:09:13,279
在通道变多变多

230
00:09:13,279 --> 00:09:16,439
就是说把你的就等我们之前有讲过

231
00:09:16,459 --> 00:09:20,319
每个通道信息就可以认为是一个空间的一个pattern

232
00:09:20,340 --> 00:09:21,600
就是一个模式

233
00:09:21,600 --> 00:09:21,919
就是说

234
00:09:21,919 --> 00:09:25,819
我不断的把我的空间信息把你压缩压缩

235
00:09:25,819 --> 00:09:26,840
变小变小变小

236
00:09:27,000 --> 00:09:27,840
通道数变多

237
00:09:27,860 --> 00:09:32,560
就是说我能把那些抽出来的压缩的信息放在不同的通道里面

238
00:09:32,580 --> 00:09:35,120
最后最后就是啊

239
00:09:35,120 --> 00:09:38,160
MLP就是把这些所有的模式拿出来

240
00:09:38,160 --> 00:09:42,280
然后通过一个多层的感知器模型

241
00:09:42,280 --> 00:09:45,280
最后能够训练到我最后的输出

242
00:09:45,280 --> 00:09:49,480
这也就是整个卷积神级网络的一个设计思想

243
00:09:49,480 --> 00:09:54,600
这个思想在未来基本上是一直是这样子的一个操作

244
00:09:54,600 --> 00:09:56,560
只是说在具体的时间还有不一样

245
00:09:56,640 --> 00:10:02,320
核心思想还是说你通过不断的卷积层把我的空间信息压缩压缩压缩

246
00:10:02,320 --> 00:10:06,760
然后把这些压缩好的加到我的通道里面

247
00:10:06,760 --> 00:10:08,600
所以通道是一直在增加

248
00:10:08,620 --> 00:10:09,360
然后我的

249
00:10:10,520 --> 00:10:12,720
我的高宽是一直在减少的

250
00:10:12,720 --> 00:10:13,600
到最后的最后

251
00:10:13,600 --> 00:10:18,640
就是说现在的神经网络最后的最后高宽会变成一通道数会变得很大

252
00:10:18,640 --> 00:10:20,080
上千

253
00:10:20,080 --> 00:10:23,240
然后就是做全连接输出

254
00:10:23,240 --> 00:10:23,520
OK

255
00:10:24,519 --> 00:10:26,799
所以这个思想一直是没有变化

256
00:10:26,799 --> 00:10:28,480
但是具体时间细节有变化

257
00:10:30,360 --> 00:10:35,159
让我们来看一下这个网络在我们真实的就是FreshM list

258
00:10:35,159 --> 00:10:37,559
我们还是我们没有M list

259
00:10:37,559 --> 00:10:39,079
M list大概就99%了

260
00:10:39,079 --> 00:10:39,960
没什么好看

261
00:10:39,960 --> 00:10:41,519
所以FreshM list难一点点

262
00:10:41,519 --> 00:10:44,079
就是说让我们看到是啊

263
00:10:44,100 --> 00:10:45,879
BatchSize256

264
00:10:45,899 --> 00:10:48,919
我们用前面那个函数把我们的啊

265
00:10:48,939 --> 00:10:49,639
漏的进来

266
00:10:50,000 --> 00:10:55,759
当然是说另外一个是说我们要用GPU了

267
00:10:55,759 --> 00:10:57,960
我们之前有讲过怎么用GPU

268
00:10:57,980 --> 00:10:59,639
Nernet你CPU还是能跑的

269
00:10:59,659 --> 00:11:02,240
但是基本上GPU还是挺快的

270
00:11:02,240 --> 00:11:07,159
Nernet是可能是我们介绍的唯一的一个CPU能跑的一个网络啊

271
00:11:07,159 --> 00:11:09,439
但是我们这里还是做一个GPU的实现

272
00:11:10,960 --> 00:11:13,480
首先我们来看一下我们怎么实现了

273
00:11:13,480 --> 00:11:14,200
首先

274
00:11:14,200 --> 00:11:16,919
因为我们的会实现一些啊

275
00:11:16,919 --> 00:11:20,839
手写的版本或者是TouchNN实现的版本啊

276
00:11:20,860 --> 00:11:22,439
如果是TouchNN的话

277
00:11:22,439 --> 00:11:27,319
那我们就是变成一个EVA的Module

278
00:11:27,319 --> 00:11:30,759
如果你没有Device没有给定的话

279
00:11:30,779 --> 00:11:35,199
那就是我就是从你的Network里面就Network的parameters

280
00:11:35,219 --> 00:11:36,679
他有一个啊

281
00:11:36,679 --> 00:11:40,120
我把他拿出来把他的就他是一个Iterator

282
00:11:40,120 --> 00:11:41,559
他把它构建成一个Iterator

283
00:11:41,579 --> 00:11:42,759
把第一个元素拿出来

284
00:11:42,759 --> 00:11:45,799
就把第一个Network的那个参数拿出来

285
00:11:45,800 --> 00:11:47,880
然后看一下他的Device在哪里

286
00:11:47,900 --> 00:11:49,000
如果你不告诉我的话

287
00:11:49,000 --> 00:11:50,840
我就看你的网络存在哪里

288
00:11:50,840 --> 00:11:51,960
我把你Device拿出来

289
00:11:53,120 --> 00:11:55,320
那接下来当然是说我做一个Accumulator

290
00:11:55,340 --> 00:11:57,280
就是一个累加器

291
00:11:57,280 --> 00:11:58,600
接下来就是说啊

292
00:11:58,600 --> 00:12:01,000
对每一个DataIterator X Y

293
00:12:01,000 --> 00:12:04,120
我们先把它挪到那个啊

294
00:12:04,120 --> 00:12:06,120
挪到他那个Device上面去

295
00:12:06,140 --> 00:12:07,160
就如果你是个List

296
00:12:07,160 --> 00:12:08,040
我就每一个挪一下

297
00:12:08,060 --> 00:12:09,440
如果你就是一个Pencil的话

298
00:12:09,440 --> 00:12:10,760
就挪一次嘛

299
00:12:10,760 --> 00:12:10,960
啊

300
00:12:10,960 --> 00:12:11,720
Y也弄过去

301
00:12:12,720 --> 00:12:14,960
然后我们就是把啊

302
00:12:14,960 --> 00:12:18,200
X放到我们的Network里面得到输出

303
00:12:18,200 --> 00:12:20,200
然后算一下Accuracy

304
00:12:20,200 --> 00:12:22,080
Accuracy我们已经啊

305
00:12:22,100 --> 00:12:23,080
之前定义过了

306
00:12:23,080 --> 00:12:25,440
我们就存在这个Detail里面啊

307
00:12:25,460 --> 00:12:28,800
最后我们要算一下我们的Y的元素个数啊

308
00:12:28,800 --> 00:12:34,000
最后是说所有的你那个分类正确的个数

309
00:12:34,000 --> 00:12:36,800
除以你的整个Y的大小得到你的Accuracy

310
00:12:37,800 --> 00:12:41,680
所以这个跟之前我们评估Accuracy是没本事区别的

311
00:12:41,960 --> 00:12:43,480
只是说我们这里加了个Device

312
00:12:43,480 --> 00:12:46,759
就是说你如果指定Device或者你的啊

313
00:12:46,759 --> 00:12:48,639
模型已经在GPU上的时候

314
00:12:48,639 --> 00:12:51,560
他的Accuracy会在GPU上做啊

315
00:12:51,560 --> 00:12:53,240
是因为啊

316
00:12:53,240 --> 00:12:55,519
你假设你的模型够大的话

317
00:12:55,519 --> 00:12:57,639
那你的forward你得也是挪去GPU

318
00:13:00,480 --> 00:13:01,759
OK

319
00:13:01,759 --> 00:13:04,840
另外一个是说我的Training的函数啊

320
00:13:04,860 --> 00:13:06,920
要稍微改一下下

321
00:13:06,920 --> 00:13:09,720
让他能在GPU上做

322
00:13:09,720 --> 00:13:12,960
这个函数我们放在Train Chapter 6里面

323
00:13:12,960 --> 00:13:15,879
就是这个函数名字挺奇怪的

324
00:13:15,899 --> 00:13:19,399
就是第六章他的训练函数

325
00:13:19,420 --> 00:13:22,759
然后之后基本上卷机神经网络都会用这个函数

326
00:13:22,759 --> 00:13:26,040
所以给大家解释一下的函数是怎么实现的

327
00:13:27,080 --> 00:13:31,560
但实际上跟之前那个第三章那个Training函数没本事区别

328
00:13:31,560 --> 00:13:35,360
他唯一的不一样就是多了个Device这个东西啊

329
00:13:35,360 --> 00:13:37,320
就是我们要用GPU了

330
00:13:38,280 --> 00:13:39,760
首先我们来看一下这个函数

331
00:13:41,760 --> 00:13:44,800
首先我们要初始化我们的Wait

332
00:13:44,800 --> 00:13:46,120
如果你是

333
00:13:47,800 --> 00:13:50,879
全连接层或者是卷机层的话

334
00:13:50,879 --> 00:13:57,320
我们就用XVR Uniform这个自定义的就定好的那个操作

335
00:13:57,320 --> 00:13:59,040
来初始化

336
00:13:59,040 --> 00:14:02,200
我们有之前讲过XVR这个初始函数

337
00:14:02,200 --> 00:14:05,000
就是说他会根据你的输入输出的大小

338
00:14:05,120 --> 00:14:09,919
使得你在随机输出用随机输入的时候

339
00:14:09,919 --> 00:14:13,279
你的输出和输入它的方差是差不多了

340
00:14:13,279 --> 00:14:16,320
保证你在模型开始的时候不要炸了

341
00:14:16,340 --> 00:14:18,240
或者是用变量0

342
00:14:18,259 --> 00:14:21,600
就是这个函数的XVR的主要的一个原理

343
00:14:23,039 --> 00:14:26,879
然后当时我们把它Apply到我们整个Wait上面

344
00:14:26,879 --> 00:14:27,960
就是Net的Apply

345
00:14:27,980 --> 00:14:30,799
就是对面的每一个Parameter

346
00:14:30,799 --> 00:14:32,279
他都去Run一下这个函数

347
00:14:32,959 --> 00:14:33,199
对吧

348
00:14:34,519 --> 00:14:37,839
然后我们打一下在哪个Device上训练

349
00:14:37,860 --> 00:14:42,360
常见的错误是说

350
00:14:42,360 --> 00:14:44,480
你说我要在GPU上跑

351
00:14:44,480 --> 00:14:46,079
实际上没有在GPU上跑

352
00:14:46,079 --> 00:14:47,559
所以半天不出结果

353
00:14:47,559 --> 00:14:49,000
所以最后给大家打一下

354
00:14:50,600 --> 00:14:52,720
然后我们就Net2Device

355
00:14:52,740 --> 00:14:54,799
我们之前有讲过是怎么样把它挪到

356
00:14:54,819 --> 00:14:58,439
把整个参数搬到GPU的内存上

357
00:14:58,439 --> 00:15:00,120
然后我们这里就直接用SGD

358
00:15:00,120 --> 00:15:01,919
就没有用特别Fancy的东西

359
00:15:02,759 --> 00:15:04,159
我们也讲过SGD了

360
00:15:04,159 --> 00:15:06,079
只要给一个Linear Rate就行了

361
00:15:07,879 --> 00:15:09,559
Loss就是一个Cross Entropy Loss

362
00:15:09,559 --> 00:15:13,559
我们就是一个多类分类问题

363
00:15:13,559 --> 00:15:13,879
对吧

364
00:15:13,879 --> 00:15:18,360
就跟softmax regression是没区别的

365
00:15:19,519 --> 00:15:22,199
然后这里有一个我们要动画一下这个效果

366
00:15:22,199 --> 00:15:25,839
就是有一个动画效果了

367
00:15:25,839 --> 00:15:27,919
我们就不讲不详细讲了

368
00:15:29,240 --> 00:15:30,720
下面其实跟差不多

369
00:15:30,960 --> 00:15:32,639
就是首先你的一个Followup

370
00:15:32,639 --> 00:15:35,320
就是对每一次数据做迭代

371
00:15:35,340 --> 00:15:38,320
然后说我要在每一次迭代

372
00:15:38,340 --> 00:15:41,040
每次数据迭代里面拿一个Batch出来

373
00:15:41,040 --> 00:15:44,920
我们把我的操作T2设0

374
00:15:44,920 --> 00:15:47,600
把你的输入和输出挪到我的GPU上

375
00:15:49,120 --> 00:15:50,840
把你的X和Y挪进去

376
00:15:50,860 --> 00:15:53,840
然后做我的前项操作

377
00:15:53,840 --> 00:15:55,480
计算我的损失

378
00:15:55,480 --> 00:15:56,960
然后计算T2

379
00:15:56,980 --> 00:15:58,399
然后再迭代

380
00:15:58,399 --> 00:16:01,360
后面这些东西就是说我要打印一些东西

381
00:16:01,360 --> 00:16:03,600
动画一些东西就不给大家讲了

382
00:16:03,620 --> 00:16:05,919
就我要把我的Loss给你打一打

383
00:16:05,919 --> 00:16:06,879
我的Accuracy打一打

384
00:16:06,899 --> 00:16:08,360
我的Validation Accuracy打一打

385
00:16:08,360 --> 00:16:10,439
所以这就是这个函数干的事情

386
00:16:10,459 --> 00:16:13,240
最后这些函数就是说都是一些打印信息了

387
00:16:13,259 --> 00:16:16,679
就核心根本跟前面是没区别的

388
00:16:16,679 --> 00:16:18,120
跟我们定义的第三兆

389
00:16:18,139 --> 00:16:19,480
那个圈函数没区别

390
00:16:19,500 --> 00:16:20,600
唯一的区别就是说

391
00:16:20,600 --> 00:16:25,199
我们要把那个输入输出给挪到那个GPU上

392
00:16:25,220 --> 00:16:26,759
每个Batch挪进去就这样

393
00:16:27,759 --> 00:16:29,120
对吧

394
00:16:29,120 --> 00:16:30,439
第二个主要的区别就是说

395
00:16:30,460 --> 00:16:32,200
我的Network也得挪过去

396
00:16:32,200 --> 00:16:32,639
对吧

397
00:16:32,639 --> 00:16:35,519
第二号后面那些长一点是因为

398
00:16:35,519 --> 00:16:38,600
我们因为我们之后想比较各个卷迹神经网络

399
00:16:38,620 --> 00:16:40,200
它的计算的性能

400
00:16:40,200 --> 00:16:41,519
它以及它的Accuracy

401
00:16:41,519 --> 00:16:42,960
我要多打印一点东西

402
00:16:42,960 --> 00:16:44,439
就是主要的区别

403
00:16:44,439 --> 00:16:46,439
所以看上去长一点

404
00:16:46,439 --> 00:16:47,439
OK

405
00:16:47,460 --> 00:16:50,000
所以大家不要背这个那么长的函数

406
00:16:50,000 --> 00:16:51,799
我们可能是定的最长的一个函数

407
00:16:51,799 --> 00:16:52,639
37行

408
00:16:52,639 --> 00:16:53,559
我还被他吓到了

409
00:16:57,679 --> 00:16:58,080
OK

410
00:16:58,080 --> 00:17:00,639
所以我们来我们这个就可以来

411
00:17:02,159 --> 00:17:03,240
我来给他跑一下

412
00:17:03,240 --> 00:17:04,599
我还没跑的这个函数

413
00:17:06,920 --> 00:17:08,319
我得给他跑一下

414
00:17:12,720 --> 00:17:14,799
我们来跑一下

415
00:17:14,799 --> 00:17:18,079
就是刚刚我就是用快捷键跑了一个

416
00:17:18,099 --> 00:17:19,079
从头开始运行

417
00:17:22,039 --> 00:17:25,160
就这个跑起来应该还是挺快的

418
00:17:25,200 --> 00:17:26,080
可以看到是

419
00:17:28,480 --> 00:17:32,279
基本上你的还是一样啊

420
00:17:32,279 --> 00:17:34,560
X是你的Data的Path

421
00:17:34,580 --> 00:17:35,720
就是Epoch

422
00:17:35,720 --> 00:17:36,720
然后你的Y呢

423
00:17:36,720 --> 00:17:37,519
就是Depends

424
00:17:37,519 --> 00:17:39,120
就说如果是蓝色的话

425
00:17:39,120 --> 00:17:39,960
就是Training Loss

426
00:17:39,960 --> 00:17:42,440
就是说你的损失应该就往下降了

427
00:17:42,440 --> 00:17:42,880
你的

428
00:17:44,640 --> 00:17:48,840
测试级的精度和你的验证级的精度

429
00:17:48,860 --> 00:17:49,759
就是在这个

430
00:17:51,160 --> 00:17:52,800
这个线上和这个绿线上

431
00:17:52,820 --> 00:17:54,640
这个红线和绿线上面

432
00:17:54,640 --> 00:18:01,800
就基本上可以看到是说测试精度和训练精度基本上是重合的

433
00:18:01,820 --> 00:18:05,000
就是说你可以看到没有什么太多Overfitting在里面

434
00:18:06,480 --> 00:18:06,920
相对来说

435
00:18:06,940 --> 00:18:09,200
这是一个比较小的一个啊

436
00:18:09,220 --> 00:18:10,520
卷体神经网络啊

437
00:18:10,540 --> 00:18:14,040
所以我们没看到特别多的Overfitting

438
00:18:14,040 --> 00:18:16,920
没有Overfitting就意外着你很有可能就是Underfitting了

439
00:18:16,940 --> 00:18:18,640
就说就说你这个模型不够强

440
00:18:20,160 --> 00:18:20,640
可以看一下

441
00:18:20,660 --> 00:18:23,920
就是说我们的精度是0.82

442
00:18:24,960 --> 00:18:26,800
然后你的测试精度是0.827

443
00:18:26,800 --> 00:18:27,800
就基本上是重合了

444
00:18:27,800 --> 00:18:28,080
对吧

445
00:18:29,560 --> 00:18:30,759
然后你的样本呢

446
00:18:30,759 --> 00:18:31,400
还挺快的

447
00:18:31,400 --> 00:18:34,200
八万个八万九千Example Second

448
00:18:34,200 --> 00:18:37,640
我们就是我们这个样本一共就五万个样本

449
00:18:37,660 --> 00:18:41,080
所以你基本上你一秒钟能够跑

450
00:18:41,080 --> 00:18:45,160
基本上将近两次数据扫数据啊

451
00:18:45,160 --> 00:18:46,520
但是你看上去没那么快

452
00:18:46,520 --> 00:18:47,720
是因为啊

453
00:18:47,720 --> 00:18:48,840
你还做验证的

454
00:18:48,840 --> 00:18:51,040
我们还做还扫了一遍做验证

455
00:18:51,040 --> 00:18:53,080
然后还得额外的开销啊

456
00:18:53,080 --> 00:18:53,800
打印打印啊

457
00:18:54,279 --> 00:18:55,759
打印这个图本上还Python

458
00:18:55,759 --> 00:18:58,559
就是很容易写的代码不好

459
00:18:58,579 --> 00:18:59,559
就搞不搞笑

460
00:18:59,559 --> 00:19:02,279
就这个图其实你要不断去Update

461
00:19:02,279 --> 00:19:04,079
这个非常搞笑

462
00:19:04,099 --> 00:19:07,039
所以他有个0点几秒的延时

463
00:19:07,039 --> 00:19:08,440
没打印一个点

464
00:19:08,440 --> 00:19:12,159
所以就导致你看上去应该是刷一下能跑出来

465
00:19:12,159 --> 00:19:15,200
但实际上你看上可能要十秒钟的样子

466
00:19:15,200 --> 00:19:15,759
OK

467
00:19:15,759 --> 00:19:18,240
所以这个就是啊

468
00:19:18,259 --> 00:19:20,039
Nonet的效果啊

469
00:19:20,039 --> 00:19:22,960
我们就后面我们当可以大家记得

470
00:19:22,960 --> 00:19:25,720
我们在之前做现行模型的时候有据说啊

471
00:19:25,720 --> 00:19:28,120
这个网络拿去实际数据验证一下

472
00:19:28,120 --> 00:19:29,440
看一下分类正确

473
00:19:29,440 --> 00:19:30,279
我们就不做了

474
00:19:30,279 --> 00:19:32,039
大家有兴趣可以去看一下啊

475
00:19:32,039 --> 00:19:33,279
当然是说啊

476
00:19:33,279 --> 00:19:36,160
这里值得注意的是说首先他很快

477
00:19:38,039 --> 00:19:38,920
可以理解对吧

478
00:19:38,920 --> 00:19:42,319
八十年代的东西的跑到现在

479
00:19:42,340 --> 00:19:46,200
那当年要跑个当年这是要超级计算机来跑了啊

480
00:19:46,220 --> 00:19:48,079
当年是为了跑Nonet

481
00:19:48,079 --> 00:19:52,200
我们用的是一个这个叫vector machine

482
00:19:52,200 --> 00:19:53,400
就是当年的啊

483
00:19:53,400 --> 00:19:58,960
也是可能要十万美金或者就是特殊定制的高端CPU跑的

484
00:19:58,980 --> 00:20:03,480
现在我们就是GPU可以跑到基本上就是瞬间出结果

485
00:20:04,840 --> 00:20:08,080
第二个大家可能注意的是说我们进度8.2

486
00:20:08,100 --> 00:20:08,799
827

487
00:20:08,799 --> 00:20:10,799
827其实不高啊

488
00:20:10,799 --> 00:20:11,319
827

489
00:20:11,319 --> 00:20:13,440
我们可以看一下我们之前的那个

490
00:20:16,840 --> 00:20:21,160
我们可以看一下之前我们那个MLP能跑多少啊

491
00:20:21,160 --> 00:20:21,800
我给大家

492
00:20:26,720 --> 00:20:28,640
多层杆子机器人来看一下MLP

493
00:20:28,660 --> 00:20:31,519
我们最多跑了多少

494
00:20:35,680 --> 00:20:37,320
差不多我就说啊

495
00:20:37,320 --> 00:20:38,519
那我们没有打印啊

496
00:20:38,519 --> 00:20:40,680
我们没有打印具体的数字多少

497
00:20:40,680 --> 00:20:42,920
当时候为了为了就是说代码简单一点

498
00:20:42,940 --> 00:20:45,680
打印就多几行代码啊

499
00:20:45,700 --> 00:20:46,200
代码一长

500
00:20:46,220 --> 00:20:46,960
别人就吓走了

501
00:20:46,960 --> 00:20:48,840
所以我们就没打印啊

502
00:20:48,839 --> 00:20:51,759
我觉得应该是比MLP高那么一点点

503
00:20:51,779 --> 00:20:53,720
但是可以看到是说卷积层网络

504
00:20:53,720 --> 00:20:57,439
他的过拟核现象比MLP要少

505
00:20:57,439 --> 00:20:59,759
因为他的模型比MLP要小

506
00:20:59,759 --> 00:21:00,839
大家可以去算一下

507
00:21:00,839 --> 00:21:02,759
Nullet他的模型有多少

508
00:21:02,759 --> 00:21:05,199
我们有讲过卷积层的模型

509
00:21:05,199 --> 00:21:08,159
他其实比全连接他等价一个受限的全连接

510
00:21:08,159 --> 00:21:08,579
对吧

511
00:21:08,579 --> 00:21:10,079
他的模型少很多

512
00:21:10,079 --> 00:21:12,679
少很多就意味着他的模型复杂度低

513
00:21:12,679 --> 00:21:13,679
所以他的模型

514
00:21:13,679 --> 00:21:15,720
他的over fitting的概率就变低

515
00:21:15,720 --> 00:21:17,159
所以看到是说

516
00:21:17,160 --> 00:21:19,080
Nullet几乎没有over fitting现象

517
00:21:19,080 --> 00:21:21,320
但是MLP是有一点点的

518
00:21:21,340 --> 00:21:24,519
而且我猜Nullet可能是比这个好那么一点点啊

519
00:21:24,540 --> 00:21:25,880
另外这个我们没调参

520
00:21:25,880 --> 00:21:27,000
就是说大家可以调一调

521
00:21:27,000 --> 00:21:29,240
这应该是可以调到0.8384

522
00:21:29,240 --> 00:21:31,240
应该是没问题的啊

523
00:21:31,259 --> 00:21:34,460
里面有很多参数其实是不合理的

524
00:21:34,460 --> 00:21:36,519
就是说在MNIST也是行啊

525
00:21:36,519 --> 00:21:40,160
但是在Fashion MNIST这个结果应该里面有很多东西是不合理的

526
00:21:40,180 --> 00:21:40,720
可以调一调

527
00:21:40,720 --> 00:21:42,800
应该调到可以调高一点点

528
00:21:42,800 --> 00:21:43,840
OK

529
00:21:43,840 --> 00:21:45,880
这就是Nullet

