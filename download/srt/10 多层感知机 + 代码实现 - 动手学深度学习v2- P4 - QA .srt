1
00:00:00,000 --> 00:00:04,280
好 多层杆子记得从零开始实现

2
00:00:04,280 --> 00:00:07,000
就是我们把重新实现一遍

3
00:00:07,000 --> 00:00:11,480
首先我们这个地方就是我把我的摄像头关一下

4
00:00:11,480 --> 00:00:15,919
这里我们跟前面是一样的

5
00:00:15,919 --> 00:00:16,480
没变化

6
00:00:16,480 --> 00:00:18,679
就是批量大小等于256

7
00:00:18,679 --> 00:00:23,199
然后我们之前我们不是定义过一个FreshM list的data

8
00:00:23,199 --> 00:00:24,519
就是我们把它漏了进来

9
00:00:24,519 --> 00:00:27,400
有一个训练和一个测试级

10
00:00:28,400 --> 00:00:29,800
就是这个事情

11
00:00:30,440 --> 00:00:31,840
好 关键是说

12
00:00:31,840 --> 00:00:33,560
关键其实是这几个函数

13
00:00:33,560 --> 00:00:34,600
这几个东西了

14
00:00:34,880 --> 00:00:37,640
就假设我们的输入是784

15
00:00:37,679 --> 00:00:39,880
就因为是28×18的图片

16
00:00:39,880 --> 00:00:41,840
我们的输出是10

17
00:00:42,280 --> 00:00:44,280
这两个是你管不了的

18
00:00:44,280 --> 00:00:46,439
这个是数据决定了

19
00:00:46,680 --> 00:00:51,240
那么这一个东西就是你的隐藏层256

20
00:00:51,240 --> 00:00:54,120
这就是一个我的超参数了

21
00:00:54,520 --> 00:00:59,440
就是他其实选的就是784和10之间的一个数了

22
00:01:00,080 --> 00:01:01,600
那么定义第一层

23
00:01:01,840 --> 00:01:04,680
第一层就是这个其实你加不加都没关系

24
00:01:04,680 --> 00:01:08,320
就是说声明是一个touch的一个parameter

25
00:01:08,680 --> 00:01:09,680
不加也没关系

26
00:01:09,680 --> 00:01:11,520
其实我们之前是没加的

27
00:01:12,000 --> 00:01:15,240
所以我们W是初始成一个随机的

28
00:01:15,439 --> 00:01:20,719
它的函数是我的输入的个数784

29
00:01:21,159 --> 00:01:22,880
列数是我的256

30
00:01:23,239 --> 00:01:24,439
那当时我要算

31
00:01:24,439 --> 00:01:25,159
因为我要更新

32
00:01:25,359 --> 00:01:26,239
所以我要剔度

33
00:01:26,560 --> 00:01:29,199
我的标我的偏差

34
00:01:29,439 --> 00:01:32,079
就是我的隐藏层的个数是一个向量

35
00:01:32,239 --> 00:01:33,640
偏差就可以设成0

36
00:01:35,319 --> 00:01:38,000
另外大家没有问过是为什么要随机

37
00:01:38,479 --> 00:01:40,439
大家可以试一下设成0会怎么样

38
00:01:40,640 --> 00:01:42,840
全部设成0或者全部设成1会怎么样

39
00:01:42,840 --> 00:01:43,679
可以试一下

40
00:01:43,679 --> 00:01:44,679
我就不公布答案了

41
00:01:44,679 --> 00:01:47,079
大家一定要动手

42
00:01:48,079 --> 00:01:50,799
然后第二层就是输出层

43
00:01:51,079 --> 00:01:53,159
那它是输入层

44
00:01:53,359 --> 00:01:55,799
就是前一个隐藏层的输出了

45
00:01:55,799 --> 00:01:57,359
就是256

46
00:01:57,359 --> 00:01:58,640
就是number of hindrance

47
00:01:59,120 --> 00:02:01,959
输出他们的那就是一个10

48
00:02:02,120 --> 00:02:03,560
同样我需要剔度的

49
00:02:03,920 --> 00:02:07,359
最后的标量是一个10

50
00:02:07,359 --> 00:02:09,240
就是因为我要分10类

51
00:02:09,719 --> 00:02:12,400
那么这样我所有的参数就是W1和B1

52
00:02:12,400 --> 00:02:13,719
就是我的第一层

53
00:02:14,479 --> 00:02:16,400
W2和B2就是第二层

54
00:02:16,960 --> 00:02:17,400
OK

55
00:02:17,400 --> 00:02:20,519
这就是我的第一

56
00:02:21,800 --> 00:02:25,080
然后我们来看一下我们的relu的函数

57
00:02:25,080 --> 00:02:28,680
relu还是就是可以看一下

58
00:02:28,680 --> 00:02:29,080
是干嘛

59
00:02:29,080 --> 00:02:29,920
就是一个max

60
00:02:29,920 --> 00:02:31,760
就给个x进来

61
00:02:31,760 --> 00:02:34,439
我生成一个zero like

62
00:02:34,439 --> 00:02:38,680
就是跟他的数据类型和它的形状都一样

63
00:02:38,680 --> 00:02:40,920
但是元素的值全是为0

64
00:02:40,920 --> 00:02:41,600
就A

65
00:02:42,200 --> 00:02:45,760
那么就是说我来把它点max

66
00:02:45,760 --> 00:02:47,600
就是求一个最大值

67
00:02:47,600 --> 00:02:48,520
那就是relu

68
00:02:50,840 --> 00:02:52,600
另外我们实现我的模型

69
00:02:54,360 --> 00:02:57,080
模型就是跟前面是一样的

70
00:02:57,439 --> 00:02:58,320
x进来

71
00:02:58,320 --> 00:03:01,520
我们先把它拉成一个二维的一个矩阵

72
00:03:01,960 --> 00:03:04,640
-1就是反正就是batch size了

73
00:03:05,080 --> 00:03:07,040
你这个就是784

74
00:03:07,040 --> 00:03:09,960
把本来28的图片拉成一个矩阵

75
00:03:10,439 --> 00:03:12,040
那么进来先做乘法

76
00:03:12,560 --> 00:03:15,320
矩阵乘法就是一个可以用这个来

77
00:03:15,320 --> 00:03:16,800
解写起来方便一点

78
00:03:16,800 --> 00:03:18,120
at符号

79
00:03:18,439 --> 00:03:19,800
就输入乘以W1

80
00:03:19,960 --> 00:03:20,960
加上B1

81
00:03:21,320 --> 00:03:23,280
就刚刚大家有说过转制不转制

82
00:03:23,280 --> 00:03:25,920
转制不转制就取决你W1是怎么创建了

83
00:03:26,280 --> 00:03:29,280
就我们创建的是x是一个p2大小

84
00:03:29,280 --> 00:03:31,240
乘一个输入的大小784

85
00:03:31,240 --> 00:03:35,040
我们的W1是生命乘784乘以256

86
00:03:35,040 --> 00:03:36,560
所以就直接乘了不要转制

87
00:03:36,960 --> 00:03:39,880
再加上你的我的256长的向量

88
00:03:40,680 --> 00:03:41,880
来进入我们的relu

89
00:03:42,680 --> 00:03:44,760
然后最后的输出就是我们的

90
00:03:44,760 --> 00:03:47,800
第一层的输出和第二层的

91
00:03:48,200 --> 00:03:49,320
权重做乘法

92
00:03:49,320 --> 00:03:52,080
再加上我们的偏差就行了

93
00:03:52,439 --> 00:03:53,360
这就是输出

94
00:03:53,560 --> 00:03:54,920
最后损失是跟前面一样

95
00:03:54,920 --> 00:03:55,960
就cross entropy了

96
00:03:59,480 --> 00:04:00,400
好训练

97
00:04:00,680 --> 00:04:02,320
训练跟前面也是一样的

98
00:04:02,320 --> 00:04:06,840
就是所以你可以看到说我们跑10个apoc

99
00:04:07,200 --> 00:04:09,360
然后能力rate选成0.1

100
00:04:09,680 --> 00:04:11,120
然后我们的sgd

101
00:04:11,439 --> 00:04:12,800
然后券也是之前的函数

102
00:04:12,800 --> 00:04:14,640
就是说代码跟前面是一样的

103
00:04:14,680 --> 00:04:16,360
所以唯一的变化可能就是这个图了

104
00:04:16,360 --> 00:04:19,520
这个图就是说我们就因为我们时间不多

105
00:04:19,520 --> 00:04:21,120
所以我们就不去重新run了

106
00:04:21,480 --> 00:04:23,560
主要你可以看一下跟前面的

107
00:04:23,840 --> 00:04:27,000
比如说跟线性跟softmax回归的区别

108
00:04:28,439 --> 00:04:29,439
比如说可以看一下

109
00:04:29,840 --> 00:04:31,160
可是你选

110
00:04:32,639 --> 00:04:34,400
我觉得我这个网络确实比较慢

111
00:04:34,400 --> 00:04:35,560
我也

112
00:04:36,759 --> 00:04:37,319
OK

113
00:04:39,879 --> 00:04:42,759
我觉得可能是今天美国回国的网络有点满

114
00:04:43,920 --> 00:04:45,759
或者别的原因

115
00:04:46,240 --> 00:04:47,120
就看到是说

116
00:04:47,120 --> 00:04:48,879
这个是我们之前没有隐藏的

117
00:04:48,879 --> 00:04:49,519
会怎么样

118
00:04:49,920 --> 00:04:53,480
可以看到是说我的损失大概是0.4多一点点

119
00:04:53,639 --> 00:04:55,759
我的精度是0.8多一点点

120
00:04:55,759 --> 00:04:57,319
我们这里没有写具体数字

121
00:04:57,319 --> 00:04:59,800
大家就是靠肉眼看一下

122
00:05:01,159 --> 00:05:03,120
然后我们这里可以看到是说

123
00:05:03,120 --> 00:05:06,199
我的损失其实是低一些了

124
00:05:06,199 --> 00:05:06,680
对吧

125
00:05:06,719 --> 00:05:08,000
我是0.4以下了

126
00:05:08,000 --> 00:05:09,839
之前是0.4以上

127
00:05:10,599 --> 00:05:11,759
是0.4以上

128
00:05:12,319 --> 00:05:14,079
但我的精度其实我看一下

129
00:05:14,359 --> 00:05:16,479
精度其实没有发生太多变化

130
00:05:16,479 --> 00:05:17,719
肉眼看不出来

131
00:05:17,719 --> 00:05:19,399
都是0.8多那么一点点

132
00:05:19,680 --> 00:05:24,240
所以就是说我的多层感知机

133
00:05:24,240 --> 00:05:28,479
就是说这个模型损失确实往下降

134
00:05:28,479 --> 00:05:31,120
但精度没有说比之前好很多

135
00:05:31,719 --> 00:05:33,279
就是说你可以看到这个现象

136
00:05:33,839 --> 00:05:36,959
我们之后再来仔细的探讨这一块是怎么回事

137
00:05:38,240 --> 00:05:38,879
所以这个就是

138
00:05:38,879 --> 00:05:41,120
但是说因为我的模型更大了

139
00:05:41,120 --> 00:05:42,599
所以我的数据理合性更好

140
00:05:42,599 --> 00:05:43,879
所以我的损失在下降

141
00:05:44,879 --> 00:05:45,359
OK

142
00:05:46,839 --> 00:05:47,039
好

143
00:05:47,039 --> 00:05:48,800
这就是从零开始

144
00:05:48,839 --> 00:05:51,759
我们快速进入一下

145
00:05:52,079 --> 00:05:55,159
从简洁实现

146
00:05:56,360 --> 00:05:57,000
简洁实现

147
00:05:57,000 --> 00:05:58,319
因为它还是一个很简单模型

148
00:05:58,360 --> 00:06:00,000
所以它的实现也不难

149
00:06:04,039 --> 00:06:05,240
就隐藏层的话

150
00:06:05,240 --> 00:06:06,759
那就是说白了

151
00:06:06,759 --> 00:06:07,719
就是唯一的变化

152
00:06:07,719 --> 00:06:08,719
就是这个地方了

153
00:06:09,199 --> 00:06:10,519
就跟之前是一样的

154
00:06:10,519 --> 00:06:12,639
我们有一个把输入

155
00:06:12,639 --> 00:06:13,879
因为是一个3D的东西

156
00:06:13,879 --> 00:06:15,879
要flatten成一个二维的东西

157
00:06:16,039 --> 00:06:17,639
就是说有个flatten的layer

158
00:06:17,839 --> 00:06:19,719
然后我们这里就是

159
00:06:21,399 --> 00:06:22,839
一个线性层

160
00:06:23,599 --> 00:06:25,279
告诉你输入是784

161
00:06:25,439 --> 00:06:26,479
输出是256

162
00:06:26,479 --> 00:06:27,719
就是我们还是用256

163
00:06:27,839 --> 00:06:29,240
然后不一样的是说

164
00:06:29,240 --> 00:06:32,560
我们再加了一个renew的激活函数

165
00:06:33,319 --> 00:06:34,439
然后这是我的输出

166
00:06:34,439 --> 00:06:35,919
就是常为10

167
00:06:36,959 --> 00:06:38,000
别的其实都一样

168
00:06:38,000 --> 00:06:40,039
别的就是怎么need to wait

169
00:06:40,360 --> 00:06:43,280
大家可以看一下前面会怎么样

170
00:06:44,400 --> 00:06:46,319
那么训练跟本质上也没区别

171
00:06:46,319 --> 00:06:48,000
就是batch size

172
00:06:48,000 --> 00:06:48,879
run rate

173
00:06:48,879 --> 00:06:50,120
跑多少轮

174
00:06:50,240 --> 00:06:51,840
用cross entropy的loss

175
00:06:51,879 --> 00:06:53,439
用SGD做trainer

176
00:06:53,560 --> 00:06:55,480
然后把数据弄出来

177
00:06:55,480 --> 00:06:59,000
然后用之前我们上一次实现的

178
00:06:59,000 --> 00:07:00,800
那一个函数作为

179
00:07:01,520 --> 00:07:03,200
把它存在detail这个包里面

180
00:07:03,240 --> 00:07:04,840
然后把它调用一次就行了

181
00:07:06,280 --> 00:07:07,400
最后看到是说

182
00:07:07,400 --> 00:07:08,520
因为在理论上

183
00:07:08,519 --> 00:07:10,079
它的实现跟前面是等价的

184
00:07:10,079 --> 00:07:12,959
所以它的损失也是0.4

185
00:07:12,959 --> 00:07:13,839
小一点点

186
00:07:14,000 --> 00:07:16,319
accuracy不是很高

187
00:07:16,319 --> 00:07:19,839
就是0.6往上去一点点

188
00:07:19,839 --> 00:07:22,719
0.8多那么一点点

189
00:07:22,719 --> 00:07:24,319
就是我们的精度了

190
00:07:25,919 --> 00:07:29,159
基本上这个就是多层感知机

191
00:07:29,359 --> 00:07:30,240
可以看到是说

192
00:07:30,240 --> 00:07:32,319
跟我们之前的softmax

193
00:07:32,319 --> 00:07:33,159
没本质区别

194
00:07:33,159 --> 00:07:33,399
对吧

195
00:07:33,399 --> 00:07:35,680
就是加了一点点东西在里面

196
00:07:36,319 --> 00:07:38,920
所以这个也是深度学习的好处

197
00:07:38,920 --> 00:07:39,639
就是说

198
00:07:40,639 --> 00:07:43,319
你的模型虽然变化很大

199
00:07:43,639 --> 00:07:47,040
但是其实我大概从代码角度来讲

200
00:07:47,040 --> 00:07:47,960
实现角度来讲

201
00:07:47,960 --> 00:07:49,240
可能就改那么一点点

202
00:07:49,240 --> 00:07:50,840
让别的东西都可以不用变

203
00:07:51,000 --> 00:07:54,199
所以就是说大家为什么现在用MLP

204
00:07:54,199 --> 00:07:55,680
而不是用SVM

205
00:07:55,720 --> 00:07:56,920
SVM的话

206
00:07:56,960 --> 00:07:58,319
可能更容易调效果

207
00:07:58,319 --> 00:07:59,199
说不定还好一些

208
00:08:00,040 --> 00:08:01,600
之所以不用是因为

209
00:08:01,639 --> 00:08:03,120
我用MLP效果不好

210
00:08:03,120 --> 00:08:04,319
我可以转卷机

211
00:08:04,439 --> 00:08:07,120
可以转我们之后的RN

212
00:08:07,120 --> 00:08:09,360
可以转我们之后的transformer

213
00:08:09,399 --> 00:08:11,159
我都不要变化太多东西

214
00:08:11,199 --> 00:08:12,680
但是转SVM的话

215
00:08:12,680 --> 00:08:13,560
相对来说

216
00:08:13,920 --> 00:08:16,639
你要调的东西可能还要更多一点

217
00:08:16,680 --> 00:08:18,959
就是从难度性来讲

218
00:08:18,959 --> 00:08:21,839
为什么MLP大家会用的多一点

219
00:08:22,439 --> 00:08:23,039
OK

220
00:08:23,039 --> 00:08:25,519
这个就是我们的实现了

