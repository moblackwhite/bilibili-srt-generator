1
00:00:00,000 --> 00:00:04,400
接下来我们来看一下softmax回归的简洁实现

2
00:00:04,400 --> 00:00:07,200
就是通过PyTorch的NN的module

3
00:00:07,200 --> 00:00:10,400
来实现我们刚刚所说的softmax回归

4
00:00:10,400 --> 00:00:12,000
从零开始实现的内容

5
00:00:12,000 --> 00:00:13,800
跟之前一样

6
00:00:13,800 --> 00:00:17,199
我们首先把我们的数据拿进来

7
00:00:17,199 --> 00:00:19,600
拿到一个数据迭代器

8
00:00:21,800 --> 00:00:26,900
接下来就是它的跟之前我们线性模型的主要区别

9
00:00:26,900 --> 00:00:32,500
就是如何构造我们的softmax回归模型

10
00:00:32,500 --> 00:00:37,700
因为softmax不会显示的去调整我们输入的形状

11
00:00:37,700 --> 00:00:40,300
所以我们首先用nflatten

12
00:00:40,300 --> 00:00:45,500
flatten就是说把我一个任何维度的一个tensor

13
00:00:45,500 --> 00:00:47,300
变成一个2D的tensor

14
00:00:47,300 --> 00:00:49,100
其中第零维度保留

15
00:00:49,100 --> 00:00:52,300
剩下的维度全部转成一个向量

16
00:00:52,300 --> 00:00:55,900
这就是我们刚刚其实在softmax回归

17
00:00:55,899 --> 00:00:58,899
从零开始里面我们有那个reshape函数

18
00:00:58,899 --> 00:01:04,500
就是对应的是说我们可以加入一个module的一个flatten layer

19
00:01:04,500 --> 00:01:06,900
接下来我们定义我们的线性层

20
00:01:06,900 --> 00:01:09,099
同样输入是784

21
00:01:09,099 --> 00:01:10,700
输出是10

22
00:01:10,700 --> 00:01:12,900
让我们把这两个串在一起

23
00:01:12,900 --> 00:01:16,500
放进一个sequential的一个类构造器里面

24
00:01:16,500 --> 00:01:19,299
就可以拿到我们的network了

25
00:01:20,700 --> 00:01:24,900
同样道理是说我们定义一个initwait的函数

26
00:01:24,900 --> 00:01:26,700
当我的这个layer

27
00:01:26,700 --> 00:01:31,700
它会对每一个layer来对应的做一次call一次

28
00:01:31,700 --> 00:01:33,900
就是m就是我们的当前layer

29
00:01:33,900 --> 00:01:36,500
如果它是一个linear layer的话

30
00:01:36,500 --> 00:01:40,900
那么我们就把它的weight

31
00:01:40,900 --> 00:01:43,100
init称成一个均值为零

32
00:01:43,100 --> 00:01:43,700
默认是零

33
00:01:43,700 --> 00:01:48,500
防差等于0.01的一个随机值

34
00:01:48,500 --> 00:01:54,100
然后我们把这一个initwait函数apply到我们的net上面

35
00:01:54,300 --> 00:01:57,700
就是按照每一层去跑一下这个函数

36
00:01:57,700 --> 00:01:59,300
就可以完成我们的初始化了

37
00:02:02,700 --> 00:02:04,700
那么当然是说

38
00:02:04,700 --> 00:02:05,700
cross entropy

39
00:02:05,700 --> 00:02:08,100
这个是一个那么常见的一个损失函数

40
00:02:08,100 --> 00:02:10,700
当然所有的框架都会有对应的实现

41
00:02:10,700 --> 00:02:14,100
我们就可以直接拿到n到cross entropy loss

42
00:02:14,100 --> 00:02:14,900
拿出我们的loss

43
00:02:16,100 --> 00:02:18,100
然后跟之前一样的

44
00:02:18,100 --> 00:02:23,300
我们在sgd里面传入我们net的所谓的参数

45
00:02:23,300 --> 00:02:25,100
我们的学习率设成0.1

46
00:02:25,100 --> 00:02:26,300
那么就拿到我们的train了

47
00:02:29,100 --> 00:02:33,300
记得之前我们有定义那个train下滑线ch3

48
00:02:33,300 --> 00:02:35,100
就第三章的函数

49
00:02:35,100 --> 00:02:38,700
所以我们重用这个函数不再重新实现了

50
00:02:38,700 --> 00:02:40,900
所以同样我们跑10个apoc

51
00:02:40,900 --> 00:02:41,900
可以看一下效果

52
00:02:44,700 --> 00:02:46,300
理论上来说

53
00:02:46,300 --> 00:02:50,500
跟我们之前的效果应该是一模一样的

54
00:02:50,500 --> 00:02:54,900
可以看到是这个曲线几乎跟我们之前的

55
00:02:54,900 --> 00:02:56,699
所做的是差不多的

56
00:02:56,699 --> 00:02:58,300
可以看到曲线

57
00:02:58,300 --> 00:02:59,500
从这个点大概开始

58
00:02:59,500 --> 00:03:02,099
这是第一次扫完数据之后

59
00:03:02,099 --> 00:03:04,900
然后差不多收敛到这个地方

60
00:03:04,900 --> 00:03:07,500
loss差不多下降到这个地方

61
00:03:07,500 --> 00:03:10,300
所以基本上就是

62
00:03:10,300 --> 00:03:14,300
这个就是使用最简单的softmax回归

63
00:03:14,300 --> 00:03:16,300
我们所能达到的地方

64
00:03:16,300 --> 00:03:18,900
这个是我们最简单的模型

65
00:03:18,900 --> 00:03:20,500
最简单的分类模型

66
00:03:20,500 --> 00:03:21,500
我们在之后

67
00:03:21,500 --> 00:03:25,099
在几乎在接下来一两个月里面

68
00:03:25,099 --> 00:03:26,900
我们会不断的介绍

69
00:03:26,900 --> 00:03:28,700
在同样在这个数据集上

70
00:03:28,700 --> 00:03:32,300
通过更深的更大的模型

71
00:03:32,300 --> 00:03:34,700
对这个数据集来进行训练

72
00:03:34,700 --> 00:03:39,099
我们可以不断的去提升我们训练的精度

73
00:03:39,099 --> 00:03:41,300
可以大家来直观比较一下

74
00:03:41,300 --> 00:03:44,099
不同的模型在计算复杂度上

75
00:03:44,099 --> 00:03:45,700
对数据的理合性上

76
00:03:45,700 --> 00:03:47,300
以及是不是容易训练上

77
00:03:47,500 --> 00:03:49,100
得到直观的体验

78
00:03:49,100 --> 00:03:51,700
当然这个是我们最基本的模型

79
00:03:51,700 --> 00:03:54,100
所以我们特意的是

80
00:03:54,100 --> 00:03:57,100
详细解释了一下它的各个细节

81
00:03:57,100 --> 00:03:58,500
然后它作为基础

82
00:03:58,500 --> 00:04:01,700
来作为我们深度学习模型的基础

83
00:04:01,700 --> 00:04:04,500
好这就是softmax回归

84
00:04:04,500 --> 00:04:07,900
对于使用PyTorch module的实现

