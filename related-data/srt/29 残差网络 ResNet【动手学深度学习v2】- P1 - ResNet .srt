1
00:00:00,000 --> 00:00:03,480
假设你在卷积神级网络里面

2
00:00:03,480 --> 00:00:05,480
只要了解一个神级网络的话

3
00:00:05,480 --> 00:00:06,960
你就了解ResNet就行了

4
00:00:07,879 --> 00:00:09,599
ResNet是一个很简单的

5
00:00:09,599 --> 00:00:11,599
也是很好用的一个网络

6
00:00:11,599 --> 00:00:16,399
这也是大家会经常在实际中使用的一个网络

7
00:00:16,399 --> 00:00:18,080
我们现在给大家介绍一下

8
00:00:20,280 --> 00:00:22,039
就它源自于一个思想

9
00:00:22,039 --> 00:00:23,440
就是说我们

10
00:00:25,039 --> 00:00:28,000
说我们要不断的去加深我的神级网络

11
00:00:28,920 --> 00:00:31,199
但加深一定会给你带来好处吗

12
00:00:31,719 --> 00:00:32,640
不一定的

13
00:00:32,640 --> 00:00:33,480
就举个例子

14
00:00:34,039 --> 00:00:35,960
就假设你这个是你的最优质

15
00:00:36,200 --> 00:00:37,880
这个是我最优质在这个地方

16
00:00:38,359 --> 00:00:41,280
然后这一个FE就是我的一个函数

17
00:00:41,960 --> 00:00:43,120
就可以认为就是说

18
00:00:43,120 --> 00:00:46,640
函数的大小代表了它的函数的复杂程度

19
00:00:47,039 --> 00:00:50,719
就是说我如果这个模型够很简单的话

20
00:00:50,719 --> 00:00:53,640
我可能就得我的模型就能在这里面去

21
00:00:53,640 --> 00:00:55,519
就能够覆盖住这一块东西

22
00:00:56,040 --> 00:00:57,760
所以我能得到最好的值

23
00:00:57,760 --> 00:01:00,760
就是在这个区间里面跟它最近的对吧

24
00:01:00,960 --> 00:01:03,920
就是所谓的模型的偏差

25
00:01:04,040 --> 00:01:06,359
就是说我最好我在这一类里面

26
00:01:06,359 --> 00:01:08,800
我在我的模型可以在这里面学出一对模型

27
00:01:09,240 --> 00:01:10,920
取得最好的那一个模型

28
00:01:11,120 --> 00:01:12,800
可能最好的这个地方里面

29
00:01:12,800 --> 00:01:13,960
跟它还需要挺远的

30
00:01:14,960 --> 00:01:16,560
我可以把模型变得更复杂

31
00:01:16,560 --> 00:01:17,200
加更多的层

32
00:01:17,200 --> 00:01:20,359
使得能够去学到更大的一块东西

33
00:01:21,000 --> 00:01:24,080
那么你就说假设我是更复杂

34
00:01:24,480 --> 00:01:26,480
一直到F6是更复杂的模型

35
00:01:27,000 --> 00:01:28,640
当然它区域变更大了

36
00:01:29,120 --> 00:01:32,200
但是它更复杂的模型一定会带来好处吗

37
00:01:32,640 --> 00:01:33,640
不一定对吧

38
00:01:33,759 --> 00:01:35,200
你比如说举个例子

39
00:01:36,640 --> 00:01:38,319
这里的最好的学在这个地方

40
00:01:39,879 --> 00:01:41,680
F2最好的这个地方对吧

41
00:01:41,840 --> 00:01:42,920
在这里

42
00:01:42,959 --> 00:01:44,920
但是你到F6你会发现说

43
00:01:45,079 --> 00:01:47,640
它的那个距离对最优点的距离

44
00:01:48,039 --> 00:01:51,480
它其实比这个距离要大

45
00:01:52,480 --> 00:01:56,719
就是说虽然你的F6这个模型更加复杂了

46
00:01:56,760 --> 00:01:59,560
但是实际上你可能学偏了

47
00:02:00,000 --> 00:02:01,560
就可能你学你去

48
00:02:01,840 --> 00:02:03,719
比较适合学到那一块

49
00:02:03,760 --> 00:02:07,439
还不如一个小模型跟最优点近

50
00:02:07,600 --> 00:02:09,520
这就是在统计级学习里面

51
00:02:09,520 --> 00:02:11,360
所谓的一个模型偏差

52
00:02:12,560 --> 00:02:14,039
那么你应该怎么做呢

53
00:02:14,240 --> 00:02:14,840
就你的

54
00:02:14,840 --> 00:02:18,120
如果你是说我每一次增加我的模型的复杂度

55
00:02:19,240 --> 00:02:20,319
增加模型复杂度

56
00:02:20,319 --> 00:02:21,599
就是说F1变到F2

57
00:02:21,599 --> 00:02:22,560
F3 F6

58
00:02:22,879 --> 00:02:27,879
每一次更复杂模型是包含前面的小模型的话

59
00:02:28,840 --> 00:02:32,479
就我这个模型里面包含小模型作为子模型的话

60
00:02:32,519 --> 00:02:35,799
所以我能学到的模型就会

61
00:02:35,959 --> 00:02:37,599
严格的比前面更大

62
00:02:37,639 --> 00:02:40,479
所以永远是至少不会变差

63
00:02:40,519 --> 00:02:41,639
不能说我会改进

64
00:02:41,639 --> 00:02:44,120
但是我更复杂模型不会变差

65
00:02:44,719 --> 00:02:47,280
所以就说RefNet想的办法是说

66
00:02:47,319 --> 00:02:49,560
我让你加更多的层

67
00:02:49,560 --> 00:02:51,680
永远不会太让你

68
00:02:51,840 --> 00:02:53,520
不会让你变差

69
00:02:53,560 --> 00:02:54,920
通常来说是变好的

70
00:02:55,520 --> 00:02:55,920
OK

71
00:02:55,920 --> 00:02:57,360
这是他的核心的思想

72
00:02:58,759 --> 00:03:00,319
然后我们具体怎么做

73
00:03:00,520 --> 00:03:02,319
具体做的其实也挺简单的

74
00:03:03,120 --> 00:03:04,240
就具体是说

75
00:03:05,240 --> 00:03:06,280
我们之前都是说

76
00:03:06,280 --> 00:03:08,439
我们要把这些层给你串起来

77
00:03:09,080 --> 00:03:10,280
这一层一层接一层

78
00:03:10,879 --> 00:03:14,159
但是假设我希望是说我通过加层

79
00:03:15,319 --> 00:03:17,360
不会影响到我的

80
00:03:17,400 --> 00:03:18,640
变复杂的话

81
00:03:18,640 --> 00:03:19,439
我怎么做呢

82
00:03:19,479 --> 00:03:22,639
就是说我做一个加法在这个地方

83
00:03:24,120 --> 00:03:26,079
就是说我的新

84
00:03:27,439 --> 00:03:30,240
我新的函数FX

85
00:03:30,319 --> 00:03:33,359
是等于我的输入加上我的一个GX

86
00:03:33,479 --> 00:03:35,960
假设我的GX是我新加的东西的话

87
00:03:37,639 --> 00:03:38,520
那么就是说

88
00:03:39,240 --> 00:03:40,280
最简单情况就是说

89
00:03:40,280 --> 00:03:42,240
我还是能得到原来那个X

90
00:03:43,079 --> 00:03:45,000
就是说原来那一块东西我还是有的

91
00:03:45,520 --> 00:03:47,240
假设我的GX什么事情都不干

92
00:03:47,719 --> 00:03:49,560
我就是没学到任何东西

93
00:03:49,560 --> 00:03:51,000
我知道还是得到原来

94
00:03:51,360 --> 00:03:53,240
X里面假设已经有一个小模型了

95
00:03:53,920 --> 00:03:55,159
我还是会得到原来的值

96
00:03:56,120 --> 00:03:59,960
那么GX上任何能够做一点点用的东西

97
00:03:59,960 --> 00:04:01,640
都会把我的东西

98
00:04:01,640 --> 00:04:03,520
会能学到东西变大一点

99
00:04:05,280 --> 00:04:06,320
所以我们来看一下

100
00:04:06,320 --> 00:04:09,879
就是说从图上面讲是什么样子

101
00:04:12,560 --> 00:04:14,719
就假设说我的是一个

102
00:04:15,240 --> 00:04:16,800
假设我是有两层的话

103
00:04:16,800 --> 00:04:18,560
比如说一个卷积层

104
00:04:18,560 --> 00:04:19,480
一个激活函数

105
00:04:19,480 --> 00:04:20,160
一个卷积层

106
00:04:20,160 --> 00:04:21,000
一个激活函数

107
00:04:21,560 --> 00:04:22,360
这是我串联

108
00:04:22,360 --> 00:04:25,480
假设那么我如果想要干的事情

109
00:04:25,480 --> 00:04:26,160
就是说

110
00:04:26,759 --> 00:04:28,280
我要做一条pass

111
00:04:28,280 --> 00:04:30,800
使得说假设我这是新加的层

112
00:04:30,960 --> 00:04:33,160
我可以说没有这个层的时候

113
00:04:33,600 --> 00:04:34,439
也没关系

114
00:04:34,480 --> 00:04:35,160
那么怎么做

115
00:04:35,160 --> 00:04:37,240
就是说我就把这个X加一条

116
00:04:38,080 --> 00:04:39,199
一条捷径

117
00:04:39,199 --> 00:04:40,000
加到这个地方

118
00:04:40,120 --> 00:04:43,560
就是说他的FX得加上

119
00:04:45,240 --> 00:04:46,280
X作为输出

120
00:04:47,439 --> 00:04:48,680
就这里的意思是说

121
00:04:48,680 --> 00:04:51,680
假设这是我的这个层是我新加的的话

122
00:04:52,560 --> 00:04:54,240
那么假设没有他的话怎么办

123
00:04:54,240 --> 00:04:54,840
没有他的话

124
00:04:54,840 --> 00:04:55,759
就前面的东西

125
00:04:55,759 --> 00:04:58,160
我就是什么东西都没学到这个地方

126
00:04:58,400 --> 00:05:00,759
但是我之前的下面的层的结果

127
00:05:00,759 --> 00:05:02,240
还是能够绕过去

128
00:05:02,439 --> 00:05:03,439
能够过去来

129
00:05:06,960 --> 00:05:08,240
就他这是一个

130
00:05:08,280 --> 00:05:10,439
所以就是说你可以认为

131
00:05:10,480 --> 00:05:12,040
假设我这里还有一个小层

132
00:05:12,160 --> 00:05:12,879
在这个地方

133
00:05:13,519 --> 00:05:14,319
在这个地方

134
00:05:14,639 --> 00:05:15,319
那么

135
00:05:16,000 --> 00:05:18,040
resnet就包含了说

136
00:05:18,040 --> 00:05:19,800
我这个是这个加了

137
00:05:19,800 --> 00:05:20,719
他之后就变复杂

138
00:05:20,719 --> 00:05:21,159
对吧

139
00:05:21,560 --> 00:05:23,920
resnet就是说我可以包含一个

140
00:05:23,920 --> 00:05:24,759
没有他

141
00:05:24,759 --> 00:05:26,360
他没有任何作用的时候

142
00:05:26,800 --> 00:05:28,480
我还是能变成一个小模型

143
00:05:28,480 --> 00:05:29,560
然后直接跳到他里面

144
00:05:29,560 --> 00:05:30,839
就是我的复杂模型

145
00:05:31,039 --> 00:05:32,560
告诉你可以包含小模型

146
00:05:34,120 --> 00:05:36,399
这就是说你从函数的角度来解释

147
00:05:36,399 --> 00:05:37,719
他为什么是有用

148
00:05:37,759 --> 00:05:39,319
但是说resnet的本身

149
00:05:39,319 --> 00:05:40,920
他其实不是这么解释的

150
00:05:41,039 --> 00:05:42,199
他本身是说

151
00:05:42,240 --> 00:05:43,399
我这个东西

152
00:05:43,400 --> 00:05:45,200
我可以把残差给你pass过去

153
00:05:45,200 --> 00:05:46,640
使得你提速更快

154
00:05:46,680 --> 00:05:47,760
但实际上你可以认为

155
00:05:48,640 --> 00:05:51,360
我网络里面可以允许你嵌入了

156
00:05:51,360 --> 00:05:53,400
之前小一点的网络

157
00:05:53,440 --> 00:05:56,000
使得你先去你和小网络

158
00:05:57,720 --> 00:05:58,240
OK

159
00:05:59,960 --> 00:06:01,320
然后他一些细节是说

160
00:06:01,320 --> 00:06:02,520
他具体的设计

161
00:06:03,000 --> 00:06:04,680
是这个样子

162
00:06:05,120 --> 00:06:07,360
他也是跟他是从BGG那里过来的

163
00:06:07,760 --> 00:06:10,000
然后三层三卷机

164
00:06:10,440 --> 00:06:12,000
后面加入batch normalization

165
00:06:12,320 --> 00:06:13,360
再加入relu

166
00:06:13,480 --> 00:06:15,000
再加入三层三卷机

167
00:06:15,040 --> 00:06:16,400
再加入batch normalization

168
00:06:16,560 --> 00:06:17,640
然后正常情况下

169
00:06:17,640 --> 00:06:17,920
对吧

170
00:06:17,920 --> 00:06:19,400
就relu就直接出去了

171
00:06:19,440 --> 00:06:21,920
但是现在是说我的x输入

172
00:06:21,920 --> 00:06:25,040
还会加在我的batch normalization的上面

173
00:06:25,440 --> 00:06:26,760
然后作为relu出去

174
00:06:27,520 --> 00:06:28,760
这是第一个实现

175
00:06:29,080 --> 00:06:30,160
第二个实现的话

176
00:06:30,200 --> 00:06:31,520
他会在这里再做一个

177
00:06:31,520 --> 00:06:33,720
一层一的卷机层来变换你的通道

178
00:06:34,080 --> 00:06:35,840
就假设我的卷机层

179
00:06:35,840 --> 00:06:37,640
要对我的通道做变换的话

180
00:06:37,800 --> 00:06:40,240
那么我这么加回去

181
00:06:40,240 --> 00:06:41,160
就直接加不回去了

182
00:06:41,200 --> 00:06:42,960
那我就加入一个一层一的卷机层

183
00:06:42,960 --> 00:06:45,080
来把我的通道输变到合适的范围

184
00:06:45,080 --> 00:06:47,800
能够按照原数这么加回去

185
00:06:48,720 --> 00:06:49,040
OK

186
00:06:49,040 --> 00:06:51,240
这是他的一个实现的细节

187
00:06:52,880 --> 00:06:57,560
然后当然是说你可以使用不同的残插块

188
00:06:58,280 --> 00:06:59,440
然后就是说

189
00:06:59,720 --> 00:07:00,920
就是说你可以说

190
00:07:00,920 --> 00:07:01,720
大家肯定会问说

191
00:07:01,720 --> 00:07:02,840
你为什么加这个地方

192
00:07:02,840 --> 00:07:04,680
实际上你可以加在很多地方

193
00:07:04,840 --> 00:07:06,120
就原始你可以加到这个地方

194
00:07:06,120 --> 00:07:06,560
对吧

195
00:07:06,720 --> 00:07:10,440
然后你说我要把这个东西加在我的batch

196
00:07:10,440 --> 00:07:12,080
normalization之前也行

197
00:07:12,359 --> 00:07:15,759
或者我说我要把在之后也行

198
00:07:15,959 --> 00:07:18,599
或者你反正你我就要把batch

199
00:07:18,599 --> 00:07:19,719
long加在我的

200
00:07:19,759 --> 00:07:21,599
我是这么我是倒着来的

201
00:07:21,599 --> 00:07:22,519
大家说

202
00:07:22,839 --> 00:07:24,079
你可以发现我是

203
00:07:24,120 --> 00:07:24,800
啊

204
00:07:24,879 --> 00:07:26,039
人路线过来

205
00:07:26,159 --> 00:07:26,800
conf

206
00:07:26,839 --> 00:07:27,919
batch normalization

207
00:07:27,919 --> 00:07:28,959
然后这么过来的

208
00:07:29,639 --> 00:07:30,959
然后当然是说

209
00:07:31,000 --> 00:07:32,479
另外一个也是

210
00:07:32,599 --> 00:07:33,919
也常见的一个

211
00:07:33,919 --> 00:07:36,519
就是说把batch normalization放在最下面

212
00:07:36,560 --> 00:07:37,959
就是这么从这里过去

213
00:07:38,319 --> 00:07:40,079
就是说你可以去试试说

214
00:07:40,079 --> 00:07:42,719
你到底那个路要放在什么地方

215
00:07:43,000 --> 00:07:44,199
就是说你在哪里

216
00:07:44,240 --> 00:07:44,959
过来

217
00:07:45,000 --> 00:07:45,680
放去哪里

218
00:07:45,680 --> 00:07:46,560
就是说你可以随便

219
00:07:46,560 --> 00:07:47,359
反正就三个层

220
00:07:47,759 --> 00:07:48,800
就卷积层

221
00:07:48,879 --> 00:07:50,519
然后batch normalization

222
00:07:50,519 --> 00:07:52,240
然后激活层就三个层

223
00:07:52,240 --> 00:07:53,719
你反正从哪里出来

224
00:07:53,719 --> 00:07:54,639
从哪里出接过去

225
00:07:54,639 --> 00:07:57,399
都你可以试各种不一样的东西

226
00:07:57,839 --> 00:08:01,039
但实际上来说可能都差不多

227
00:08:01,159 --> 00:08:04,399
可能原始的可能还效果更好一点

228
00:08:05,240 --> 00:08:06,439
就大家可以去试

229
00:08:07,719 --> 00:08:08,079
OK

230
00:08:08,079 --> 00:08:09,479
然后我们来再来看一下

231
00:08:09,480 --> 00:08:10,759
就是说整个rest net

232
00:08:10,759 --> 00:08:11,800
它的架构什么样

233
00:08:11,840 --> 00:08:14,120
就rest net叫recedual net

234
00:08:14,240 --> 00:08:15,200
它的最核心的

235
00:08:15,200 --> 00:08:17,480
就是说我加入了一个加法

236
00:08:17,480 --> 00:08:17,800
在这里

237
00:08:17,920 --> 00:08:18,920
从一条过去

238
00:08:18,960 --> 00:08:20,080
这是它最核心的

239
00:08:20,280 --> 00:08:22,840
但它自己本身的rest net

240
00:08:22,840 --> 00:08:25,120
其实你可以认为它跟VGG很像

241
00:08:25,680 --> 00:08:26,520
就是说

242
00:08:27,640 --> 00:08:30,800
就它有两种rest net block

243
00:08:31,560 --> 00:08:36,080
就第一种是说高宽减半的rest net block

244
00:08:36,800 --> 00:08:39,040
就是说所谓的

245
00:08:39,920 --> 00:08:40,800
高宽减半

246
00:08:40,800 --> 00:08:41,520
就是说

247
00:08:41,600 --> 00:08:43,920
它在卷第一个卷积层里面

248
00:08:44,600 --> 00:08:45,960
幅度等于2

249
00:08:46,000 --> 00:08:47,920
就等于是把高宽给你减半了

250
00:08:48,120 --> 00:08:50,120
然后通常来说

251
00:08:50,120 --> 00:08:52,840
你也会把你的通道数给你增加一倍

252
00:08:53,200 --> 00:08:54,400
我们之前有讲过

253
00:08:54,440 --> 00:08:55,879
所以你一层一的卷积

254
00:08:55,879 --> 00:08:56,520
就干这个事情

255
00:08:56,520 --> 00:08:58,879
把你输入也通道数也增加

256
00:08:58,879 --> 00:09:00,160
然后这样子能加起来

257
00:09:00,480 --> 00:09:02,560
然后接下来就是说

258
00:09:02,560 --> 00:09:05,240
这个是高宽减半的rest net块

259
00:09:05,240 --> 00:09:06,519
然后之后就是说

260
00:09:06,519 --> 00:09:08,480
strat全部等于1的时候

261
00:09:08,480 --> 00:09:10,720
就接多个这样子高宽不减半的

262
00:09:11,240 --> 00:09:13,200
整个这东西叫做rest net块

263
00:09:13,200 --> 00:09:15,799
然后你把n个东西重复几次

264
00:09:15,840 --> 00:09:17,200
第一块就是会

265
00:09:17,200 --> 00:09:17,919
不复等于2

266
00:09:17,919 --> 00:09:19,560
有一个步骤

267
00:09:19,560 --> 00:09:20,960
然后后面的就是接一堆

268
00:09:20,960 --> 00:09:21,720
差不多了

269
00:09:22,200 --> 00:09:24,440
然后通过不同的

270
00:09:24,480 --> 00:09:26,560
到底有多少个rest net块

271
00:09:26,560 --> 00:09:28,080
以及它的输出通道数

272
00:09:28,120 --> 00:09:30,320
你可以得到不同的rest net的架构

273
00:09:31,240 --> 00:09:33,600
就这个东西大家是可以理解

274
00:09:33,680 --> 00:09:34,840
就是它的架构

275
00:09:34,840 --> 00:09:36,680
你可以认为是很类似于VGG

276
00:09:36,800 --> 00:09:37,720
或者Google net

277
00:09:37,720 --> 00:09:39,320
就是有5个stage

278
00:09:40,639 --> 00:09:43,879
然后基本上就是一个

279
00:09:44,040 --> 00:09:45,720
7层7的卷积出来

280
00:09:45,960 --> 00:09:48,000
这里就看见加入batch normalization

281
00:09:48,240 --> 00:09:49,759
然后加入一个33的pooling

282
00:09:49,800 --> 00:09:52,360
然后就是不同的rest net的块

283
00:09:52,360 --> 00:09:53,680
就是不同的块

284
00:09:53,720 --> 00:09:55,879
最后有一个global average pooling

285
00:09:55,879 --> 00:09:58,120
就是全局的平均赤化层

286
00:09:59,279 --> 00:09:59,840
OK

287
00:09:59,879 --> 00:10:01,600
这就是rest net的架构

288
00:10:01,800 --> 00:10:03,639
我们等会来看具体的实现

289
00:10:03,800 --> 00:10:05,720
这里就是可以认为

290
00:10:06,279 --> 00:10:07,360
所以我说

291
00:10:07,720 --> 00:10:08,480
之前有说过

292
00:10:08,600 --> 00:10:10,399
架构其实都长差不多了

293
00:10:10,399 --> 00:10:11,920
就是5个stage

294
00:10:12,159 --> 00:10:13,639
然后你在每个stage

295
00:10:13,639 --> 00:10:14,440
前面stage

296
00:10:14,440 --> 00:10:16,960
大家都一般是用7层7的卷积

297
00:10:17,080 --> 00:10:18,240
然后33的pooling

298
00:10:18,279 --> 00:10:19,120
后面的话

299
00:10:19,120 --> 00:10:21,399
你就是替换成自己想要的样子

300
00:10:21,720 --> 00:10:22,800
你不同的网络

301
00:10:22,800 --> 00:10:24,399
可以有不同的自己的块

302
00:10:24,680 --> 00:10:25,800
所以基本上是

303
00:10:26,080 --> 00:10:28,000
目前来说主流是这么设计的

304
00:10:30,600 --> 00:10:31,120
OK

305
00:10:32,399 --> 00:10:33,240
然后我们来看一下

306
00:10:33,240 --> 00:10:35,040
就是说rest net

307
00:10:35,240 --> 00:10:36,040
512

308
00:10:36,040 --> 00:10:39,759
这个已经是被改良过两次三次的版本了

309
00:10:40,440 --> 00:10:41,560
就最原始的版本

310
00:10:41,560 --> 00:10:43,879
我记得rest net 512是在这个地方

311
00:10:44,519 --> 00:10:45,720
就最原始的版本

312
00:10:45,840 --> 00:10:48,080
然后在过去很多年里面

313
00:10:48,080 --> 00:10:49,680
大家对它持续的改进

314
00:10:49,720 --> 00:10:52,800
包括了你整个里面的那些东西的调整

315
00:10:52,840 --> 00:10:54,920
以及整个训练算法的调整

316
00:10:56,120 --> 00:10:56,879
就是说

317
00:10:56,879 --> 00:10:59,000
整更好的data augmentation

318
00:10:59,120 --> 00:11:00,160
更好的学习率

319
00:11:00,519 --> 00:11:02,279
现在都用一个扩散的学习率

320
00:11:03,039 --> 00:11:05,240
我们整本书都是用的是

321
00:11:05,279 --> 00:11:06,159
就固定学习率

322
00:11:06,279 --> 00:11:07,559
现在你可以用一个

323
00:11:07,879 --> 00:11:09,000
学习率可以增大变小

324
00:11:09,000 --> 00:11:09,639
增大变小

325
00:11:09,639 --> 00:11:10,079
对吧

326
00:11:10,120 --> 00:11:11,360
这种子的技术

327
00:11:11,399 --> 00:11:12,799
可以把整个目前来说

328
00:11:12,799 --> 00:11:14,639
整个rest net最好的进度在这个地方

329
00:11:15,039 --> 00:11:15,679
就512

330
00:11:15,679 --> 00:11:17,559
就是但是你下面会有

331
00:11:17,559 --> 00:11:19,480
我记得这个是101版

332
00:11:19,519 --> 00:11:21,199
就152是这样

333
00:11:21,199 --> 00:11:22,919
你有152个剪辑层

334
00:11:23,639 --> 00:11:24,759
这也是挺吓人的

335
00:11:25,240 --> 00:11:27,679
你还rest net还有201层

336
00:11:28,120 --> 00:11:28,679
就是说

337
00:11:28,680 --> 00:11:30,520
然后这个地方可能下面这个东西

338
00:11:30,520 --> 00:11:33,160
是rest net34层的rest net

339
00:11:34,160 --> 00:11:36,880
当然是说你的层数越少

340
00:11:36,880 --> 00:11:37,760
就是说白了

341
00:11:37,760 --> 00:11:39,120
不同层就是不同的

342
00:11:39,160 --> 00:11:40,120
你有多少个block

343
00:11:41,480 --> 00:11:44,160
比如说层数越少

344
00:11:44,160 --> 00:11:45,240
当然你越快了

345
00:11:45,240 --> 00:11:46,360
进度低一点

346
00:11:46,360 --> 00:11:47,480
层数越多

347
00:11:47,520 --> 00:11:49,160
通常来说进度越高

348
00:11:49,560 --> 00:11:52,280
rest net512是经常用来刷分的一个模型

349
00:11:52,920 --> 00:11:54,960
实际情况大家用的非常少

350
00:11:55,280 --> 00:11:56,520
这个东西太贵了

351
00:11:56,879 --> 00:11:58,720
实际情况下rest net34

352
00:11:58,720 --> 00:12:01,000
或者这个可能是rest net50

353
00:12:01,000 --> 00:12:04,000
我猜或者有可能是50是这个

354
00:12:04,360 --> 00:12:05,360
可能这是50

355
00:12:05,360 --> 00:12:06,480
这是34

356
00:12:06,480 --> 00:12:07,960
这可能是rest net18

357
00:12:09,039 --> 00:12:10,439
大家可以去看一下

358
00:12:11,519 --> 00:12:12,600
链接里面可以去看一下

359
00:12:12,600 --> 00:12:13,559
具体长什么样子

360
00:12:13,799 --> 00:12:14,759
一般的情况下

361
00:12:14,799 --> 00:12:16,799
我觉得34是用的比较多的

362
00:12:16,879 --> 00:12:18,279
就34个剪辑层

363
00:12:19,279 --> 00:12:20,919
然后你如果觉得还不行

364
00:12:20,919 --> 00:12:21,960
你可以上50

365
00:12:22,159 --> 00:12:24,480
但是一般来说用101

366
00:12:24,840 --> 00:12:26,720
要152这种比较很大

367
00:12:26,720 --> 00:12:27,320
很大rest net

368
00:12:27,320 --> 00:12:28,840
是用的很少的一件事情

369
00:12:28,960 --> 00:12:30,480
除非你就是用来刷棒而已

370
00:12:31,639 --> 00:12:32,120
OK

371
00:12:32,120 --> 00:12:32,960
所以这个就是

372
00:12:33,519 --> 00:12:34,200
所以你可以看到

373
00:12:34,200 --> 00:12:34,960
就是说

374
00:12:35,360 --> 00:12:36,879
虽然他现在看起来

375
00:12:36,879 --> 00:12:38,159
他不是最高的

376
00:12:38,159 --> 00:12:39,480
但实际上那些高的

377
00:12:39,600 --> 00:12:40,759
不管是这一些

378
00:12:41,560 --> 00:12:42,680
不管是这一块

379
00:12:43,560 --> 00:12:45,960
基本上都是rest net的一些变种

380
00:12:46,560 --> 00:12:48,000
都需要rest

381
00:12:48,120 --> 00:12:49,840
所以说特别是说

382
00:12:50,639 --> 00:12:51,320
rest net

383
00:12:51,320 --> 00:12:53,279
他那个reset那个链接

384
00:12:53,319 --> 00:12:55,039
就连过去那个思想

385
00:12:55,079 --> 00:12:56,639
不仅在rest net被使用

386
00:12:56,679 --> 00:12:57,919
几乎你看到

387
00:12:57,959 --> 00:12:59,919
现在所有新的网络

388
00:12:59,959 --> 00:13:01,839
这一个链接都会被使用

389
00:13:01,879 --> 00:13:03,279
不管是BERT也好

390
00:13:03,319 --> 00:13:04,720
Traform这些模型也好

391
00:13:05,120 --> 00:13:06,120
这个residual connection

392
00:13:06,120 --> 00:13:07,600
也是一个标配了

393
00:13:07,600 --> 00:13:08,480
在现在来说

394
00:13:09,639 --> 00:13:10,159
OK

395
00:13:10,199 --> 00:13:11,919
所以这就是rest net

396
00:13:13,360 --> 00:13:14,159
然后我们来看一下

397
00:13:14,159 --> 00:13:14,919
就是说总结一下

398
00:13:14,919 --> 00:13:15,720
就是说

399
00:13:15,839 --> 00:13:16,759
残差快

400
00:13:16,759 --> 00:13:18,039
就是你加一个

401
00:13:18,079 --> 00:13:19,120
翘卷那个层

402
00:13:19,159 --> 00:13:22,519
可以使得训练很深的网络

403
00:13:22,519 --> 00:13:23,240
更加容易

404
00:13:24,279 --> 00:13:25,639
是因为说

405
00:13:25,679 --> 00:13:26,959
我就算我再生

406
00:13:26,959 --> 00:13:28,559
因为我的链接存在

407
00:13:28,559 --> 00:13:30,240
使得我总是里面可以

408
00:13:30,480 --> 00:13:32,000
包含住一个小的网络

409
00:13:32,159 --> 00:13:33,480
就是说不管再生

410
00:13:33,519 --> 00:13:34,919
我也是先把下面那些

411
00:13:34,919 --> 00:13:36,559
因为我有这种翘卷层

412
00:13:36,839 --> 00:13:38,720
我总会把下面那些小的

413
00:13:38,720 --> 00:13:39,559
先训练好

414
00:13:39,720 --> 00:13:41,759
然后再慢慢的去训练更深的

415
00:13:41,799 --> 00:13:43,559
所以大家发现rest net

416
00:13:43,559 --> 00:13:44,959
可以训练到1000层

417
00:13:45,919 --> 00:13:47,519
就只要你的内存能撑得下

418
00:13:47,519 --> 00:13:49,439
你的优惠算法是能做的

419
00:13:49,879 --> 00:13:51,959
然后你的残差的网络

420
00:13:51,960 --> 00:13:54,200
对随后的深度神经网络

421
00:13:54,200 --> 00:13:55,800
产生了非常深远的影响

422
00:13:55,840 --> 00:13:57,280
就residual connection

423
00:13:57,280 --> 00:13:58,800
就是那个跳转

424
00:13:58,840 --> 00:14:01,639
几乎是所有的网络

425
00:14:01,639 --> 00:14:03,560
现在都会或多或少用到它

426
00:14:04,120 --> 00:14:06,120
这样子才能让你做到更深

427
00:14:06,759 --> 00:14:07,320
OK

428
00:14:07,320 --> 00:14:08,720
所以这就是rest net

