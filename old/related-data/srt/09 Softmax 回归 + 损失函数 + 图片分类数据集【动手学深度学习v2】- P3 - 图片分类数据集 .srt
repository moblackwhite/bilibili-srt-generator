1
00:00:00,000 --> 00:00:03,480
在我们介绍Softmax回归的实现之前

2
00:00:03,480 --> 00:00:04,919
我们先来介绍一下

3
00:00:04,919 --> 00:00:09,240
我们怎么来读取一个多类分类的数据集

4
00:00:09,240 --> 00:00:13,919
这个数据集也在今后我们的所谓的学习中一直被使用

5
00:00:13,919 --> 00:00:17,280
所以我们特意来跟大家先介绍一下

6
00:00:17,280 --> 00:00:21,760
我们知道说M-List的数据集是我们图像分类里面

7
00:00:21,760 --> 00:00:24,560
使得最广泛的数据集之一

8
00:00:24,560 --> 00:00:27,400
它就是说对手写数字的识别

9
00:00:27,600 --> 00:00:32,760
但是它是1989年或者86年提出来的

10
00:00:32,760 --> 00:00:36,160
这个数据集对于现在来说过于简单了

11
00:00:36,160 --> 00:00:38,800
所以我们用一个非常近似

12
00:00:38,800 --> 00:00:42,800
但是又稍微复杂一点的叫做Fashion M-List的数据集

13
00:00:42,800 --> 00:00:47,400
作为我们的一个数据集来验证不同模型的区别

14
00:00:48,240 --> 00:00:52,960
好 我们在这里我们先来导入我们要的一些库

15
00:00:52,960 --> 00:00:56,359
首先我们使用了Touch Vision这个库

16
00:00:56,359 --> 00:01:01,519
它是一个PyTorch对于计算机视觉一些模型实现的一个库

17
00:01:02,519 --> 00:01:04,719
然后我们还是跟之前一样的

18
00:01:04,719 --> 00:01:08,519
我们在Utils里面导入我们的Data

19
00:01:08,519 --> 00:01:13,120
这样子我们能方便一些读取数据一些小批量的一些函数

20
00:01:13,719 --> 00:01:15,519
然后我们在Touch Vision里面

21
00:01:15,519 --> 00:01:21,439
把Transforms这个对数据进行操作的一个Module给你导入进来

22
00:01:21,799 --> 00:01:28,200
最后我们知道说我们将一些函数实现好之后存在D2L里面

23
00:01:28,200 --> 00:01:31,200
就是我们从对它的Touch的实现

24
00:01:31,200 --> 00:01:32,599
我们导入D2L

25
00:01:33,400 --> 00:01:37,400
这里我们使用D2L Use SVG Display

26
00:01:37,400 --> 00:01:40,799
就是说我们用SVG来显示我们的图片

27
00:01:40,799 --> 00:01:42,599
这样子清晰度高一点

28
00:01:43,599 --> 00:01:45,200
OK 这是我们的导入

29
00:01:45,960 --> 00:01:46,359
好

30
00:01:51,359 --> 00:01:52,960
接下来看一下是说

31
00:01:55,560 --> 00:01:59,960
我们使用框架自己带的一个函数

32
00:02:00,960 --> 00:02:04,960
FreshMList来下载并读取这个数据到内存里面

33
00:02:04,960 --> 00:02:06,159
因为这个数据不大

34
00:02:06,159 --> 00:02:08,960
大概就是一两百来兆的样子

35
00:02:08,960 --> 00:02:10,759
我们就直接读入内存了

36
00:02:11,759 --> 00:02:13,560
首先我们要做一个操作

37
00:02:14,120 --> 00:02:17,719
我们需要把我们的图片转成我们的一个

38
00:02:17,719 --> 00:02:19,719
PyTorch的一个Tensor

39
00:02:19,719 --> 00:02:21,719
那就是Transforms to Tensor

40
00:02:21,719 --> 00:02:25,719
就是我们要做一个最简单的一个预处理

41
00:02:26,319 --> 00:02:29,920
接下来就是说我们把整个Data Set

42
00:02:30,719 --> 00:02:33,920
从Torch Vision Data Set里面把FreshMList拿到

43
00:02:33,920 --> 00:02:37,719
下载到我们的一个上一期目录的Data下面

44
00:02:38,319 --> 00:02:40,719
然后我们来换一个行

45
00:02:41,680 --> 00:02:43,879
可以看到是说Trend等于True是说

46
00:02:43,879 --> 00:02:46,879
这样子对应的是下载的是训练数据集

47
00:02:47,879 --> 00:02:49,879
然后它的Transforms等于Trans

48
00:02:49,879 --> 00:02:52,280
就是说我们然后拿出来之后

49
00:02:52,280 --> 00:02:55,280
我们需要得到的是一个PyTorch的Tensor

50
00:02:55,280 --> 00:02:56,879
而不是一堆图片

51
00:02:57,479 --> 00:03:00,680
最后说因为我们Download等于True

52
00:03:00,680 --> 00:03:02,680
是说我们默认从网上下载

53
00:03:03,280 --> 00:03:05,680
但如果大家这里不方便的话

54
00:03:05,680 --> 00:03:06,879
可以事先下载好

55
00:03:06,879 --> 00:03:08,879
存在Data这个文件夹里面

56
00:03:09,039 --> 00:03:11,039
这样子可以不用指定Download了

57
00:03:11,039 --> 00:03:13,840
对于Test的数据集

58
00:03:13,840 --> 00:03:14,840
就测试数据集

59
00:03:14,840 --> 00:03:17,639
我们大概没有讲训练和测试的区别

60
00:03:17,639 --> 00:03:19,240
但是可以简单认为是说

61
00:03:19,240 --> 00:03:21,039
测试数据集是用来验证

62
00:03:21,039 --> 00:03:23,039
我们模型的好坏的一个数据集

63
00:03:23,840 --> 00:03:24,840
它不参与训练

64
00:03:24,840 --> 00:03:26,840
但是来预测一下我们模型的好坏

65
00:03:27,240 --> 00:03:29,240
同样的话就是一样的操作

66
00:03:29,240 --> 00:03:32,039
但是我们唯一的区别是我们的Trend

67
00:03:32,039 --> 00:03:33,039
它等于False

68
00:03:33,039 --> 00:03:35,039
它对应的下载MList里面

69
00:03:35,039 --> 00:03:37,240
FreshMList里面那个测试集

70
00:03:38,200 --> 00:03:40,200
然后我们拿到数据集之后

71
00:03:40,200 --> 00:03:41,200
我们可以看一下

72
00:03:41,200 --> 00:03:42,400
我们MList的Trend

73
00:03:42,400 --> 00:03:44,400
大概是有6万个图片

74
00:03:44,400 --> 00:03:46,400
我们的MList的Test

75
00:03:46,400 --> 00:03:47,600
是有1万张图片

76
00:03:48,800 --> 00:03:51,000
然后我们接下来可以看到是说

77
00:03:51,000 --> 00:03:54,400
我们拿到0

78
00:03:54,400 --> 00:03:56,200
就是第0个Example

79
00:03:56,800 --> 00:03:58,600
这一个就表示我的图片

80
00:03:58,600 --> 00:04:00,600
因为它的1表示它的标号

81
00:04:00,600 --> 00:04:03,000
就是我们看到第一张图片

82
00:04:03,000 --> 00:04:05,000
它的形状就是一个

83
00:04:05,360 --> 00:04:07,960
首先它是一个Tosh的一个Tensor

84
00:04:07,960 --> 00:04:09,560
然后它是一个黑白图片

85
00:04:09,560 --> 00:04:12,159
所以它的RGB的Channel数是等于1

86
00:04:12,159 --> 00:04:13,360
它就是一个Channel

87
00:04:13,960 --> 00:04:16,160
然后它的长和宽都是28

88
00:04:16,160 --> 00:04:17,759
就是一个比较小的图片了

89
00:04:20,560 --> 00:04:21,560
接下来是说

90
00:04:21,560 --> 00:04:23,680
我们这里定义了两个函数

91
00:04:23,680 --> 00:04:25,759
来就是画一下我们这个数据集

92
00:04:26,959 --> 00:04:29,079
就是说因为这个数据集

93
00:04:29,079 --> 00:04:30,279
我们是有10个类别

94
00:04:30,279 --> 00:04:32,959
它是一个关于衣服的一个数据集

95
00:04:32,959 --> 00:04:34,919
它里面标号都是什么T-shirt

96
00:04:36,239 --> 00:04:39,519
裤子 上衣 包 鞋子

97
00:04:39,519 --> 00:04:40,719
这样的标号

98
00:04:41,519 --> 00:04:43,719
然后这里另外就是我们使用

99
00:04:43,719 --> 00:04:45,919
Matte Plot Libs来画几张

100
00:04:46,719 --> 00:04:48,519
把这个图片一个一个画出来

101
00:04:48,519 --> 00:04:49,919
我们就不详细解释了

102
00:04:49,919 --> 00:04:50,919
大家可以看一下

103
00:04:53,719 --> 00:04:55,319
我们主要是看到是说

104
00:04:55,719 --> 00:04:56,919
我们之前有讲过

105
00:04:56,919 --> 00:04:59,719
我们构造了PyTorch的数据集之后

106
00:04:59,919 --> 00:05:01,919
我们可以放进一个Data Loader里面

107
00:05:02,080 --> 00:05:03,879
然后指定一个Batch Size

108
00:05:03,879 --> 00:05:06,480
就可以拿到一个大小为固定数字的

109
00:05:06,480 --> 00:05:09,080
一个批量的一个数据

110
00:05:09,080 --> 00:05:10,080
然后我们

111
00:05:10,080 --> 00:05:11,879
因为它是一个Python的Iterator

112
00:05:11,879 --> 00:05:14,080
我们用Iter 构造出一个Iterator

113
00:05:14,080 --> 00:05:16,879
然后Next就是拿到第一个小批量

114
00:05:16,879 --> 00:05:19,080
我们记为一个X和Y

115
00:05:19,480 --> 00:05:21,280
可以看到是说整个这一块

116
00:05:21,280 --> 00:05:23,879
虽然我们的数据发生了变化

117
00:05:23,879 --> 00:05:25,280
但是跟我们之前

118
00:05:25,280 --> 00:05:26,879
那个人工数据集的

119
00:05:26,879 --> 00:05:29,480
基本上读数据其实一样的

120
00:05:30,439 --> 00:05:33,640
然后我们就是把X Reshape成一个

121
00:05:33,640 --> 00:05:35,240
它是一个Batch等于18

122
00:05:35,240 --> 00:05:38,040
就是说我们把Channel数就不用了

123
00:05:38,040 --> 00:05:39,840
就是把它Reshape成一个

124
00:05:39,840 --> 00:05:41,439
Number of Examples

125
00:05:41,439 --> 00:05:43,640
然后你的宽很高

126
00:05:44,040 --> 00:05:45,640
然后我们画出来

127
00:05:45,640 --> 00:05:47,640
就是说我来画两行

128
00:05:47,640 --> 00:05:50,240
每一行有9张图片

129
00:05:50,439 --> 00:05:51,640
最后我们的Title

130
00:05:51,640 --> 00:05:53,439
就是它的每张图片的标号

131
00:05:53,439 --> 00:05:55,640
是等于我们从Label数拿出来

132
00:05:56,040 --> 00:05:57,840
因为它Y里面

133
00:05:58,000 --> 00:05:59,800
它已经是一个数值的标号了

134
00:05:59,800 --> 00:06:01,400
那么我们需要从我们的

135
00:06:01,400 --> 00:06:02,400
字符串的标号里面

136
00:06:02,400 --> 00:06:05,000
把对应的字符串拿出来

137
00:06:05,200 --> 00:06:06,600
可以看到是说

138
00:06:06,600 --> 00:06:08,400
我们画了每一张图片

139
00:06:08,400 --> 00:06:10,000
这是每张图片的样子

140
00:06:10,000 --> 00:06:11,000
它是黑白图片

141
00:06:11,000 --> 00:06:13,000
虽然这里上了一点色

142
00:06:13,400 --> 00:06:14,800
然后它是对应的标号

143
00:06:14,800 --> 00:06:16,000
在上面表示

144
00:06:16,000 --> 00:06:16,320
好

145
00:06:16,320 --> 00:06:19,200
这样子我们就能够把图片拿出来了

146
00:06:22,400 --> 00:06:25,920
然后我们怎么来读一个批量

147
00:06:25,920 --> 00:06:27,600
我们之前已经讲过了

148
00:06:27,920 --> 00:06:29,800
然后这里唯一的区别是说

149
00:06:29,800 --> 00:06:31,200
我们定了一个小函数

150
00:06:31,200 --> 00:06:33,200
它就是返回一个4

151
00:06:33,520 --> 00:06:34,920
4是什么意思

152
00:06:34,920 --> 00:06:37,800
就是每一次读一个数据

153
00:06:37,800 --> 00:06:39,000
它其实不容易

154
00:06:39,000 --> 00:06:40,200
因为它要把图片

155
00:06:40,200 --> 00:06:41,800
我们现在已经在内存里面

156
00:06:41,800 --> 00:06:42,920
一般来说

157
00:06:42,920 --> 00:06:44,600
我们图片可能放在硬盘上

158
00:06:44,600 --> 00:06:47,800
我们可能需要多个进程

159
00:06:47,800 --> 00:06:50,680
来进行数据的读取操作

160
00:06:50,680 --> 00:06:53,000
以及做一些预读取

161
00:06:53,120 --> 00:06:55,400
所以我们这里使用一个

162
00:06:55,400 --> 00:06:57,760
常为4的进程数来进行读取

163
00:06:57,760 --> 00:06:59,080
你可以选择一个

164
00:06:59,080 --> 00:07:00,040
根据你的CPU

165
00:07:00,040 --> 00:07:01,280
来选择一个小一点的数

166
00:07:01,280 --> 00:07:02,880
或者大一点的数都没关系

167
00:07:04,240 --> 00:07:06,320
所以给定两个

168
00:07:06,320 --> 00:07:07,960
给定了数据集之后

169
00:07:07,960 --> 00:07:10,360
我们知道可以使用data loader

170
00:07:10,360 --> 00:07:11,880
给定我们的batch size

171
00:07:11,880 --> 00:07:16,080
来指定我们是不是要随机打乱顺序

172
00:07:16,080 --> 00:07:18,040
如果是用训练集的话

173
00:07:18,040 --> 00:07:19,200
我们是需要随机的

174
00:07:19,200 --> 00:07:20,760
测试集就不一定了

175
00:07:21,160 --> 00:07:22,800
然后我们来告诉你说

176
00:07:22,800 --> 00:07:23,720
number of workers

177
00:07:23,720 --> 00:07:24,800
就是要多少进程

178
00:07:24,840 --> 00:07:26,319
我们现在给4

179
00:07:26,920 --> 00:07:29,000
最后我们的batch size等于256

180
00:07:29,520 --> 00:07:30,560
可以看到是说

181
00:07:30,560 --> 00:07:32,920
我们之前定义过timer这个函数

182
00:07:32,920 --> 00:07:34,840
用来测试我们的速度

183
00:07:34,840 --> 00:07:36,280
那么就是我们构造出

184
00:07:36,280 --> 00:07:37,280
iterator之后

185
00:07:37,280 --> 00:07:41,280
我们就可以用for x y in data train

186
00:07:41,879 --> 00:07:43,879
train iterator来

187
00:07:43,879 --> 00:07:46,800
一个一个访问我们所有的batch

188
00:07:47,439 --> 00:07:48,360
然后我们看一下

189
00:07:48,480 --> 00:07:50,160
我们整个扫一遍数据

190
00:07:50,160 --> 00:07:52,199
它的时间是1.72秒

191
00:07:52,759 --> 00:07:53,680
作为一个练习

192
00:07:53,680 --> 00:07:54,639
大家可以试一下

193
00:07:54,639 --> 00:07:57,040
使用不同的进程来读取数据

194
00:07:57,040 --> 00:07:58,480
看看时间的变化

195
00:07:58,840 --> 00:08:01,680
这个数据是根据你的CPU来

196
00:08:01,680 --> 00:08:03,319
性能来有不一样的区别

197
00:08:03,759 --> 00:08:05,680
我们这里需要知道的是说

198
00:08:05,680 --> 00:08:08,639
我们读一次数据的时间是1.72秒

199
00:08:08,920 --> 00:08:12,079
所以经常大家会遇到的一个性能问题

200
00:08:12,079 --> 00:08:14,800
是说很有可能你的模型训练挺快的

201
00:08:14,800 --> 00:08:16,639
但你的数据读不过来

202
00:08:16,800 --> 00:08:17,879
所以通常来说

203
00:08:17,879 --> 00:08:20,120
我们会去在训练之前

204
00:08:20,120 --> 00:08:21,519
我们会去看一下

205
00:08:21,519 --> 00:08:23,240
我们的数据读取有多快

206
00:08:23,240 --> 00:08:26,519
我们需要读的至少要比训练要快

207
00:08:27,040 --> 00:08:30,439
一般要快一些快很多是最好的

208
00:08:30,840 --> 00:08:32,840
所以这也是一个常见的性能瓶颈

209
00:08:33,120 --> 00:08:35,039
所以这也是告诉大家说

210
00:08:35,039 --> 00:08:37,159
怎么去来benchmark一下

211
00:08:37,159 --> 00:08:38,240
我们的数据读取

212
00:08:40,639 --> 00:08:43,240
最后我们把所有的之前讲过的

213
00:08:43,799 --> 00:08:47,120
这一些函数放在一个文件

214
00:08:47,159 --> 00:08:48,600
一个叫load data

215
00:08:48,720 --> 00:08:50,279
fashml的函数里面

216
00:08:50,320 --> 00:08:52,120
使得我们之后能够重用

217
00:08:52,679 --> 00:08:54,279
但我们唯一的加了一个是说

218
00:08:54,279 --> 00:08:55,600
一个resize的选项

219
00:08:55,600 --> 00:08:58,399
就是说我们需要把是不是把图片

220
00:08:58,639 --> 00:09:00,320
因为它就是28×28

221
00:09:00,320 --> 00:09:01,720
之后我们的模型

222
00:09:01,759 --> 00:09:03,840
可能需要更大一点的输入

223
00:09:03,879 --> 00:09:05,720
我们可以用通过resize

224
00:09:06,080 --> 00:09:08,440
如果我们只需要resize的话

225
00:09:08,440 --> 00:09:10,000
我们在transformer里面

226
00:09:10,279 --> 00:09:12,279
再加入一个resize的一个操作

227
00:09:12,279 --> 00:09:13,960
使得把图片变得更大一点

228
00:09:14,440 --> 00:09:16,240
奇异的就是我们之前的

229
00:09:16,519 --> 00:09:18,440
它反馈的就是两个data loader

230
00:09:18,440 --> 00:09:19,159
一个是train

231
00:09:19,159 --> 00:09:20,919
和一个是test data loader

232
00:09:21,439 --> 00:09:21,959
OK

233
00:09:21,959 --> 00:09:24,559
这就是我们的数据读取

