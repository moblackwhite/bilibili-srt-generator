1
00:00:00,000 --> 00:00:01,520
好

2
00:00:01,520 --> 00:00:02,800
第一个问题是说

3
00:00:03,120 --> 00:00:06,040
感觉SVM从理论上来讲

4
00:00:06,040 --> 00:00:07,360
应该对分类

5
00:00:08,640 --> 00:00:10,320
总体效果不错

6
00:00:10,359 --> 00:00:12,800
和神机网络比缺点在哪里

7
00:00:13,400 --> 00:00:18,879
SVM的它的一个缺点其实是

8
00:00:19,039 --> 00:00:23,679
首先SVM它是通过一个kernel来匹配

9
00:00:23,719 --> 00:00:27,199
我的模型复杂度的

10
00:00:27,560 --> 00:00:29,920
假设你是用kernel的SVM的话

11
00:00:29,920 --> 00:00:32,600
它其实算起来不容易

12
00:00:32,760 --> 00:00:36,160
就是说SVM很难做到100万个数据点

13
00:00:38,520 --> 00:00:40,760
但是对于多层感知器的话

14
00:00:40,760 --> 00:00:42,480
我们通过随机提速下降

15
00:00:42,520 --> 00:00:44,280
很容易做到100万1000万

16
00:00:44,320 --> 00:00:47,240
就是说SVM当你数据不大的话

17
00:00:47,240 --> 00:00:49,480
几万个点几千个点几万个点

18
00:00:49,480 --> 00:00:51,680
或10万个点都是可以做的

19
00:00:51,719 --> 00:00:53,280
而且是比较容易解的

20
00:00:53,320 --> 00:00:56,120
但是一大的话就挺难的

21
00:00:56,439 --> 00:00:59,439
这就是SVM的一个主要的缺点

22
00:01:00,920 --> 00:01:03,399
另外一块就是SVM的缺点

23
00:01:03,399 --> 00:01:05,879
在于是说它能调的东西不多

24
00:01:05,879 --> 00:01:09,359
就是反正就是一个很平化的一个东西

25
00:01:09,359 --> 00:01:11,560
就是说你可以调一些东西

26
00:01:11,719 --> 00:01:14,400
比如说kernel的宽度

27
00:01:14,599 --> 00:01:16,040
或者不同的kernel

28
00:01:16,040 --> 00:01:17,640
但是实际上调来调去

29
00:01:17,640 --> 00:01:20,000
大家觉得好像也没有太多效果

30
00:01:20,200 --> 00:01:21,760
这个也是它的就是说

31
00:01:21,799 --> 00:01:23,760
可调性不是很行

32
00:01:25,200 --> 00:01:27,040
或者说或者反过来讲

33
00:01:27,200 --> 00:01:29,120
就是说大家想了解它的缺点

34
00:01:29,120 --> 00:01:32,280
或者你可以了解神经网络的优点是什么

35
00:01:32,920 --> 00:01:34,439
神经网络的主要的优点

36
00:01:34,439 --> 00:01:37,840
对我从我这里这是一个很大的一个观点

37
00:01:38,120 --> 00:01:41,280
我觉得神经网络比别的领域的优点在于

38
00:01:41,280 --> 00:01:43,840
是说它是一个语言

39
00:01:45,120 --> 00:01:48,040
就是说神经网络本身是一种语言

40
00:01:49,400 --> 00:01:50,600
你通过它的语言

41
00:01:50,600 --> 00:01:52,200
那么也有一些语句

42
00:01:52,480 --> 00:01:54,240
比如说不同的layer

43
00:01:54,280 --> 00:01:56,560
它就是我的里面的一些小工具

44
00:01:56,600 --> 00:01:57,800
然后不同的连起来

45
00:01:57,800 --> 00:01:58,840
我可以是Holoop

46
00:01:59,520 --> 00:02:01,120
我可以把它一句句写出来

47
00:02:01,160 --> 00:02:05,680
就是说神经网络通过神经网络这种语言

48
00:02:05,680 --> 00:02:07,840
我们对它进行编程

49
00:02:08,319 --> 00:02:10,439
来描述我们对整个物体

50
00:02:10,439 --> 00:02:11,520
整个世界

51
00:02:11,520 --> 00:02:13,640
或整个我们要解决问题的一种理解

52
00:02:15,120 --> 00:02:16,520
这个里面很玄学

53
00:02:17,560 --> 00:02:20,040
就是说它不是一个不像编程语言一样的

54
00:02:20,040 --> 00:02:22,759
真的就是我一行一行写下来

55
00:02:22,759 --> 00:02:23,680
很有逻辑性

56
00:02:23,759 --> 00:02:27,400
就是说神经网络其实是一个

57
00:02:28,360 --> 00:02:30,200
比较不那么直观的

58
00:02:30,200 --> 00:02:33,560
但是它编程性特别好的一种框架

59
00:02:33,920 --> 00:02:35,680
我可以做很灵活的编程

60
00:02:35,920 --> 00:02:38,000
反正自动求导把我的T2求出来

61
00:02:38,400 --> 00:02:39,200
就行了

62
00:02:40,200 --> 00:02:43,319
所以你相对于说别的机器学习的模型

63
00:02:43,319 --> 00:02:44,560
SVM也好

64
00:02:44,599 --> 00:02:46,960
它有很好的数学解释

65
00:02:47,000 --> 00:02:49,400
但是它的可编程性会差很多

66
00:02:49,719 --> 00:02:52,040
就SVM能解决的问题

67
00:02:52,719 --> 00:02:54,719
它会比神经网络会少很多

68
00:02:55,319 --> 00:02:57,560
但是如果纯从分类角度来讲

69
00:02:57,560 --> 00:02:59,080
SVM确实问题不大

70
00:02:59,120 --> 00:03:01,039
除了你scale不上去之外

71
00:03:01,039 --> 00:03:03,479
就是说SVM做ImageNet就很难

72
00:03:04,199 --> 00:03:05,240
在神经网络

73
00:03:05,280 --> 00:03:07,400
它确实可以做到很大的数据集

74
00:03:08,439 --> 00:03:09,879
另外一块我们会讲到

75
00:03:09,879 --> 00:03:11,680
就是说神经网络可以通过卷器

76
00:03:11,680 --> 00:03:13,400
做比较好的特征的提取

77
00:03:13,840 --> 00:03:16,000
就是说SVM你可以把它简单的

78
00:03:16,000 --> 00:03:18,680
看成说SVM需要做特征的抽取

79
00:03:18,719 --> 00:03:20,520
和SVM本身是个分类器

80
00:03:20,639 --> 00:03:22,319
在神经网络其实是说

81
00:03:22,360 --> 00:03:23,840
特征的提取和分类

82
00:03:23,840 --> 00:03:25,360
它其实是放在一起做了

83
00:03:25,479 --> 00:03:27,439
就一起通过神经网络来进行表达

84
00:03:27,439 --> 00:03:29,520
可以做真正的原始数据集上的

85
00:03:30,319 --> 00:03:32,439
一些end to end的一些训练

86
00:03:33,240 --> 00:03:34,599
就我们不展开太多

87
00:03:34,599 --> 00:03:36,360
我们慢慢的会给大家介绍

88
00:03:36,360 --> 00:03:37,560
更复杂的神经网络

89
00:03:37,599 --> 00:03:39,159
大家会理解它跟

90
00:03:39,840 --> 00:03:41,080
别的基础学习的算法

91
00:03:41,080 --> 00:03:42,159
是什么样子的区别

92
00:03:45,319 --> 00:03:46,520
就第二个问题

93
00:03:46,520 --> 00:03:50,400
其实说我们除了权重衰退dropout

94
00:03:50,400 --> 00:03:51,199
还能不能介绍

95
00:03:51,199 --> 00:03:52,280
边我们会介绍

96
00:03:52,280 --> 00:03:53,680
边是batch normalization

97
00:03:53,680 --> 00:03:54,599
我们肯定会介绍

98
00:03:54,640 --> 00:03:57,319
模型减支我们distillation

99
00:03:57,360 --> 00:03:59,200
模型减支和distillation

100
00:03:59,200 --> 00:04:00,240
其实它你可以认为

101
00:04:00,240 --> 00:04:01,760
它不是让你做

102
00:04:03,159 --> 00:04:06,319
它其实不是真的给你做

103
00:04:06,920 --> 00:04:08,040
规约

104
00:04:08,040 --> 00:04:09,640
就不上你模型机动减少

105
00:04:09,640 --> 00:04:10,879
只是说让你得到一个

106
00:04:10,879 --> 00:04:11,680
比较小的模型

107
00:04:11,680 --> 00:04:13,120
你的部署起来好一点

108
00:04:13,159 --> 00:04:16,280
我们不会特别的讲部署

109
00:04:16,319 --> 00:04:18,680
但是也许大家感兴趣的话

110
00:04:18,680 --> 00:04:19,280
大家可以留言

111
00:04:19,280 --> 00:04:19,800
感兴趣的话

112
00:04:19,800 --> 00:04:21,319
我可以录一点别的视频

113
00:04:21,319 --> 00:04:22,399
比如说10分钟的视频

114
00:04:22,399 --> 00:04:23,399
给你介绍

115
00:04:23,800 --> 00:04:24,920
怎么样我一个模型

116
00:04:25,040 --> 00:04:26,160
怎么样把它减支

117
00:04:26,160 --> 00:04:28,319
或者distillation到一个很小的

118
00:04:28,319 --> 00:04:29,199
比如说recipe pie

119
00:04:29,639 --> 00:04:31,120
或者要在我笔记本上

120
00:04:31,120 --> 00:04:32,240
跑4K的图片

121
00:04:32,240 --> 00:04:32,759
跑事实

122
00:04:32,759 --> 00:04:34,120
这这里可以做

123
00:04:34,319 --> 00:04:36,680
但是我们课就不仔细讲了

124
00:04:36,680 --> 00:04:38,160
因为这边东西太多了

125
00:04:38,160 --> 00:04:39,920
那我可以给大家讲怎么用

126
00:04:39,920 --> 00:04:41,840
不介绍这些什么东西

127
00:04:46,079 --> 00:04:47,960
就说训练误差是training

128
00:04:47,960 --> 00:04:50,160
在training dataset上的arrow

129
00:04:50,199 --> 00:04:52,600
就泛化误差是在testing dataset

130
00:04:52,600 --> 00:04:53,600
这是没错的

131
00:04:53,600 --> 00:04:55,760
但是你这个testing一定要解决

132
00:04:55,760 --> 00:04:57,840
说我们经常说test dataset

133
00:04:57,840 --> 00:04:59,640
其实是一个validation dataset

134
00:04:59,920 --> 00:05:00,960
就所谓的validation

135
00:05:00,960 --> 00:05:02,560
我们是可以通过它来去

136
00:05:02,600 --> 00:05:04,439
调我们的超参数

137
00:05:04,560 --> 00:05:05,600
但是testing就是说

138
00:05:05,600 --> 00:05:07,240
一定是用完一次就没了

139
00:05:07,240 --> 00:05:08,360
就是我们打比赛

140
00:05:08,520 --> 00:05:09,760
打比赛有一个

141
00:05:10,480 --> 00:05:11,320
有一个

142
00:05:12,480 --> 00:05:14,520
private的leaderboard

143
00:05:14,520 --> 00:05:16,000
就是说大家不会公布成绩

144
00:05:16,000 --> 00:05:16,920
到最后的最后

145
00:05:16,920 --> 00:05:17,840
大家一次性公布

146
00:05:17,840 --> 00:05:19,480
然后就定了排行榜

147
00:05:19,560 --> 00:05:21,080
这个是test dataset

148
00:05:21,080 --> 00:05:23,320
或者是说我训练一个模型

149
00:05:23,319 --> 00:05:26,240
把它真正的部署到实际生产中

150
00:05:26,240 --> 00:05:27,719
看明天的结果怎么样

151
00:05:27,719 --> 00:05:29,040
这个是testing dataset

152
00:05:29,719 --> 00:05:32,519
所以严格意义上说泛化误差

153
00:05:32,519 --> 00:05:34,719
是说在未来还没有

154
00:05:34,719 --> 00:05:36,319
现在不在手上的

155
00:05:36,360 --> 00:05:38,000
data set上的泛化误差

156
00:05:40,279 --> 00:05:41,120
我们第4个问题

157
00:05:41,120 --> 00:05:42,600
还是说SVM和神经网络

158
00:05:42,719 --> 00:05:44,519
我们就不仔细的来讲了

159
00:05:47,199 --> 00:05:50,519
就是说我说训练测试验证

160
00:05:50,519 --> 00:05:52,639
三个数据级的划分的比例标准

161
00:05:52,639 --> 00:05:53,279
是什么

162
00:05:53,319 --> 00:05:54,480
如果是比赛的话

163
00:05:54,480 --> 00:05:56,159
不知道测试数据级的分布

164
00:05:56,159 --> 00:05:58,959
怎么设计验证级和验证数据级

165
00:05:58,959 --> 00:06:00,360
有什么指导原则吗

166
00:06:01,480 --> 00:06:03,120
我们会有一个比赛

167
00:06:03,120 --> 00:06:05,600
就是我们可能在下周就来

168
00:06:05,839 --> 00:06:06,879
就是我们

169
00:06:09,360 --> 00:06:10,759
这个比赛我可以提前说一下

170
00:06:10,759 --> 00:06:12,600
其实是一个比较好玩的东西

171
00:06:12,600 --> 00:06:15,000
是说我给大家

172
00:06:15,800 --> 00:06:18,480
去预测我们的房价的卖的

173
00:06:18,480 --> 00:06:19,680
房价的价格

174
00:06:20,360 --> 00:06:23,279
然后我是把2020年

175
00:06:23,280 --> 00:06:26,080
的房的湾区还是加州

176
00:06:26,080 --> 00:06:26,640
我都忘了

177
00:06:26,640 --> 00:06:30,240
应该是整个加州的房子给下了下来

178
00:06:30,360 --> 00:06:33,320
然后在训练级

179
00:06:33,320 --> 00:06:36,840
其实是应该是1月份到5月份的房子

180
00:06:37,320 --> 00:06:38,560
验证级的

181
00:06:38,560 --> 00:06:39,680
我们的测试级

182
00:06:39,680 --> 00:06:41,280
应该是5月份以后的房子

183
00:06:41,520 --> 00:06:43,440
然后我们一个公开的

184
00:06:43,800 --> 00:06:45,040
公开出来的验证级

185
00:06:45,040 --> 00:06:45,720
其实你可以认为

186
00:06:45,720 --> 00:06:47,880
它就是一个validation data set

187
00:06:47,880 --> 00:06:48,640
就验证级了

188
00:06:48,640 --> 00:06:52,680
就是应该是6月份到10月份

189
00:06:52,680 --> 00:06:55,240
但是私有的数据级

190
00:06:55,240 --> 00:06:57,480
也就是真正的测试级是后面的

191
00:06:58,160 --> 00:07:00,079
就它确实会有一个分布

192
00:07:00,319 --> 00:07:02,560
就是说这里面涉及到一个事情

193
00:07:02,560 --> 00:07:04,519
是说你的测试数据级

194
00:07:04,519 --> 00:07:06,639
很有可能跟你的验证数据的分布

195
00:07:06,639 --> 00:07:07,480
是不一样的

196
00:07:08,879 --> 00:07:11,439
假设我是用过去的数据来训练模型

197
00:07:11,439 --> 00:07:13,160
去预测明天的数据级的话

198
00:07:13,199 --> 00:07:15,199
很有可能这个世界会发生变化

199
00:07:15,560 --> 00:07:17,319
这是一个非常大的一个问题

200
00:07:17,360 --> 00:07:19,120
我们这里没有特别去讲

201
00:07:19,120 --> 00:07:20,000
这个问题会怎么样

202
00:07:20,000 --> 00:07:21,960
这个叫covariance shift

203
00:07:22,000 --> 00:07:23,840
就是说整个分布会发生变化

204
00:07:24,080 --> 00:07:26,400
我们这里很多时候

205
00:07:26,440 --> 00:07:27,560
你就假设数据

206
00:07:27,560 --> 00:07:29,440
是一个独立同分布的情况下

207
00:07:29,840 --> 00:07:31,280
你的验证数据级

208
00:07:31,280 --> 00:07:32,360
通常来说

209
00:07:34,360 --> 00:07:35,840
够大就行了

210
00:07:35,880 --> 00:07:38,000
就没有特别多的

211
00:07:39,720 --> 00:07:40,240
区别

212
00:07:40,240 --> 00:07:41,600
比如说经常来说

213
00:07:41,600 --> 00:07:42,840
我有一些数据级

214
00:07:42,840 --> 00:07:44,320
我可以选择30%

215
00:07:44,320 --> 00:07:46,240
作为测试数据

216
00:07:46,440 --> 00:07:48,120
70%作为训练数据

217
00:07:48,280 --> 00:07:49,960
在70%的数据级上

218
00:07:49,960 --> 00:07:51,480
做一个5折交叉验证

219
00:07:52,240 --> 00:07:54,280
就说每一次就拿20%

220
00:07:54,280 --> 00:07:55,400
作为验证数据级

221
00:07:55,400 --> 00:07:56,280
然后做5次

222
00:07:56,320 --> 00:07:57,880
这是最常用的办法

223
00:07:58,080 --> 00:07:58,720
或者是说

224
00:07:58,720 --> 00:08:00,000
如果你数据够多的话

225
00:08:00,000 --> 00:08:01,200
那么你可以砍一半

226
00:08:02,040 --> 00:08:04,200
一半作为测试数据

227
00:08:04,200 --> 00:08:05,480
一半作为训练数据

228
00:08:05,520 --> 00:08:06,440
在训练数据上

229
00:08:06,440 --> 00:08:08,040
我还是做k折交叉验证

230
00:08:08,960 --> 00:08:10,080
或者是说你说

231
00:08:10,080 --> 00:08:12,320
对于imageNet这种做法的是说

232
00:08:12,320 --> 00:08:13,720
我说我有一千类

233
00:08:14,480 --> 00:08:15,240
我一千类

234
00:08:15,240 --> 00:08:17,280
平均每一个类应该是

235
00:08:18,920 --> 00:08:19,920
5000张样本

236
00:08:19,920 --> 00:08:20,920
还是多少张样本

237
00:08:20,920 --> 00:08:21,720
我不记得了

238
00:08:21,880 --> 00:08:22,680
就他的验证

239
00:08:22,680 --> 00:08:23,600
他的测试数据

240
00:08:23,600 --> 00:08:24,480
或者验证数据

241
00:08:24,480 --> 00:08:25,640
他的做法是说

242
00:08:25,640 --> 00:08:26,520
对每个类

243
00:08:26,520 --> 00:08:28,320
我随机挑50张图片出来

244
00:08:29,080 --> 00:08:30,200
然后这样子的话

245
00:08:30,200 --> 00:08:31,080
那就是

246
00:08:31,480 --> 00:08:32,680
最后得到一个5万的

247
00:08:32,680 --> 00:08:33,720
一个1000类的话

248
00:08:33,720 --> 00:08:34,960
那就是一个5万大小的

249
00:08:34,960 --> 00:08:36,320
一个验证数据

250
00:08:36,320 --> 00:08:38,040
剩下数据全部做了测试

251
00:08:38,399 --> 00:08:40,840
这也是另外一种做法

252
00:08:44,680 --> 00:08:45,440
问题6

253
00:08:46,080 --> 00:08:47,560
不是用training set

254
00:08:47,560 --> 00:08:48,480
和testing set

255
00:08:48,480 --> 00:08:49,360
来看over fitting

256
00:08:49,360 --> 00:08:50,360
under fitting吗

257
00:08:52,399 --> 00:08:53,519
我们等会看一下

258
00:08:53,519 --> 00:08:54,399
给大家演示一下

259
00:08:54,399 --> 00:08:55,680
就是说你其实不应该

260
00:08:55,680 --> 00:08:57,440
还是不能应该叫test set

261
00:08:57,480 --> 00:08:58,840
应该叫validation set

262
00:08:58,840 --> 00:09:00,080
就是说你是用validation

263
00:09:00,080 --> 00:09:01,360
来看一下over fitting

264
00:09:05,000 --> 00:09:06,560
就是说另外一个问题是说

265
00:09:06,560 --> 00:09:08,240
如果时序上

266
00:09:10,040 --> 00:09:12,200
对时序上的数据节

267
00:09:12,440 --> 00:09:13,840
如果有

268
00:09:14,399 --> 00:09:16,360
自相关心怎么办

269
00:09:16,399 --> 00:09:16,600
好

270
00:09:16,600 --> 00:09:18,120
这是一个挺有意思的问题

271
00:09:18,120 --> 00:09:18,840
就是说

272
00:09:18,879 --> 00:09:20,840
你做股票的时候

273
00:09:21,320 --> 00:09:22,840
你的验证级

274
00:09:22,879 --> 00:09:24,160
就时序序列的话

275
00:09:24,160 --> 00:09:25,560
你要保证的是说

276
00:09:25,560 --> 00:09:26,360
你的测试级

277
00:09:26,360 --> 00:09:28,560
一定是在训练级之后的

278
00:09:29,000 --> 00:09:30,600
你不能在中间做

279
00:09:30,600 --> 00:09:31,399
那是不行的

280
00:09:31,480 --> 00:09:32,879
就是说我要做股票预测

281
00:09:32,879 --> 00:09:33,680
我不能说

282
00:09:33,720 --> 00:09:35,920
我把过去一个月的数据拿出来

283
00:09:35,960 --> 00:09:38,600
然后把中间随机采样一些点来

284
00:09:38,840 --> 00:09:39,960
终结一些天来

285
00:09:39,960 --> 00:09:41,240
作为我的验证级

286
00:09:41,280 --> 00:09:42,280
那肯定是不行的

287
00:09:42,320 --> 00:09:44,120
你唯一能干的事情是说

288
00:09:44,120 --> 00:09:45,200
过去一个月的数据

289
00:09:45,200 --> 00:09:47,160
股票数据作为训练级

290
00:09:47,200 --> 00:09:49,000
然后比如说

291
00:09:49,519 --> 00:09:50,919
把中间切一块

292
00:09:52,039 --> 00:09:53,759
前一个星期的作为验证级

293
00:09:53,759 --> 00:09:55,720
这个星期以前的作为训练级

294
00:09:57,240 --> 00:09:59,120
这是一般的时序序列的做法

295
00:10:05,039 --> 00:10:05,840
另外一个是说

296
00:10:05,840 --> 00:10:06,759
验证数据级

297
00:10:06,759 --> 00:10:08,759
和训练数据级的数据清理

298
00:10:08,799 --> 00:10:09,759
比如说异常处理

299
00:10:10,000 --> 00:10:10,879
特征构造

300
00:10:11,080 --> 00:10:12,519
是不是要放一起处理

301
00:10:17,120 --> 00:10:18,679
你应该是要

302
00:10:18,680 --> 00:10:19,480
看你怎么样

303
00:10:19,640 --> 00:10:20,320
就两种

304
00:10:20,320 --> 00:10:21,080
就是说

305
00:10:21,120 --> 00:10:22,080
最简单是说

306
00:10:22,080 --> 00:10:23,080
我要做标准化

307
00:10:23,080 --> 00:10:23,560
标准化

308
00:10:23,560 --> 00:10:23,840
就是说

309
00:10:23,840 --> 00:10:25,840
把这一数据减去它的均值

310
00:10:25,840 --> 00:10:26,920
除以它的方差

311
00:10:27,200 --> 00:10:29,240
就是说你均值和方差怎么算

312
00:10:29,680 --> 00:10:30,680
你有两种算法

313
00:10:30,720 --> 00:10:31,600
一种算法是说

314
00:10:31,600 --> 00:10:34,160
我确实把训练级和测试级

315
00:10:34,160 --> 00:10:35,440
所有的级都拿过来

316
00:10:35,440 --> 00:10:37,600
放在一起算均值和算方差

317
00:10:37,600 --> 00:10:38,560
这个也问题不大

318
00:10:38,560 --> 00:10:39,640
不大的是说

319
00:10:39,640 --> 00:10:40,880
因为你没有看到标号

320
00:10:40,880 --> 00:10:42,480
你只看到了一些的

321
00:10:42,520 --> 00:10:44,120
一些它的值

322
00:10:44,120 --> 00:10:47,120
这很有可能在实际生产中是OK的

323
00:10:47,360 --> 00:10:48,440
另外一种做法是说

324
00:10:48,440 --> 00:10:53,000
我确实是只在训练级上做算均值

325
00:10:53,000 --> 00:10:53,720
算误差

326
00:10:53,720 --> 00:10:55,800
然后把均值和方差

327
00:10:55,800 --> 00:10:57,680
作用到我的验证数据集取

328
00:10:58,480 --> 00:10:59,800
就是说一般来说

329
00:10:59,800 --> 00:11:01,760
你后者会保险一点

330
00:11:02,200 --> 00:11:03,040
实际情况下

331
00:11:03,040 --> 00:11:03,720
你可以做

332
00:11:03,720 --> 00:11:04,400
你可以看

333
00:11:04,400 --> 00:11:06,960
就当前面就会好一点点

334
00:11:07,040 --> 00:11:09,120
就是说它对分布的变化

335
00:11:09,120 --> 00:11:10,680
会更加鲁莽一点

336
00:11:11,160 --> 00:11:12,760
所以实际情况

337
00:11:12,760 --> 00:11:15,760
我觉得你应该去看你的实际的应用

338
00:11:15,880 --> 00:11:17,200
假设你在你实际

339
00:11:17,200 --> 00:11:18,560
你要训一个模型

340
00:11:18,600 --> 00:11:20,440
去部署的话

341
00:11:20,440 --> 00:11:21,160
你看一看

342
00:11:21,160 --> 00:11:24,280
你是不是能拿到验证数据上的数据

343
00:11:24,280 --> 00:11:24,800
你标号

344
00:11:24,800 --> 00:11:25,759
我假设你拿不到

345
00:11:26,520 --> 00:11:28,120
这是你看看能拿不到

346
00:11:28,120 --> 00:11:30,840
能不能拿到这些验证数据的数据

347
00:11:30,840 --> 00:11:31,720
如果你能拿到

348
00:11:31,720 --> 00:11:33,960
你就可以做同一做处理

349
00:11:33,960 --> 00:11:34,759
如果你拿不到

350
00:11:34,759 --> 00:11:36,280
那你只能用训练数据集

351
00:11:40,960 --> 00:11:41,600
嗯

352
00:11:43,480 --> 00:11:45,840
第9个问题是说深度学习

353
00:11:45,840 --> 00:11:47,320
一般训练级比较大

354
00:11:47,360 --> 00:11:49,080
所以K者交叉验证

355
00:11:49,080 --> 00:11:49,960
是不是没什么用

356
00:11:49,960 --> 00:11:50,800
训练成本太高

357
00:11:50,800 --> 00:11:53,240
这个对待是这样子

358
00:11:53,360 --> 00:11:55,519
就是说K者交叉验证

359
00:11:55,519 --> 00:11:56,560
因为要做K次

360
00:11:56,759 --> 00:11:58,280
就是说训练起来比较难

361
00:11:58,320 --> 00:12:00,720
所以在比较大的

362
00:12:01,000 --> 00:12:02,720
数据上我们很少用

363
00:12:02,800 --> 00:12:04,600
所以我的前提是说

364
00:12:04,600 --> 00:12:06,080
你做K者交叉验证

365
00:12:06,080 --> 00:12:08,879
是说一定是你的数据级不够

366
00:12:09,000 --> 00:12:10,840
就不够大的情况下你可以做

367
00:12:10,879 --> 00:12:12,879
但是在于传统基因学习

368
00:12:12,879 --> 00:12:13,920
我们一般是做的

369
00:12:13,960 --> 00:12:15,480
深度学习确实做的不多

370
00:12:15,480 --> 00:12:18,320
因为比较贵

371
00:12:23,600 --> 00:12:25,519
就为什么cross validation会好

372
00:12:25,519 --> 00:12:28,519
其实也没有解决数据来源的问题

373
00:12:28,560 --> 00:12:30,639
cross validation只是给你选择

374
00:12:30,680 --> 00:12:32,680
超参数的

375
00:12:32,680 --> 00:12:33,960
它不能解决别的问题

376
00:12:34,159 --> 00:12:35,120
就是数据来源

377
00:12:35,120 --> 00:12:36,879
当然你怎么采用数据

378
00:12:36,879 --> 00:12:38,639
使得它的分布比较好

379
00:12:38,639 --> 00:12:40,480
不要跟训练级和验证级

380
00:12:40,480 --> 00:12:42,279
两个数据长得非常不一样

381
00:12:42,320 --> 00:12:44,800
或者说你怎么采用比较好数据

382
00:12:44,800 --> 00:12:46,200
当然不是cross validation

383
00:12:46,200 --> 00:12:46,880
干的事情

384
00:12:47,200 --> 00:12:50,520
整个那一块是data science

385
00:12:50,560 --> 00:12:52,080
要去怎么样去弄数据

386
00:12:52,240 --> 00:12:53,680
就一个data scientist

387
00:12:53,720 --> 00:12:56,360
80%时间都是在搞数据

388
00:12:56,560 --> 00:12:58,240
我们也许可以讲一讲

389
00:12:58,240 --> 00:12:59,480
怎么去排发数据

390
00:12:59,640 --> 00:13:00,800
怎么样选择东西

391
00:13:00,800 --> 00:13:03,120
但这里面其实挺大一块

392
00:13:07,480 --> 00:13:09,640
就是说可以理解是说

393
00:13:09,640 --> 00:13:10,800
一共有训练数据级

394
00:13:10,800 --> 00:13:12,280
验证数据级和测试数据级

395
00:13:12,280 --> 00:13:13,120
三种数据级

396
00:13:13,840 --> 00:13:14,919
他

397
00:13:16,919 --> 00:13:17,960
就看你怎么理解

398
00:13:17,960 --> 00:13:20,240
就是说所谓的三种数据级

399
00:13:20,240 --> 00:13:22,480
是说我数据级其实是一个

400
00:13:22,480 --> 00:13:23,840
就是理论上说

401
00:13:23,879 --> 00:13:25,080
比如说我做图片分类

402
00:13:25,080 --> 00:13:26,480
我的所有的图片

403
00:13:26,600 --> 00:13:27,679
我所有的图片在一起

404
00:13:27,679 --> 00:13:28,600
比如说1万张

405
00:13:28,639 --> 00:13:30,000
那就是我一个数据级

406
00:13:30,120 --> 00:13:31,720
我会把数据级做分开

407
00:13:32,039 --> 00:13:34,200
不同的数据做不同的事情

408
00:13:38,120 --> 00:13:39,279
K是怎么确定

409
00:13:40,200 --> 00:13:42,399
K最重要的一个K的确定

410
00:13:42,399 --> 00:13:45,279
是说你在你的能承受的

411
00:13:45,319 --> 00:13:47,399
计算成本里面

412
00:13:48,079 --> 00:13:48,879
你K越大

413
00:13:48,879 --> 00:13:50,480
其实K越大效果越好了

414
00:13:50,480 --> 00:13:51,879
但是你K越大的话

415
00:13:51,879 --> 00:13:54,399
你的计算成本也是现行的增加

416
00:13:54,399 --> 00:13:55,639
所以你选一个

417
00:13:55,639 --> 00:13:58,600
你觉得还能承受的训练的

418
00:14:00,120 --> 00:14:01,199
代价就行了

419
00:14:02,879 --> 00:14:05,199
第13是说模型的参数

420
00:14:05,199 --> 00:14:07,319
和超参数不一样吗

421
00:14:07,360 --> 00:14:08,240
不一样的

422
00:14:08,399 --> 00:14:10,120
模型的参数是讲

423
00:14:10,120 --> 00:14:13,480
W和B里面的那些元素的值

424
00:14:13,600 --> 00:14:14,799
我们整个模型训练的

425
00:14:14,799 --> 00:14:16,240
要给解决的问题

426
00:14:16,399 --> 00:14:18,399
超参数就是hyperparameter

427
00:14:18,840 --> 00:14:20,879
是讲你这个模型

428
00:14:21,000 --> 00:14:22,039
我是选用

429
00:14:22,039 --> 00:14:24,120
我是不是选现行模型

430
00:14:24,159 --> 00:14:26,399
还是选我的多层感知机

431
00:14:26,440 --> 00:14:28,039
如果是多层感知机的话

432
00:14:28,039 --> 00:14:29,240
我是选多少层

433
00:14:29,279 --> 00:14:30,320
每一层有多大

434
00:14:30,360 --> 00:14:31,080
我训练的时候

435
00:14:31,080 --> 00:14:32,799
我的学习率选多少

436
00:14:32,799 --> 00:14:34,519
所有那些选择别的

437
00:14:34,560 --> 00:14:36,759
就模型就是说

438
00:14:37,720 --> 00:14:39,399
模型参数以外的

439
00:14:39,399 --> 00:14:40,199
所有东西可以

440
00:14:40,199 --> 00:14:41,319
我们可以来选的

441
00:14:41,319 --> 00:14:42,480
都是超参数

442
00:14:48,679 --> 00:14:49,879
这cross validation

443
00:14:49,879 --> 00:14:51,279
每一块训练时

444
00:14:51,279 --> 00:14:52,759
获得的最终模型参数

445
00:14:52,759 --> 00:14:53,759
可能是不同的

446
00:14:55,480 --> 00:14:56,840
应该选哪个模型

447
00:14:57,000 --> 00:14:58,639
就其实你cross validation的话

448
00:14:58,639 --> 00:15:00,679
你最后报告的是

449
00:15:00,679 --> 00:15:02,319
你的平均的精度

450
00:15:02,439 --> 00:15:04,120
但他每一块告诉你的

451
00:15:04,120 --> 00:15:04,559
不一样

452
00:15:04,559 --> 00:15:06,039
就是你取个平均

453
00:15:07,039 --> 00:15:08,720
这个在统计上是有很多意义的

454
00:15:08,759 --> 00:15:09,440
就大树定理

455
00:15:15,160 --> 00:15:16,480
简单说一下

456
00:15:16,480 --> 00:15:18,240
bias和error的区别

457
00:15:18,480 --> 00:15:19,800
其实我不是特别懂

458
00:15:19,800 --> 00:15:21,399
这句话是什么意思

459
00:15:21,759 --> 00:15:23,000
我在想

460
00:15:23,000 --> 00:15:25,240
可能你是讲统计学系里面的

461
00:15:25,279 --> 00:15:27,680
模型的bias和模型的

462
00:15:28,240 --> 00:15:29,600
或者是说

463
00:15:30,440 --> 00:15:32,360
那个叫做variation

464
00:15:33,000 --> 00:15:34,519
我们就不再解释了

465
00:15:34,680 --> 00:15:36,639
因为那一块是统计学系

466
00:15:36,680 --> 00:15:38,080
里面一大块东西

467
00:15:38,080 --> 00:15:40,120
我们也不可能一两句话能解清楚

468
00:15:45,200 --> 00:15:46,480
问题16是说

469
00:15:46,480 --> 00:15:48,320
所以是出现了over fitting

470
00:15:48,320 --> 00:15:49,360
或者under fitting

471
00:15:49,360 --> 00:15:51,600
才需要hyperparameter的tuning

472
00:15:51,800 --> 00:15:52,600
就是不是training

473
00:15:53,080 --> 00:15:54,200
他其实不是这个意思

474
00:15:54,200 --> 00:15:55,000
就是说

475
00:15:57,960 --> 00:15:59,840
就是说所谓的调参

476
00:16:00,040 --> 00:16:02,080
就是要调一个比较好的参数

477
00:16:02,080 --> 00:16:03,759
使得犯坏精度比较好

478
00:16:04,960 --> 00:16:06,240
这什么是不好的参数

479
00:16:06,680 --> 00:16:07,720
over fitting不好的

480
00:16:07,720 --> 00:16:08,920
under fitting也不好的

481
00:16:09,759 --> 00:16:11,080
所以是说over fitting

482
00:16:11,080 --> 00:16:13,000
under fitting是大概会告诉你说

483
00:16:13,000 --> 00:16:14,440
哪个参数比较好

484
00:16:16,320 --> 00:16:18,200
就是说你调总是要调的

485
00:16:18,200 --> 00:16:19,920
就是说你一般你调一调

486
00:16:19,920 --> 00:16:20,600
就是说

487
00:16:21,240 --> 00:16:22,639
但是你不多调两次

488
00:16:22,639 --> 00:16:23,960
你其实你也无法看到

489
00:16:23,960 --> 00:16:25,879
我们等会可以直接给大家讲一下

490
00:16:25,879 --> 00:16:26,519
under fitting

491
00:16:26,519 --> 00:16:28,080
over fitting到底长什么样子

492
00:16:28,440 --> 00:16:29,240
就是说

493
00:16:29,279 --> 00:16:31,120
不是说你出现了才会调

494
00:16:31,120 --> 00:16:32,840
是说这个东西告诉你说

495
00:16:32,840 --> 00:16:33,680
什么样的是好的

496
00:16:33,680 --> 00:16:34,720
什么样是不好的

497
00:16:35,519 --> 00:16:36,240
是这个意思

498
00:16:38,519 --> 00:16:42,560
如何有效的设计超参数

499
00:16:42,560 --> 00:16:43,879
是不是只能搜索

500
00:16:44,200 --> 00:16:45,759
最好的搜索是BS

501
00:16:45,759 --> 00:16:46,560
还是网格

502
00:16:46,560 --> 00:16:47,360
还是随机

503
00:16:47,399 --> 00:16:48,279
有没有推荐

504
00:16:48,480 --> 00:16:50,120
这个是一个挺好的问题

505
00:16:51,440 --> 00:16:52,360
这个是一大块

506
00:16:52,519 --> 00:16:54,160
就是AutoML里面有一个大块

507
00:16:54,160 --> 00:16:56,080
叫做hyperparameter tuning

508
00:16:56,080 --> 00:16:56,960
叫HPO

509
00:16:57,360 --> 00:16:58,120
我们

510
00:17:00,920 --> 00:17:03,120
这一块我们这一堂课

511
00:17:03,120 --> 00:17:05,319
这一节课没有去讲

512
00:17:05,319 --> 00:17:07,079
我也许可以给大家补充一下

513
00:17:07,079 --> 00:17:07,679
这一块的

514
00:17:07,679 --> 00:17:09,039
如果大家感兴趣的话

515
00:17:09,279 --> 00:17:10,720
就是说对面两件事情

516
00:17:10,720 --> 00:17:12,599
一个是说你怎么设计超参数

517
00:17:12,679 --> 00:17:13,919
就是说你到底要

518
00:17:14,199 --> 00:17:14,960
我要选

519
00:17:14,960 --> 00:17:16,720
比如说我要10个里面

520
00:17:16,759 --> 00:17:17,759
选一个好的

521
00:17:17,759 --> 00:17:19,319
那么这个10个长什么样子

522
00:17:19,519 --> 00:17:20,839
我们昨天有讲过

523
00:17:20,839 --> 00:17:22,319
我们MLP怎么设计

524
00:17:22,480 --> 00:17:23,519
怎么宽一点窄一点

525
00:17:23,519 --> 00:17:24,439
大概多少大

526
00:17:24,480 --> 00:17:25,639
就这个是设计

527
00:17:26,000 --> 00:17:27,480
第二个是说我给你10种

528
00:17:27,480 --> 00:17:29,240
或者一般来说

529
00:17:29,240 --> 00:17:32,359
我可以给你是几百或者上千种

530
00:17:33,079 --> 00:17:34,000
做组合

531
00:17:34,279 --> 00:17:35,079
我可以告诉你说

532
00:17:35,079 --> 00:17:35,639
我的学习力

533
00:17:35,640 --> 00:17:37,680
可以在0.1 0.01 0.001

534
00:17:37,680 --> 00:17:38,600
三种选择

535
00:17:38,759 --> 00:17:41,040
那么我的MLP

536
00:17:41,040 --> 00:17:43,360
我可以一层两层三层三种选项

537
00:17:43,400 --> 00:17:44,280
每一层的话

538
00:17:44,280 --> 00:17:44,880
我可以说

539
00:17:44,880 --> 00:17:47,600
32 64 18 18

540
00:17:47,600 --> 00:17:49,440
然后最后你的你是一个乘的

541
00:17:49,440 --> 00:17:51,800
就是说你是三乘以三乘以三

542
00:17:51,800 --> 00:17:52,440
一直乘下去

543
00:17:52,440 --> 00:17:54,000
你一个指数级的爆炸

544
00:17:54,160 --> 00:17:56,040
就是你很有可能设计出一个

545
00:17:56,480 --> 00:17:57,440
超参数的空间

546
00:17:57,440 --> 00:17:58,720
有一百万种可能

547
00:17:59,240 --> 00:18:01,320
那么说接下来一个问题是说

548
00:18:01,320 --> 00:18:03,360
你不可能把每一个都编了一次

549
00:18:03,360 --> 00:18:04,960
而你能编了一次就没问题了

550
00:18:04,960 --> 00:18:05,880
你不能编的话

551
00:18:05,880 --> 00:18:06,519
你怎么办

552
00:18:07,120 --> 00:18:07,880
所有的网格

553
00:18:07,880 --> 00:18:09,319
就是所有的编了一次

554
00:18:10,000 --> 00:18:10,840
随机的话

555
00:18:10,840 --> 00:18:11,600
就是我不能编

556
00:18:11,600 --> 00:18:13,600
那我随机的采样做一些东西

557
00:18:13,759 --> 00:18:15,680
我还有可能是做一个

558
00:18:15,680 --> 00:18:17,240
在上面再训练一个模型

559
00:18:18,240 --> 00:18:19,240
我的

560
00:18:20,160 --> 00:18:21,319
我的个人推荐

561
00:18:23,079 --> 00:18:25,160
第一个超参数的设计

562
00:18:25,160 --> 00:18:27,440
靠专家的

563
00:18:27,440 --> 00:18:28,840
就靠自己的经验

564
00:18:29,279 --> 00:18:30,079
就我们会说

565
00:18:30,400 --> 00:18:31,279
给定一个数据集

566
00:18:31,279 --> 00:18:33,000
我觉得哪样子的模型会比较好

567
00:18:33,000 --> 00:18:34,480
哪样子的超参数比较好

568
00:18:34,720 --> 00:18:36,319
这一般是靠自己来设定

569
00:18:36,319 --> 00:18:37,400
最好不要设太大

570
00:18:37,400 --> 00:18:38,240
也不要太小

571
00:18:38,240 --> 00:18:38,720
太大

572
00:18:38,720 --> 00:18:40,039
搜不出来太少

573
00:18:40,319 --> 00:18:42,400
你有可能错过了很多好的选择

574
00:18:42,400 --> 00:18:44,039
所以这一块目前来看

575
00:18:44,039 --> 00:18:45,240
没有特别好的选项

576
00:18:45,240 --> 00:18:46,759
只有可能自己来设计

577
00:18:47,000 --> 00:18:48,680
我们今后可能会做的好一点

578
00:18:49,160 --> 00:18:49,880
第二个是说

579
00:18:49,880 --> 00:18:52,720
如果怎么样选最好的搜索

580
00:18:52,720 --> 00:18:53,640
有两种做法

581
00:18:53,640 --> 00:18:55,480
一种是我们昨天说过的

582
00:18:55,480 --> 00:18:56,720
就是自己调

583
00:18:57,440 --> 00:18:58,440
就自己试一个

584
00:18:58,440 --> 00:18:59,200
看一看精度

585
00:18:59,200 --> 00:19:00,120
然后再试下一个

586
00:19:00,120 --> 00:19:01,599
然后根据我的上一个

587
00:19:02,080 --> 00:19:04,720
一个当前的结果来判断

588
00:19:04,720 --> 00:19:05,960
下一个往哪边走

589
00:19:06,720 --> 00:19:08,400
就有点老中医

590
00:19:08,560 --> 00:19:09,800
或者是说

591
00:19:09,800 --> 00:19:10,680
另外一个是说

592
00:19:10,680 --> 00:19:12,160
大家我建议就用随机

593
00:19:12,360 --> 00:19:13,560
如果你自己不想调的话

594
00:19:13,560 --> 00:19:14,200
就随机

595
00:19:14,320 --> 00:19:15,880
随机的意思是说

596
00:19:16,160 --> 00:19:18,640
每一次我随机的选取一个组合

597
00:19:19,160 --> 00:19:19,840
训练一次

598
00:19:19,840 --> 00:19:23,000
看一下我的验证精度

599
00:19:23,120 --> 00:19:25,200
然后随机个100次

600
00:19:25,440 --> 00:19:27,320
然后把最好的那一个超参数

601
00:19:27,320 --> 00:19:28,080
选出来就行了

602
00:19:28,080 --> 00:19:28,960
这就随机

603
00:19:29,120 --> 00:19:30,680
我推荐用随机

604
00:19:30,880 --> 00:19:32,080
Bas你也可以做

605
00:19:32,080 --> 00:19:33,000
但Bas的话

606
00:19:33,000 --> 00:19:33,799
你得肯定是

607
00:19:33,799 --> 00:19:35,320
你得训练个100次

608
00:19:35,320 --> 00:19:35,759
1000次

609
00:19:35,759 --> 00:19:36,519
1万次

610
00:19:36,519 --> 00:19:38,279
Bas的方法才会好一点

611
00:19:40,920 --> 00:19:43,400
这一块其实是很大的一个领域

612
00:19:43,560 --> 00:19:44,360
就HPO

613
00:19:45,080 --> 00:19:46,279
大家可以有兴趣

614
00:19:46,279 --> 00:19:49,120
大家可以再跟我讲一讲

615
00:19:49,120 --> 00:19:50,960
我们也许可以做一个专题

616
00:19:50,960 --> 00:19:52,160
来给大家解释一下

617
00:19:54,279 --> 00:19:55,440
第问题18

618
00:19:55,440 --> 00:19:57,200
假设我做一个二分类的问题

619
00:19:57,200 --> 00:19:59,200
实际情况是1比9的比例

620
00:19:59,240 --> 00:20:01,240
我的训练级的两个

621
00:20:02,200 --> 00:20:03,880
我的训练级的两种类型比例

622
00:20:03,880 --> 00:20:05,799
应该是1比1还是1比9

623
00:20:10,039 --> 00:20:11,600
就是说我理解你的意思

624
00:20:11,600 --> 00:20:12,920
是说你有个两分的问题

625
00:20:12,920 --> 00:20:13,920
一类是

626
00:20:14,400 --> 00:20:15,799
就是假设你有10个样本

627
00:20:15,799 --> 00:20:17,000
一类是有9个样本

628
00:20:17,000 --> 00:20:17,799
一类是一个样本

629
00:20:17,799 --> 00:20:19,000
就是非常不平衡

630
00:20:19,000 --> 00:20:20,000
那你怎么做呢

631
00:20:20,519 --> 00:20:23,680
我觉得你的验证数据级

632
00:20:23,680 --> 00:20:25,559
肯定要保证是

633
00:20:26,680 --> 00:20:27,799
其实都没关系

634
00:20:27,799 --> 00:20:31,000
就是说你有看你多少数据了

635
00:20:31,000 --> 00:20:32,159
假设你的数据很多的话

636
00:20:32,159 --> 00:20:33,799
你就是就随便了

637
00:20:33,799 --> 00:20:34,680
就随机

638
00:20:35,119 --> 00:20:37,079
就是随便看一道都可以

639
00:20:37,119 --> 00:20:39,720
假设你的数据级不那么大的话

640
00:20:39,759 --> 00:20:41,039
那么我的建议是

641
00:20:41,039 --> 00:20:43,319
你的验证数据级上

642
00:20:43,359 --> 00:20:47,359
最好是两类都有差不多样数的多

643
00:20:49,519 --> 00:20:50,720
原因是说

644
00:20:50,720 --> 00:20:52,000
假设你是

645
00:20:52,839 --> 00:20:53,519
不然的话

646
00:20:53,519 --> 00:20:54,639
你做的不好的话

647
00:20:54,639 --> 00:20:56,440
那很容易是说我就是

648
00:20:56,440 --> 00:20:57,559
那我如果是

649
00:20:57,559 --> 00:20:58,919
因为这个对分类器来讲

650
00:20:58,919 --> 00:20:59,759
我很容易是

651
00:20:59,759 --> 00:21:02,039
我就把你所有东西都给正类

652
00:21:02,039 --> 00:21:03,519
假设正类是多的那一类

653
00:21:03,559 --> 00:21:04,919
那我就所谓的分类器

654
00:21:04,960 --> 00:21:06,879
不管谁我都给你判正类

655
00:21:06,919 --> 00:21:08,279
那我的精度是90%

656
00:21:09,159 --> 00:21:11,319
那你可能就是从数据上来说

657
00:21:11,319 --> 00:21:14,119
我这个模型器90%精度挺好的

658
00:21:16,159 --> 00:21:16,839
所以

659
00:21:17,639 --> 00:21:19,559
那么你对于小的那一类

660
00:21:19,559 --> 00:21:21,559
你就会忽略掉很多事情

661
00:21:21,839 --> 00:21:23,000
所以你把验证级

662
00:21:23,000 --> 00:21:24,039
你把验证数据级

663
00:21:24,039 --> 00:21:25,079
把它平衡一下

664
00:21:25,079 --> 00:21:26,720
那么你至少是50%的精度

665
00:21:26,720 --> 00:21:27,039
对吧

666
00:21:27,039 --> 00:21:28,159
在研制数据级上

667
00:21:28,839 --> 00:21:31,200
也可以避免是说

668
00:21:31,200 --> 00:21:31,920
你的模型

669
00:21:31,920 --> 00:21:34,319
太偏好于多的那一类

670
00:21:34,599 --> 00:21:35,720
你有很多种办法

671
00:21:35,720 --> 00:21:36,839
你可以通过加权重

672
00:21:36,839 --> 00:21:37,920
来避免这个事情

673
00:21:42,359 --> 00:21:44,039
问题是就是个很好的问题

674
00:21:44,039 --> 00:21:44,759
就是说

675
00:21:45,279 --> 00:21:47,200
K折交叉验证的目的是

676
00:21:47,200 --> 00:21:49,039
确定超参数吗

677
00:21:49,119 --> 00:21:50,399
而且用这个超参数

678
00:21:50,399 --> 00:21:51,519
再训练一次

679
00:21:51,519 --> 00:21:53,079
全数据吗

680
00:21:53,240 --> 00:21:54,319
这个问题挺好的

681
00:21:54,319 --> 00:21:55,759
就是说你要两种做法

682
00:21:56,279 --> 00:21:57,640
一种做法是说

683
00:21:57,680 --> 00:22:00,160
我的K折交叉验证的

684
00:22:00,680 --> 00:22:01,920
就N种做法

685
00:22:02,039 --> 00:22:03,319
第一种是怎么做呢

686
00:22:03,599 --> 00:22:05,160
第一种就是你说的那一种

687
00:22:05,799 --> 00:22:06,920
K折交叉验证

688
00:22:06,920 --> 00:22:08,759
就是来确定一个超参数

689
00:22:09,039 --> 00:22:10,079
确定好之后

690
00:22:10,079 --> 00:22:11,519
我在整个数据体上

691
00:22:11,519 --> 00:22:13,400
再全部重新训练一次

692
00:22:15,799 --> 00:22:17,759
这个是几乎是

693
00:22:17,759 --> 00:22:19,480
你最常见的一个做法

694
00:22:20,400 --> 00:22:21,960
第二个做法是说

695
00:22:22,559 --> 00:22:24,319
我不再重新训练了

696
00:22:24,919 --> 00:22:27,639
我就把K折交叉验证中的

697
00:22:27,639 --> 00:22:30,000
那一个选定K参数

698
00:22:30,200 --> 00:22:31,879
选定好的超参数

699
00:22:31,879 --> 00:22:33,079
里面找出

700
00:22:33,119 --> 00:22:36,399
随便找一个一折里面的模型

701
00:22:36,399 --> 00:22:37,399
或者是说

702
00:22:37,439 --> 00:22:39,679
找出精度最好的

703
00:22:39,679 --> 00:22:41,240
那一折的模型拿出来

704
00:22:41,439 --> 00:22:42,919
你的代价当然是说

705
00:22:43,119 --> 00:22:44,240
你的训练的模型训练

706
00:22:44,240 --> 00:22:45,039
可以少一点点

707
00:22:45,039 --> 00:22:47,399
要你的少看了一些训练集

708
00:22:49,599 --> 00:22:51,039
第三种还有什么做法

709
00:22:51,039 --> 00:22:51,919
是怎么做呢

710
00:22:52,080 --> 00:22:55,600
就是你就是把K折交叉验证的

711
00:22:55,920 --> 00:22:57,920
K个模型全部拿下来

712
00:22:57,960 --> 00:22:59,800
然后真的做预测的时候

713
00:22:59,800 --> 00:23:01,840
你把一个测试数据集

714
00:23:02,640 --> 00:23:03,800
全部放到

715
00:23:03,920 --> 00:23:04,759
这个K个模型

716
00:23:04,759 --> 00:23:06,600
每一个都预测一次

717
00:23:06,720 --> 00:23:08,720
然后把它的预测结果去均值

718
00:23:09,600 --> 00:23:11,800
这个其实是一个不错的选择

719
00:23:12,120 --> 00:23:13,440
但是它的代价是说

720
00:23:13,440 --> 00:23:15,519
你的预测的时候

721
00:23:15,519 --> 00:23:17,640
你的代价是变成K倍了

722
00:23:17,920 --> 00:23:19,360
你之前你再过一遍

723
00:23:19,360 --> 00:23:20,800
但现在你要过K遍

724
00:23:20,799 --> 00:23:22,119
但是你的好处是说

725
00:23:22,119 --> 00:23:23,079
这样子能增加

726
00:23:23,079 --> 00:23:24,519
你的一些模型的稳定性

727
00:23:24,559 --> 00:23:25,919
因为你做了一个voting

728
00:23:30,680 --> 00:23:31,519
问题20

729
00:23:31,519 --> 00:23:33,079
validation出现了误差

730
00:23:33,079 --> 00:23:33,599
是什么误差

731
00:23:33,599 --> 00:23:34,720
就是validation误差

732
00:23:34,720 --> 00:23:36,000
就是验证误差

733
00:23:41,639 --> 00:23:42,399
问题21

734
00:23:42,399 --> 00:23:44,480
为什么SVN打败了多层感觉机

735
00:23:44,480 --> 00:23:46,960
后来深度学习又打败了SVN呢

736
00:23:49,240 --> 00:23:50,000
简单来讲

737
00:23:50,640 --> 00:23:51,920
它简单来讲

738
00:23:51,920 --> 00:23:53,440
它不是打败是流行

739
00:23:53,559 --> 00:23:55,680
就是说你会发现整个学术界

740
00:23:55,839 --> 00:23:57,200
它其实是一个

741
00:23:57,200 --> 00:23:58,480
你可以认为是一个时尚界

742
00:23:58,480 --> 00:23:59,799
大家都是赶时髦

743
00:24:00,119 --> 00:24:03,000
然后SVN打败了多层感知机

744
00:24:03,000 --> 00:24:04,079
是两个原因

745
00:24:04,079 --> 00:24:04,759
一个原因

746
00:24:04,759 --> 00:24:06,480
它确实比较简单

747
00:24:06,720 --> 00:24:08,799
但它SVN的精度

748
00:24:08,799 --> 00:24:10,640
并没有比多层感知机要好

749
00:24:10,680 --> 00:24:12,759
但它不那么要挑餐

750
00:24:12,759 --> 00:24:13,960
这是它的第一个好点

751
00:24:14,000 --> 00:24:16,079
第二个是SVN它有数学理论

752
00:24:16,920 --> 00:24:17,960
它有数学理论

753
00:24:18,000 --> 00:24:18,720
有人推

754
00:24:19,240 --> 00:24:20,240
大家就火了

755
00:24:21,440 --> 00:24:23,279
然后深度学习又打败了SVN

756
00:24:24,600 --> 00:24:26,279
深度学习打败SVN是

757
00:24:27,360 --> 00:24:28,839
深度学习说我没理论

758
00:24:28,839 --> 00:24:29,279
没理论不要紧

759
00:24:29,279 --> 00:24:31,039
我实际效果很好

760
00:24:31,039 --> 00:24:32,680
我在ImageNet上拿第一了

761
00:24:33,079 --> 00:24:35,600
我们之后会讲这个故事

762
00:24:35,799 --> 00:24:36,559
就是说SVN

763
00:24:36,559 --> 00:24:40,079
你其实在之前ImageNet的冠军

764
00:24:40,079 --> 00:24:41,079
都是用SVN的

765
00:24:41,400 --> 00:24:43,559
然后深度学习说AlexNet出来

766
00:24:43,559 --> 00:24:45,759
把SVN精度高了很多

767
00:24:46,120 --> 00:24:47,759
那就是实用性更好

768
00:24:48,559 --> 00:24:50,000
另外一块就是大家说

769
00:24:50,000 --> 00:24:51,319
也不要太纠结这个事情

770
00:24:51,319 --> 00:24:52,799
这个学术界

771
00:24:52,799 --> 00:24:55,160
就是一波又一波的

772
00:24:55,519 --> 00:24:58,480
今天我们深度学习火了几年了

773
00:24:58,640 --> 00:25:00,000
我们再讲深度学习

774
00:25:00,400 --> 00:25:01,599
可能三年之后

775
00:25:01,599 --> 00:25:01,960
五年之后

776
00:25:01,960 --> 00:25:02,879
说不定就不火了

777
00:25:03,440 --> 00:25:04,240
你可以看一看

778
00:25:04,240 --> 00:25:07,879
比如说编程语言

779
00:25:08,720 --> 00:25:11,240
每几年我们都有新的语言出来

780
00:25:11,720 --> 00:25:12,960
然后火了

781
00:25:12,960 --> 00:25:13,920
然后又不火了

782
00:25:14,359 --> 00:25:15,440
然后反过来讲

783
00:25:15,440 --> 00:25:17,359
我们所有东西都是一波又一波的

784
00:25:17,359 --> 00:25:17,640
对吧

785
00:25:17,640 --> 00:25:18,720
你所有的公司

786
00:25:18,759 --> 00:25:20,440
现在厉害的公司

787
00:25:20,440 --> 00:25:21,520
10年前都不厉害

788
00:25:22,440 --> 00:25:23,759
或者20年前都不厉害

789
00:25:23,920 --> 00:25:24,920
现在厉害的公司

790
00:25:24,920 --> 00:25:26,080
很有80%

791
00:25:26,080 --> 00:25:27,480
20年后之后也不厉害

792
00:25:27,480 --> 00:25:28,960
就是说这是一个发展的观点

793
00:25:29,240 --> 00:25:30,560
所以说我们这门课

794
00:25:30,560 --> 00:25:31,080
它

795
00:25:31,720 --> 00:25:32,759
当然一块是说

796
00:25:32,759 --> 00:25:34,040
我们讲一些实用的东西

797
00:25:34,040 --> 00:25:35,680
告诉你说现在流行什么东西

798
00:25:35,680 --> 00:25:36,759
什么东西比较重要

799
00:25:37,200 --> 00:25:37,840
另外一块

800
00:25:37,840 --> 00:25:39,160
我觉得最关心的是说

801
00:25:39,160 --> 00:25:41,160
我给大家讲一些核心的一些东西

802
00:25:41,160 --> 00:25:41,759
就是说

803
00:25:42,440 --> 00:25:44,560
一些经验

804
00:25:44,759 --> 00:25:45,440
就是说

805
00:25:45,600 --> 00:25:46,320
开车来说

806
00:25:46,319 --> 00:25:47,720
我说大家练车

807
00:25:47,720 --> 00:25:49,000
我们说来一辆车

808
00:25:49,000 --> 00:25:49,639
我们练

809
00:25:50,399 --> 00:25:52,359
当然是说你得熟悉这辆车

810
00:25:52,799 --> 00:25:53,960
但是熟悉这辆车

811
00:25:53,960 --> 00:25:55,319
你的不是你最终的目的

812
00:25:55,319 --> 00:25:57,079
最终目的是你要学会怎么开车

813
00:25:57,079 --> 00:25:58,119
换一辆新的车

814
00:25:58,519 --> 00:26:01,039
过几年新的模型出来

815
00:26:01,200 --> 00:26:03,519
就但是你核心的思路是没变的

816
00:26:03,639 --> 00:26:05,000
比如说over fitting

817
00:26:05,000 --> 00:26:06,119
under fitting不会变

818
00:26:06,200 --> 00:26:07,799
数据集怎么弄不会变

819
00:26:07,839 --> 00:26:09,159
然后你模型会变了

820
00:26:09,200 --> 00:26:10,439
或者深度学习

821
00:26:10,439 --> 00:26:12,319
深度学习可能不火了

822
00:26:12,359 --> 00:26:13,839
但是我们其实很多

823
00:26:15,039 --> 00:26:15,960
结构性的东西

824
00:26:15,960 --> 00:26:16,519
深度学习

825
00:26:16,519 --> 00:26:19,400
它没有改变机器学习结构性的东西

826
00:26:19,559 --> 00:26:20,600
我们要搞数据

827
00:26:20,640 --> 00:26:21,880
我们要训练无差

828
00:26:21,920 --> 00:26:22,840
饭盒无差

829
00:26:22,840 --> 00:26:24,559
我们要模型capacity

830
00:26:24,559 --> 00:26:26,600
我们要尽量的避免过拟核

831
00:26:26,600 --> 00:26:27,240
我们要进度

832
00:26:27,240 --> 00:26:29,400
这些东西都是在那里的

833
00:26:29,600 --> 00:26:30,279
就是说

834
00:26:30,279 --> 00:26:31,519
所以我们是说

835
00:26:31,559 --> 00:26:33,480
希望在通过学习

836
00:26:33,480 --> 00:26:35,039
现在流行的情况下

837
00:26:35,039 --> 00:26:36,120
一方面大家说

838
00:26:36,120 --> 00:26:38,360
我们能用这个东西来做实际的工作

839
00:26:38,400 --> 00:26:40,720
另外一块是理解它背后的一些知识

840
00:26:40,759 --> 00:26:42,079
这些是不变的东西

841
00:26:42,840 --> 00:26:45,160
另外一块很有可能机器学习不火了

842
00:26:45,279 --> 00:26:47,519
因为我当年学机器学习是20年前

843
00:26:47,759 --> 00:26:48,960
机器学习也不那么火

844
00:26:48,960 --> 00:26:49,920
其实也就是

845
00:26:50,480 --> 00:26:51,440
one topic而已

846
00:26:51,720 --> 00:26:52,759
我就随便挑了一个

847
00:26:52,759 --> 00:26:53,480
没想到火了

848
00:26:53,480 --> 00:26:54,440
就运气比较好

849
00:26:54,560 --> 00:26:56,279
所以你很难说机器学习

850
00:26:56,279 --> 00:26:57,640
在10年以后20年之后

851
00:26:57,640 --> 00:26:58,320
还会这么火

852
00:26:58,320 --> 00:26:59,040
有可能不火了

853
00:26:59,920 --> 00:27:01,680
所以但是反过来讲

854
00:27:01,680 --> 00:27:02,440
我觉得

855
00:27:02,800 --> 00:27:04,160
从我的角度来看

856
00:27:04,600 --> 00:27:06,120
整个功课都长得差不多

857
00:27:06,120 --> 00:27:07,200
你别说CS了

858
00:27:07,200 --> 00:27:08,160
其实我在CMU

859
00:27:08,160 --> 00:27:09,519
读PhD的时候学过很多

860
00:27:09,519 --> 00:27:10,440
那些宝藏的课

861
00:27:10,600 --> 00:27:13,279
比如说operational research

862
00:27:13,440 --> 00:27:16,680
然后各种机制怎么定义

863
00:27:16,839 --> 00:27:18,039
这种竞价怎么定义

864
00:27:18,200 --> 00:27:20,399
或者我还学过一些更奇怪的一些课

865
00:27:20,480 --> 00:27:22,160
但我觉得整个功课

866
00:27:22,160 --> 00:27:24,079
其实本质上说都差不多

867
00:27:24,399 --> 00:27:26,160
都是思路上都是差不多

868
00:27:26,160 --> 00:27:27,000
整个工程学

869
00:27:27,000 --> 00:27:28,039
engineering

870
00:27:28,279 --> 00:27:28,799
都一样

871
00:27:28,799 --> 00:27:30,519
就是说学到最后你会发现

872
00:27:30,639 --> 00:27:31,519
其实都长差不多

873
00:27:31,519 --> 00:27:35,799
你这再有一块深入进去了

874
00:27:35,799 --> 00:27:38,480
把整个它背后的那些逻辑思维能力

875
00:27:38,480 --> 00:27:39,319
得到之后

876
00:27:39,359 --> 00:27:40,599
你要转行到别的地方去

877
00:27:40,599 --> 00:27:41,960
其实也就这样

878
00:27:41,960 --> 00:27:42,960
也挺容易的

879
00:27:43,079 --> 00:27:43,759
所以就是说

880
00:27:43,759 --> 00:27:44,960
反正你得学这个东西

881
00:27:45,200 --> 00:27:46,840
为什么不学当下最流行的

882
00:27:46,840 --> 00:27:47,920
把当下最流行的

883
00:27:47,920 --> 00:27:49,319
作为一个切入点往下走

884
00:27:49,600 --> 00:27:51,279
然后学到一些背后的东西

885
00:27:51,319 --> 00:27:52,440
就是说你要学会

886
00:27:53,360 --> 00:27:54,720
世界总是会发生变化

887
00:27:54,720 --> 00:27:55,799
新的模型总会出来

888
00:27:55,799 --> 00:27:56,920
就是说你要去

889
00:27:56,960 --> 00:27:58,680
不断去学习它不变的东西

890
00:27:58,840 --> 00:28:00,079
有些东西是每年变

891
00:28:00,079 --> 00:28:01,079
有些东西是5年一变

892
00:28:01,079 --> 00:28:01,960
有些东西10年一变

893
00:28:01,960 --> 00:28:02,880
有些东西是50年

894
00:28:02,880 --> 00:28:03,840
或者100年一变

895
00:28:03,960 --> 00:28:05,160
所以你尽量的要去

896
00:28:05,200 --> 00:28:06,240
慢慢的往下走

897
00:28:07,160 --> 00:28:08,200
但你越往下走

898
00:28:08,200 --> 00:28:10,279
你会发现越接近世界的本质

899
00:28:10,279 --> 00:28:11,680
就是一些很简单的事情了

900
00:28:13,559 --> 00:28:16,120
好

901
00:28:16,120 --> 00:28:18,920
所有的验证数据上的loss

902
00:28:18,920 --> 00:28:21,360
都是这种先下降后上升的吗

903
00:28:21,759 --> 00:28:24,680
所以这个是我知道这个东西

904
00:28:24,680 --> 00:28:26,319
一定会很误解

905
00:28:26,319 --> 00:28:28,079
就是说我们有讲过那一条线

906
00:28:28,440 --> 00:28:30,319
就是说验证数据集上的

907
00:28:30,319 --> 00:28:31,559
那个验证误差是往下

908
00:28:31,559 --> 00:28:32,920
我给大家讲一下这个东西

909
00:28:33,160 --> 00:28:34,600
我觉得挺容易

910
00:28:34,600 --> 00:28:35,160
我讲的时候

911
00:28:35,160 --> 00:28:36,319
我会觉得是可能会

912
00:28:36,319 --> 00:28:37,680
大家会有一点点误解

913
00:28:38,600 --> 00:28:39,360
就这个东西

914
00:28:39,759 --> 00:28:40,559
这个东西首先

915
00:28:40,840 --> 00:28:42,920
X轴是模型

916
00:28:43,720 --> 00:28:44,240
理解吗

917
00:28:47,079 --> 00:28:48,799
就这个东西是一个模型

918
00:28:48,799 --> 00:28:51,000
就每一个点是一个新的模型

919
00:28:52,400 --> 00:28:53,480
不同的模型

920
00:28:54,279 --> 00:28:56,319
这个比如说是MLP

921
00:28:57,799 --> 00:28:59,400
这个比如说是一个MLP

922
00:28:59,400 --> 00:29:02,200
就是一个最简单的

923
00:29:02,799 --> 00:29:03,680
那个MLP

924
00:29:03,680 --> 00:29:06,120
这个可能是一个很深很深的MLP了

925
00:29:06,120 --> 00:29:07,519
这是一个简单MLP

926
00:29:07,920 --> 00:29:09,559
所以我们网上的图

927
00:29:09,559 --> 00:29:10,880
我们的计时本的图

928
00:29:10,880 --> 00:29:11,640
这个图不一样

929
00:29:11,640 --> 00:29:12,759
我们计时本的X轴

930
00:29:12,759 --> 00:29:14,720
是我的数据的迭代次数

931
00:29:14,720 --> 00:29:16,080
我是一个模型

932
00:29:17,480 --> 00:29:18,400
就是我是一个

933
00:29:18,400 --> 00:29:19,960
我们的网上都是这样子的

934
00:29:21,200 --> 00:29:23,000
我是X轴是epoch

935
00:29:23,400 --> 00:29:24,920
我数就是讲一个

936
00:29:24,920 --> 00:29:26,800
我们的误差是一个这样子的

937
00:29:27,160 --> 00:29:30,120
这是说我一个模型

938
00:29:30,160 --> 00:29:32,440
在通过不断的学习过程中

939
00:29:32,440 --> 00:29:34,120
我会发现它的误差往下降

940
00:29:35,040 --> 00:29:36,800
这一个点是表示

941
00:29:37,000 --> 00:29:38,080
我给你换一个线

942
00:29:38,960 --> 00:29:39,440
这一个点

943
00:29:39,440 --> 00:29:40,360
最终这个点

944
00:29:40,360 --> 00:29:42,680
会对应到一个这样子的点

945
00:29:43,040 --> 00:29:44,040
我还可以换一个别的

946
00:29:44,040 --> 00:29:45,000
一个这样子的模型

947
00:29:45,000 --> 00:29:45,240
对吧

948
00:29:45,240 --> 00:29:45,880
换一个点

949
00:29:46,080 --> 00:29:46,920
对到这个地方

950
00:29:46,920 --> 00:29:49,000
就是每一个点是不同的模型

951
00:29:49,000 --> 00:29:50,200
所以你就是

952
00:29:50,880 --> 00:29:52,440
所以这是说你不同的模型

953
00:29:52,440 --> 00:29:53,640
它会有不同的区别

954
00:29:53,640 --> 00:29:54,920
是比较模型用的

955
00:29:54,960 --> 00:29:56,720
它不是一个模型的训练的

956
00:29:56,720 --> 00:29:57,280
那个

957
00:29:57,360 --> 00:29:58,280
紧度的误差

958
00:29:59,280 --> 00:29:59,760
OK

959
00:30:03,120 --> 00:30:05,000
模型的容量一般指的是什么

960
00:30:05,000 --> 00:30:05,680
还模型容量

961
00:30:05,680 --> 00:30:06,280
就是

962
00:30:06,800 --> 00:30:09,280
就是模型能够拟合还是的能力了

963
00:30:10,880 --> 00:30:16,320
随机森林在深度学习有常见的用吗

964
00:30:17,720 --> 00:30:20,520
深度学习有一些做随机森林的东西

965
00:30:20,520 --> 00:30:21,080
但是

966
00:30:21,640 --> 00:30:22,880
它不属于

967
00:30:22,920 --> 00:30:23,680
深度学习

968
00:30:23,680 --> 00:30:24,800
我们一般来说

969
00:30:24,800 --> 00:30:26,760
特指神经网络这一块

970
00:30:26,800 --> 00:30:28,920
确实是有把神随机森林

971
00:30:28,920 --> 00:30:30,440
做到神经网络里面

972
00:30:30,440 --> 00:30:31,640
但是它的最大的bug

973
00:30:31,640 --> 00:30:32,880
说随机森林的训练

974
00:30:32,880 --> 00:30:34,400
它不是通过T2下降的

975
00:30:34,880 --> 00:30:36,640
所以你不好做join的training

976
00:30:37,000 --> 00:30:39,240
所以我们用的也有用

977
00:30:39,240 --> 00:30:41,720
就是说一般来说常见的应用是

978
00:30:42,359 --> 00:30:43,000
做example

979
00:30:43,000 --> 00:30:45,039
我给一个数据

980
00:30:45,039 --> 00:30:47,359
我训练一个随机森林的模型

981
00:30:47,400 --> 00:30:48,799
我再训练一个别的模型

982
00:30:48,839 --> 00:30:50,400
我再训练一个深度学习模型

983
00:30:50,440 --> 00:30:53,559
n个模型最后做做average来投票

984
00:30:53,839 --> 00:30:55,200
这是常见的做法

985
00:30:55,240 --> 00:30:57,440
但是把随机森林结合进深度学习

986
00:30:57,440 --> 00:30:58,240
做的比较少

987
00:30:58,279 --> 00:31:00,720
主要是你T2不好传

988
00:31:04,720 --> 00:31:07,559
做k则交叉验证的时候

989
00:31:07,559 --> 00:31:08,480
会训练k次

990
00:31:08,480 --> 00:31:10,160
这样子k次训练出来的模型

991
00:31:10,160 --> 00:31:11,200
能不能融合在一起

992
00:31:11,200 --> 00:31:13,360
会不会比单个模型有更好的表达能力

993
00:31:13,960 --> 00:31:15,839
有的我们刚刚有讲过

994
00:31:18,400 --> 00:31:19,519
就是说你有k个

995
00:31:19,519 --> 00:31:21,640
然后做测试的时候

996
00:31:21,680 --> 00:31:23,360
我进k个里面

997
00:31:23,360 --> 00:31:24,799
就是5个里面进5个模型

998
00:31:24,799 --> 00:31:27,440
然后把所有的预测的结果做平均

999
00:31:27,480 --> 00:31:28,640
这样子我的结果

1000
00:31:28,640 --> 00:31:30,400
可能很有可能会好一点

1001
00:31:32,519 --> 00:31:34,920
还有更奇葩的做法

1002
00:31:34,920 --> 00:31:37,759
是说其实Google他们经常干的事情

1003
00:31:37,799 --> 00:31:38,839
为了打比赛

1004
00:31:38,839 --> 00:31:40,119
我同样一个模型

1005
00:31:40,160 --> 00:31:43,559
然后记不记得我们是我们模型是有

1006
00:31:43,680 --> 00:31:46,440
我们的权重的初始是随机权重的

1007
00:31:46,640 --> 00:31:48,599
那么我就把模型训练5遍

1008
00:31:48,759 --> 00:31:51,960
就每一次用不同的随机值来初始化模型

1009
00:31:52,000 --> 00:31:52,960
最后得到5个模型

1010
00:31:52,960 --> 00:31:55,160
最后做in jumble就是做average

1011
00:31:55,200 --> 00:31:56,519
效果也挺好的

1012
00:31:58,799 --> 00:32:00,400
标号是什么

1013
00:32:00,559 --> 00:32:02,519
标注标号的标注是一个叫label

1014
00:32:02,519 --> 00:32:05,839
我经常我中文其实我也不是那么清楚

1015
00:32:05,839 --> 00:32:07,119
反正我也经常混着用

1016
00:32:07,200 --> 00:32:08,160
标号标注

1017
00:32:08,160 --> 00:32:09,160
label

1018
00:32:09,160 --> 00:32:11,200
就是一个东西

1019
00:32:12,440 --> 00:32:14,200
我们有无限维的算法是什么

1020
00:32:14,200 --> 00:32:15,840
无限维的算法多了去了

1021
00:32:15,840 --> 00:32:22,280
无限维其实正常的深度学习的模型

1022
00:32:22,280 --> 00:32:25,520
都是都有可能是无限维的

1023
00:32:25,840 --> 00:32:29,000
就是说你如果不做

1024
00:32:29,000 --> 00:32:30,360
就是说不做它的限制

1025
00:32:30,360 --> 00:32:31,400
不做比如犯话

1026
00:32:31,400 --> 00:32:33,280
不做政策化

1027
00:32:33,400 --> 00:32:34,120
不做那些东西

1028
00:32:34,120 --> 00:32:35,320
很有可能就是无限维的

1029
00:32:37,480 --> 00:32:42,320
k者叫他训练出了k个模型

1030
00:32:42,320 --> 00:32:45,120
最后选择证证误差最小的模型

1031
00:32:45,120 --> 00:32:46,600
你也可以这么做

1032
00:32:46,800 --> 00:32:47,920
就是说我们有提过

1033
00:32:47,960 --> 00:32:49,400
你可以选择物色最小的

1034
00:32:49,400 --> 00:32:52,280
或者是说你在整个数据上重新训练一遍

1035
00:32:52,320 --> 00:32:53,920
或者每个都做好都可以

1036
00:32:59,920 --> 00:33:02,640
就VC衡量的好坏没有听懂

1037
00:33:03,000 --> 00:33:06,040
这个东西我们不特别展开了

1038
00:33:06,039 --> 00:33:08,240
就是说VC维就尽量的

1039
00:33:08,279 --> 00:33:10,639
简单认为是说我一个模型

1040
00:33:10,799 --> 00:33:13,720
我能记住的最大的数据级长什么样子

1041
00:33:15,119 --> 00:33:16,960
就是说我给你一个

1042
00:33:17,000 --> 00:33:19,319
比如说100个样本数据级

1043
00:33:19,319 --> 00:33:20,879
每个数据级的样本是

1044
00:33:21,039 --> 00:33:22,359
1000维的话

1045
00:33:22,480 --> 00:33:25,200
那么假设我能记住这个数据级

1046
00:33:25,359 --> 00:33:28,399
而且不管你数据怎么

1047
00:33:28,399 --> 00:33:29,480
你的结果是什么样子

1048
00:33:29,480 --> 00:33:30,279
不管是什么样子

1049
00:33:30,279 --> 00:33:31,319
我都能记住它

1050
00:33:31,359 --> 00:33:32,879
那我就VC dimension就等于

1051
00:33:32,879 --> 00:33:34,200
至少是大于等于100

1052
00:33:35,160 --> 00:33:38,440
就是说你就是说你判断一个模型的

1053
00:33:38,480 --> 00:33:40,039
compass的大小的

1054
00:33:40,039 --> 00:33:42,559
就是说我能记住多复杂的数据级

1055
00:33:42,799 --> 00:33:44,360
就一个人的记忆能力的好坏

1056
00:33:44,360 --> 00:33:45,279
我能记住

1057
00:33:45,319 --> 00:33:46,799
比如说我记

1058
00:33:47,120 --> 00:33:49,720
举个不那么确定的例子

1059
00:33:50,360 --> 00:33:52,080
就我判断一个人的记忆力的好坏

1060
00:33:52,080 --> 00:33:53,799
那就解释你能记住

1061
00:33:53,799 --> 00:33:55,680
比如说云州绿能记100位

1062
00:33:55,680 --> 00:33:56,600
还是记10位

1063
00:33:56,640 --> 00:33:57,759
假设你记100位

1064
00:33:57,759 --> 00:33:59,080
那你的VC dimension是100

1065
00:33:59,080 --> 00:34:00,680
假设你能记10位的话

1066
00:34:00,880 --> 00:34:01,559
只能记10位

1067
00:34:01,559 --> 00:34:02,319
那就是10

1068
00:34:02,319 --> 00:34:04,879
那么当然记100的记忆力比你记10的好一点

1069
00:34:05,159 --> 00:34:06,960
或者是说你记单词

1070
00:34:07,079 --> 00:34:08,319
你最多能记多少个

1071
00:34:08,319 --> 00:34:09,239
记1万个

1072
00:34:09,519 --> 00:34:11,119
那么就VC dimension 1

1073
00:34:11,119 --> 00:34:12,719
那比只能记100个人的话

1074
00:34:12,719 --> 00:34:14,119
你的记忆当然会好一点

1075
00:34:14,199 --> 00:34:15,799
就可以简单这么理解

1076
00:34:15,799 --> 00:34:17,679
OK

1077
00:34:18,799 --> 00:34:19,960
我们在这里

1078
00:34:20,000 --> 00:34:22,199
我觉得这个就是一个概念上的东西

1079
00:34:22,199 --> 00:34:23,039
我们确实

1080
00:34:23,079 --> 00:34:25,679
多花点时间搞清楚是没问题的

1081
00:34:26,559 --> 00:34:28,159
来

1082
00:34:32,519 --> 00:34:33,680
还有那么多

1083
00:34:37,320 --> 00:34:37,680
OK

1084
00:34:37,680 --> 00:34:38,400
我刷新了一下

1085
00:34:38,400 --> 00:34:39,280
大家又出来了

1086
00:34:39,280 --> 00:34:42,600
K者加上验证是第一次

1087
00:34:42,600 --> 00:34:44,559
分完后就确定分组了吗

1088
00:34:44,600 --> 00:34:46,240
如果每次都随机打乱数据

1089
00:34:46,240 --> 00:34:49,000
取出KN分机做验证

1090
00:34:49,039 --> 00:34:50,160
是另外一种方式

1091
00:34:50,280 --> 00:34:51,840
有没有区别

1092
00:34:51,960 --> 00:34:53,240
还是说一般来说都差不多

1093
00:34:55,760 --> 00:34:56,559
就是说

1094
00:34:56,920 --> 00:34:57,600
一般来说

1095
00:34:57,600 --> 00:35:00,240
我们是做K者的话

1096
00:35:00,240 --> 00:35:01,760
一般就是给一个样本

1097
00:35:01,760 --> 00:35:03,440
随机打乱一次就把它切好了

1098
00:35:03,440 --> 00:35:04,880
就不会下次不再切了

1099
00:35:05,120 --> 00:35:06,640
你可以说

1100
00:35:06,680 --> 00:35:08,120
你可以随机

1101
00:35:08,120 --> 00:35:09,800
你下一次可以随机打乱

1102
00:35:09,800 --> 00:35:10,680
这个叫backing

1103
00:35:11,040 --> 00:35:14,120
它其实跟K者加上验证是有

1104
00:35:14,880 --> 00:35:16,040
一点不一样的地方

1105
00:35:16,080 --> 00:35:17,280
确实你也可以随机打乱

1106
00:35:17,280 --> 00:35:17,840
这个没关系

1107
00:35:17,840 --> 00:35:19,160
其实你做backing就是了

1108
00:35:19,160 --> 00:35:20,320
做backing的

1109
00:35:20,320 --> 00:35:22,760
大家一般做backing的这种

1110
00:35:23,120 --> 00:35:24,240
做法是说

1111
00:35:24,280 --> 00:35:25,440
我就是为了真的

1112
00:35:25,440 --> 00:35:27,760
最后就是为了得到K个模型

1113
00:35:27,760 --> 00:35:30,040
这样子我做平均预测

1114
00:35:30,039 --> 00:35:31,840
这是大家常见的一种做法

1115
00:35:35,360 --> 00:35:37,039
就是说神经网络是一种语言

1116
00:35:37,039 --> 00:35:38,400
它是利用神经网络

1117
00:35:38,400 --> 00:35:39,920
对万事万物见为

1118
00:35:40,679 --> 00:35:42,880
它理论能力和所有的函数

1119
00:35:43,199 --> 00:35:45,840
其实我还不仅仅是讲这个东西

1120
00:35:46,119 --> 00:35:47,320
我其实讲的

1121
00:35:51,320 --> 00:35:52,719
我其实是说

1122
00:35:53,039 --> 00:35:54,119
理论上来说

1123
00:35:54,119 --> 00:35:57,599
你的单层单影行层的MLP能力

1124
00:35:57,599 --> 00:35:59,199
和所有的函数

1125
00:35:59,719 --> 00:36:00,599
理论上

1126
00:36:00,679 --> 00:36:02,279
他能力和所有不需要

1127
00:36:02,480 --> 00:36:04,599
我们就基本上昨天就讲完了

1128
00:36:04,599 --> 00:36:05,279
理论上

1129
00:36:05,799 --> 00:36:06,839
实际上不是的

1130
00:36:06,839 --> 00:36:08,879
实际上你训练不出来

1131
00:36:09,359 --> 00:36:10,079
就是说

1132
00:36:11,119 --> 00:36:12,319
就是说我觉得就是说

1133
00:36:12,319 --> 00:36:12,839
等于是说

1134
00:36:12,879 --> 00:36:14,599
我觉得我能做到这个事情

1135
00:36:14,599 --> 00:36:15,119
我觉得我

1136
00:36:15,159 --> 00:36:17,279
但是我实际上我就做不到

1137
00:36:17,319 --> 00:36:18,000
成绩做不到

1138
00:36:18,000 --> 00:36:18,759
对吧

1139
00:36:19,239 --> 00:36:20,119
所以说

1140
00:36:20,279 --> 00:36:22,559
所有的神经网络

1141
00:36:22,559 --> 00:36:24,679
最后的最后你去CNN也好

1142
00:36:24,679 --> 00:36:25,439
RNN也好

1143
00:36:25,439 --> 00:36:26,639
什么东西都也好

1144
00:36:26,679 --> 00:36:28,039
他其实是说

1145
00:36:28,360 --> 00:36:30,320
我知道MLP能力和你

1146
00:36:30,320 --> 00:36:32,199
但是MLP

1147
00:36:32,320 --> 00:36:33,559
基本上训练不出来

1148
00:36:33,719 --> 00:36:36,440
我要做一个比较好的结构

1149
00:36:36,480 --> 00:36:39,199
使得尽量帮助你的训练

1150
00:36:40,440 --> 00:36:42,079
比如说CNN尽量的说

1151
00:36:42,079 --> 00:36:42,960
帮助神经网络

1152
00:36:42,960 --> 00:36:43,639
OK

1153
00:36:43,719 --> 00:36:45,519
CNN他本质上就是一个MLP

1154
00:36:45,519 --> 00:36:46,360
本质上没区别

1155
00:36:46,360 --> 00:36:47,079
我给大家讲

1156
00:36:47,079 --> 00:36:48,239
他是做了一些限制

1157
00:36:48,239 --> 00:36:49,880
就把一些等于是一些位

1158
00:36:49,880 --> 00:36:52,320
给你固定住了

1159
00:36:53,119 --> 00:36:55,119
就是说我通过设计CNN

1160
00:36:55,119 --> 00:36:56,159
告诉你神经网络说

1161
00:36:56,559 --> 00:36:58,920
我觉得这个数据有空间信息

1162
00:36:58,960 --> 00:37:00,920
这样子我来告诉你说

1163
00:37:01,119 --> 00:37:02,519
你去这样子去

1164
00:37:02,639 --> 00:37:04,480
去处理空间信息

1165
00:37:05,039 --> 00:37:06,559
就RNN也是一样的

1166
00:37:06,719 --> 00:37:08,960
我觉得这个数据有时序信息

1167
00:37:09,039 --> 00:37:09,719
我告诉你说

1168
00:37:09,839 --> 00:37:11,480
这个东西这么走

1169
00:37:11,480 --> 00:37:13,399
来做时序的信息

1170
00:37:13,519 --> 00:37:14,319
就是说

1171
00:37:15,440 --> 00:37:16,319
整个东西就是说

1172
00:37:16,319 --> 00:37:17,480
整个深度神经网络

1173
00:37:17,480 --> 00:37:17,759
就是说

1174
00:37:17,759 --> 00:37:19,559
我是通过神经网络

1175
00:37:19,599 --> 00:37:22,119
尽量的去用他的方法

1176
00:37:22,119 --> 00:37:24,519
来去描述这个

1177
00:37:24,559 --> 00:37:27,239
这个数据的特性

1178
00:37:27,359 --> 00:37:28,519
使得你训练起来

1179
00:37:28,519 --> 00:37:29,639
更好训练一些

1180
00:37:30,519 --> 00:37:31,920
就是你可以认为是这个意思

1181
00:37:32,000 --> 00:37:32,839
就是说所谓的

1182
00:37:32,839 --> 00:37:34,000
就是说总结来讲

1183
00:37:34,000 --> 00:37:34,719
就是说

1184
00:37:34,759 --> 00:37:36,199
我通过神经网络来

1185
00:37:36,799 --> 00:37:39,199
来描述我对这个问题的理解

1186
00:37:42,719 --> 00:37:44,480
但很多时候你就是试一下

1187
00:37:44,519 --> 00:37:46,239
很多时候其实说白了

1188
00:37:46,239 --> 00:37:48,159
就是你拍拍脑袋

1189
00:37:48,199 --> 00:37:49,599
拍5个脑袋试一下

1190
00:37:50,199 --> 00:37:52,239
有发现一个想法不错

1191
00:37:52,279 --> 00:37:53,119
把它写出来

1192
00:37:53,119 --> 00:37:56,319
然后在上面再随便找个理由

1193
00:37:57,199 --> 00:37:58,519
但是真正的好的

1194
00:37:58,519 --> 00:37:59,480
就是你会发现很多

1195
00:37:59,480 --> 00:38:01,279
这样子的经典的论文

1196
00:38:01,279 --> 00:38:03,799
它确实效果很好

1197
00:38:03,799 --> 00:38:05,759
但是他一开始找的理由都是错的

1198
00:38:05,759 --> 00:38:06,639
就是说

1199
00:38:06,839 --> 00:38:07,759
所以说

1200
00:38:09,119 --> 00:38:10,920
所以神经网络很多时候

1201
00:38:11,639 --> 00:38:12,279
扯远一点

1202
00:38:12,279 --> 00:38:14,079
就是说我觉得这个世界有三种东西

1203
00:38:14,079 --> 00:38:15,199
一个是叫艺术

1204
00:38:15,319 --> 00:38:16,239
一个叫工程

1205
00:38:16,239 --> 00:38:17,239
一个叫科学

1206
00:38:17,360 --> 00:38:20,200
就是艺术是说

1207
00:38:20,240 --> 00:38:21,160
其实我都不知道

1208
00:38:21,160 --> 00:38:22,240
我都不知道怎么

1209
00:38:22,240 --> 00:38:23,440
就是我做了一个事情

1210
00:38:23,440 --> 00:38:25,080
我其实都不好解释这个东西

1211
00:38:25,080 --> 00:38:27,000
为什么应该长这样子

1212
00:38:27,000 --> 00:38:27,960
而不应该长这样子

1213
00:38:27,960 --> 00:38:29,280
我觉得就应该长这样

1214
00:38:29,280 --> 00:38:30,520
我觉得这样子比较好看

1215
00:38:30,760 --> 00:38:31,480
这是艺术

1216
00:38:31,680 --> 00:38:34,400
工程是说我确实还是能够很

1217
00:38:34,760 --> 00:38:36,200
很能够描述说

1218
00:38:36,200 --> 00:38:37,320
应该这么走

1219
00:38:37,320 --> 00:38:37,680
这么走

1220
00:38:37,680 --> 00:38:39,520
就是说我的CS是个工程

1221
00:38:39,640 --> 00:38:41,320
因为我做的东西

1222
00:38:41,320 --> 00:38:42,840
都可以通过实际来验证

1223
00:38:42,840 --> 00:38:45,360
我做我照亮车也是个工程

1224
00:38:45,440 --> 00:38:46,320
就反正每个东西

1225
00:38:46,320 --> 00:38:47,920
我都可以通过定理来描述

1226
00:38:48,360 --> 00:38:50,800
科学是说我去理解为什么

1227
00:38:51,680 --> 00:38:52,519
所以神经网络

1228
00:38:53,039 --> 00:38:54,280
我们希望它是科学

1229
00:38:54,280 --> 00:38:55,720
但实际上它是一个工程

1230
00:38:56,760 --> 00:38:57,640
反过来讲

1231
00:38:57,640 --> 00:38:59,440
它里面有50%是一个艺术

1232
00:38:59,920 --> 00:39:01,440
至少一开始是个艺术

1233
00:39:01,840 --> 00:39:03,840
一开始是说你去把这个东西

1234
00:39:03,840 --> 00:39:05,120
我们设计一个网络

1235
00:39:05,240 --> 00:39:06,760
发现我这么设计

1236
00:39:06,760 --> 00:39:07,640
觉得很好

1237
00:39:07,920 --> 00:39:09,880
设计完出来之后

1238
00:39:10,039 --> 00:39:10,680
确实很好

1239
00:39:10,680 --> 00:39:11,559
大家都说好

1240
00:39:11,559 --> 00:39:12,559
但是我一开始

1241
00:39:12,559 --> 00:39:14,280
我要卖我这个想法

1242
00:39:14,280 --> 00:39:15,160
我得给解释一下

1243
00:39:15,160 --> 00:39:15,600
为什么好

1244
00:39:15,600 --> 00:39:15,840
对吧

1245
00:39:15,840 --> 00:39:16,519
你写个论文

1246
00:39:16,519 --> 00:39:17,320
你不解释为什么

1247
00:39:17,320 --> 00:39:18,120
是肯定中不了的

1248
00:39:19,200 --> 00:39:20,720
所以你得解释为什么

1249
00:39:20,920 --> 00:39:22,079
你得想个理由

1250
00:39:22,200 --> 00:39:23,840
很有可能你想的是不对的

1251
00:39:24,519 --> 00:39:26,600
就是说你发现了一个

1252
00:39:26,640 --> 00:39:27,840
很work的东西

1253
00:39:27,960 --> 00:39:29,600
但是你觉得它work的理由

1254
00:39:29,600 --> 00:39:30,720
很有可能是不对的

1255
00:39:31,559 --> 00:39:32,600
所以是说

1256
00:39:32,640 --> 00:39:33,920
很多时候这就是一个艺术

1257
00:39:33,920 --> 00:39:34,400
对吧

1258
00:39:35,079 --> 00:39:37,840
但是一般只要你真的work

1259
00:39:38,120 --> 00:39:40,120
总会有人去慢慢的把你找出来

1260
00:39:40,120 --> 00:39:41,000
为什么work

1261
00:39:41,240 --> 00:39:42,559
那就是engineer的问题

1262
00:39:42,559 --> 00:39:43,760
可能再过很多年

1263
00:39:44,160 --> 00:39:46,400
最简单是说

1264
00:39:46,400 --> 00:39:47,920
蒸汽机发明的时候

1265
00:39:48,080 --> 00:39:49,120
过了100年之后

1266
00:39:49,120 --> 00:39:50,240
才有热力学出现

1267
00:39:50,240 --> 00:39:52,120
就是说蒸汽机先出来

1268
00:39:52,200 --> 00:39:54,040
然后物理的那些坏东西

1269
00:39:54,040 --> 00:39:55,440
是100年以后才出来的

1270
00:39:55,440 --> 00:39:56,960
所以我们希望神经网络

1271
00:39:57,040 --> 00:39:59,320
目前我们知道这些东西挺好的

1272
00:39:59,360 --> 00:40:00,040
挺工作的

1273
00:40:00,040 --> 00:40:01,520
但是不是很知道为什么

1274
00:40:02,440 --> 00:40:03,440
在我们在研究

1275
00:40:03,440 --> 00:40:05,400
但是肯定有一个滞后性

1276
00:40:05,400 --> 00:40:07,120
希望10年或者20年

1277
00:40:07,120 --> 00:40:08,040
或者50年

1278
00:40:08,200 --> 00:40:08,640
对吧

1279
00:40:14,120 --> 00:40:14,480
嗯

1280
00:40:16,240 --> 00:40:17,840
就是说剪质和蒸馏

1281
00:40:17,840 --> 00:40:19,480
是可以提高模型性能

1282
00:40:19,480 --> 00:40:22,240
是看你怎么说

1283
00:40:22,240 --> 00:40:23,440
比如说蒸馏

1284
00:40:23,440 --> 00:40:25,880
就是把一个复杂的网络

1285
00:40:25,880 --> 00:40:26,760
把它变小

1286
00:40:26,880 --> 00:40:28,440
使得它的人力

1287
00:40:28,440 --> 00:40:30,000
跟复杂的网络尽量一样

1288
00:40:30,200 --> 00:40:31,520
但你看你怎么说

1289
00:40:31,520 --> 00:40:32,920
就是说它比

1290
00:40:32,960 --> 00:40:34,040
在你小模型

1291
00:40:34,040 --> 00:40:35,720
它比同样另外一个小模型

1292
00:40:35,720 --> 00:40:37,000
直接训练出来的小模型

1293
00:40:37,000 --> 00:40:38,080
可能会效果好

1294
00:40:38,080 --> 00:40:39,880
是提升精度

1295
00:40:39,920 --> 00:40:41,600
但很重要对复杂

1296
00:40:41,639 --> 00:40:42,880
对你大的

1297
00:40:43,119 --> 00:40:44,519
开始的复杂模型来说

1298
00:40:44,519 --> 00:40:45,119
你的小模型

1299
00:40:45,119 --> 00:40:46,519
可能精度还会低一点

1300
00:40:46,599 --> 00:40:47,559
所以是说

1301
00:40:47,599 --> 00:40:49,239
你看你从哪个角度来讲

1302
00:40:49,239 --> 00:40:50,159
是性能提升

1303
00:40:54,480 --> 00:40:56,279
就同样模型的结构

1304
00:40:56,279 --> 00:40:57,639
同样的训练机

1305
00:40:57,639 --> 00:40:59,839
为什么只是随机初始化不一样

1306
00:40:59,839 --> 00:41:02,000
最后的集成一定会很好

1307
00:41:02,839 --> 00:41:03,759
是因为

1308
00:41:04,239 --> 00:41:05,679
就是说这里要涉及到

1309
00:41:05,679 --> 00:41:06,559
另外一个概念

1310
00:41:06,559 --> 00:41:07,440
一个很大的概念

1311
00:41:07,440 --> 00:41:08,400
一个叫统计学

1312
00:41:08,400 --> 00:41:10,000
一个叫做优化

1313
00:41:10,280 --> 00:41:11,840
就是说我的模型

1314
00:41:11,840 --> 00:41:13,159
是一个统计学的模型

1315
00:41:14,000 --> 00:41:16,760
我的优化是一个数值优化

1316
00:41:16,840 --> 00:41:18,880
所以你最后的模型是统计模型

1317
00:41:18,880 --> 00:41:19,960
就是模型的定义

1318
00:41:19,960 --> 00:41:21,599
加上你怎么优化的结果

1319
00:41:22,360 --> 00:41:23,679
就假设模型一样

1320
00:41:23,719 --> 00:41:25,719
它就统计模型是一样的情况下

1321
00:41:25,719 --> 00:41:27,320
通过随机初始化不一样

1322
00:41:27,320 --> 00:41:28,840
最后得到的结果不一样

1323
00:41:29,039 --> 00:41:30,199
就是说我随机初

1324
00:41:30,199 --> 00:41:31,119
就是优化

1325
00:41:31,119 --> 00:41:33,000
就是反正我就是从一个随机点

1326
00:41:33,000 --> 00:41:34,519
开始往前走一走

1327
00:41:34,679 --> 00:41:36,719
然后你这个平面够复杂的话

1328
00:41:36,800 --> 00:41:38,679
就是说你在一个很复杂的山里面

1329
00:41:38,679 --> 00:41:40,319
我把你随机丢在一些地方

1330
00:41:40,519 --> 00:41:41,599
你每次随机走一走

1331
00:41:41,599 --> 00:41:42,919
可能走的地方都不一样

1332
00:41:43,919 --> 00:41:44,440
对吧

1333
00:41:44,599 --> 00:41:47,319
但是说所以你

1334
00:41:47,759 --> 00:41:49,359
你这样子不一样的话

1335
00:41:50,039 --> 00:41:52,279
最后的集成都一定会好

1336
00:41:52,279 --> 00:41:53,519
就一般来说

1337
00:41:53,519 --> 00:41:54,799
统计上来说会好一点

1338
00:41:54,799 --> 00:41:56,839
就是说你每个模型都有一定的

1339
00:41:57,599 --> 00:41:59,399
就是说你模型都是个

1340
00:41:59,719 --> 00:42:01,359
就模型都是个偏的

1341
00:42:02,399 --> 00:42:03,759
就模型都是个假的

1342
00:42:03,759 --> 00:42:05,919
就模型都不能理和真实的世界

1343
00:42:05,919 --> 00:42:07,399
就是说模型都是有一个偏移

1344
00:42:07,399 --> 00:42:08,440
偏移是固定的

1345
00:42:08,559 --> 00:42:09,599
但它有个方差

1346
00:42:10,360 --> 00:42:12,280
方差是说每次优化或者什么样子

1347
00:42:12,280 --> 00:42:13,639
后有一点噪音在里面

1348
00:42:13,800 --> 00:42:15,639
我通过做n个模型

1349
00:42:15,760 --> 00:42:19,360
把它做放在一起做均值

1350
00:42:19,360 --> 00:42:21,039
我能降低这个方差

1351
00:42:22,360 --> 00:42:25,240
降低这个方差很有可能会提升你的精度

1352
00:42:25,679 --> 00:42:28,840
就他最后没有把模型的偏移给做掉

1353
00:42:28,840 --> 00:42:29,599
就是模型

1354
00:42:29,840 --> 00:42:31,360
他就是不是那么好

1355
00:42:31,360 --> 00:42:32,480
就还是不是那么好

1356
00:42:32,480 --> 00:42:34,200
但是说每一次训练

1357
00:42:34,200 --> 00:42:36,400
因为我都没有拿到这个模型的最好解

1358
00:42:36,400 --> 00:42:37,360
就是一个随机解

1359
00:42:37,360 --> 00:42:39,400
所以我做n次的话

1360
00:42:39,400 --> 00:42:41,920
我能够降低一些方差

1361
00:42:45,160 --> 00:42:46,240
问题33

1362
00:42:46,240 --> 00:42:48,760
数据集中的噪音比例多少最好

1363
00:42:48,760 --> 00:42:50,160
还是清除所有噪音

1364
00:42:50,320 --> 00:42:51,200
这个东西

1365
00:42:51,519 --> 00:42:52,599
你数据集的噪音

1366
00:42:52,599 --> 00:42:54,079
你当然希望越少越好

1367
00:42:54,240 --> 00:42:56,000
只是说我们现在是

1368
00:42:56,880 --> 00:42:59,519
我们现在是做人工数据集

1369
00:42:59,680 --> 00:43:00,920
就是给你加一点噪音

1370
00:43:00,920 --> 00:43:03,039
所以当然是清除

1371
00:43:03,200 --> 00:43:06,599
实际数据来说能清除噪音最好

1372
00:43:06,599 --> 00:43:08,039
我们只是说给一点

1373
00:43:08,039 --> 00:43:09,440
我们现在是给点人工数据集

1374
00:43:09,440 --> 00:43:10,279
给大家演示一下

1375
00:43:10,279 --> 00:43:11,319
所以加一点噪音

1376
00:43:14,559 --> 00:43:16,159
如果训练是不平衡的话

1377
00:43:16,159 --> 00:43:18,799
是否先考虑测试级也是不平衡的

1378
00:43:18,920 --> 00:43:21,599
再是否决定使用一个平衡的验证机

1379
00:43:24,519 --> 00:43:25,360
对就是说

1380
00:43:25,360 --> 00:43:26,279
但是你的

1381
00:43:26,319 --> 00:43:27,639
我觉得正常情况

1382
00:43:27,920 --> 00:43:28,679
就是说

1383
00:43:30,319 --> 00:43:31,719
你可以不平衡

1384
00:43:31,719 --> 00:43:35,440
但是你应该让通过加权来使得它平衡

1385
00:43:35,480 --> 00:43:36,159
就是说

1386
00:43:36,200 --> 00:43:37,200
假设我有两类

1387
00:43:37,200 --> 00:43:38,320
还是前面那个类

1388
00:43:38,320 --> 00:43:39,679
假设一类有90%

1389
00:43:39,679 --> 00:43:41,119
一类就10%

1390
00:43:41,840 --> 00:43:42,840
那么你要去看

1391
00:43:42,840 --> 00:43:44,440
你要去想的一个事情是说

1392
00:43:44,840 --> 00:43:45,360
一

1393
00:43:45,679 --> 00:43:47,800
我是不是真实的世界中

1394
00:43:47,800 --> 00:43:49,880
我就是90%的是这样子

1395
00:43:49,880 --> 00:43:51,119
10%是那样子

1396
00:43:51,880 --> 00:43:52,920
如果是的话

1397
00:43:53,440 --> 00:43:54,840
那么你就是应该

1398
00:43:55,280 --> 00:43:56,920
把主流的做好

1399
00:43:56,920 --> 00:43:57,360
对吧

1400
00:43:57,360 --> 00:43:58,639
把它90%的做好

1401
00:43:58,639 --> 00:44:00,240
10%的话尽量做好

1402
00:44:00,480 --> 00:44:02,320
所以如果是这样子的话是没关系

1403
00:44:03,240 --> 00:44:04,159
反过来讲

1404
00:44:04,239 --> 00:44:05,920
如果你现在这个情况

1405
00:44:05,920 --> 00:44:07,839
只是因为你采样没采样好

1406
00:44:09,000 --> 00:44:11,440
就是说你觉得10%其实挺重要的

1407
00:44:11,440 --> 00:44:12,519
只是说你这个数据集

1408
00:44:12,519 --> 00:44:13,879
你没有把它那个

1409
00:44:14,199 --> 00:44:15,359
东西都拿过来

1410
00:44:15,519 --> 00:44:16,519
这样子的情况下

1411
00:44:16,519 --> 00:44:18,799
你就是应该把10%

1412
00:44:18,799 --> 00:44:21,719
小的那一个东西的权重提升

1413
00:44:22,719 --> 00:44:23,359
最简单说

1414
00:44:23,359 --> 00:44:26,239
你把10%的样本全部复制10遍

1415
00:44:26,279 --> 00:44:27,920
那就变成1比1了

1416
00:44:28,119 --> 00:44:28,759
复制9遍

1417
00:44:28,879 --> 00:44:29,599
变成1比1了

1418
00:44:30,920 --> 00:44:33,319
就是说你不复制的话

1419
00:44:33,320 --> 00:44:35,360
你可以通过在loss里面加权

1420
00:44:35,720 --> 00:44:37,720
使得它给它一个更大的权重

1421
00:44:37,720 --> 00:44:39,200
小的类给更大的权重

1422
00:44:43,920 --> 00:44:44,680
问题35

1423
00:44:45,080 --> 00:44:46,000
在训练的时候

1424
00:44:46,000 --> 00:44:47,880
X轴是迭代次数

1425
00:44:48,080 --> 00:44:49,800
在验证数据集上

1426
00:44:49,800 --> 00:44:52,680
也会发生这种先下降后上升的

1427
00:44:52,680 --> 00:44:53,640
那不是错误

1428
00:44:53,640 --> 00:44:54,840
那就是过铃盒

1429
00:44:55,400 --> 00:44:58,800
那就是你的验证数据集会下降再上升

1430
00:44:58,800 --> 00:45:00,559
那就是发生了过铃盒

1431
00:45:00,800 --> 00:45:01,320
OK

