1
00:00:00,000 --> 00:00:02,000
装GP比较难的话

2
00:00:02,000 --> 00:00:03,439
你用collab用

3
00:00:03,439 --> 00:00:04,799
大家也知道

4
00:00:04,839 --> 00:00:06,240
Google的collab

5
00:00:06,240 --> 00:00:07,919
我们大概也提一下

6
00:00:08,080 --> 00:00:08,640
就是说

7
00:00:09,480 --> 00:00:10,880
假设你去我们的

8
00:00:11,240 --> 00:00:12,120
我们的

9
00:00:13,080 --> 00:00:14,560
假设我们去我们的课程

10
00:00:15,640 --> 00:00:18,519
比如说我们要之后要讲的卷积生成网络

11
00:00:18,519 --> 00:00:20,039
我随便调一个复杂一点的

12
00:00:20,240 --> 00:00:21,080
叫AlexNet

13
00:00:21,519 --> 00:00:24,480
就是说你去网页去点这个东西

14
00:00:25,879 --> 00:00:26,519
点它

15
00:00:27,280 --> 00:00:28,480
如果打不开

16
00:00:28,480 --> 00:00:30,760
那就大家要知道自己学会怎么样

17
00:00:30,920 --> 00:00:32,359
因为你看一下这个是对吧

18
00:00:32,359 --> 00:00:33,640
这个网址你打不开的话

19
00:00:33,640 --> 00:00:34,640
大家得想办法

20
00:00:34,640 --> 00:00:34,960
对

21
00:00:35,320 --> 00:00:36,799
我们这就不介绍方法了

22
00:00:38,039 --> 00:00:39,039
它是带GPU的

23
00:00:39,039 --> 00:00:41,600
就是说它可以让你免费用一点GPU

24
00:00:42,600 --> 00:00:44,640
就是说它的runtime里面

25
00:00:44,879 --> 00:00:45,840
就是说

26
00:00:47,120 --> 00:00:51,039
就是说它其实我就是我们默认是给它开GPU了

27
00:00:51,039 --> 00:00:52,240
就是说你可以选GPU

28
00:00:52,439 --> 00:00:55,519
然后如果你不费不付费的话

29
00:00:55,519 --> 00:00:56,679
你免费的话

30
00:00:56,679 --> 00:00:59,560
有概率是能拿到GPU的

31
00:00:59,560 --> 00:01:01,679
但是如果大家都在用的话

32
00:01:01,679 --> 00:01:04,599
当然你可能就轮不到你的

33
00:01:05,680 --> 00:01:06,879
当然你可以付费了

34
00:01:07,039 --> 00:01:09,640
我付过一阵子费也挺好的

35
00:01:09,640 --> 00:01:13,000
就一个月也就10美金还是11美金

36
00:01:14,159 --> 00:01:15,400
所以这一个是说

37
00:01:15,400 --> 00:01:16,239
但是你得

38
00:01:17,480 --> 00:01:18,519
就你没有GPU

39
00:01:19,120 --> 00:01:20,840
想办法去上网页

40
00:01:21,719 --> 00:01:23,200
具体方法大家

41
00:01:23,680 --> 00:01:25,400
八仙过海各显神通

42
00:01:25,800 --> 00:01:29,000
所以唯一的是说你通过

43
00:01:30,560 --> 00:01:31,800
这个角落这个地方

44
00:01:32,200 --> 00:01:34,080
我们现在这是m3的版本

45
00:01:34,200 --> 00:01:37,120
你得仿如果我们教的是PyTorch的话

46
00:01:37,280 --> 00:01:39,680
你得去点一下PyTorch

47
00:01:39,680 --> 00:01:41,480
就你点到PyTorch里面

48
00:01:41,480 --> 00:01:42,560
然后它就会

49
00:01:42,760 --> 00:01:44,400
打开的是PyTorch的版本

50
00:01:44,400 --> 00:01:44,719
对吧

51
00:01:44,719 --> 00:01:45,400
你看这里

52
00:01:45,680 --> 00:01:46,080
点一下

53
00:01:46,760 --> 00:01:47,280
OK

54
00:01:47,560 --> 00:01:52,120
我们就不讲怎么样去搞GPU了

55
00:01:52,600 --> 00:01:52,800
好

56
00:01:52,800 --> 00:01:53,320
我们

57
00:01:54,320 --> 00:01:56,040
我们主要讲一下

58
00:01:58,320 --> 00:01:59,440
我们主要讲一下

59
00:01:59,440 --> 00:02:03,760
在最简单的单GPU上怎么样做运算

60
00:02:05,000 --> 00:02:06,920
首先你得确认一下你有GPU

61
00:02:07,480 --> 00:02:09,040
你得运行一下感叹号

62
00:02:09,040 --> 00:02:10,520
NVIDIA-SMI

63
00:02:10,520 --> 00:02:13,240
就是NVIDIA出了一个东西

64
00:02:13,240 --> 00:02:14,200
就是自己的

65
00:02:14,280 --> 00:02:15,920
你可以看到你的

66
00:02:16,200 --> 00:02:16,960
有没有GPU

67
00:02:16,960 --> 00:02:18,920
像这个机器我是在云上面

68
00:02:19,640 --> 00:02:21,640
是有我是有4块

69
00:02:21,800 --> 00:02:23,880
特斯拉V100的GPU在这个地方的

70
00:02:24,520 --> 00:02:26,280
然后你再可以看到我的

71
00:02:27,560 --> 00:02:28,320
内存

72
00:02:28,320 --> 00:02:30,760
就是一共每个GPU16G内存

73
00:02:30,760 --> 00:02:34,280
就GPU内存和CPU内存是分开的

74
00:02:35,280 --> 00:02:37,360
然后这里就表示我用了

75
00:02:37,360 --> 00:02:39,400
这一个GPU我用了3G了

76
00:02:39,400 --> 00:02:39,880
已经

77
00:02:40,760 --> 00:02:42,200
然后接下来这个是你的GPU

78
00:02:42,200 --> 00:02:42,760
utilization

79
00:02:42,760 --> 00:02:44,120
就是说你在运行的时候

80
00:02:44,160 --> 00:02:45,920
你可以去看它用了多少

81
00:02:46,080 --> 00:02:47,960
就是说如果你发现就用了个50%

82
00:02:48,080 --> 00:02:49,520
或者50%以下的话

83
00:02:49,560 --> 00:02:51,000
就表示你用的

84
00:02:51,199 --> 00:02:54,039
你的模型不是那么的好

85
00:02:54,680 --> 00:02:56,879
然后后面就不用管了

86
00:02:57,039 --> 00:02:57,719
另外一个是说

87
00:02:57,719 --> 00:02:59,319
你可以看到你的CUDA的version

88
00:02:59,319 --> 00:03:01,240
就是说你CUDA的版本是10.0

89
00:03:01,280 --> 00:03:02,159
就是说在你

90
00:03:02,199 --> 00:03:04,280
你可能需要装对应的

91
00:03:04,400 --> 00:03:06,879
CUDA版本的框架

92
00:03:07,759 --> 00:03:08,319
OK

93
00:03:08,360 --> 00:03:09,639
这个就是说

94
00:03:09,840 --> 00:03:11,599
这个东西主要的好处是说

95
00:03:11,599 --> 00:03:12,039
一

96
00:03:12,360 --> 00:03:13,840
确保一下你有GPU

97
00:03:14,400 --> 00:03:15,120
第二的话

98
00:03:15,120 --> 00:03:16,080
你可以看一下

99
00:03:16,080 --> 00:03:16,960
你可以不断运行

100
00:03:16,960 --> 00:03:18,960
它来看你的GPU的使用率

101
00:03:19,360 --> 00:03:20,400
最下面你可以看到

102
00:03:20,400 --> 00:03:20,879
是说

103
00:03:21,439 --> 00:03:23,599
谁在用我的GPU

104
00:03:24,640 --> 00:03:25,240
OK

105
00:03:27,159 --> 00:03:27,360
好

106
00:03:27,360 --> 00:03:28,840
如果你有了GPU的话

107
00:03:29,360 --> 00:03:30,719
那么接下来就是说

108
00:03:30,759 --> 00:03:33,280
我怎么样表达我的硬件

109
00:03:34,800 --> 00:03:36,000
就是说默认

110
00:03:36,240 --> 00:03:37,560
所有的深度学习框架

111
00:03:37,560 --> 00:03:39,759
默认都是在CPU上做运算的

112
00:03:40,240 --> 00:03:41,439
你得去指定

113
00:03:41,439 --> 00:03:42,680
你要去GPU

114
00:03:43,960 --> 00:03:45,920
在Touch里面

115
00:03:46,640 --> 00:03:48,680
就是说它是通过

116
00:03:48,800 --> 00:03:51,719
Touch device CPU

117
00:03:51,920 --> 00:03:52,960
其实就是说默认

118
00:03:52,960 --> 00:03:54,360
就是CPU那个device

119
00:03:55,439 --> 00:03:56,520
然后你可以去说

120
00:03:56,760 --> 00:03:59,760
Touch.cuda.device

121
00:03:59,879 --> 00:04:00,760
CUDA

122
00:04:01,120 --> 00:04:02,200
这个也是

123
00:04:02,240 --> 00:04:04,240
我觉得他们是一开始没做好的地方

124
00:04:04,240 --> 00:04:04,840
反正

125
00:04:05,640 --> 00:04:06,920
就是用的CUDA

126
00:04:07,080 --> 00:04:07,640
但实际上

127
00:04:07,760 --> 00:04:09,680
CUDA跟GPU又不是那么的等价

128
00:04:09,680 --> 00:04:11,400
所以反正讲错就错

129
00:04:11,560 --> 00:04:12,400
就叫CUDA

130
00:04:14,040 --> 00:04:15,480
所以这个就是

131
00:04:15,520 --> 00:04:16,720
表示的是

132
00:04:16,920 --> 00:04:18,199
第0个GPU

133
00:04:18,199 --> 00:04:19,159
就是说你没有写0

134
00:04:19,159 --> 00:04:20,639
但是它是第0个GPU

135
00:04:21,719 --> 00:04:23,519
然后你可以说我要访问

136
00:04:23,519 --> 00:04:25,680
如果你有多个GPU的话

137
00:04:26,159 --> 00:04:27,759
你可以访问第

138
00:04:27,959 --> 00:04:29,279
第1个GPU

139
00:04:29,920 --> 00:04:31,839
就是加个冒号1

140
00:04:33,240 --> 00:04:33,480
对吧

141
00:04:33,480 --> 00:04:34,719
就表示这个东西

142
00:04:36,079 --> 00:04:37,240
另外的话

143
00:04:37,240 --> 00:04:39,560
你可以通过Touch.cuda.device

144
00:04:39,560 --> 00:04:40,360
Count来看

145
00:04:40,360 --> 00:04:41,560
你有多少个GPU

146
00:04:42,000 --> 00:04:43,120
这里我是三个

147
00:04:43,120 --> 00:04:44,800
为什么我们刚看到4个

148
00:04:44,800 --> 00:04:45,480
我是三个

149
00:04:45,639 --> 00:04:46,279
是因为

150
00:04:46,480 --> 00:04:48,840
我就是把最后一个屏蔽掉了

151
00:04:49,160 --> 00:04:50,920
最后一个我用来留给

152
00:04:51,160 --> 00:04:53,720
我们整个动手学生都学习跑

153
00:04:53,760 --> 00:04:56,040
每次提交一个任务跑

154
00:04:57,400 --> 00:04:58,240
我们要跑一下

155
00:04:58,240 --> 00:04:58,880
是不是能跑

156
00:04:59,040 --> 00:05:00,200
我把最后一个GPU留给

157
00:05:00,960 --> 00:05:02,720
防止我把所有的GPU用掉

158
00:05:03,640 --> 00:05:04,720
所以我们看到的是

159
00:05:04,720 --> 00:05:07,160
只看到了三个GPU的地方

160
00:05:07,360 --> 00:05:08,200
同样的道理的话

161
00:05:08,200 --> 00:05:09,480
你可以看一下你的device

162
00:05:09,480 --> 00:05:10,640
如果是0的话

163
00:05:10,640 --> 00:05:11,840
就表示你没有GPU

164
00:05:16,320 --> 00:05:17,440
OK我们来

165
00:05:19,400 --> 00:05:21,440
接下来我们定几个函数

166
00:05:21,440 --> 00:05:24,080
我们之后就是比较方便的用的话

167
00:05:24,240 --> 00:05:25,280
之后我们说

168
00:05:25,520 --> 00:05:26,720
Try GPU

169
00:05:26,760 --> 00:05:29,880
就我们尝试把第1个GPU拿出来

170
00:05:30,280 --> 00:05:32,040
就如果存在的话就返回

171
00:05:32,040 --> 00:05:34,040
如果不存在就把CPU返回来

172
00:05:34,080 --> 00:05:36,160
就保证我们的代码能跑

173
00:05:36,160 --> 00:05:36,640
对吧

174
00:05:37,080 --> 00:05:38,360
所以说如果就很简单

175
00:05:38,360 --> 00:05:39,880
如果是你的device count

176
00:05:39,880 --> 00:05:40,800
大于他的话

177
00:05:40,840 --> 00:05:42,760
那就返回他对应的GPU

178
00:05:42,760 --> 00:05:43,640
如果不然的话

179
00:05:43,680 --> 00:05:44,920
我就把CPU返回出来

180
00:05:45,920 --> 00:05:46,560
第二个函数

181
00:05:46,560 --> 00:05:48,920
就是说我能把所有的GPU都返回出来

182
00:05:49,879 --> 00:05:51,160
所以看到是说

183
00:05:51,960 --> 00:05:53,640
第一个我Try GPU的话

184
00:05:53,759 --> 00:05:56,520
第一个就是把第1个GPU返回出来

185
00:05:56,840 --> 00:05:58,439
然后Try GPU 10

186
00:05:58,480 --> 00:06:00,120
我没有10个GPU对吧

187
00:06:00,120 --> 00:06:01,639
我就返回了我的CPU

188
00:06:02,040 --> 00:06:03,639
最后Try all GPU

189
00:06:03,639 --> 00:06:05,520
就是把我的012返回来

190
00:06:05,680 --> 00:06:07,160
就第3个我就屏蔽掉了

191
00:06:07,160 --> 00:06:08,439
所以访问不到

192
00:06:11,160 --> 00:06:12,520
好我们接下来就是说

193
00:06:12,519 --> 00:06:15,519
来看一下我们怎么样在

194
00:06:15,639 --> 00:06:18,839
张亮在GPU上创建我们的Tensor

195
00:06:19,759 --> 00:06:20,839
首先我们看一下

196
00:06:20,839 --> 00:06:22,199
我们默认创建的Tensor

197
00:06:22,199 --> 00:06:23,240
就我们之前有讲过

198
00:06:23,240 --> 00:06:24,599
0123

199
00:06:24,839 --> 00:06:26,120
把它放成一个Tensor

200
00:06:26,319 --> 00:06:28,319
可以通过点device去看他

201
00:06:28,359 --> 00:06:29,319
现在在哪里

202
00:06:29,879 --> 00:06:31,279
默认是在CPU上

203
00:06:31,839 --> 00:06:34,719
你看到他的device的type是CPU

204
00:06:35,000 --> 00:06:37,319
就默认他是在我的CPU的内存上面

205
00:06:40,959 --> 00:06:41,759
那么接下来

206
00:06:42,719 --> 00:06:43,560
接下来是说

207
00:06:43,560 --> 00:06:45,680
我可以在创建的时候

208
00:06:45,680 --> 00:06:46,680
告诉你说

209
00:06:46,680 --> 00:06:48,719
我要放在GPU上

210
00:06:49,839 --> 00:06:51,839
就是说我在创建它的时候

211
00:06:52,479 --> 00:06:53,719
加入一个device

212
00:06:53,719 --> 00:06:56,120
等于我们就Try GPU

213
00:06:56,120 --> 00:06:57,599
就返回了我们有GPU

214
00:06:57,919 --> 00:07:00,479
我们就返回我们第0个GPU

215
00:07:00,479 --> 00:07:01,399
那个device

216
00:07:02,120 --> 00:07:04,079
那么再去打印x的话

217
00:07:04,199 --> 00:07:06,279
你可以看到唯一的区别是说

218
00:07:06,319 --> 00:07:08,639
device等于kula冒号0

219
00:07:09,120 --> 00:07:10,399
就表示说

220
00:07:10,520 --> 00:07:13,680
这一个张量是创建在GPU0

221
00:07:13,680 --> 00:07:14,920
就GPU是个卡

222
00:07:14,920 --> 00:07:16,720
GPU是里面有内存的

223
00:07:17,160 --> 00:07:20,840
就是我把我的CPU的内存的数据

224
00:07:20,880 --> 00:07:21,720
搬过去了

225
00:07:24,600 --> 00:07:25,480
同样道理

226
00:07:26,200 --> 00:07:29,960
我可以在我的第2个GPU上

227
00:07:30,040 --> 00:07:30,840
就是

228
00:07:31,960 --> 00:07:33,240
它的index是1

229
00:07:33,960 --> 00:07:36,480
创建一个random的2乘3的一个

230
00:07:36,520 --> 00:07:37,120
矩阵

231
00:07:37,560 --> 00:07:38,880
那么可以看到他的device

232
00:07:38,880 --> 00:07:40,240
就是kuda冒号1

233
00:07:41,400 --> 00:07:44,320
OK

234
00:07:44,760 --> 00:07:46,040
就是第1个第2个GPU

235
00:07:46,040 --> 00:07:47,120
我们都有值了

236
00:07:48,360 --> 00:07:50,640
接下来是说我要做运算的话

237
00:07:51,080 --> 00:07:53,800
我要算x加y的话

238
00:07:53,840 --> 00:07:54,640
我是怎么

239
00:07:54,840 --> 00:07:56,680
我会发生在什么地方呢

240
00:07:57,240 --> 00:08:00,480
就它会发生在x和y

241
00:08:00,520 --> 00:08:02,760
对应的device上面

242
00:08:04,320 --> 00:08:04,920
回忆一下

243
00:08:05,080 --> 00:08:06,360
之前我们的xy

244
00:08:06,360 --> 00:08:08,760
都是创建在CPU的memory里面

245
00:08:09,039 --> 00:08:10,240
那么它的默认计算

246
00:08:10,240 --> 00:08:11,360
就会发生在CPU

247
00:08:13,360 --> 00:08:14,159
现在是说

248
00:08:14,159 --> 00:08:16,439
假设我们要在GPU上的话

249
00:08:16,719 --> 00:08:17,759
我们要保证

250
00:08:18,279 --> 00:08:19,759
一个事情是说

251
00:08:19,800 --> 00:08:22,599
x和y都在GPU上

252
00:08:23,240 --> 00:08:26,959
而且它必须在同一个GPU上

253
00:08:29,240 --> 00:08:30,680
就是说我们知道

254
00:08:30,680 --> 00:08:32,519
我们的x放在第0个GPU

255
00:08:32,519 --> 00:08:34,360
我们的y放在第1个GPU

256
00:08:34,399 --> 00:08:35,720
那么我们这个地方

257
00:08:35,720 --> 00:08:37,200
需要创建一个z

258
00:08:37,879 --> 00:08:40,040
把x挪到

259
00:08:40,560 --> 00:08:42,040
第1个

260
00:08:42,040 --> 00:08:43,759
就是就第2个GPU了

261
00:08:44,960 --> 00:08:45,879
所以你可以看一下

262
00:08:45,879 --> 00:08:46,640
就是说

263
00:08:46,960 --> 00:08:47,720
x.cuda

264
00:08:47,720 --> 00:08:48,400
就你可以认为

265
00:08:48,400 --> 00:08:49,440
就是把x的值

266
00:08:49,440 --> 00:08:50,080
从一个GPU

267
00:08:50,080 --> 00:08:51,240
copy到另外一个GPU

268
00:08:51,280 --> 00:08:52,680
从0copy到1

269
00:08:53,879 --> 00:08:55,560
那么你print xz的话

270
00:08:55,680 --> 00:08:57,840
就是x的device在这个地方

271
00:08:58,240 --> 00:08:59,560
z的device在这个地方

272
00:09:01,080 --> 00:09:01,800
就是说

273
00:09:01,960 --> 00:09:03,000
device不一样了

274
00:09:04,600 --> 00:09:05,920
那么这样子的话

275
00:09:05,919 --> 00:09:08,759
我的y和z都在我的

276
00:09:08,839 --> 00:09:10,519
第1号GPU上

277
00:09:11,120 --> 00:09:12,439
那我就可以做计算了

278
00:09:13,000 --> 00:09:13,879
就是说

279
00:09:13,919 --> 00:09:15,399
因为它在同一个GPU上

280
00:09:15,399 --> 00:09:16,360
它的加法

281
00:09:16,399 --> 00:09:18,199
会在GPU完成

282
00:09:18,240 --> 00:09:19,479
就会用GPU来加速

283
00:09:19,479 --> 00:09:20,360
它做计算

284
00:09:21,000 --> 00:09:22,240
可以看到它的结果

285
00:09:22,240 --> 00:09:24,479
也会写在GPU上

286
00:09:26,679 --> 00:09:27,240
OK

287
00:09:29,799 --> 00:09:30,599
就是说

288
00:09:31,000 --> 00:09:31,759
可以解释一下

289
00:09:31,759 --> 00:09:33,159
为什么要做这个事情

290
00:09:33,199 --> 00:09:34,120
是因为

291
00:09:34,120 --> 00:09:36,039
当你可以从实际上来说

292
00:09:36,039 --> 00:09:36,720
你可以说

293
00:09:36,720 --> 00:09:38,320
x加上y

294
00:09:38,320 --> 00:09:40,799
x和y在不同的device上面

295
00:09:40,799 --> 00:09:41,600
是没关系的

296
00:09:41,600 --> 00:09:42,200
就是说

297
00:09:42,240 --> 00:09:44,320
从实际上来说

298
00:09:44,320 --> 00:09:45,039
一点问题都没有

299
00:09:45,039 --> 00:09:45,639
我就把它

300
00:09:45,639 --> 00:09:47,159
要么就copy到

301
00:09:47,240 --> 00:09:48,879
的第1个device上去

302
00:09:49,080 --> 00:09:51,639
就之所以不这么做的历史原因

303
00:09:51,639 --> 00:09:54,799
是因为在GPU之间挪数据

304
00:09:54,840 --> 00:09:56,639
特别是GPU的数据

305
00:09:56,639 --> 00:09:58,600
挪到CPU是一件很慢的事情

306
00:09:59,720 --> 00:10:01,159
就如果你不小心

307
00:10:01,200 --> 00:10:03,480
说我一个网络

308
00:10:03,480 --> 00:10:05,200
然后基本上所有的层

309
00:10:05,200 --> 00:10:07,320
都创建在GPU上

310
00:10:07,320 --> 00:10:08,720
但不小心把某一层

311
00:10:08,720 --> 00:10:10,000
创建在CPU上

312
00:10:10,000 --> 00:10:12,759
那你是不是要来来回回挪东西

313
00:10:13,200 --> 00:10:14,159
这个东西很容易会

314
00:10:14,159 --> 00:10:15,480
造成你的性能问题

315
00:10:17,200 --> 00:10:17,840
就是说

316
00:10:17,840 --> 00:10:19,639
而且这个东西比较很难debug

317
00:10:19,759 --> 00:10:21,000
所以还不如说

318
00:10:21,159 --> 00:10:22,000
我就给你报个错

319
00:10:22,000 --> 00:10:22,600
说

320
00:10:22,800 --> 00:10:23,519
不好意思

321
00:10:23,560 --> 00:10:26,840
那有一个layer放在CPU上

322
00:10:26,840 --> 00:10:27,840
你不能做运算

323
00:10:28,200 --> 00:10:29,240
所以这就是

324
00:10:29,279 --> 00:10:31,759
它纯粹是给大家造成困难

325
00:10:31,759 --> 00:10:33,000
纯粹是让大家去想

326
00:10:33,240 --> 00:10:34,679
我的东西要放在

327
00:10:34,720 --> 00:10:36,799
在哪一个device上运行

328
00:10:37,519 --> 00:10:38,519
纯粹是因为

329
00:10:38,519 --> 00:10:40,200
如果不让你想的话

330
00:10:40,240 --> 00:10:41,720
性能很难保证

331
00:10:41,919 --> 00:10:43,919
就是一个性能的考虑

332
00:10:46,200 --> 00:10:47,600
另外一个当然是说

333
00:10:47,600 --> 00:10:49,679
如果Z已经在第一个

334
00:10:49,679 --> 00:10:51,000
第一号GPU的话

335
00:10:51,000 --> 00:10:52,200
那么他点CUDA1

336
00:10:52,200 --> 00:10:53,600
他会不会返回任何事情

337
00:10:53,600 --> 00:10:55,279
他就是返回自己

338
00:10:55,279 --> 00:10:56,120
就不会

339
00:10:56,159 --> 00:10:58,799
不会把GPU从自己copy到自己

340
00:10:58,799 --> 00:11:00,200
这也是一个性能的考虑

341
00:11:03,279 --> 00:11:03,559
好

342
00:11:03,559 --> 00:11:04,799
我们看到了说

343
00:11:04,840 --> 00:11:07,200
怎么样在GPU上做

344
00:11:08,039 --> 00:11:09,200
张量的运算

345
00:11:09,240 --> 00:11:11,200
那么接下来怎么做神级网络呢

346
00:11:12,200 --> 00:11:13,759
神级网络跟之前一样

347
00:11:14,039 --> 00:11:16,720
我们可以创建我们的神级网络

348
00:11:16,720 --> 00:11:18,240
我们通常在CPU上

349
00:11:18,240 --> 00:11:20,279
把你的权重给你出售好

350
00:11:20,639 --> 00:11:23,919
然后你可以调用.to这个method

351
00:11:25,600 --> 00:11:28,200
把它挪到某一个device上面去

352
00:11:28,879 --> 00:11:30,799
net.to device

353
00:11:30,799 --> 00:11:31,480
try GPU

354
00:11:31,480 --> 00:11:33,480
就是挪到第0号GPU去

355
00:11:35,879 --> 00:11:37,360
然后记得我的X

356
00:11:37,360 --> 00:11:39,360
就已经创建在0号GPU上了

357
00:11:39,399 --> 00:11:42,120
所以我的network

358
00:11:42,159 --> 00:11:43,000
就是说这个函数

359
00:11:43,000 --> 00:11:45,159
意味着把我们所有的参数

360
00:11:45,920 --> 00:11:48,840
在我的第0号GPU上copy一份

361
00:11:51,320 --> 00:11:54,680
然后我的X也在我的0号GPU上

362
00:11:54,720 --> 00:11:57,159
所以这个的前项运算

363
00:11:57,200 --> 00:11:58,879
就是在0号GPU进行的

364
00:11:58,920 --> 00:12:01,200
那我们就在GPU上做运算了

365
00:12:01,639 --> 00:12:03,039
可以看到它的结果

366
00:12:03,080 --> 00:12:05,560
也是在我的第0号GPU上

367
00:12:08,159 --> 00:12:09,560
但你可以确认一下

368
00:12:09,560 --> 00:12:10,680
我们的weights

369
00:12:10,879 --> 00:12:12,039
你看到我们的

370
00:12:12,039 --> 00:12:14,399
我的第一个全连接层的

371
00:12:14,399 --> 00:12:16,320
weights.data.device

372
00:12:16,360 --> 00:12:17,639
就在我的CUDA上面

373
00:12:17,639 --> 00:12:18,840
对index.00

374
00:12:18,840 --> 00:12:20,039
就是第0号GPU

375
00:12:20,399 --> 00:12:22,039
可以确认一下我的模型参数

376
00:12:22,039 --> 00:12:23,680
在GPU上

377
00:12:24,879 --> 00:12:25,560
OK

378
00:12:25,600 --> 00:12:26,879
这就是说

379
00:12:26,879 --> 00:12:28,960
怎么样用GPU做运算

380
00:12:29,080 --> 00:12:31,320
就是用GPU是很简单的事情

381
00:12:31,640 --> 00:12:32,440
比较难的事情

382
00:12:32,440 --> 00:12:34,440
一是说你得去弄个GPU来

383
00:12:34,480 --> 00:12:35,680
第二个是说

384
00:12:36,000 --> 00:12:38,160
你得装一装CUDA

385
00:12:39,040 --> 00:12:41,560
那东西其实也现在已经很好装了

386
00:12:41,560 --> 00:12:43,040
就以前还是挺难装的

387
00:12:43,040 --> 00:12:44,760
就我们当时最早的时候

388
00:12:44,800 --> 00:12:46,200
还是问题挺多

389
00:12:46,200 --> 00:12:47,879
现在已经好很多了

390
00:12:48,400 --> 00:12:49,120
就是说

391
00:12:49,160 --> 00:12:51,080
假设你已经全部有的话

392
00:12:51,120 --> 00:12:55,520
那么你要在GPU上做运算的办法

393
00:12:55,520 --> 00:12:56,680
就是把我的数据

394
00:12:56,680 --> 00:12:58,000
罗到我的GPU上

395
00:12:58,400 --> 00:13:00,040
只要你数据在GPU上

396
00:13:00,039 --> 00:13:02,279
那么对应的操作

397
00:13:02,279 --> 00:13:04,120
也会在GPU上完成

398
00:13:04,959 --> 00:13:07,159
而且你要保证是说

399
00:13:07,279 --> 00:13:09,439
你得手动的copy到GPU上

400
00:13:09,480 --> 00:13:10,079
就是说

401
00:13:10,079 --> 00:13:11,559
而且你有多个GPU的话

402
00:13:11,559 --> 00:13:12,719
你得保证你的数据

403
00:13:12,719 --> 00:13:14,399
在同一个GPU上做运行

404
00:13:15,079 --> 00:13:16,759
所以对神纪网络是一样的

405
00:13:16,799 --> 00:13:19,000
神纪网络要在一个GPU上做运算

406
00:13:19,000 --> 00:13:20,360
那么你要干的事情是

407
00:13:20,360 --> 00:13:21,559
把它的权重

408
00:13:21,599 --> 00:13:22,799
copy到GPU上

409
00:13:22,919 --> 00:13:25,240
把它的输入也copy到GPU上

410
00:13:25,319 --> 00:13:27,319
那么就可以在GPU上做forward

411
00:13:27,480 --> 00:13:28,599
forward做完之后

412
00:13:28,600 --> 00:13:29,399
你backward

413
00:13:29,440 --> 00:13:31,800
应该也是会在同一个GPU上做运算

414
00:13:32,040 --> 00:13:32,600
OK

415
00:13:32,639 --> 00:13:34,120
这就是我们的GPU

