1
00:00:00,000 --> 00:00:06,360
好 接下来我们来讲一下softmax回归的从零开始的实现

2
00:00:06,360 --> 00:00:10,759
我们已经在线性回归里面介绍了从零开始实现

3
00:00:10,759 --> 00:00:13,320
对于softmax回归也是一样

4
00:00:13,320 --> 00:00:15,880
我们觉得这是一个非常重要的模型

5
00:00:15,880 --> 00:00:19,000
这也是以后所有深度学习模型的基础

6
00:00:19,000 --> 00:00:21,359
所以我们应该从零开始实现一遍

7
00:00:21,359 --> 00:00:23,400
让大家理解所有的细节

8
00:00:23,400 --> 00:00:28,760
我们之前有定义我们怎么样读取我们的FreshM list的data

9
00:00:29,080 --> 00:00:32,800
所以在这里我们就直接import了我们的数据

10
00:00:32,800 --> 00:00:37,200
可以看到是说我们定义的我们的batch size等于256

11
00:00:37,200 --> 00:00:40,320
每一次我们随机读256张图片

12
00:00:40,320 --> 00:00:45,400
然后我们能返回一个训练的iterator和我们的test的iterator

13
00:00:45,400 --> 00:00:48,200
就是我们训练级和测试级的迭来器

14
00:00:51,000 --> 00:00:56,480
好 接下来是说我们知道我们之前每张图片是一个长

15
00:00:56,640 --> 00:00:59,600
和宽都为28的一张图片

16
00:01:00,160 --> 00:01:02,680
然后它的通道数为1

17
00:01:02,680 --> 00:01:05,200
它其实就是一个3D的一张输入

18
00:01:05,200 --> 00:01:07,800
但是对于softmax回归来讲

19
00:01:07,800 --> 00:01:10,040
我的输入需要是一个向量

20
00:01:10,600 --> 00:01:12,719
所以我们需要把整个图片拉长

21
00:01:12,719 --> 00:01:14,960
把它拉成一个直直的向量

22
00:01:15,640 --> 00:01:17,159
但我们之后会讲说

23
00:01:17,159 --> 00:01:20,120
这个拉成向量会损失掉很多空间信息

24
00:01:20,120 --> 00:01:23,120
这个我们留给卷积神级网络来继续了

25
00:01:23,400 --> 00:01:26,120
在这里我们简单的把它拉成一条向量

26
00:01:26,320 --> 00:01:29,400
因为28x28等于784

27
00:01:29,400 --> 00:01:34,680
所以我们的softmax回归的输入就是784的向量

28
00:01:35,680 --> 00:01:38,400
因为我是一个数据集有10个类

29
00:01:38,400 --> 00:01:41,000
所以我的模型输出的维度为10

30
00:01:41,640 --> 00:01:45,560
这里我们就定义了我们的两个根据数据相关的参数

31
00:01:45,560 --> 00:01:46,880
784和10

32
00:01:47,439 --> 00:01:50,600
那么首先我们要定义我们的权重

33
00:01:51,439 --> 00:01:52,040
w

34
00:01:52,880 --> 00:01:54,120
w跟之前一样

35
00:01:54,120 --> 00:01:58,160
我们把它初始成一个高斯随机分布的值

36
00:01:58,760 --> 00:02:00,240
它的均值为0

37
00:02:00,240 --> 00:02:01,800
方差为0.01

38
00:02:02,400 --> 00:02:04,359
它的形状这里是关键

39
00:02:04,480 --> 00:02:06,760
它的形状首先是它的行数

40
00:02:06,800 --> 00:02:08,480
等于你输入的个数

41
00:02:08,520 --> 00:02:09,719
就是784

42
00:02:10,000 --> 00:02:12,360
你的列数就等于输出的个数

43
00:02:12,640 --> 00:02:14,240
等于10

44
00:02:15,240 --> 00:02:16,439
因为我们要计算T组

45
00:02:16,439 --> 00:02:19,200
所以我们的requires grad等于true

46
00:02:20,159 --> 00:02:21,800
同样的偏移

47
00:02:21,960 --> 00:02:23,360
对每一个输出

48
00:02:23,360 --> 00:02:25,280
我们都需要有一个偏移

49
00:02:25,280 --> 00:02:28,640
那么所以我们的偏移就是一个成为10的一个向量

50
00:02:29,120 --> 00:02:31,120
同样我们需要计算T度

51
00:02:31,160 --> 00:02:33,719
我们requires grad等于true

52
00:02:38,240 --> 00:02:38,560
好

53
00:02:38,560 --> 00:02:41,960
接下来我们要定义我们的softmax操作

54
00:02:42,320 --> 00:02:45,680
在定义之前我们稍微回顾一下

55
00:02:46,040 --> 00:02:47,640
对一个矩阵

56
00:02:47,640 --> 00:02:50,280
我们可以按照某个轴来求和

57
00:02:51,159 --> 00:02:54,479
这里我们定义了一个2乘以3的一个矩阵

58
00:02:55,479 --> 00:02:56,680
可以看到是说

59
00:02:56,719 --> 00:02:58,240
如果按照维度

60
00:02:58,240 --> 00:03:01,960
等于0来求和的话

61
00:03:02,120 --> 00:03:05,639
那就是把我的形状shape中的第0号元素

62
00:03:05,800 --> 00:03:08,039
变成了从2变成了1

63
00:03:08,360 --> 00:03:09,759
keep dimension等于true的话

64
00:03:09,759 --> 00:03:12,240
还是一个二维的一个矩阵

65
00:03:12,319 --> 00:03:14,360
那么它的输出就是一个

66
00:03:14,560 --> 00:03:16,159
航向量

67
00:03:16,919 --> 00:03:20,879
如果我们按照维度1来求和的话

68
00:03:20,919 --> 00:03:23,960
那就是把shape中的第1个元素变成1

69
00:03:24,199 --> 00:03:26,000
那么它就是变成一个列向量

70
00:03:26,000 --> 00:03:27,719
就是有2乘以的一个列向量

71
00:03:29,319 --> 00:03:31,919
这个我们在之前先行代数有讲过

72
00:03:32,639 --> 00:03:34,519
有了这个操作之后

73
00:03:34,519 --> 00:03:37,240
我们就可以实现我们的softmax操作了

74
00:03:37,599 --> 00:03:40,240
回忆一下softmax的定义是这样子

75
00:03:40,439 --> 00:03:41,919
唯一不同的是说

76
00:03:41,960 --> 00:03:45,079
现在我们的x不再是一个航向量

77
00:03:45,080 --> 00:03:46,040
不再是一个向量

78
00:03:46,040 --> 00:03:46,960
而是一个矩阵

79
00:03:46,960 --> 00:03:48,800
对一个矩阵来讲

80
00:03:48,840 --> 00:03:50,400
我们就是按每一行

81
00:03:50,760 --> 00:03:52,080
对它做softmax

82
00:03:53,480 --> 00:03:54,560
所以我们怎么做呢

83
00:03:54,760 --> 00:03:57,720
首先我们使用touch.exp

84
00:03:57,880 --> 00:04:01,640
对每一个元素做指数计算

85
00:04:02,280 --> 00:04:03,240
接下来

86
00:04:03,480 --> 00:04:06,280
我们按照维度为1来求和

87
00:04:06,320 --> 00:04:08,480
就是把每一行进行求和

88
00:04:08,760 --> 00:04:10,160
然后keep dimension等于true

89
00:04:10,160 --> 00:04:13,520
就它还是一个二维的一个矩阵

90
00:04:14,280 --> 00:04:16,400
接下来利用广播机制

91
00:04:16,680 --> 00:04:18,199
把它的每个元素

92
00:04:18,680 --> 00:04:20,480
除以它的partition

93
00:04:21,120 --> 00:04:22,600
它的意思就是说

94
00:04:22,600 --> 00:04:23,840
对每一行

95
00:04:23,879 --> 00:04:25,079
对第2行

96
00:04:25,120 --> 00:04:27,600
我都除以partition中第1个元素

97
00:04:28,079 --> 00:04:29,360
这样子我的每一行

98
00:04:29,360 --> 00:04:32,280
就会除以了我们下面这一个向量

99
00:04:33,600 --> 00:04:34,759
我们接下来可以验证一下

100
00:04:34,759 --> 00:04:36,240
我们这个是不是正确的

101
00:04:36,680 --> 00:04:37,680
我们怎么做呢

102
00:04:38,000 --> 00:04:38,920
就是说

103
00:04:39,319 --> 00:04:42,000
我们创建一个随机的一个

104
00:04:42,639 --> 00:04:43,319
均值为0

105
00:04:43,319 --> 00:04:44,519
方程为1的一个

106
00:04:44,519 --> 00:04:46,600
两行五列的一个矩阵x

107
00:04:47,240 --> 00:04:50,120
我们把它放进softmax之后

108
00:04:50,279 --> 00:04:51,040
你可以发现

109
00:04:51,040 --> 00:04:54,560
首先它的形状是不会发生变化的

110
00:04:54,560 --> 00:04:55,839
所以xprop

111
00:04:55,959 --> 00:04:58,240
还是一个两行五列的一个矩阵

112
00:04:58,759 --> 00:05:00,240
但是不一样的是说

113
00:05:00,600 --> 00:05:02,759
它的所有的值都是一个正的了

114
00:05:02,759 --> 00:05:03,959
因为你是均

115
00:05:04,639 --> 00:05:05,399
正台分母

116
00:05:05,399 --> 00:05:06,959
所以你理论上是一个正负

117
00:05:06,959 --> 00:05:08,079
应该比例是一样的

118
00:05:08,240 --> 00:05:10,240
现在全部变成了正的

119
00:05:10,400 --> 00:05:11,280
其次

120
00:05:11,319 --> 00:05:13,920
如果我们按照行来做加的话

121
00:05:14,040 --> 00:05:15,800
那么就是说会得到一个

122
00:05:15,960 --> 00:05:16,759
常为2的

123
00:05:16,759 --> 00:05:17,519
因为是两行

124
00:05:17,920 --> 00:05:20,079
每一行的元素就应该是1.0

125
00:05:20,680 --> 00:05:22,079
这样子我们就是说

126
00:05:22,120 --> 00:05:23,879
不管是什么样的输入

127
00:05:23,879 --> 00:05:25,240
softmax之后

128
00:05:25,400 --> 00:05:27,240
它的每个元素值为正

129
00:05:27,600 --> 00:05:28,639
至少为非负

130
00:05:28,879 --> 00:05:31,400
而且它的每一行合为1

131
00:05:32,800 --> 00:05:33,319
OK

132
00:05:35,319 --> 00:05:35,519
好

133
00:05:35,519 --> 00:05:36,680
这样子我们就可以来实现

134
00:05:36,680 --> 00:05:39,120
我们的softmax回归的模型

135
00:05:39,199 --> 00:05:41,399
它其实也是挺简单的一个计算

136
00:05:42,160 --> 00:05:43,280
大家可以看一下

137
00:05:44,040 --> 00:05:46,439
所以说首先对于输入x

138
00:05:46,879 --> 00:05:48,160
因为我们需要的是一个

139
00:05:48,160 --> 00:05:51,480
p量大小乘以输入为数的一个矩阵

140
00:05:51,680 --> 00:05:54,280
所以我们把它reshape成一个2D的矩阵

141
00:05:54,560 --> 00:05:56,800
-1就表示你自己给我算一下

142
00:05:57,000 --> 00:05:59,480
其实这一个维度应该是等于p量大小

143
00:06:00,079 --> 00:06:03,319
然后w的shape0就是784

144
00:06:03,720 --> 00:06:05,000
它就变成了一个

145
00:06:05,040 --> 00:06:07,079
我们记得我们的batchsize是256

146
00:06:07,079 --> 00:06:07,560
对吧

147
00:06:07,680 --> 00:06:10,439
所以x就会被reshape成一个256

148
00:06:10,480 --> 00:06:12,079
乘以784的矩阵

149
00:06:12,839 --> 00:06:16,680
然后我们再对x和w进行矩阵乘法

150
00:06:16,959 --> 00:06:19,120
加上我们的通过广播机制

151
00:06:19,120 --> 00:06:20,319
加上我们的偏移

152
00:06:20,800 --> 00:06:23,480
最后我们放进我们的softmax里面

153
00:06:23,759 --> 00:06:27,639
拿到一个所有的元素值大于0

154
00:06:27,800 --> 00:06:30,800
而且行合为1的一个输出

155
00:06:31,079 --> 00:06:33,560
这就是我们整个softmax回归模型

156
00:06:35,920 --> 00:06:36,360
OK

157
00:06:36,920 --> 00:06:40,680
接下来我们要实现我们的交叉商损失

158
00:06:41,120 --> 00:06:44,080
在实现之前我们还是补一个细节

159
00:06:44,840 --> 00:06:47,160
怎么样在我的预测值里面

160
00:06:47,160 --> 00:06:48,240
根据我的标号

161
00:06:48,240 --> 00:06:50,280
把我们的对应的预测值拿出来

162
00:06:51,720 --> 00:06:52,600
举个例子

163
00:06:53,720 --> 00:06:57,920
我们创建一个长度为2的向量

164
00:06:58,160 --> 00:06:59,360
它是一个整数型

165
00:06:59,400 --> 00:07:00,439
第一个元素是0

166
00:07:00,480 --> 00:07:01,480
第二个元素是2

167
00:07:01,560 --> 00:07:04,840
这里表示的是两个真实的标号

168
00:07:04,960 --> 00:07:05,759
这是y

169
00:07:06,480 --> 00:07:08,480
y hat就是我的预测值

170
00:07:08,800 --> 00:07:10,639
假设我们是有三类的话

171
00:07:10,800 --> 00:07:12,920
对两个样本做预测的话

172
00:07:12,920 --> 00:07:14,560
那就是一个2乘3的矩阵

173
00:07:14,560 --> 00:07:17,360
就是这个是第0样本的预测值

174
00:07:17,680 --> 00:07:22,000
这个是第一个样本的预测值

175
00:07:23,040 --> 00:07:25,240
那么接下来我们要干的事情是说

176
00:07:25,280 --> 00:07:27,000
对于第0样本

177
00:07:27,000 --> 00:07:30,879
我们把它对应标号的预测值拿出来

178
00:07:31,240 --> 00:07:32,199
就是说

179
00:07:33,639 --> 00:07:35,120
对第0号样本

180
00:07:35,280 --> 00:07:38,160
拿出来y0对应的那一个元素

181
00:07:38,959 --> 00:07:40,240
对第一个样本

182
00:07:40,280 --> 00:07:43,519
拿出y1里面下标所对应的输出

183
00:07:44,199 --> 00:07:45,560
那么这里可以看到是说

184
00:07:45,560 --> 00:07:47,199
因为y0是等于0

185
00:07:47,240 --> 00:07:48,879
所以对第0号样本

186
00:07:48,879 --> 00:07:50,480
我们拿到是第0个元素

187
00:07:50,480 --> 00:07:51,319
0.1

188
00:07:52,199 --> 00:07:54,079
y1等于2

189
00:07:54,280 --> 00:07:55,759
所以对于这个来说

190
00:07:55,759 --> 00:07:57,280
我们就拿到了0.5

191
00:07:58,120 --> 00:08:00,040
所以这个操作的意思是说

192
00:08:00,079 --> 00:08:02,240
我给了你一个预测值

193
00:08:02,360 --> 00:08:04,560
然后我去拿出真实标号

194
00:08:04,560 --> 00:08:07,439
我对真实标号类的预测值是多少

195
00:08:07,439 --> 00:08:08,439
给你拿出来

196
00:08:09,319 --> 00:08:09,639
好

197
00:08:09,680 --> 00:08:10,959
有了这个之后

198
00:08:11,000 --> 00:08:12,120
我们就可以来

199
00:08:12,680 --> 00:08:15,199
实现我们的交叉商损失函数了

200
00:08:16,120 --> 00:08:17,360
这个函数是说

201
00:08:17,399 --> 00:08:20,000
给定我的y hat

202
00:08:20,000 --> 00:08:22,959
我的预测和我的真实标号y

203
00:08:23,240 --> 00:08:24,160
我怎么办呢

204
00:08:24,360 --> 00:08:26,160
首先对于每一行

205
00:08:26,560 --> 00:08:27,759
拿出来

206
00:08:27,879 --> 00:08:28,720
这是range

207
00:08:28,720 --> 00:08:29,519
生成一个0

208
00:08:29,519 --> 00:08:33,559
一直到长度为n的一个向量

209
00:08:33,840 --> 00:08:36,000
然后拿出来

210
00:08:36,560 --> 00:08:39,400
对应的真实标号的预测值

211
00:08:40,360 --> 00:08:42,400
回一下我们的cross entropy的定义

212
00:08:43,200 --> 00:08:44,000
然后

213
00:08:44,760 --> 00:08:46,040
然后求lock

214
00:08:46,120 --> 00:08:47,160
然后求负数

215
00:08:48,680 --> 00:08:49,200
ok

216
00:08:49,200 --> 00:08:50,200
所以验证一下

217
00:08:50,840 --> 00:08:53,720
y hat是一个2乘3的一个预测值

218
00:08:53,760 --> 00:08:56,120
y是一个长为2的向量

219
00:08:56,320 --> 00:08:58,600
那么做了cross entropy之后

220
00:08:58,640 --> 00:09:02,160
那就会拿到一个长度为2的一个损失

221
00:09:02,799 --> 00:09:04,519
这是样本0的损失

222
00:09:04,519 --> 00:09:05,839
样本1的损失

223
00:09:06,039 --> 00:09:08,319
所以都是大于0的

224
00:09:09,319 --> 00:09:09,639
ok

225
00:09:09,639 --> 00:09:11,279
这就是cross entropy的定义

226
00:09:15,199 --> 00:09:15,839
然后

227
00:09:16,039 --> 00:09:17,799
因为我们做的是分类问题

228
00:09:17,799 --> 00:09:19,360
所以我们要判断说

229
00:09:19,399 --> 00:09:21,360
我们的预测值

230
00:09:21,399 --> 00:09:23,319
预测的类别和真实的类别

231
00:09:23,319 --> 00:09:24,839
是不是正确的

232
00:09:25,279 --> 00:09:26,919
我们这里实现一个小函数

233
00:09:27,120 --> 00:09:29,199
说给定我们的预测值y hat

234
00:09:29,199 --> 00:09:30,679
和我们的真实的值y

235
00:09:30,840 --> 00:09:33,600
我们来计算我们分类正确的类别数

236
00:09:34,400 --> 00:09:37,400
首先如果你的y hat是一个二维矩阵的话

237
00:09:37,600 --> 00:09:40,280
那就是说它的shape大于1

238
00:09:40,600 --> 00:09:43,040
而且它的列数也大于1的时候

239
00:09:43,200 --> 00:09:45,280
我就按照每一行axis等于1

240
00:09:45,320 --> 00:09:47,360
按照每一行就argmax

241
00:09:47,560 --> 00:09:48,880
就每一行中

242
00:09:48,920 --> 00:09:51,960
元数值最大的那个小标

243
00:09:52,040 --> 00:09:53,360
乘到y hat里面

244
00:09:53,720 --> 00:09:55,680
这是我的预测的分类的类别

245
00:09:56,840 --> 00:09:57,920
然后因为

246
00:09:58,559 --> 00:10:00,719
我的y hat和我的y可能

247
00:10:00,719 --> 00:10:03,719
我的数据类型可能不一样

248
00:10:03,759 --> 00:10:06,399
那我就把y hat转成y的数据类型

249
00:10:06,519 --> 00:10:07,599
然后做比较

250
00:10:08,360 --> 00:10:11,399
变成一个波的tensor

251
00:10:11,559 --> 00:10:14,679
然后我又把它转成跟y一样的形状

252
00:10:14,719 --> 00:10:15,599
求和

253
00:10:15,959 --> 00:10:18,240
那就是然后转成一个伏点数

254
00:10:18,799 --> 00:10:21,039
那么整个这个函数表示的是说

255
00:10:21,079 --> 00:10:24,839
我们找出来预测正确的样本数

256
00:10:25,559 --> 00:10:27,639
那么再除以整个y的长度

257
00:10:27,639 --> 00:10:29,519
那么就是预测正确的概率

258
00:10:29,919 --> 00:10:30,879
这里我们可以看一下

259
00:10:30,919 --> 00:10:31,840
就是等于0.5了

260
00:10:31,840 --> 00:10:32,759
就是我们是随机

261
00:10:36,879 --> 00:10:37,199
好

262
00:10:37,199 --> 00:10:39,000
接下来这一个函数是说

263
00:10:39,039 --> 00:10:41,600
我给我一个模型

264
00:10:41,600 --> 00:10:43,559
给我一个数据迭代器

265
00:10:43,720 --> 00:10:45,840
我来计算一下这个模型

266
00:10:45,840 --> 00:10:48,080
在这个数据迭代器上的精度

267
00:10:48,919 --> 00:10:49,159
好

268
00:10:49,159 --> 00:10:50,759
首先可以看一下这个实现

269
00:10:50,919 --> 00:10:53,480
就如果我是一个用了touch n

270
00:10:53,480 --> 00:10:55,000
实现的一个模型的话

271
00:10:55,039 --> 00:10:56,799
我把它转成一个评估模式

272
00:10:56,800 --> 00:10:58,440
就是说不要计算t度了

273
00:10:58,440 --> 00:10:59,960
我们只做一个forward pass

274
00:11:01,480 --> 00:11:03,840
接下来对于迭代器中

275
00:11:03,840 --> 00:11:06,240
每一次拿到一个p的x和y

276
00:11:06,880 --> 00:11:10,520
我们先把它的x放到我的net里面

277
00:11:10,520 --> 00:11:12,640
算出我的评测值

278
00:11:12,840 --> 00:11:14,800
然后计算我们所有的

279
00:11:14,800 --> 00:11:16,760
预测正确的样本数

280
00:11:17,240 --> 00:11:20,640
然后这个是我们整个样本的总数

281
00:11:20,840 --> 00:11:23,200
我们放进一个accumulator里面

282
00:11:23,200 --> 00:11:24,360
就是一个迭代器

283
00:11:24,360 --> 00:11:25,800
就是做一个累加器

284
00:11:26,200 --> 00:11:27,880
就是不断的加入累加器

285
00:11:28,680 --> 00:11:31,400
然后最后返回的是说

286
00:11:31,400 --> 00:11:33,360
分类正确的样本数

287
00:11:33,720 --> 00:11:35,840
和总样本数一除

288
00:11:35,840 --> 00:11:37,760
我们就拿到精度了

289
00:11:38,280 --> 00:11:38,800
对吧

290
00:11:39,480 --> 00:11:42,000
所以下面是一个整个accumulator

291
00:11:42,000 --> 00:11:42,680
怎么实现的

292
00:11:42,680 --> 00:11:43,760
其实就是一个很简单的

293
00:11:43,760 --> 00:11:46,480
一个不断做加法的一个过程

294
00:11:46,480 --> 00:11:47,840
我们就不解释了

295
00:11:49,000 --> 00:11:49,920
那么可以看一下

296
00:11:49,920 --> 00:11:52,440
我们随机出来的

297
00:11:52,480 --> 00:11:55,840
模型会是什么样子

298
00:11:56,040 --> 00:11:57,560
随机出来的模型

299
00:11:57,600 --> 00:12:00,200
和我们的测试迭代器

300
00:12:00,720 --> 00:12:03,040
可以看到我们的精度是0.118

301
00:12:03,160 --> 00:12:04,680
因为我们的类别数是10

302
00:12:05,200 --> 00:12:06,400
所以因为是随机的话

303
00:12:06,400 --> 00:12:09,000
那么应该是10%的正确率

304
00:12:09,040 --> 00:12:10,480
所以几乎是这个值

305
00:12:10,480 --> 00:12:12,080
可能我的随机值很近

306
00:12:12,080 --> 00:12:14,080
所以基本上认为是一个随机的

307
00:12:18,120 --> 00:12:18,680
好

308
00:12:18,680 --> 00:12:19,360
这样的话

309
00:12:19,399 --> 00:12:23,519
我就来实现我们整个训练脚本了

310
00:12:23,800 --> 00:12:24,840
这个函数是说

311
00:12:24,840 --> 00:12:27,399
我们在对整个数据迭代一次的时候

312
00:12:27,399 --> 00:12:29,039
我们整个逻辑是什么样子

313
00:12:29,360 --> 00:12:32,120
而且我们这个函数稍微长一点点

314
00:12:32,159 --> 00:12:33,720
这是因为我们要处理

315
00:12:33,879 --> 00:12:35,240
如果我们的模型

316
00:12:35,240 --> 00:12:36,720
要么是手动定义的

317
00:12:36,720 --> 00:12:39,039
要么是用的torch nn的模具

318
00:12:39,039 --> 00:12:40,919
我们两个情况都可以考虑一下

319
00:12:41,120 --> 00:12:41,720
这样子的话

320
00:12:41,720 --> 00:12:43,840
这个函数在之后就可以重用了

321
00:12:44,920 --> 00:12:47,879
首先如果我是用nn模具实现的话

322
00:12:47,879 --> 00:12:48,639
我告诉你说

323
00:12:48,799 --> 00:12:50,360
我要开启我的训练模式

324
00:12:50,399 --> 00:12:52,240
就是告诉pytorch说

325
00:12:52,240 --> 00:12:54,080
我要计算几度

326
00:12:55,319 --> 00:12:58,439
然后我们会用一个长度为3的迭代器

327
00:12:58,439 --> 00:13:00,919
来累加我们的一些需要的信息

328
00:13:01,600 --> 00:13:03,799
接下来就是扫一遍我们的数据

329
00:13:04,039 --> 00:13:04,960
一个for loop

330
00:13:05,600 --> 00:13:07,960
首先我们来计算y hat

331
00:13:08,840 --> 00:13:12,960
然后我们再通过交叉伤损失函数

332
00:13:12,960 --> 00:13:14,200
来计算我们的L

333
00:13:14,919 --> 00:13:16,319
当然我们会考虑说

334
00:13:16,320 --> 00:13:20,280
如果的updater是我的pytorch的

335
00:13:20,280 --> 00:13:21,440
一个optimizer的话

336
00:13:21,640 --> 00:13:22,960
就跟之前定义的一样

337
00:13:23,120 --> 00:13:24,640
我们应该的应该是说

338
00:13:24,680 --> 00:13:26,560
先把t度设成0

339
00:13:26,840 --> 00:13:28,480
然后再计算t度

340
00:13:28,920 --> 00:13:30,879
然后再update一下

341
00:13:30,920 --> 00:13:32,879
就是对我的参数进行一次更新

342
00:13:33,800 --> 00:13:35,400
最后我们会算一下

343
00:13:35,600 --> 00:13:36,800
算一下就是说

344
00:13:37,760 --> 00:13:39,120
做一些样本数

345
00:13:39,480 --> 00:13:41,360
正确的分类数

346
00:13:41,360 --> 00:13:43,120
这些东西放进了一家器里面

347
00:13:44,120 --> 00:13:46,960
如果我们是用的自己实现的话

348
00:13:46,960 --> 00:13:48,320
从头开始实现的话

349
00:13:48,679 --> 00:13:50,799
我们的L出来就是一个向量

350
00:13:51,279 --> 00:13:52,879
我们把它做求和

351
00:13:53,120 --> 00:13:54,879
然后算t度

352
00:13:55,279 --> 00:13:57,320
然后再update一下

353
00:13:57,320 --> 00:13:58,799
就是根据我的p端大小

354
00:13:58,799 --> 00:13:59,919
放进update一下

355
00:14:00,679 --> 00:14:02,039
最后我们同样的是说

356
00:14:02,039 --> 00:14:02,720
我们记录一下

357
00:14:02,720 --> 00:14:04,000
我们分类的正确的个数

358
00:14:04,720 --> 00:14:06,799
最后我们返回的是一个

359
00:14:06,840 --> 00:14:07,639
我们的

360
00:14:08,399 --> 00:14:09,960
你可以看到我们的

361
00:14:10,159 --> 00:14:11,680
第一个是我们的损失

362
00:14:11,880 --> 00:14:13,760
这是我们所有的loss的

363
00:14:13,960 --> 00:14:14,640
累加

364
00:14:14,840 --> 00:14:16,960
除以我们的样本数

365
00:14:17,800 --> 00:14:19,480
除以我们的样本总数

366
00:14:19,520 --> 00:14:21,840
和我们所有分类正确的

367
00:14:22,080 --> 00:14:22,720
样本数

368
00:14:22,720 --> 00:14:24,120
除以我们的总样本数

369
00:14:24,200 --> 00:14:26,400
这是我们最后返回的结果

370
00:14:28,200 --> 00:14:30,840
这是我们对整个数据迭代一次

371
00:14:30,840 --> 00:14:33,400
所需要实现的

372
00:14:35,320 --> 00:14:35,960
那么

373
00:14:36,560 --> 00:14:39,520
这个就是我们要实现一个辅助函数

374
00:14:39,519 --> 00:14:41,720
就是一个小动画

375
00:14:41,840 --> 00:14:44,679
在这可以让大家来实时的看到

376
00:14:45,840 --> 00:14:48,199
我们在训练过程中的变化

377
00:14:48,960 --> 00:14:50,279
我们就不详细解释了

378
00:14:50,439 --> 00:14:51,600
这里面就是一些

379
00:14:51,639 --> 00:14:53,240
怎么样用matplotlib

380
00:14:53,240 --> 00:14:55,279
来画各种图的一个东西

381
00:14:56,399 --> 00:14:56,919
好

382
00:14:58,279 --> 00:14:59,720
这是最后我们的

383
00:14:59,919 --> 00:15:00,840
所有的函数

384
00:15:00,840 --> 00:15:03,079
就是在这个train里面

385
00:15:03,639 --> 00:15:06,720
ch3就是我们是第3章的训练函数

386
00:15:06,759 --> 00:15:08,319
我们在之后还会有

387
00:15:08,320 --> 00:15:10,440
不断的去完善这个训练函数

388
00:15:10,760 --> 00:15:12,000
使它变得更复杂

389
00:15:12,200 --> 00:15:14,600
这里还是相对来说比较简单

390
00:15:15,000 --> 00:15:16,800
首先我们定了我们的一个

391
00:15:16,800 --> 00:15:18,160
可视化的一个animator

392
00:15:18,320 --> 00:15:19,440
就大家可以忽略掉

393
00:15:19,640 --> 00:15:21,120
这个具体参数怎么样子

394
00:15:21,360 --> 00:15:22,800
它的核心是说

395
00:15:22,840 --> 00:15:24,680
我给我一个num of epochs

396
00:15:24,720 --> 00:15:27,000
就是我扫n遍数据

397
00:15:27,440 --> 00:15:30,440
首先我们就训练第一次

398
00:15:30,440 --> 00:15:30,760
对吧

399
00:15:30,760 --> 00:15:31,480
训练一次

400
00:15:31,480 --> 00:15:32,280
就是跟

401
00:15:32,360 --> 00:15:34,120
按照刚刚我们定的函数

402
00:15:34,240 --> 00:15:35,480
对整个数据扫一次

403
00:15:35,480 --> 00:15:36,720
更新我们的模型

404
00:15:37,040 --> 00:15:38,720
然后我们把训练的一些

405
00:15:38,720 --> 00:15:40,000
一些matrix拿下来

406
00:15:40,000 --> 00:15:42,000
就是训练的一些误差

407
00:15:42,519 --> 00:15:44,639
接下来我们要在我们的

408
00:15:44,920 --> 00:15:46,519
在测试数据集上

409
00:15:46,519 --> 00:15:48,440
我们来评估一下我的精度

410
00:15:48,680 --> 00:15:50,560
放在一个测试的精度

411
00:15:50,960 --> 00:15:53,040
然后我们就在我们的animator里面

412
00:15:53,080 --> 00:15:53,840
显示一下

413
00:15:53,840 --> 00:15:55,120
我们的训练的误差

414
00:15:55,360 --> 00:15:56,320
训练的精度

415
00:15:56,560 --> 00:15:57,399
测试的误差

416
00:15:57,399 --> 00:15:58,200
测试的精度

417
00:15:58,600 --> 00:15:59,160
OK

418
00:16:00,320 --> 00:16:01,759
这个就是我们的

419
00:16:01,960 --> 00:16:03,160
整个的逻辑

420
00:16:05,120 --> 00:16:06,600
最后的最后就是

421
00:16:06,879 --> 00:16:07,639
跟之前一样

422
00:16:07,639 --> 00:16:09,399
我们实现了我们的SGD

423
00:16:09,800 --> 00:16:11,320
我们把SGD包装一下

424
00:16:11,519 --> 00:16:13,200
我们的参数是W和B

425
00:16:13,440 --> 00:16:16,040
能力rate是我们设成了1.1了

426
00:16:16,360 --> 00:16:17,560
我们的批量大小

427
00:16:17,920 --> 00:16:19,120
这是我们的updater

428
00:16:19,639 --> 00:16:20,040
好

429
00:16:20,040 --> 00:16:21,759
接下来我们就可以开始训练了

430
00:16:22,000 --> 00:16:24,840
我们这里训练10个迭代周期

431
00:16:25,000 --> 00:16:27,800
给大家看一下我们的训练的过程

432
00:16:28,879 --> 00:16:31,279
首先我们因为我们有个animator函数

433
00:16:31,320 --> 00:16:33,040
所以它会不断去显示

434
00:16:33,120 --> 00:16:34,720
每一次扫完数据之后

435
00:16:34,759 --> 00:16:35,879
我们的变化

436
00:16:36,159 --> 00:16:37,399
可以看到是说

437
00:16:37,439 --> 00:16:38,360
蓝色的线

438
00:16:38,360 --> 00:16:40,480
就是我们的训练损失

439
00:16:40,799 --> 00:16:41,759
我们的训练损失

440
00:16:41,759 --> 00:16:43,240
就是cross entropy的loss

441
00:16:43,399 --> 00:16:45,480
会随着我的不断的下降

442
00:16:45,720 --> 00:16:48,679
然后我的红色的线

443
00:16:48,720 --> 00:16:52,039
是我的全训练数据机上的精度

444
00:16:52,360 --> 00:16:53,639
然后我的绿色的线

445
00:16:53,639 --> 00:16:55,720
是我的测试上面的精度

446
00:16:56,120 --> 00:16:58,639
因为我们一般关心的是测试的精度

447
00:16:58,840 --> 00:17:00,039
所以看到是说

448
00:17:00,159 --> 00:17:01,840
一开始在这个地方

449
00:17:02,000 --> 00:17:04,319
随着一开始其实在这个地方

450
00:17:04,319 --> 00:17:04,759
对吧

451
00:17:04,759 --> 00:17:06,879
因为这是第一次迭代之后的

452
00:17:07,000 --> 00:17:08,960
扫了一遍数据之后的事

453
00:17:09,200 --> 00:17:11,319
所以随着我的训练在进行

454
00:17:11,319 --> 00:17:13,400
我的精度在不断的上升

455
00:17:13,599 --> 00:17:15,160
最后在这个点的时候

456
00:17:15,279 --> 00:17:17,039
其实也还没有完全收敛

457
00:17:17,039 --> 00:17:20,200
可以看到我们的损失还在往下降

458
00:17:20,400 --> 00:17:22,000
为了时间关系

459
00:17:22,000 --> 00:17:23,640
我们就只跑10个周期

460
00:17:23,680 --> 00:17:25,799
大家可以试一试跑更长会怎么样

461
00:17:26,599 --> 00:17:29,079
但是也许可以看到是说

462
00:17:29,079 --> 00:17:31,720
我的精度会还是会有一点点提升

463
00:17:32,039 --> 00:17:34,400
大家可以自己去实验一下

464
00:17:34,920 --> 00:17:37,640
通过设置不同的数据周期

465
00:17:37,880 --> 00:17:40,320
然后不同的学习率

466
00:17:40,320 --> 00:17:43,160
来大家可以来实验一下不同的效果

467
00:17:43,720 --> 00:17:44,800
这就是训练

468
00:17:45,440 --> 00:17:47,600
最后当然我们可以做一个预测了

469
00:17:47,960 --> 00:17:49,120
预测是说

470
00:17:49,320 --> 00:17:51,480
我来预测一些

471
00:17:51,519 --> 00:17:53,759
就是说我对我的测试的数据集

472
00:17:53,759 --> 00:17:55,440
里面拿出一个样本

473
00:17:55,800 --> 00:17:56,960
然后我们来

474
00:17:57,240 --> 00:17:59,840
把它的真实的标号拿出来

475
00:18:00,000 --> 00:18:02,400
然后我的预测标号也拿出来

476
00:18:02,519 --> 00:18:04,280
就通过我们之前定义的

477
00:18:04,920 --> 00:18:06,560
把我们预测的一些数值

478
00:18:06,560 --> 00:18:10,000
变成真正的字符串的标号

479
00:18:10,200 --> 00:18:11,600
让我们再plot一下

480
00:18:12,160 --> 00:18:13,440
基本上可以看到

481
00:18:13,640 --> 00:18:15,400
这个地方我们运气还不错

482
00:18:15,640 --> 00:18:18,000
就是我们看了6张图片

483
00:18:18,160 --> 00:18:21,120
基本上是说最上面的是我的真实标号

484
00:18:21,160 --> 00:18:22,680
下面是我的预测标号

485
00:18:22,759 --> 00:18:24,040
可以看到是说

486
00:18:24,440 --> 00:18:25,680
我们都做对了

487
00:18:25,920 --> 00:18:27,640
这个是运气

488
00:18:27,640 --> 00:18:29,480
因为可以看到我们大概精度

489
00:18:29,480 --> 00:18:30,600
也就80%

490
00:18:30,600 --> 00:18:31,519
90%的样子

491
00:18:31,920 --> 00:18:33,240
所以你当然画多一点

492
00:18:33,240 --> 00:18:35,119
你会看到有一些做错的地方

493
00:18:36,160 --> 00:18:36,599
好

494
00:18:36,599 --> 00:18:40,319
这个就是我们的从头开始实现

495
00:18:40,599 --> 00:18:41,960
softmax回归

