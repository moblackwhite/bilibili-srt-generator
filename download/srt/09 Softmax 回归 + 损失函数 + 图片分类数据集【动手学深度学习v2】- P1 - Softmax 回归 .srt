1
00:00:00,000 --> 00:00:02,240
第一个问题是说

2
00:00:03,520 --> 00:00:07,799
能提一下soft label的训练策略吗

3
00:00:07,799 --> 00:00:09,080
以及为什么有效

4
00:00:09,080 --> 00:00:11,599
soft label是说

5
00:00:11,599 --> 00:00:14,200
我们刚刚有讲到

6
00:00:14,240 --> 00:00:15,359
我们怎么用

7
00:00:15,359 --> 00:00:19,000
一味有效来表示一个标号

8
00:00:19,000 --> 00:00:19,879
就是说

9
00:00:19,960 --> 00:00:21,039
你有n类的话

10
00:00:21,039 --> 00:00:22,879
我就把你变成一个很长的项量

11
00:00:22,960 --> 00:00:24,960
其中只有正确那一类为一

12
00:00:24,960 --> 00:00:27,879
剩下的所有的全部变成零

13
00:00:28,039 --> 00:00:31,759
所以然后我们用一个softmax去逼近

14
00:00:32,640 --> 00:00:33,920
纯零一的分布

15
00:00:34,439 --> 00:00:35,679
但是它的问题是什么

16
00:00:36,239 --> 00:00:37,560
它的问题是说

17
00:00:37,960 --> 00:00:40,200
你用了指数的话

18
00:00:40,239 --> 00:00:44,599
你很难用指数去逼近一个1

19
00:00:44,719 --> 00:00:46,599
因为指数变成1的话

20
00:00:46,599 --> 00:00:49,120
你想让它完全变成1的话

21
00:00:49,120 --> 00:00:51,519
你必须是你的输出

22
00:00:51,519 --> 00:00:53,320
几乎接近于无穷大

23
00:00:53,359 --> 00:00:55,480
而剩下的东西都很小

24
00:00:55,520 --> 00:00:58,000
这就是挺难用softmax

25
00:00:58,000 --> 00:01:01,240
是逼近一个0和1的一个极端的数值

26
00:01:02,200 --> 00:01:04,480
所以他们提出了一个方案是说

27
00:01:04,719 --> 00:01:05,840
我就不用那么极端

28
00:01:06,240 --> 00:01:07,040
我就说

29
00:01:07,040 --> 00:01:09,240
如果你是正确那一类

30
00:01:09,400 --> 00:01:11,200
我就把你记成0.9

31
00:01:11,359 --> 00:01:13,040
你剩下那些不正确的类

32
00:01:13,040 --> 00:01:15,560
那就是0.1除以n分之一

33
00:01:15,680 --> 00:01:17,760
这就是soft label

34
00:01:17,880 --> 00:01:19,000
这样的好处是说

35
00:01:19,000 --> 00:01:21,400
使得你用softmax

36
00:01:21,400 --> 00:01:23,680
真的去完全你和0.9

37
00:01:23,720 --> 00:01:25,160
和那些很小的数的时候

38
00:01:25,160 --> 00:01:26,200
是有可能的

39
00:01:26,680 --> 00:01:29,000
这个是一个常用的技巧

40
00:01:29,320 --> 00:01:31,200
在图片分类里面

41
00:01:31,400 --> 00:01:32,480
这是默认的

42
00:01:32,480 --> 00:01:34,480
大家会使用的一个小trick

43
00:01:36,880 --> 00:01:37,880
第二个问题

44
00:01:38,440 --> 00:01:42,080
softmax回归和logistic regression

45
00:01:42,080 --> 00:01:43,719
它是不是一样的

46
00:01:44,160 --> 00:01:45,480
如果不一样的话

47
00:01:45,480 --> 00:01:46,920
哪些地方不一样

48
00:01:47,480 --> 00:01:49,360
其实你可以认为它是一样的

49
00:01:49,719 --> 00:01:50,480
就是说

50
00:01:50,920 --> 00:01:52,200
logistic regression

51
00:01:52,799 --> 00:01:53,880
你可以认为是一个

52
00:01:53,880 --> 00:01:55,600
当你只有两类的时候

53
00:01:55,600 --> 00:01:57,200
你做softmax会怎么样

54
00:01:58,640 --> 00:02:00,200
就是说等于是你输出

55
00:02:00,200 --> 00:02:00,879
只有两个

56
00:02:00,879 --> 00:02:02,280
一个正义一个负义

57
00:02:02,879 --> 00:02:05,359
但是注意到是说我的softmax

58
00:02:05,520 --> 00:02:07,560
它的加起来是等于1的

59
00:02:07,680 --> 00:02:10,400
所以如果我就做一个0和1分布的话

60
00:02:10,400 --> 00:02:11,520
我根本不需要管

61
00:02:11,520 --> 00:02:13,400
我只要预测为0的

62
00:02:13,840 --> 00:02:15,319
或者正义或者负义

63
00:02:15,319 --> 00:02:17,520
我只要预测负义的那一个类

64
00:02:18,080 --> 00:02:18,960
因为你正义

65
00:02:18,960 --> 00:02:20,000
我只要预测了负义

66
00:02:20,000 --> 00:02:22,199
正义那就一定是1减去概率

67
00:02:22,879 --> 00:02:24,400
这就是logistic regression

68
00:02:24,400 --> 00:02:25,680
和softmax的区别

69
00:02:25,680 --> 00:02:28,319
所以logistic regression

70
00:02:28,319 --> 00:02:30,439
虽然是一个二分类的问题

71
00:02:30,560 --> 00:02:34,120
但是它实际上只要输出一个元素

72
00:02:34,120 --> 00:02:37,120
你可以是要么是对正义类的预测

73
00:02:37,120 --> 00:02:38,479
要么是负义类的预测

74
00:02:38,520 --> 00:02:39,639
那么你剩下的一个类

75
00:02:39,680 --> 00:02:41,120
那就是等于1减去它

76
00:02:41,280 --> 00:02:43,759
所以你就是可以认为是

77
00:02:43,920 --> 00:02:45,599
logistic regression

78
00:02:45,599 --> 00:02:48,439
是softmax regression的一个特例

79
00:02:48,599 --> 00:02:49,599
一般来说

80
00:02:49,599 --> 00:02:51,919
我们在接下来深度学习中

81
00:02:51,919 --> 00:02:54,560
很少遇到两分类的问题

82
00:02:54,560 --> 00:02:56,759
所以我们就是直接跳过了

83
00:02:56,759 --> 00:02:57,800
logistic regression

84
00:02:57,800 --> 00:02:59,639
直接讲softmax回归

85
00:03:01,199 --> 00:03:02,120
第三个问题

86
00:03:02,120 --> 00:03:04,439
和第二个问题其实是一样的

87
00:03:05,159 --> 00:03:07,079
第4个问题是说

88
00:03:07,400 --> 00:03:10,719
为什么使用交叉商

89
00:03:10,719 --> 00:03:12,240
而不用相对商

90
00:03:12,240 --> 00:03:16,719
或者别的信息衡量标准

91
00:03:17,120 --> 00:03:19,080
大家如果学过的话

92
00:03:19,240 --> 00:03:21,840
知道相对商mutual information

93
00:03:21,840 --> 00:03:24,639
就表示的是一个两个

94
00:03:25,039 --> 00:03:27,639
统计两个概率之间的一个区别

95
00:03:27,879 --> 00:03:30,680
它比交叉商的好处是说

96
00:03:30,680 --> 00:03:31,639
它是没有

97
00:03:31,639 --> 00:03:33,360
它是一个对称的关系

98
00:03:33,599 --> 00:03:34,479
就是说

99
00:03:35,000 --> 00:03:36,439
现在h p q

100
00:03:36,439 --> 00:03:38,039
它不等于h q p

101
00:03:38,079 --> 00:03:39,960
但是相互商量

102
00:03:39,960 --> 00:03:42,159
就是i p q是等于i q p的

103
00:03:43,960 --> 00:03:45,439
至于为什么不用

104
00:03:45,680 --> 00:03:48,079
其实也没有特别大的原因不用

105
00:03:48,400 --> 00:03:50,560
互信息最大的问题是不好算

106
00:03:50,840 --> 00:03:51,919
那种算起来

107
00:03:52,240 --> 00:03:54,479
不如交叉商来的那么简单

108
00:03:54,800 --> 00:03:55,719
对于我们来讲

109
00:03:55,719 --> 00:03:58,039
其实我们真的只关心一个

110
00:03:58,479 --> 00:04:00,079
两个分布的一个距离

111
00:04:00,079 --> 00:04:02,240
就跟我们讲过的损失函数一样

112
00:04:02,520 --> 00:04:03,560
你用L1

113
00:04:03,560 --> 00:04:04,319
用L2

114
00:04:04,319 --> 00:04:06,319
或者用Hubbard的loss

115
00:04:06,319 --> 00:04:08,039
其实差不多

116
00:04:08,240 --> 00:04:08,960
最后的最后

117
00:04:08,960 --> 00:04:10,000
我们会选一个

118
00:04:10,199 --> 00:04:12,000
算起来相对来说简单一点的

119
00:04:12,560 --> 00:04:16,120
第5个问题

120
00:04:17,120 --> 00:04:19,480
Y产业log

121
00:04:19,480 --> 00:04:20,360
Y hat

122
00:04:20,360 --> 00:04:22,839
我们只关心正确的类

123
00:04:22,920 --> 00:04:24,680
不关心不正确的类

124
00:04:24,720 --> 00:04:27,720
如果不关心不正确的类的效果

125
00:04:27,720 --> 00:04:28,959
有没有可能更好

126
00:04:29,639 --> 00:04:33,759
其实我们不是不关心不正确的类

127
00:04:33,759 --> 00:04:34,560
是因为我们

128
00:04:37,040 --> 00:04:39,079
Watt的编码

129
00:04:39,719 --> 00:04:43,000
把剩下的类的概率变成0了

130
00:04:43,000 --> 00:04:45,000
所以导致我们计算的时候

131
00:04:45,000 --> 00:04:47,680
可以忽略掉不正确的类

132
00:04:47,919 --> 00:04:51,560
如果我们使用之前我们提到的soft label

133
00:04:51,560 --> 00:04:52,919
那就是不正确的类

134
00:04:52,919 --> 00:04:56,000
它是也是有存在一个非0的概率的情况下

135
00:04:56,399 --> 00:04:58,839
我们确实会关心不正确的类了

136
00:04:59,799 --> 00:05:00,279
OK

137
00:05:00,279 --> 00:05:01,479
我再刷新一下

138
00:05:01,680 --> 00:05:02,240
看看

139
00:05:02,680 --> 00:05:03,560
还有没有

140
00:05:04,560 --> 00:05:05,399
别的问题

141
00:05:09,279 --> 00:05:13,399
这样的n类问题

142
00:05:13,399 --> 00:05:15,039
对于一个类别来说

143
00:05:15,039 --> 00:05:17,120
是不是认为只有一个正类

144
00:05:17,120 --> 00:05:18,599
n-1个负类

145
00:05:18,599 --> 00:05:21,199
会不会类别不平衡

146
00:05:21,199 --> 00:05:24,240
会有这样子的情况

147
00:05:24,399 --> 00:05:26,519
但是相对来说好处是说

148
00:05:26,560 --> 00:05:29,399
可以看到我们刚刚损失函数

149
00:05:29,399 --> 00:05:31,799
如果是你用01这个编码的话

150
00:05:31,839 --> 00:05:34,680
其实我并不关心别的类的一个

151
00:05:34,799 --> 00:05:36,479
别的类会怎么样

152
00:05:36,479 --> 00:05:38,120
所以我只关心当前类

153
00:05:38,120 --> 00:05:39,160
或者是说

154
00:05:39,160 --> 00:05:41,079
其实大家不用关心说

155
00:05:41,079 --> 00:05:42,399
是不是不平衡

156
00:05:42,399 --> 00:05:43,759
你要关心的是说

157
00:05:43,759 --> 00:05:45,240
是不是存在一些类

158
00:05:45,240 --> 00:05:47,560
它那些类没有足够多的样本

159
00:05:48,120 --> 00:05:49,280
没有足够多的样本

160
00:05:49,280 --> 00:05:50,480
那就会比较麻烦

161
00:05:50,639 --> 00:05:53,120
如果你每一个类有足够多的样本

162
00:05:53,120 --> 00:05:55,319
比如说img net每一个类

163
00:05:55,319 --> 00:05:57,879
我大概都有5000个样本的情况下

164
00:05:58,040 --> 00:06:00,920
那么其实我们是每个类还是差不多的

165
00:06:04,199 --> 00:06:06,160
另外一个问题是说

166
00:06:06,439 --> 00:06:08,800
损失函数的红色箭头

167
00:06:08,800 --> 00:06:11,640
和橙色的线之间的关系

168
00:06:11,680 --> 00:06:13,240
再讲解一下

169
00:06:14,120 --> 00:06:16,080
可以我们来可以讲解一下

170
00:06:17,760 --> 00:06:18,760
就是说

171
00:06:19,120 --> 00:06:21,160
其实就是一个很直观的表示

172
00:06:21,360 --> 00:06:24,040
就是这个箭头就表示

173
00:06:24,040 --> 00:06:24,920
我们还记不记得

174
00:06:24,920 --> 00:06:26,560
就是在我们讲

175
00:06:28,240 --> 00:06:29,680
T度下降的时候

176
00:06:29,720 --> 00:06:31,280
每一次我有一个点

177
00:06:31,320 --> 00:06:32,520
然后我沿着这个点

178
00:06:32,520 --> 00:06:34,360
沿着这个往前走一步

179
00:06:34,680 --> 00:06:36,639
那么步长取决于两个东西

180
00:06:36,639 --> 00:06:37,199
是不是

181
00:06:37,240 --> 00:06:38,600
一个是我的T度

182
00:06:38,600 --> 00:06:39,879
那个本身的大小

183
00:06:39,960 --> 00:06:41,439
一个是我的学习率

184
00:06:42,240 --> 00:06:43,199
这里我们假设

185
00:06:43,199 --> 00:06:45,040
我们的学习率是固定的情况下

186
00:06:45,160 --> 00:06:47,639
那么我的步长

187
00:06:47,639 --> 00:06:49,120
就真的取决于我的T度

188
00:06:49,120 --> 00:06:50,639
那些数字的大小

189
00:06:51,600 --> 00:06:54,759
数值大小就由橙色线来表示

190
00:06:55,040 --> 00:06:57,120
所以这个线就意味着是说

191
00:06:57,120 --> 00:06:59,840
我不管是在正义还是负义

192
00:06:59,840 --> 00:07:01,480
你T度的绝对值

193
00:07:01,480 --> 00:07:02,639
就是那个长度

194
00:07:02,680 --> 00:07:03,560
其实是一样的

195
00:07:03,560 --> 00:07:05,280
所以你在不管哪一个地方

196
00:07:05,280 --> 00:07:06,399
往前走一步

197
00:07:06,439 --> 00:07:08,759
你的在你基本上学习率

198
00:07:08,759 --> 00:07:09,959
在不变的情况下

199
00:07:10,280 --> 00:07:12,000
那么你的步长都差不多

200
00:07:12,000 --> 00:07:14,000
所以这里的意思是说

201
00:07:14,040 --> 00:07:15,920
每一个红色的箭头的步

202
00:07:16,120 --> 00:07:17,360
发的差不多

203
00:07:17,920 --> 00:07:19,759
假设我要往前走的话

204
00:07:19,759 --> 00:07:21,439
比如说是L2 loss的话

205
00:07:21,639 --> 00:07:23,040
那么可以看到是说

206
00:07:23,360 --> 00:07:25,720
跟原点隔得比较远的地方

207
00:07:25,759 --> 00:07:27,759
我的T度的绝对值比较大

208
00:07:27,920 --> 00:07:28,840
比较大的话

209
00:07:28,840 --> 00:07:30,840
在学习率固定的情况下

210
00:07:30,840 --> 00:07:33,079
那么这个步长就会显得会大一点

211
00:07:33,920 --> 00:07:35,399
反过来说

212
00:07:35,399 --> 00:07:37,280
到了快到零点的时候

213
00:07:37,319 --> 00:07:39,600
我的T度的绝对值变小

214
00:07:39,600 --> 00:07:41,280
那么我的步长也会变小

215
00:07:41,439 --> 00:07:43,040
就是说你可以认为是说

216
00:07:43,280 --> 00:07:44,480
这个损失函数

217
00:07:44,480 --> 00:07:47,199
是怎么样把我一个远的一个节

218
00:07:47,879 --> 00:07:50,079
拉向我的最优节

219
00:07:50,279 --> 00:07:51,240
L2的话

220
00:07:51,240 --> 00:07:53,000
就是说如果你跟我离得很远

221
00:07:53,120 --> 00:07:54,199
我就会尽力的把你

222
00:07:54,199 --> 00:07:55,800
很快的把你拉过来

223
00:07:55,840 --> 00:07:56,920
如果是L1的话

224
00:07:57,040 --> 00:07:58,360
我就是一个均匀的速度

225
00:07:58,360 --> 00:07:59,160
把你拉过来

226
00:08:00,079 --> 00:08:00,560
OK

227
00:08:03,639 --> 00:08:05,120
问题8是说

228
00:08:06,199 --> 00:08:10,399
对于MSE和最大四安的估计

229
00:08:10,720 --> 00:08:12,480
能不能稍微提一下

230
00:08:13,040 --> 00:08:15,600
其实和第9问题是差不多的

231
00:08:15,600 --> 00:08:16,399
就是说

232
00:08:16,879 --> 00:08:18,680
四安函数的曲线

233
00:08:18,680 --> 00:08:19,759
是怎么得出来的

234
00:08:21,160 --> 00:08:22,600
我们之所以没有讲

235
00:08:22,600 --> 00:08:23,920
是因为整个这一块

236
00:08:23,920 --> 00:08:25,639
其实统计里面的概念

237
00:08:26,160 --> 00:08:29,279
我们其实尽量的

238
00:08:29,319 --> 00:08:31,759
没有涉及到太多的统计

239
00:08:31,759 --> 00:08:32,639
因为为什么

240
00:08:32,639 --> 00:08:34,799
是因为统计是可以用来

241
00:08:34,799 --> 00:08:36,399
解释我们模型的一个工具

242
00:08:36,559 --> 00:08:37,679
但反过来讲

243
00:08:37,720 --> 00:08:40,000
深度学习在后面的模型

244
00:08:40,000 --> 00:08:42,000
跟统计没有太多的关系

245
00:08:42,039 --> 00:08:44,399
所以我们这一次主要讲的是

246
00:08:44,399 --> 00:08:45,199
线性代数

247
00:08:45,199 --> 00:08:47,120
因为你的所谓的数据结构

248
00:08:47,120 --> 00:08:48,080
是一个线性代数

249
00:08:49,159 --> 00:08:52,319
所以我们可以大概简单讲一下

250
00:08:53,000 --> 00:08:55,600
最小化我的一个损失

251
00:08:56,120 --> 00:08:58,120
就等价于最大化

252
00:08:58,120 --> 00:08:59,720
我的一个四安函数

253
00:09:01,159 --> 00:09:02,200
就是说四安函数

254
00:09:02,600 --> 00:09:05,160
我有个模型

255
00:09:06,080 --> 00:09:11,040
然后我们去给定数据的情况下

256
00:09:11,080 --> 00:09:12,160
我们这个模型

257
00:09:12,160 --> 00:09:13,520
就是说所谓的模型

258
00:09:13,520 --> 00:09:14,920
就是我的权重

259
00:09:14,960 --> 00:09:16,640
出现的概率需要多大

260
00:09:17,480 --> 00:09:19,160
那么我们需要最大化

261
00:09:19,200 --> 00:09:21,240
我们也给定我们观察的东西

262
00:09:21,240 --> 00:09:22,520
我们要找到一个模型

263
00:09:22,520 --> 00:09:23,800
使得给定数据

264
00:09:23,800 --> 00:09:25,400
我们出现的概率是最大的

265
00:09:25,440 --> 00:09:27,520
最合理的解释

266
00:09:28,280 --> 00:09:30,160
所以就是说

267
00:09:30,959 --> 00:09:34,159
你可以看到我们这个线

268
00:09:34,679 --> 00:09:36,439
虽然我们是要最小化

269
00:09:36,439 --> 00:09:38,399
我们的损失函数

270
00:09:38,519 --> 00:09:40,279
也就是等价于最大化

271
00:09:40,279 --> 00:09:41,959
我们最大四安函数

272
00:09:42,199 --> 00:09:43,480
最大四安在这里

273
00:09:43,480 --> 00:09:44,959
其实可以等价是一个

274
00:09:46,039 --> 00:09:46,799
统计上来说

275
00:09:46,799 --> 00:09:48,000
叫做一个指数

276
00:09:49,039 --> 00:09:50,199
family的一个分布

277
00:09:50,919 --> 00:09:51,639
所以的话

278
00:09:51,639 --> 00:09:52,919
你就是有可以对应到

279
00:09:52,919 --> 00:09:54,159
概率那一套东西

280
00:09:54,199 --> 00:09:55,919
但我们并没有特别讲

281
00:09:55,919 --> 00:09:58,959
所以大家可以去看一下书

282
00:09:58,960 --> 00:10:00,759
书里面确实是有讲四安函数

283
00:10:00,759 --> 00:10:01,360
怎么样子

284
00:10:01,400 --> 00:10:03,840
如果大家想更深刻理解的话

285
00:10:03,840 --> 00:10:05,480
可以学一下统计

286
00:10:05,720 --> 00:10:08,360
或者说统计学习

287
00:10:08,360 --> 00:10:10,080
就是statistic learning

288
00:10:10,080 --> 00:10:11,360
我们会稍微讲一点点

289
00:10:11,360 --> 00:10:12,879
但是不会深入太多

290
00:10:18,639 --> 00:10:20,000
OK第10个问题

291
00:10:20,000 --> 00:10:22,040
其实我们也有讲过

292
00:10:25,480 --> 00:10:26,800
其实就是说

293
00:10:26,960 --> 00:10:28,160
当你的

294
00:10:29,080 --> 00:10:30,480
就是说这个问题是说

295
00:10:30,480 --> 00:10:32,560
在不同的损失函数

296
00:10:32,560 --> 00:10:33,560
下降的速度

297
00:10:33,560 --> 00:10:35,400
和昨天讲的学习率的关系

298
00:10:35,400 --> 00:10:36,320
是什么样子

299
00:10:36,720 --> 00:10:37,840
昨天理解的学习率

300
00:10:37,840 --> 00:10:39,560
好像是t度下降的步长

301
00:10:39,840 --> 00:10:41,560
步长和速度有点搞不清

302
00:10:41,800 --> 00:10:42,600
就是说

303
00:10:42,720 --> 00:10:43,760
我们刚刚也讲过

304
00:10:44,640 --> 00:10:46,040
其实你w

305
00:10:46,320 --> 00:10:48,440
每次对它的做更新的时候

306
00:10:48,440 --> 00:10:49,080
是有两项

307
00:10:49,080 --> 00:10:49,600
对吧

308
00:10:49,640 --> 00:10:51,960
一项是你的负的t度方向

309
00:10:52,000 --> 00:10:53,360
一项是你的学习率

310
00:10:54,000 --> 00:10:56,680
所以你就是被两个地方控制

311
00:10:56,680 --> 00:10:58,040
一个是你t度

312
00:10:58,040 --> 00:10:59,800
你的每个数值有多少

313
00:11:00,560 --> 00:11:02,200
另外一个是说你的学习率

314
00:11:02,200 --> 00:11:03,640
就是我是人为控制的

315
00:11:04,040 --> 00:11:06,600
所以这里的理解是说

316
00:11:06,640 --> 00:11:09,040
假设我的学习率是固定的情况下

317
00:11:09,440 --> 00:11:11,440
那么我不同的损失函数

318
00:11:11,440 --> 00:11:13,320
会导致我算t度的值

319
00:11:13,320 --> 00:11:14,200
会发生变化

320
00:11:14,680 --> 00:11:16,440
就是说在别的全部固定

321
00:11:16,440 --> 00:11:18,040
就是学习率固定的情况下

322
00:11:18,080 --> 00:11:19,240
不同的损失函数

323
00:11:19,240 --> 00:11:21,600
会导致在不同的地方

324
00:11:21,600 --> 00:11:24,320
会带来不一样的t度的那一些值

325
00:11:24,360 --> 00:11:25,760
使得我们的走的

326
00:11:26,120 --> 00:11:28,480
每次走的步伐会有点不一样

327
00:11:32,519 --> 00:11:35,519
第11个问题是说

328
00:11:35,559 --> 00:11:36,799
order regression

329
00:11:36,960 --> 00:11:38,120
其实我不是特别理解

330
00:11:38,120 --> 00:11:39,120
order regression是什么

331
00:11:39,120 --> 00:11:40,759
我假设你是说ranking

332
00:11:41,319 --> 00:11:42,639
所谓的ranking就是说

333
00:11:42,679 --> 00:11:44,000
你得去排序

334
00:11:44,840 --> 00:11:47,080
你说A应该排在B的前面

335
00:11:47,200 --> 00:11:48,480
那就意味着说

336
00:11:48,679 --> 00:11:49,720
其实我并不关心

337
00:11:49,720 --> 00:11:50,759
你的概率什么样子

338
00:11:50,759 --> 00:11:51,960
你的数值是什么样子

339
00:11:51,960 --> 00:11:53,840
我只关心我们的相对值

340
00:11:54,320 --> 00:11:56,120
其实你可以用softmax来做

341
00:11:56,360 --> 00:11:57,600
其实也问题不大

342
00:11:57,600 --> 00:11:59,680
但是你可以把它做成一个ranking的问题

343
00:11:59,680 --> 00:12:01,519
ranking就是用margin来做

344
00:12:02,480 --> 00:12:05,440
其实说一张话会有一点点区别

345
00:12:05,440 --> 00:12:09,280
但实际使用中区别不那么大

346
00:12:09,720 --> 00:12:10,840
其实你那些区别

347
00:12:10,840 --> 00:12:12,560
跟你的数据

348
00:12:12,560 --> 00:12:14,399
跟你的最后的你模型

349
00:12:14,399 --> 00:12:16,560
其实差别还是挺明显的

350
00:12:16,560 --> 00:12:18,080
所以每个地方都有区别

351
00:12:18,400 --> 00:12:21,400
所以当然可以用

352
00:12:21,400 --> 00:12:22,840
softmax总是可以用的

353
00:12:24,040 --> 00:12:25,399
OK我们应该是

354
00:12:25,399 --> 00:12:27,840
这也是我们的所有问题

355
00:12:27,840 --> 00:12:28,639
我再刷新一下

356
00:12:30,040 --> 00:12:31,240
刚刚有同学问到

357
00:12:31,480 --> 00:12:33,920
我们是在直播还是在录播

358
00:12:34,160 --> 00:12:35,440
现在是在直播

359
00:12:35,639 --> 00:12:37,519
不过今天我确实想试一下

360
00:12:37,720 --> 00:12:39,160
是不是能提前录一下

361
00:12:39,160 --> 00:12:40,360
效果会不会好

362
00:12:40,720 --> 00:12:43,000
我其实在大概三个小时之前

363
00:12:43,160 --> 00:12:44,000
录了几段

364
00:12:44,360 --> 00:12:45,480
主要有两个目的

365
00:12:45,639 --> 00:12:47,800
我们刚好时间还有24分钟

366
00:12:47,800 --> 00:12:49,000
我们应该来得及

367
00:12:49,360 --> 00:12:52,960
聊一下我们整个是怎么设置的

368
00:12:53,560 --> 00:12:54,960
我们我今天想试一下

369
00:12:54,960 --> 00:12:55,840
录制一下

370
00:12:55,840 --> 00:12:56,320
为什么

371
00:12:56,519 --> 00:12:58,320
是因为前面几次

372
00:12:58,519 --> 00:13:00,840
我们有一次是摄像头

373
00:13:01,280 --> 00:13:02,440
相机有点发热

374
00:13:03,720 --> 00:13:05,639
另外一块就是有一次是说

375
00:13:05,639 --> 00:13:06,680
我们解释一个东西

376
00:13:06,680 --> 00:13:07,920
其实是解释错了

377
00:13:07,960 --> 00:13:09,400
还有一次其实是

378
00:13:09,759 --> 00:13:12,840
我们的有一个我们的notebook里面

379
00:13:12,840 --> 00:13:16,080
没有slides没有写出来

380
00:13:16,240 --> 00:13:19,360
所以我感觉这些东西都会给直播

381
00:13:19,519 --> 00:13:20,920
给大家浪费大家时间

382
00:13:20,920 --> 00:13:22,639
特别是浪费大家几分钟时间

383
00:13:22,960 --> 00:13:24,240
所以我在想说

384
00:13:25,120 --> 00:13:28,280
先试着我们先录一次

385
00:13:28,280 --> 00:13:29,000
录一次的话

386
00:13:29,000 --> 00:13:30,080
如果中间出现问题

387
00:13:30,080 --> 00:13:31,360
我可以把它剪掉

388
00:13:31,440 --> 00:13:34,400
所以就说不用去节省大家的时间

389
00:13:35,120 --> 00:13:36,519
第二个也是看一下

390
00:13:36,519 --> 00:13:40,480
我自己能不能承受开销

391
00:13:40,480 --> 00:13:42,200
因为直播容易一些

392
00:13:42,200 --> 00:13:43,360
就直播我就讲过去

393
00:13:43,360 --> 00:13:44,400
反正一个半小时

394
00:13:44,600 --> 00:13:45,600
我讲也是讲

395
00:13:45,600 --> 00:13:49,040
我站在这里也是时间也花掉了

396
00:13:49,200 --> 00:13:50,840
所以我要去录的话

397
00:13:50,840 --> 00:13:52,759
其实我还得多花一两个小时

398
00:13:52,759 --> 00:13:55,639
我在想我能不能承受这个事件

399
00:13:55,639 --> 00:13:57,759
因为我主要是周末干这个事情

400
00:13:58,159 --> 00:14:00,480
周末我们现在是还是在疫情期间

401
00:14:00,720 --> 00:14:02,279
我跟我太太两个人

402
00:14:02,279 --> 00:14:05,000
在周末主要的活动是带了我们两个娃

403
00:14:05,000 --> 00:14:06,439
一个4岁一个1岁

404
00:14:06,639 --> 00:14:08,879
基本上是要等他们睡午觉的时候

405
00:14:08,879 --> 00:14:10,600
我能够抽一点时间来做一点

406
00:14:10,600 --> 00:14:11,399
干一点事情

407
00:14:11,600 --> 00:14:12,799
所以我就想看看

408
00:14:12,799 --> 00:14:14,360
能不能在睡午觉

409
00:14:14,360 --> 00:14:15,879
他们的一个半小时里面

410
00:14:15,919 --> 00:14:16,960
能不能压出来

411
00:14:16,960 --> 00:14:17,879
能不能录一段

412
00:14:18,120 --> 00:14:19,960
给大家提供更好的体验

413
00:14:20,400 --> 00:14:22,960
当然我们我希望是不要录

414
00:14:22,960 --> 00:14:25,280
我希望直播能达到我们今天

415
00:14:25,280 --> 00:14:26,920
除了今天录播我少放

416
00:14:26,920 --> 00:14:28,639
我有个order没放对之外

417
00:14:28,639 --> 00:14:31,120
我希望直播能达到我们的

418
00:14:31,120 --> 00:14:32,920
录播的效果

419
00:14:32,920 --> 00:14:35,400
但是目前来说我们这个是

420
00:14:35,560 --> 00:14:37,560
我这个也是一个业余

421
00:14:37,800 --> 00:14:38,960
想做到一个

422
00:14:38,960 --> 00:14:40,480
因为想露脸的话

423
00:14:40,480 --> 00:14:43,000
因为露脸主要是会有一点交互性

424
00:14:43,000 --> 00:14:45,240
大家会觉得可能会

425
00:14:45,240 --> 00:14:47,400
有motivate去看一点

426
00:14:48,040 --> 00:14:49,320
当然不是我长得怎么样

427
00:14:49,879 --> 00:14:52,240
所以我在想说

428
00:14:52,240 --> 00:14:53,720
因为为了露脸

429
00:14:53,720 --> 00:14:56,360
所以导致整个其实挺麻烦的一个事情

430
00:14:56,360 --> 00:14:56,520
好

431
00:14:56,520 --> 00:14:57,480
我给大家可以看一下

432
00:14:57,480 --> 00:15:00,400
我们现在setup是怎么样子

433
00:15:00,640 --> 00:15:02,200
其实我现在车库里面

434
00:15:02,200 --> 00:15:04,040
车库这个地方真的是

435
00:15:04,160 --> 00:15:05,640
冬天很冷

436
00:15:05,640 --> 00:15:07,120
夏天很热

437
00:15:07,120 --> 00:15:09,280
所以可以看到是说

438
00:15:09,280 --> 00:15:11,320
这里是一块绿布

439
00:15:11,320 --> 00:15:13,920
就导致我们的背景的透明

440
00:15:13,920 --> 00:15:17,680
然后我们这里有个灯

441
00:15:17,840 --> 00:15:20,080
这个灯其实也不是那么的好

442
00:15:20,080 --> 00:15:21,920
其实我一直想买一个灯

443
00:15:21,920 --> 00:15:24,040
一个新品

444
00:15:24,040 --> 00:15:25,360
然后但我等了两个月

445
00:15:25,360 --> 00:15:26,360
还是没到货

446
00:15:26,360 --> 00:15:28,760
因为可能是疫情的缘故

447
00:15:28,760 --> 00:15:30,600
所以灯感觉没照出来

448
00:15:30,600 --> 00:15:33,040
所以这个灯我们有时候会

449
00:15:33,040 --> 00:15:34,240
我觉得不是很亮

450
00:15:34,240 --> 00:15:36,240
就背景会有一点点灰

451
00:15:36,360 --> 00:15:38,240
然后下面其实更麻烦

452
00:15:38,240 --> 00:15:39,800
下面我有一个相机

453
00:15:39,800 --> 00:15:40,880
我这里有个Mac

454
00:15:40,880 --> 00:15:42,320
我这里还有一个MacBook

455
00:15:42,440 --> 00:15:43,520
然后那里有显示器

456
00:15:43,520 --> 00:15:45,160
我的屏幕是在这个地方

457
00:15:45,319 --> 00:15:46,519
我这里有一个摄像头

458
00:15:46,519 --> 00:15:47,719
在这个地方

459
00:15:47,719 --> 00:15:48,879
有个麦克风在这里

460
00:15:48,879 --> 00:15:51,159
下面其实本来我在一个iPad

461
00:15:51,159 --> 00:15:52,879
本来我如果要手写的话

462
00:15:52,879 --> 00:15:53,600
我可以用它

463
00:15:54,360 --> 00:15:55,639
所以整个设置

464
00:15:55,639 --> 00:15:57,799
其实是挺麻烦的一个设置

465
00:15:57,959 --> 00:16:01,959
经常出错的概率特别大

466
00:16:01,959 --> 00:16:03,159
所以我在想说

467
00:16:03,159 --> 00:16:05,120
在我确实能

468
00:16:05,120 --> 00:16:07,439
而且这个设置会在不断的变

469
00:16:07,439 --> 00:16:09,399
我每天可能会想一想说

470
00:16:09,399 --> 00:16:12,079
是不是有办法显得更专业一点

471
00:16:12,199 --> 00:16:13,039
我们做一件事情

472
00:16:13,039 --> 00:16:15,839
尽量做到最好

473
00:16:15,839 --> 00:16:17,599
我觉得这个也是想

474
00:16:17,719 --> 00:16:18,839
不管做什么事情

475
00:16:18,839 --> 00:16:20,919
虽然这个东西可能觉得

476
00:16:20,919 --> 00:16:23,480
你找一个专业人士来帮你就解决了

477
00:16:23,480 --> 00:16:26,360
但我觉得你需要去

478
00:16:26,399 --> 00:16:28,799
我们这门课就动手学

479
00:16:28,799 --> 00:16:30,959
就是你要去动手

480
00:16:30,959 --> 00:16:32,319
不管什么事情都去动手

481
00:16:32,319 --> 00:16:35,240
这个是我的一个哲学

482
00:16:35,279 --> 00:16:35,919
OK

483
00:16:36,480 --> 00:16:37,319
所以我们尽量

484
00:16:37,519 --> 00:16:38,959
我尽量还是在学习

485
00:16:38,959 --> 00:16:41,559
怎么样做比较好的课程

486
00:16:41,600 --> 00:16:43,760
然后确实比较得心应手了

487
00:16:43,760 --> 00:16:45,960
我们就直接直播就行了

488
00:16:45,960 --> 00:16:46,560
好

489
00:16:46,760 --> 00:16:48,120
这样子我也简单一点

490
00:16:48,120 --> 00:16:51,120
但是确实我觉得还是内容优先

491
00:16:51,120 --> 00:16:54,240
因为我多花一个小时

492
00:16:54,240 --> 00:16:56,120
如果有100个人或1000个人

493
00:16:56,120 --> 00:16:57,680
能够节省5分钟

494
00:16:57,680 --> 00:16:58,680
或看得更好的话

495
00:16:58,680 --> 00:16:59,720
我觉得还是值的

496
00:17:00,120 --> 00:17:00,640
OK

497
00:17:01,040 --> 00:17:03,600
所以我们来直接来切换到我们的问题

498
00:17:03,600 --> 00:17:11,039
就是说

499
00:17:11,039 --> 00:17:12,960
第一问题12是说

500
00:17:12,960 --> 00:17:16,960
我执行你下载命令后显示

501
00:17:16,960 --> 00:17:18,160
http arrow

502
00:17:18,160 --> 00:17:20,160
这个东西我还真不知道

503
00:17:20,360 --> 00:17:23,039
他们是我们用的是Torch的

504
00:17:23,039 --> 00:17:26,519
自己的官网下载

505
00:17:26,519 --> 00:17:28,839
我希望他们不是放在Google上面

506
00:17:28,839 --> 00:17:31,599
这样子可能会下载会比较麻烦一点

507
00:17:31,599 --> 00:17:33,160
当然你如果不行的话

508
00:17:33,160 --> 00:17:34,920
你就是自己手动下载也行

509
00:17:34,920 --> 00:17:36,720
你可以看一下Pytorch的网页上

510
00:17:36,720 --> 00:17:38,600
怎么把这个数据机手动下载

511
00:17:38,600 --> 00:17:41,480
问题13

512
00:17:41,480 --> 00:17:43,800
Data loader的number of workers

513
00:17:43,800 --> 00:17:44,640
并行了吗

514
00:17:44,640 --> 00:17:45,080
是的

515
00:17:45,080 --> 00:17:45,800
就是说

516
00:17:45,840 --> 00:17:47,200
取决于你的时限

517
00:17:47,200 --> 00:17:52,000
应该是Pytorch应该是用的是进程来实现的

518
00:17:52,000 --> 00:17:52,480
就是说

519
00:17:52,480 --> 00:17:53,440
你number of workers

520
00:17:53,440 --> 00:17:54,640
测试是4的话

521
00:17:54,640 --> 00:17:57,880
那么你会在后端开4个python的进程

522
00:17:57,880 --> 00:17:59,000
来帮你做并行

523
00:17:59,759 --> 00:18:02,759
问题14也是一个相关的问题

524
00:18:03,000 --> 00:18:06,119
你直接写4是不是不好

525
00:18:08,119 --> 00:18:11,119
进程这样简单还是有必要写吗

526
00:18:11,359 --> 00:18:12,440
是不是不好

527
00:18:14,079 --> 00:18:16,559
其实也可以直接写了

528
00:18:16,559 --> 00:18:17,640
这个没关系

529
00:18:17,759 --> 00:18:20,519
我们之前之所以这是一个历史原因

530
00:18:20,519 --> 00:18:21,839
之所以做了这个函数

531
00:18:21,839 --> 00:18:25,319
是因为在以前在早点版本里面

532
00:18:25,319 --> 00:18:27,039
他Windows不支持

533
00:18:27,079 --> 00:18:28,879
所以我们这个函数里面要判一下

534
00:18:28,920 --> 00:18:30,000
如果是Windows的话

535
00:18:30,000 --> 00:18:30,640
你不要用

536
00:18:30,640 --> 00:18:32,279
你只能你不能开多线程

537
00:18:32,399 --> 00:18:33,240
多进程

538
00:18:33,279 --> 00:18:36,119
所以然后我们就一直保留下来了

539
00:18:36,119 --> 00:18:37,359
但你写不写都没关系

540
00:18:37,359 --> 00:18:38,559
我觉得不写没事

541
00:18:43,279 --> 00:18:46,519
另外是方差0.01有什么讲究吗

542
00:18:46,799 --> 00:18:47,559
没有什么讲究

543
00:18:47,559 --> 00:18:49,720
就是现在没有什么讲究

544
00:18:49,720 --> 00:18:50,559
之后会有

545
00:18:50,879 --> 00:18:52,839
现在我们就选了一个比较小的值

546
00:18:52,879 --> 00:18:54,799
我们之后会来解释一下

547
00:18:54,799 --> 00:18:56,119
我们应该是下一下节课

548
00:18:56,119 --> 00:18:57,879
会来解释一下方差

549
00:18:57,879 --> 00:19:00,039
这个东西其实是挺有讲究的

550
00:19:00,039 --> 00:19:01,359
对于深度神经网络

551
00:19:01,360 --> 00:19:03,000
方差是很重要的一件事情

552
00:19:03,800 --> 00:19:05,040
但是我们只是放在这里

553
00:19:05,040 --> 00:19:08,240
大家知道这是一个超参数就行了

554
00:19:10,240 --> 00:19:11,280
问题是7

555
00:19:11,280 --> 00:19:13,600
PyTorch训练好的模型

556
00:19:14,000 --> 00:19:16,480
在测试的时候

557
00:19:16,480 --> 00:19:18,840
不论是Batch Size设为1还是更多

558
00:19:18,880 --> 00:19:21,600
测试的时间差不多

559
00:19:23,040 --> 00:19:24,680
如果正常理解

560
00:19:24,680 --> 00:19:25,680
如果设成4

561
00:19:25,680 --> 00:19:28,320
不应该就是1的4倍速度吗

562
00:19:28,480 --> 00:19:29,680
当然不是的

563
00:19:29,799 --> 00:19:30,560
就是说

564
00:19:31,400 --> 00:19:34,000
就是说你不管你Batch Size等于几

565
00:19:34,000 --> 00:19:36,560
你的计算量是不会发生变化的

566
00:19:36,920 --> 00:19:39,759
只是说唯一的发生变化的是说

567
00:19:40,000 --> 00:19:40,920
我的并行度

568
00:19:40,920 --> 00:19:42,440
我计算的时候的并行度

569
00:19:42,440 --> 00:19:43,680
是不是能增加

570
00:19:43,720 --> 00:19:45,400
使得我的整个

571
00:19:46,080 --> 00:19:48,560
这个执行的效率能不能增加

572
00:19:49,320 --> 00:19:51,519
所以说你如果Batch Size变大

573
00:19:51,519 --> 00:19:52,880
变小都没区别的话

574
00:19:52,880 --> 00:19:54,600
很有可能是说你这个模型

575
00:19:54,600 --> 00:19:56,279
其实就很小

576
00:19:56,279 --> 00:19:57,519
本来就并行度不够

577
00:19:57,519 --> 00:19:58,920
所以就看不出区别

578
00:19:58,920 --> 00:20:00,039
但一般来说

579
00:20:00,240 --> 00:20:01,160
而且在CPU上

580
00:20:01,160 --> 00:20:02,920
可能大家也看不出太多区别

581
00:20:03,279 --> 00:20:05,120
主要的区别是在于

582
00:20:05,440 --> 00:20:07,720
在GPU上会有比较大的区别

583
00:20:07,720 --> 00:20:11,480
因为GPU上现在都是上百或上千个核

584
00:20:11,519 --> 00:20:12,920
当然会有一点区别

585
00:20:14,039 --> 00:20:15,720
CPU上你可能确实没区别

586
00:20:18,080 --> 00:20:18,880
问题是吧

587
00:20:18,880 --> 00:20:21,560
这两个等号等号相连起什么作用

588
00:20:21,560 --> 00:20:23,519
应该就是Python的Pen

589
00:20:23,519 --> 00:20:24,759
两个东西是不是相等

590
00:20:25,080 --> 00:20:27,600
看每个元素之间是不是相等

591
00:20:27,720 --> 00:20:31,520
我们在数据操作那一节讲过这个问题

592
00:20:31,520 --> 00:20:32,680
大家可以回顾一下

593
00:20:35,920 --> 00:20:38,200
为什么不在Accuracy的函数里面

594
00:20:38,200 --> 00:20:40,200
把除员NanceY呢

595
00:20:40,480 --> 00:20:42,800
之所以不做这个事情是说

596
00:20:43,080 --> 00:20:44,600
你有可能还记不记得

597
00:20:44,600 --> 00:20:47,760
我们如果去读一个Batch的时候

598
00:20:47,880 --> 00:20:50,960
最后Batch很有可能是没读满的

599
00:20:51,760 --> 00:20:53,320
如果你在Accuracy

600
00:20:53,320 --> 00:20:54,880
就是说我要对N个Batch

601
00:20:54,880 --> 00:20:56,240
然后一直一直算下来

602
00:20:56,359 --> 00:20:57,799
如果你都除了的话

603
00:20:57,799 --> 00:21:00,200
其实它如果你最后一个Batch

604
00:21:00,200 --> 00:21:03,519
它的Batch Size它的Number of Examples

605
00:21:03,519 --> 00:21:05,720
它不等于你Batch Size的时候

606
00:21:06,039 --> 00:21:07,680
你这个是不正确的

607
00:21:07,680 --> 00:21:11,039
所以你应该是把所有东西的正例和负例

608
00:21:11,039 --> 00:21:12,960
在所有的群面加起来

609
00:21:12,960 --> 00:21:15,240
然后最后再除就是等价的

610
00:21:20,559 --> 00:21:24,440
CMP的Type YType是必要的吗

611
00:21:25,440 --> 00:21:27,200
你可以去试一下

612
00:21:28,039 --> 00:21:28,720
你可以试一下

613
00:21:28,720 --> 00:21:29,840
比如必要

614
00:21:30,519 --> 00:21:31,920
可能是不必要的

615
00:21:32,880 --> 00:21:34,480
就是说我们这个是给代码

616
00:21:34,480 --> 00:21:35,360
都能Run

617
00:21:35,360 --> 00:21:36,160
你就去Run一下

618
00:21:36,160 --> 00:21:40,279
而且我们其实里面代码

619
00:21:40,279 --> 00:21:41,160
很多代码

620
00:21:41,160 --> 00:21:43,000
其实我觉得是有一点点多余的

621
00:21:43,000 --> 00:21:45,080
你可以去尝试去简化它

622
00:21:45,440 --> 00:21:47,000
我们之所以写成那样子

623
00:21:47,000 --> 00:21:49,120
很多时候有很多一些历史原因

624
00:21:49,160 --> 00:21:52,279
很多时候是老的PyTorch版本不支持

625
00:21:52,600 --> 00:21:54,360
我们如果做的比较早的话

626
00:21:54,400 --> 00:21:57,400
我们PyTorch也是每三个月更新一次的话

627
00:21:57,760 --> 00:21:59,320
那么新的版本也许支持了

628
00:21:59,320 --> 00:22:01,600
但我们不一定是能改过来

629
00:22:02,240 --> 00:22:04,840
但我们还在联系PyTorch团队说

630
00:22:04,840 --> 00:22:07,040
我们把整个我们这本书

631
00:22:07,040 --> 00:22:09,600
放进他们的官方的一个测试里面

632
00:22:09,640 --> 00:22:12,600
这样子他们可以帮我们也来看一下

633
00:22:14,280 --> 00:22:17,040
Accuracy函数能不能再讲一下

634
00:22:17,360 --> 00:22:18,440
我觉得Accuracy函数

635
00:22:18,440 --> 00:22:20,800
你可以去仔细看一下

636
00:22:20,800 --> 00:22:23,600
其实还是比较简单的一个计算

637
00:22:24,280 --> 00:22:26,360
我们之后会再讲一次Accuracy

638
00:22:26,360 --> 00:22:28,000
就是在GPU上怎么算

639
00:22:28,200 --> 00:22:30,280
所以我们你可以先看一看

640
00:22:30,280 --> 00:22:31,200
如果你还没看到

641
00:22:31,200 --> 00:22:32,920
我们下一次再讲Accuracy的时候

642
00:22:32,920 --> 00:22:33,840
你可以再听一听

643
00:22:35,880 --> 00:22:37,560
在计算精度的时候

644
00:22:37,920 --> 00:22:41,280
为什么用EVO将模型设成评估模式

645
00:22:42,000 --> 00:22:44,160
是因为其实你不设没关系

646
00:22:44,600 --> 00:22:46,080
就是为了一个好的习惯

647
00:22:46,240 --> 00:22:46,880
你就设一下

648
00:22:47,280 --> 00:22:49,240
就是说设成EVO模式的话

649
00:22:49,240 --> 00:22:51,560
那就是他默认会补去开

650
00:22:51,600 --> 00:22:52,760
补去对你的体

651
00:22:52,759 --> 00:22:54,759
他就知道我不要算T度了

652
00:22:55,039 --> 00:22:56,720
我很多跟T度相关的事情

653
00:22:56,720 --> 00:22:57,519
我可不用做

654
00:22:57,720 --> 00:22:59,319
性能上可能会好一点

655
00:22:59,319 --> 00:23:01,879
但是我觉得你不设也没关系

656
00:23:05,200 --> 00:23:08,079
这里的W和B是怎么从网络模型中

657
00:23:08,079 --> 00:23:10,039
抽出来放进updater的

658
00:23:10,519 --> 00:23:11,559
其实我不是很知道

659
00:23:11,559 --> 00:23:12,799
你是讲哪一个时限

660
00:23:13,039 --> 00:23:14,000
如果你是讲的

661
00:23:14,000 --> 00:23:16,039
我们的从零开始的时限的话

662
00:23:16,039 --> 00:23:18,079
我们就是直接拿出来放进去

663
00:23:18,079 --> 00:23:19,640
W和B是我们创建的

664
00:23:20,079 --> 00:23:22,640
如果你是讲的是NN模具的时限的话

665
00:23:22,879 --> 00:23:25,960
我们在构造那个optimizer的时候

666
00:23:25,960 --> 00:23:28,559
是把net的点parameters

667
00:23:29,079 --> 00:23:30,640
他net的点parameters

668
00:23:30,640 --> 00:23:32,920
就把我们所谓的W和B拿出来

669
00:23:32,920 --> 00:23:35,240
放进了他的updater里面

670
00:23:35,240 --> 00:23:36,559
这就是我做step的时候

671
00:23:36,559 --> 00:23:39,879
我就已经知道这些所有的模型了

672
00:23:44,400 --> 00:23:45,319
问题23

673
00:23:45,839 --> 00:23:47,680
在多次迭代之后

674
00:23:47,680 --> 00:23:51,319
如果测试精度出现上升再下降

675
00:23:51,319 --> 00:23:52,559
是过离合的吗

676
00:23:52,960 --> 00:23:56,200
是不是可以提前终止

677
00:23:56,759 --> 00:23:59,799
很有可能是过离合了

678
00:24:00,119 --> 00:24:01,799
但是你可以再等一等

679
00:24:01,799 --> 00:24:04,079
如果你确实一直是下降的话

680
00:24:04,960 --> 00:24:06,960
那有可能是过离合

681
00:24:07,759 --> 00:24:11,599
通常你的我们可能会后面会讲一些策略

682
00:24:11,599 --> 00:24:13,200
让你来尽量的避免

683
00:24:13,440 --> 00:24:14,720
一个办法是说

684
00:24:15,039 --> 00:24:17,839
你可以通过比较好的去微调能学习率

685
00:24:17,839 --> 00:24:19,680
能够避免一些事情

686
00:24:19,680 --> 00:24:21,359
当然可以通过加各种振奏项

687
00:24:21,800 --> 00:24:24,640
我们之后会来讲这种实际的

688
00:24:24,640 --> 00:24:26,120
碰到这种情况应该怎么办

689
00:24:26,120 --> 00:24:28,640
而且我们会有竞赛

690
00:24:28,640 --> 00:24:30,640
真实数据大家都来跑

691
00:24:30,640 --> 00:24:32,360
大家都来跑的好处是说

692
00:24:32,360 --> 00:24:33,600
因为现在是我来讲

693
00:24:33,600 --> 00:24:36,560
我来讲可能就是就讲很标准的一些模型

694
00:24:37,040 --> 00:24:38,440
但如果我们做竞赛的话

695
00:24:38,440 --> 00:24:41,320
我们希望是说大家都能分享

696
00:24:41,320 --> 00:24:43,000
我们给定个数据集

697
00:24:43,000 --> 00:24:46,640
大家分享不同的模型调参的一些观察

698
00:24:46,880 --> 00:24:49,080
这个事情是我们想做一个竞赛

699
00:24:49,080 --> 00:24:50,400
让大家来做事情

700
00:24:50,519 --> 00:24:53,480
就是大家把各种学到的东西去试一下

701
00:24:53,480 --> 00:24:55,240
然后以后碰到了新的问题

702
00:24:55,240 --> 00:24:56,480
我们可以一起来讨论

703
00:25:00,120 --> 00:25:04,160
CNN的网络学习到底是什么信息

704
00:25:04,160 --> 00:25:05,840
是纹理还是轮廓

705
00:25:05,840 --> 00:25:07,759
还是所有内容的综合

706
00:25:07,759 --> 00:25:08,840
说不清的那种

707
00:25:08,960 --> 00:25:10,320
我们还没有讲CNN

708
00:25:10,320 --> 00:25:12,160
CNN是我们接下来的话题

709
00:25:12,160 --> 00:25:14,200
我们会讲到所有CNN里面

710
00:25:14,400 --> 00:25:17,800
从80年代到最近的一些模型

711
00:25:18,080 --> 00:25:19,920
但如果大家懂的

712
00:25:19,920 --> 00:25:21,680
就是说懂的话

713
00:25:21,680 --> 00:25:25,200
就是说我们其实也不好说CNN学到什么

714
00:25:25,240 --> 00:25:27,080
目前至少几年前

715
00:25:27,080 --> 00:25:30,560
大家觉得CNN其实主要是学纹理

716
00:25:30,920 --> 00:25:33,279
而轮廓它其实不那么在意

717
00:25:35,039 --> 00:25:35,600
OK

718
00:25:35,960 --> 00:25:38,519
但是学到纹理也不错

719
00:25:38,519 --> 00:25:40,840
纹理就人看纹理不那么仔细

720
00:25:40,840 --> 00:25:43,279
但计算机看纹理其实是非常强的

721
00:25:43,279 --> 00:25:50,039
是不是要对权力阶层的输出

722
00:25:50,039 --> 00:25:52,480
做成L2规划

723
00:25:52,519 --> 00:25:55,000
对最后的损失和精度会有什么影响

724
00:25:55,160 --> 00:25:57,599
我们会专门有一节讲这一个东西

725
00:25:57,759 --> 00:25:59,240
这是一个很重要的项目

726
00:25:59,839 --> 00:26:02,519
通过规划来控制过你喝

727
00:26:02,519 --> 00:26:03,799
我们可以稍微等一下

728
00:26:03,799 --> 00:26:05,480
等到应该是下周或下周

729
00:26:05,639 --> 00:26:06,599
下周我们没课

730
00:26:06,799 --> 00:26:08,039
下周不是长假

731
00:26:08,480 --> 00:26:10,319
下下周我们应该会讲到

732
00:26:10,319 --> 00:26:15,639
下下下周我们要讲感知机

733
00:26:17,919 --> 00:26:21,639
自己实现的softmax和PyTorch实现的

734
00:26:21,639 --> 00:26:23,399
softmax谁快

735
00:26:23,399 --> 00:26:25,039
这个东西你就比一下

736
00:26:25,039 --> 00:26:30,799
但很有可能他们实现的稳定性要高

737
00:26:30,799 --> 00:26:32,559
你可以去看我们的书

738
00:26:32,559 --> 00:26:35,039
书里面有个练习题来跟大家讲说

739
00:26:35,119 --> 00:26:37,559
你为什么这么实现会有一点的问题

740
00:26:37,559 --> 00:26:38,919
有数据精度的问题

741
00:26:39,560 --> 00:26:41,240
但是你说谁快

742
00:26:41,240 --> 00:26:44,039
大家一定要去比一下

743
00:26:44,039 --> 00:26:46,039
你千万不要信说我说我快

744
00:26:46,039 --> 00:26:46,720
你说你快

745
00:26:46,720 --> 00:26:51,080
这些东西都是很主观的一些事情

746
00:26:53,800 --> 00:26:57,440
如果是自己的图片数据机

747
00:26:57,440 --> 00:27:00,039
需要怎么做才能用于训练

748
00:27:00,160 --> 00:27:02,880
怎么样根据本地图片训练机和测试

749
00:27:02,880 --> 00:27:04,560
机创建迭代器

750
00:27:04,800 --> 00:27:07,640
你可以去看一下PyTorch的文档

751
00:27:07,680 --> 00:27:09,680
里面有从文件

752
00:27:09,680 --> 00:27:10,720
就是说你怎么办

753
00:27:10,800 --> 00:27:11,840
最简单就是说

754
00:27:12,080 --> 00:27:13,680
比如说你的图片有10个类

755
00:27:13,880 --> 00:27:16,280
有猫狗马什么

756
00:27:16,440 --> 00:27:18,680
你就猫创建一个文件夹

757
00:27:18,680 --> 00:27:20,160
狗创建一个文件夹

758
00:27:20,280 --> 00:27:22,920
然后把猫的图片放在猫的文件夹下

759
00:27:22,920 --> 00:27:24,880
狗的图片放在狗的文件夹下

760
00:27:24,920 --> 00:27:27,160
然后PyTorch能够说

761
00:27:27,160 --> 00:27:29,200
我把你的上一层目录告诉他

762
00:27:29,240 --> 00:27:30,240
他就去扫一遍

763
00:27:30,240 --> 00:27:31,759
是能够直接读进来的

764
00:27:32,040 --> 00:27:33,960
你可以去看一下PyTorch的文档

765
00:27:34,240 --> 00:27:37,000
其实你不管TensorFlow还是MSNet

766
00:27:37,000 --> 00:27:39,279
它是都是支持这样子的一个操作的

767
00:27:43,039 --> 00:27:44,359
另外一个问题是说

768
00:27:44,640 --> 00:27:48,799
在后续讲解和现在英文版的内容是一样吗

769
00:27:49,000 --> 00:27:50,519
看了一下英文版有一千页

770
00:27:50,519 --> 00:27:51,359
是的

771
00:27:51,359 --> 00:27:52,359
我们尽量把

772
00:27:52,599 --> 00:27:55,599
我们在不断的把英文版翻译到中文版

773
00:27:55,599 --> 00:27:59,200
然后我们希望这个课能够把整个内容讲完

774
00:28:00,720 --> 00:28:03,240
但我们可能会不会讲推荐系统

775
00:28:03,240 --> 00:28:05,400
因为为什么是因为推荐系统

776
00:28:05,400 --> 00:28:07,440
我觉得写的还不那么好

777
00:28:07,440 --> 00:28:10,120
我们还一个真正的教科书

778
00:28:10,320 --> 00:28:14,280
能被好的大学采用作为教科书

779
00:28:14,720 --> 00:28:15,640
还有一定的差距

780
00:28:15,640 --> 00:28:17,080
所以那一张有一两张

781
00:28:17,080 --> 00:28:18,960
我觉得质量还不那么高

782
00:28:18,960 --> 00:28:21,080
所以我们可能会这一次会不会讲

783
00:28:21,920 --> 00:28:26,000
但我们接下来会找这方面的专家来继续往下写

784
00:28:26,000 --> 00:28:29,920
这样子如果到了我们觉得非常专业的一个程度了

785
00:28:29,920 --> 00:28:31,800
我们可以开始来给大家讲一下

786
00:28:32,639 --> 00:28:37,440
Softmax最近的学到了什么

787
00:28:37,559 --> 00:28:40,079
它的解释性可以怎么理解

788
00:28:40,399 --> 00:28:43,680
有没有指标可以衡量神经网络的解释性

789
00:28:44,680 --> 00:28:46,559
这一块如果你想理解的话

790
00:28:46,559 --> 00:28:47,759
Softmax的话

791
00:28:48,119 --> 00:28:49,200
还是有机会的

792
00:28:49,200 --> 00:28:51,159
就是说我们还没有讲深度神经网络

793
00:28:51,680 --> 00:28:54,720
所以我们只讲了一个Softmax回归

794
00:28:55,000 --> 00:28:57,559
大家可以去看一下

795
00:28:57,559 --> 00:28:59,159
你如果想真的知道的话

796
00:28:59,160 --> 00:29:02,759
去看一下统计学习

797
00:29:02,759 --> 00:29:04,440
就是statistic learning

798
00:29:04,720 --> 00:29:06,640
就是说它里面是有很多可解释的

799
00:29:06,640 --> 00:29:08,920
对于整个模型的capacity

800
00:29:09,440 --> 00:29:11,360
它的convergence

801
00:29:11,360 --> 00:29:13,720
各种能力

802
00:29:13,840 --> 00:29:15,120
它都是可以做的

803
00:29:16,040 --> 00:29:22,120
所以在我你可以去参考一下那些相对应的书

804
00:29:22,320 --> 00:29:25,279
我们这里就不会特别深入的讲这一块了

805
00:29:25,279 --> 00:29:27,560
因为我们想法还是说

806
00:29:28,559 --> 00:29:30,759
给大家讲一下最基础的模型

807
00:29:30,879 --> 00:29:31,960
实现一遍

808
00:29:32,159 --> 00:29:34,279
然后我们大概差不多了

809
00:29:34,279 --> 00:29:35,200
我们就往前走

810
00:29:35,200 --> 00:29:37,679
因为Softmax回归也好

811
00:29:38,200 --> 00:29:39,119
线性回归也好

812
00:29:39,119 --> 00:29:42,039
那都是50年前的东西了

813
00:29:42,240 --> 00:29:46,319
跟现在的神经网络确实差的

814
00:29:46,399 --> 00:29:47,799
虽然本质上没有区别

815
00:29:47,799 --> 00:29:50,720
但是它的各种细节还是差别比较大的

816
00:29:50,759 --> 00:29:53,759
所以我们想尽量的快速的往前走

817
00:29:54,240 --> 00:29:55,359
所以这一课

818
00:29:55,400 --> 00:29:58,920
这个课我们不会那么的去讲一些理论性的东西

819
00:29:59,840 --> 00:30:01,800
所以这个也是我们这个课的局限性

820
00:30:02,440 --> 00:30:03,120
大家要理解

821
00:30:03,120 --> 00:30:04,320
我们这个课其实很

822
00:30:04,360 --> 00:30:07,560
我们是在讲一点点数学的前提下

823
00:30:08,760 --> 00:30:10,680
尽量的往实用走

824
00:30:10,680 --> 00:30:12,560
就是讲这是什么

825
00:30:12,880 --> 00:30:13,720
怎么实现

826
00:30:13,760 --> 00:30:14,560
怎么用

827
00:30:14,800 --> 00:30:16,720
然后还有一点点东西

828
00:30:16,720 --> 00:30:18,280
我们会讲一下一些数学

829
00:30:18,520 --> 00:30:21,200
为什么是因为如果我全讲数学

830
00:30:21,200 --> 00:30:22,560
大家早就走掉了

831
00:30:22,879 --> 00:30:25,039
因为毕竟我们不是在

832
00:30:25,039 --> 00:30:27,720
我们尝试过在伯克利的统计系

833
00:30:27,720 --> 00:30:31,079
就是我们两年前在UC伯克利的统计系

834
00:30:31,079 --> 00:30:32,839
那也是世界上最好的统计系了

835
00:30:32,839 --> 00:30:33,759
Michael Jordan

836
00:30:34,319 --> 00:30:35,679
宇宾他们都在那里

837
00:30:36,039 --> 00:30:37,960
我们给统计系的本质上讲数学

838
00:30:37,960 --> 00:30:39,679
大家都觉得这个东西有点难

839
00:30:39,679 --> 00:30:41,599
所以我们觉得还是

840
00:30:42,480 --> 00:30:46,639
我们尽量的实用化一点

841
00:30:46,839 --> 00:30:48,240
这是我们的课程

842
00:30:49,119 --> 00:30:49,639
OK

843
00:30:49,639 --> 00:30:51,240
我来最后刷新一下

844
00:30:51,240 --> 00:30:53,079
如果这是所有的问题的话

845
00:30:53,079 --> 00:30:55,480
我们今天刚好讲到这里

846
00:30:55,720 --> 00:30:57,200
时间上刚好也是

847
00:30:57,640 --> 00:30:58,799
我来最后看一眼

848
00:30:58,960 --> 00:30:59,599
我们应该

