1
00:00:00,000 --> 00:00:03,960
这个notebook是说给大家看一下模型选择和

2
00:00:03,960 --> 00:00:05,879
嵌礼盒过礼盒的一些现象

3
00:00:05,879 --> 00:00:10,640
我们通过一个很简单的人工数据集来探索一下这个事情

4
00:00:10,640 --> 00:00:13,160
就我们做一个什么样的人工数据集呢

5
00:00:13,160 --> 00:00:19,960
就是用一个三阶多项式来生成一个训练集合测试机

6
00:00:19,960 --> 00:00:23,000
就是你的真实的标号是一个5

7
00:00:23,000 --> 00:00:24,760
加上1.2乘以个x

8
00:00:24,760 --> 00:00:25,839
x是一个表量

9
00:00:25,839 --> 00:00:29,400
减去3.4乘以x平方除以2的感叹号

10
00:00:29,800 --> 00:00:33,719
就之所以除感叹号是说你不要那个x不要长得特别大

11
00:00:34,079 --> 00:00:36,760
然后再加上另外一个再加上一些噪音

12
00:00:37,640 --> 00:00:43,680
所以实际生成的话我们会生成一个特征为20的一个东西

13
00:00:43,680 --> 00:00:44,520
我再放大一点

14
00:00:44,520 --> 00:00:49,079
我们用100个训练样本

15
00:00:49,079 --> 00:00:50,280
100个测试样本

16
00:00:50,280 --> 00:00:51,359
还是一样的

17
00:00:51,480 --> 00:00:53,079
测试其实是验证的意思

18
00:00:53,560 --> 00:00:56,120
我们就偷懒就都叫测试

19
00:00:57,120 --> 00:01:01,480
我们的真实的也是一个常为w是一个常为20的w

20
00:01:01,480 --> 00:01:03,400
就是第一个是5

21
00:01:04,000 --> 00:01:05,160
第二个是1.2

22
00:01:05,160 --> 00:01:06,840
然后-3.4和5.6

23
00:01:06,840 --> 00:01:08,240
剩下的w全是零

24
00:01:08,320 --> 00:01:10,280
就是后面那些全是一些噪音项

25
00:01:10,920 --> 00:01:13,480
然后我们下面就不再仔细过了

26
00:01:13,480 --> 00:01:17,240
就是我的会产生一个20维的一个项量

27
00:01:17,240 --> 00:01:19,719
然后其实只有前面的5个是

28
00:01:19,960 --> 00:01:21,240
4个是真正有值的

29
00:01:21,240 --> 00:01:22,360
后面都是一些零

30
00:01:22,600 --> 00:01:24,120
就给你一些噪音

31
00:01:24,680 --> 00:01:26,840
然后我们再最后加一点噪音进去

32
00:01:30,200 --> 00:01:31,760
然后我们也看一下

33
00:01:31,760 --> 00:01:34,520
就是我们就特征的

34
00:01:34,520 --> 00:01:35,480
比如说前两维

35
00:01:35,719 --> 00:01:38,719
然后这些东西我们就不仔细看了

36
00:01:38,800 --> 00:01:40,159
就是能数据长什么样子

37
00:01:41,760 --> 00:01:43,600
然后另外我们实现一个样本

38
00:01:43,600 --> 00:01:45,120
就是说评估一下

39
00:01:45,280 --> 00:01:46,960
给你个网络

40
00:01:47,120 --> 00:01:48,159
给你个iterator

41
00:01:48,159 --> 00:01:49,000
就给你个数据

42
00:01:49,000 --> 00:01:50,159
给你个损失函数

43
00:01:50,280 --> 00:01:52,000
来评估一下所有的损失

44
00:01:52,000 --> 00:01:53,480
这个是一个很简单的函数

45
00:01:53,480 --> 00:01:55,920
就是说每一次data iterator里面

46
00:01:56,280 --> 00:01:57,760
就把xy拿出来

47
00:01:57,920 --> 00:01:59,159
把x丢到net里面

48
00:01:59,159 --> 00:02:00,079
拿到一个输入

49
00:02:00,719 --> 00:02:01,560
输出

50
00:02:01,600 --> 00:02:04,600
然后把y跟输出的形状搞成一样的

51
00:02:04,920 --> 00:02:06,079
最后放到loss里面

52
00:02:06,079 --> 00:02:07,000
拿到一个L

53
00:02:07,120 --> 00:02:08,480
然后把这些东西都放进去

54
00:02:08,480 --> 00:02:10,039
最后在整个数据上

55
00:02:10,039 --> 00:02:11,520
就算一个平均的损失

56
00:02:11,680 --> 00:02:12,439
这是一个

57
00:02:12,800 --> 00:02:13,800
evaluate loss

58
00:02:13,800 --> 00:02:14,680
我们会存起来

59
00:02:14,680 --> 00:02:15,920
之后也会可以用

60
00:02:19,319 --> 00:02:20,280
这里有个训练函数

61
00:02:20,280 --> 00:02:21,960
其实也是一个很简单的训练函数

62
00:02:21,960 --> 00:02:23,480
他干个什么事情

63
00:02:23,680 --> 00:02:25,560
他就是我给你特征

64
00:02:25,560 --> 00:02:26,760
给你训练的

65
00:02:26,879 --> 00:02:28,920
测试的给你的label

66
00:02:29,040 --> 00:02:29,960
然后告诉你说

67
00:02:29,960 --> 00:02:31,960
我要训练少多少次数据

68
00:02:32,319 --> 00:02:33,000
我们用

69
00:02:33,000 --> 00:02:35,120
square的loss

70
00:02:35,600 --> 00:02:38,360
然后这里就是我们定了一个很简单的

71
00:02:38,360 --> 00:02:40,520
一个单层的一个信息网络

72
00:02:40,520 --> 00:02:42,080
就是一个

73
00:02:42,879 --> 00:02:43,879
信息回归网络

74
00:02:43,879 --> 00:02:44,400
很简单

75
00:02:44,400 --> 00:02:46,120
就是一个linear layer

76
00:02:46,280 --> 00:02:47,400
BIOS的没用

77
00:02:48,800 --> 00:02:49,520
然后batch size

78
00:02:49,520 --> 00:02:50,960
我们随便给了一个10

79
00:02:51,240 --> 00:02:52,640
然后把数据漏的进来

80
00:02:52,800 --> 00:02:53,840
用sgd训练

81
00:02:53,840 --> 00:02:54,719
别的东西都一样

82
00:02:54,719 --> 00:02:55,680
就后面东西都一样

83
00:02:55,680 --> 00:02:56,800
就唯一的是说

84
00:02:56,840 --> 00:02:59,280
我给定他这个模型不干什么事情

85
00:02:59,280 --> 00:03:01,120
就是我给一个数据进来

86
00:03:01,159 --> 00:03:01,920
然后我

87
00:03:02,200 --> 00:03:02,840
生

88
00:03:03,240 --> 00:03:04,719
做一个线性回归

89
00:03:04,719 --> 00:03:05,480
然后看一下

90
00:03:05,480 --> 00:03:06,640
给你打印一下

91
00:03:07,680 --> 00:03:10,560
打一下整个曲线的变化

92
00:03:10,600 --> 00:03:12,400
然后看一下你的weight

93
00:03:12,400 --> 00:03:13,560
穴道是什么样子

94
00:03:14,879 --> 00:03:15,480
OK

95
00:03:16,240 --> 00:03:16,439
好

96
00:03:16,439 --> 00:03:17,800
我们就给大家直接看一下

97
00:03:17,800 --> 00:03:19,240
这个结果会是什么样子

98
00:03:20,240 --> 00:03:21,120
就第一个是说

99
00:03:21,120 --> 00:03:22,960
其实第一个是比较好的一个情况

100
00:03:22,960 --> 00:03:23,640
就是说

101
00:03:23,680 --> 00:03:28,360
我把整个样本的前面撕裂给你

102
00:03:28,920 --> 00:03:30,280
就记得我们这个样本

103
00:03:30,280 --> 00:03:34,400
其实就前面撕裂是有我的权重

104
00:03:34,680 --> 00:03:36,200
后面都是连接噪音

105
00:03:36,879 --> 00:03:37,840
然后就是说

106
00:03:37,840 --> 00:03:39,400
这里我是给你一个

107
00:03:39,520 --> 00:03:41,320
我也知道我的真实数据

108
00:03:41,320 --> 00:03:42,840
是一个线性的模型

109
00:03:42,840 --> 00:03:43,160
所以

110
00:03:45,160 --> 00:03:47,120
数据和模型是匹配的

111
00:03:47,120 --> 00:03:47,760
这个地方

112
00:03:48,159 --> 00:03:49,679
数据把前面撕裂拿过来

113
00:03:49,679 --> 00:03:51,120
我们模型是个线性模型

114
00:03:51,159 --> 00:03:52,039
所以拿进来之后

115
00:03:52,039 --> 00:03:52,519
你会发现

116
00:03:52,519 --> 00:03:54,519
我的穴的权重都是还可以的

117
00:03:54,519 --> 00:03:56,199
就是跟我真实的是差不多

118
00:03:56,239 --> 00:03:57,959
然后仔细看一下

119
00:03:57,959 --> 00:03:59,000
你会看到这个东西

120
00:03:59,959 --> 00:04:00,799
就是说

121
00:04:00,840 --> 00:04:03,239
我的训练和测试

122
00:04:04,039 --> 00:04:05,120
两个东西比较重要

123
00:04:05,879 --> 00:04:06,519
一个是说

124
00:04:06,519 --> 00:04:09,199
最后的我们的精度

125
00:04:09,799 --> 00:04:12,439
就最后你最后到的值是什么样子

126
00:04:12,479 --> 00:04:14,079
我们训练和测试

127
00:04:14,079 --> 00:04:17,480
都到了大概是0.01的上面一点点

128
00:04:18,159 --> 00:04:19,480
另外一个是说

129
00:04:19,519 --> 00:04:21,800
训练和测试之间的gap

130
00:04:21,920 --> 00:04:22,879
这里当时有gap

131
00:04:22,879 --> 00:04:24,519
但是最后的gap也还可以

132
00:04:24,519 --> 00:04:24,920
对吧

133
00:04:25,039 --> 00:04:26,199
就是说这里没有

134
00:04:26,240 --> 00:04:27,920
就是说测试和训练级的

135
00:04:28,319 --> 00:04:29,920
损失其实差不多了

136
00:04:29,920 --> 00:04:32,560
就是我们没有发生太多的overfitting

137
00:04:33,879 --> 00:04:34,439
OK

138
00:04:36,000 --> 00:04:37,240
我们看下一个

139
00:04:37,920 --> 00:04:38,879
下一个什么样子

140
00:04:38,879 --> 00:04:39,599
下一个的变化

141
00:04:39,599 --> 00:04:41,680
是说我只把前两个给你

142
00:04:42,399 --> 00:04:44,399
就是说我这个数据

143
00:04:44,519 --> 00:04:45,759
都没有给全给你

144
00:04:45,759 --> 00:04:47,079
然后你训练的话

145
00:04:47,079 --> 00:04:48,519
你能看到是说

146
00:04:51,199 --> 00:04:52,039
这个地方

147
00:04:52,639 --> 00:04:55,879
它的最后的损失非常高

148
00:04:55,879 --> 00:04:58,359
大概是0点几的样子

149
00:04:58,959 --> 00:04:59,719
看到了吧

150
00:04:59,800 --> 00:05:03,399
就是说跟之前比之前是0.01的样子

151
00:05:04,199 --> 00:05:05,839
所以这个就是underfitting

152
00:05:05,839 --> 00:05:07,519
就是说你根本就没下降

153
00:05:07,759 --> 00:05:08,879
你根本就没下降

154
00:05:09,000 --> 00:05:10,279
你之间训练误差

155
00:05:10,279 --> 00:05:11,479
测试误差之间的区别

156
00:05:11,479 --> 00:05:12,240
我就不管了

157
00:05:12,279 --> 00:05:14,799
就是说underfitting就欠你

158
00:05:14,920 --> 00:05:17,600
是说我的损失根本就没降

159
00:05:17,600 --> 00:05:19,199
就是说降的太高了

160
00:05:19,199 --> 00:05:21,120
就是说根本就没有训练好

161
00:05:21,120 --> 00:05:22,800
我的模型

162
00:05:22,879 --> 00:05:25,120
我们这里不是模型的容量不够

163
00:05:25,120 --> 00:05:26,120
是我们的数据

164
00:05:27,160 --> 00:05:27,960
没有给全

165
00:05:27,960 --> 00:05:29,080
所以你模型再好

166
00:05:29,080 --> 00:05:30,520
你也训练不出我的数据

167
00:05:30,960 --> 00:05:31,960
所以你可以简单认为

168
00:05:31,960 --> 00:05:32,960
这是一个欠你和

169
00:05:33,879 --> 00:05:34,879
过你和呢

170
00:05:36,800 --> 00:05:38,560
过你和可以看到是说

171
00:05:38,840 --> 00:05:39,879
我给你

172
00:05:39,920 --> 00:05:41,160
我把你整个数据给你

173
00:05:41,160 --> 00:05:43,160
其实你我这里有20列

174
00:05:43,160 --> 00:05:44,800
最后的16列都是

175
00:05:44,800 --> 00:05:46,439
理论上我的weight都是零

176
00:05:47,040 --> 00:05:48,840
然后在我把噪音都给你了

177
00:05:48,960 --> 00:05:50,280
我的模型不变的情况下

178
00:05:50,280 --> 00:05:51,400
我的数据变得

179
00:05:52,600 --> 00:05:54,840
更加有误解

180
00:05:54,840 --> 00:05:55,920
有误导性

181
00:05:55,960 --> 00:05:57,920
所以你会发现我会学到

182
00:05:57,960 --> 00:05:58,480
大量的

183
00:05:58,480 --> 00:06:00,800
我会基本上把后面的那些w

184
00:06:00,800 --> 00:06:01,600
本来是应该0的

185
00:06:01,600 --> 00:06:02,760
全部给你学出来了

186
00:06:02,879 --> 00:06:04,320
其实都是一些噪音

187
00:06:04,320 --> 00:06:06,240
我信息模型都帮你拟和了

188
00:06:06,560 --> 00:06:08,200
然后你能看到是说

189
00:06:09,960 --> 00:06:12,520
最后虽然你看到我的最后的loss

190
00:06:12,519 --> 00:06:14,039
没有真正的降太多

191
00:06:14,039 --> 00:06:14,839
但是你可以看到

192
00:06:14,839 --> 00:06:15,839
这个之间的gap

193
00:06:16,719 --> 00:06:18,359
就是说你可以多运行一次

194
00:06:18,359 --> 00:06:20,399
但是说中间是说你会发现

195
00:06:20,439 --> 00:06:21,919
他确实测试精度

196
00:06:21,919 --> 00:06:24,039
你其实跑到这个地方的时候

197
00:06:24,279 --> 00:06:25,680
可能跑到这个地方的时候

198
00:06:25,680 --> 00:06:28,000
你还是只把前面那几个东西

199
00:06:28,000 --> 00:06:28,799
学出来了

200
00:06:29,039 --> 00:06:31,240
但是你持续往下跑

201
00:06:31,240 --> 00:06:32,719
我就持续去学学学学学

202
00:06:32,719 --> 00:06:34,120
就把后面那一些

203
00:06:34,240 --> 00:06:35,279
不应该学的

204
00:06:35,279 --> 00:06:37,319
应该是0的东西全部学出来

205
00:06:37,399 --> 00:06:39,519
最后导致最后我的训练

206
00:06:39,560 --> 00:06:41,279
测试精度还测试误差

207
00:06:41,279 --> 00:06:42,240
还在往上升

208
00:06:42,839 --> 00:06:43,479
OK

209
00:06:43,599 --> 00:06:47,079
这个就是一个很直观的过拟和的解释

