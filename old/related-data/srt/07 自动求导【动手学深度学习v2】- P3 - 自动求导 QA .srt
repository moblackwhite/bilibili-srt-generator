1
00:00:00,000 --> 00:00:08,320
好,我们问题说,第一个是说PPT上的影视构造和显示构造看上去为啥差不多

2
00:00:08,320 --> 00:00:19,320
就是说,对,它看上去是差不多,但是它主要的区别是说显示构造是说我先把整个计算写出来

3
00:00:19,320 --> 00:00:29,039
然后再去给值,就是说你可以认为是说我要用Python来实现一个函数和用数学来实现是不一样的

4
00:00:29,399 --> 00:00:38,399
这个我们不会特别展开讲,但是它其实在你真的用的时候显示构造会非常不方便在很多时候

5
00:00:40,679 --> 00:00:45,359
问题18,需要正向和反向都算一遍吗?

6
00:00:45,960 --> 00:00:50,840
需要的,其实我不是很理解这个什么意思

7
00:00:50,840 --> 00:00:55,600
就是说当你在神级网络球T2的时候,你需要正着算一遍反着算一遍

8
00:00:56,039 --> 00:00:57,519
这个是肯定是要的

9
00:00:57,840 --> 00:01:08,359
然后当然是说反着算,就是说这种球导的时候你只要反向算,你就不需要正着再算一遍了

10
00:01:08,359 --> 00:01:13,760
就是说正向是说你需要把整个Y最后你的函数值给我算出来,对吧,然后我再反着来

11
00:01:15,840 --> 00:01:20,520
第19,为什么Python只会默认累积T2?

12
00:01:22,520 --> 00:01:25,359
就是说这个是一个设计上的一个理念

13
00:01:25,400 --> 00:01:32,280
就是说为什么,就是说我理解,就是说我当年我设计这个东西的时候说不会这么设计

14
00:01:32,280 --> 00:01:34,319
就是说为什么Python会这么设计呢?

15
00:01:34,319 --> 00:01:37,640
是因为通常你什么时候需要累积T2

16
00:01:38,159 --> 00:01:41,359
就是说假设我有一个批量

17
00:01:42,159 --> 00:01:45,920
然后呢,因为我是记得我们刚刚提过说内存耗的比较多

18
00:01:45,920 --> 00:01:49,200
因为你反向传播的话,你要把中间结果存起来

19
00:01:49,560 --> 00:01:52,680
Python对内存的管理一向来不是那么的好

20
00:01:53,000 --> 00:02:00,520
所以它如果对一个比较大的批量,你算不下怎么办?

21
00:02:01,080 --> 00:02:03,800
就是说假设我一个批量大小是128

22
00:02:03,800 --> 00:02:06,800
然后我算一次性算不出T2怎么办呢?

23
00:02:06,800 --> 00:02:12,800
我就把128的批量切成4下,每一个就是64

24
00:02:13,080 --> 00:02:17,280
64算一下,然后第二个64算一下,再算一下,算4下

25
00:02:17,520 --> 00:02:20,400
T2累加起来,就可以发出去了

26
00:02:20,719 --> 00:02:23,520
这就是它的一个比较重要的好处

27
00:02:23,800 --> 00:02:27,760
也另外一个是说,当你做一些Multi-Modality的时候

28
00:02:27,760 --> 00:02:33,680
就是说我有两个,我有一个weight在不同的模型之间share的时候

29
00:02:33,680 --> 00:02:35,599
我这样子累加也是有好处的

30
00:02:39,480 --> 00:02:43,680
问题20,我不是那么理解

31
00:02:44,040 --> 00:02:49,159
0246就是说,其实我不是那么懂

32
00:02:49,159 --> 00:02:53,639
应该我理解的意思就是说x的平方应该就是2乘x

33
00:02:53,639 --> 00:02:57,759
然后你就2乘以023,它就变成0246吧

34
00:03:01,960 --> 00:03:06,960
就为什么深度学习一般对标量求导,而不是对矩阵或者向量

35
00:03:07,479 --> 00:03:10,919
是因为你的Loss通常是一个标量

36
00:03:10,919 --> 00:03:15,000
你的精度也好,你的很多Loss,它的基础学习它都是一个标量

37
00:03:15,359 --> 00:03:18,719
如果你对于,如果你的Loss变成一个向量Loss的话

38
00:03:18,719 --> 00:03:24,960
那你就会麻烦,那就向量关于矩阵的Loss就变成一个矩阵

39
00:03:25,520 --> 00:03:28,719
那么矩阵再往下走一下就变成一个四维矩阵

40
00:03:28,719 --> 00:03:32,759
那么你神经网络一升,那你就变成一个特别大的张量

41
00:03:32,759 --> 00:03:35,759
你就算不出来了

42
00:03:35,919 --> 00:03:37,800
所以Loss通常是一个标量

43
00:03:41,560 --> 00:03:45,960
对,问题23,就今天的求导会在PyTorch讲实现吗

44
00:03:45,960 --> 00:03:47,159
我们确实讲了实现

45
00:03:47,199 --> 00:03:51,759
但我们没有讲就是说具体那一个我们说到的公式的实现

46
00:03:51,759 --> 00:03:52,719
大家可以去试一下

47
00:03:52,719 --> 00:03:57,680
我们不是在PBT里面有讲过说我们给一个公式怎么样把它求出来吗

48
00:03:57,680 --> 00:04:00,879
大家可以去试一下这个东西到底在PyTorch怎么实现

49
00:04:00,879 --> 00:04:03,560
然后去算一下PyTorch是不是算对了

50
00:04:04,000 --> 00:04:08,359
当然是说PyTorch对于我们转来转去它那个东西是没搞对的

51
00:04:08,359 --> 00:04:09,719
大家可以去体会一下

52
00:04:09,719 --> 00:04:21,319
多个Loss分别反向的时候是不是需要累积提读

53
00:04:21,319 --> 00:04:26,279
是的,就是说假设你在之后你的神经网络有多个损失函数的话

54
00:04:26,279 --> 00:04:27,879
你是需要做累加提读的

55
00:04:27,879 --> 00:04:31,319
这也是说刚刚提到为什么PyTorch默认是累加提读

56
00:04:34,439 --> 00:04:37,360
就为什么获取Grid的钱需要backward

57
00:04:37,480 --> 00:04:40,280
就是说你不去做backward的话

58
00:04:40,280 --> 00:04:41,720
它不会去计算T读

59
00:04:41,720 --> 00:04:43,680
因为计算T读是很贵的一件事情

60
00:04:43,680 --> 00:04:47,920
就是说我把比如说我给另一个input x

61
00:04:47,920 --> 00:04:49,160
我要把y算出来

62
00:04:49,160 --> 00:04:50,560
我已经花了一秒钟

63
00:04:50,560 --> 00:04:52,040
那么我要算T读的话

64
00:04:52,040 --> 00:04:53,960
同样的话要花另外一秒钟

65
00:04:53,960 --> 00:04:57,199
所以除非你一定说我需要T读

66
00:04:57,199 --> 00:04:59,360
就是说你一定告诉我说你要去run backward

67
00:04:59,360 --> 00:05:00,960
不然我自动不会帮你跑

68
00:05:00,960 --> 00:05:01,960
因为太贵了

69
00:05:02,879 --> 00:05:06,319
而且这个东西是说我需要内存

70
00:05:06,319 --> 00:05:07,680
占用非常多的内存

71
00:05:09,719 --> 00:05:12,719
求导过程是不是都是有向图

72
00:05:12,719 --> 00:05:14,599
也就是可以用数状来表示

73
00:05:14,599 --> 00:05:16,959
有没有其他环状结构

74
00:05:16,959 --> 00:05:18,279
有是有的

75
00:05:18,279 --> 00:05:19,439
就是循环神经网络

76
00:05:19,439 --> 00:05:21,639
你可以变成一个环状结构

77
00:05:21,639 --> 00:05:23,959
这样子的话会

78
00:05:23,959 --> 00:05:27,039
但是我们在计算上面会不会还是把它展开

79
00:05:27,039 --> 00:05:29,159
就是说到时候我们会去说

80
00:05:29,159 --> 00:05:32,079
它其实就是一个做T读累加了

81
00:05:32,079 --> 00:05:34,120
就是说在图上表示上

82
00:05:34,120 --> 00:05:35,439
我会把它展开

83
00:05:35,439 --> 00:05:37,120
虽然从逻辑上来上

84
00:05:37,120 --> 00:05:39,279
它是一个有环的一个图

85
00:05:42,439 --> 00:05:45,800
就是说PyTorch和M3可以实现矢量求导吗

86
00:05:45,800 --> 00:05:46,319
可以的

87
00:05:46,319 --> 00:05:48,759
就是说就高阶求导

88
00:05:48,759 --> 00:05:50,680
大家可以去看一下

89
00:05:50,680 --> 00:05:54,519
这high order gradient是支持的

90
00:05:54,519 --> 00:05:58,319
但是那个东西通常是说

91
00:05:58,319 --> 00:05:59,920
你要在优化算法里面

92
00:05:59,920 --> 00:06:01,800
用二阶优化算法是可以做

93
00:06:01,800 --> 00:06:02,920
但是那个东西很慢

94
00:06:02,920 --> 00:06:04,680
就是说大家可以去实验一下

