1
00:00:00,000 --> 00:00:05,879
我们刚刚讲过的是说标量的链式法则

2
00:00:05,879 --> 00:00:08,880
Y是一个U的一个函数

3
00:00:08,880 --> 00:00:11,400
U是关于X的一个函数

4
00:00:11,400 --> 00:00:13,679
那么Y对X求导的话

5
00:00:13,679 --> 00:00:18,679
那就是说我先把Y把U做成一个变量进来求导

6
00:00:18,679 --> 00:00:21,920
然后把U做成一个函数关于X的导数

7
00:00:21,920 --> 00:00:24,400
你要拓展到向量

8
00:00:24,400 --> 00:00:28,400
拓展到向量最大的问题是说你要把形状搞对

9
00:00:28,800 --> 00:00:31,320
就是说当你的Y是一个标量

10
00:00:31,320 --> 00:00:33,200
X是一个向量

11
00:00:33,200 --> 00:00:37,000
那么首先U也是一个标量

12
00:00:37,000 --> 00:00:40,000
那么这个当然是一个标量

13
00:00:40,000 --> 00:00:43,680
U关于X它就是一个一乘以N的一个东西

14
00:00:43,680 --> 00:00:45,680
那么一乘它就变成一乘N

15
00:00:45,680 --> 00:00:48,480
就是这个形状不发生变化

16
00:00:48,480 --> 00:00:52,160
那假设U是一个向量怎么办呢

17
00:00:52,160 --> 00:00:54,240
那就Y关于U

18
00:00:54,240 --> 00:00:55,400
那还是一个一乘K

19
00:00:55,400 --> 00:00:57,800
假设U是一个K位的一个向量

20
00:00:57,800 --> 00:00:58,840
那是一乘K

21
00:00:58,840 --> 00:01:02,000
那么U关于X它是一个K乘N的一个矩阵

22
00:01:02,000 --> 00:01:04,160
那么它一乘还会变成一乘N

23
00:01:04,160 --> 00:01:06,680
然后我们来挡住了

24
00:01:06,680 --> 00:01:09,159
然后你的Y是一个向量

25
00:01:09,159 --> 00:01:10,439
X是一个向量

26
00:01:10,439 --> 00:01:12,040
U也是一个向量

27
00:01:12,040 --> 00:01:13,040
同样的道理

28
00:01:13,040 --> 00:01:15,719
假设你的是一个M乘N的话

29
00:01:15,719 --> 00:01:18,400
那么它就是U是一个长为K的话

30
00:01:18,400 --> 00:01:19,719
那么它是一个M乘K

31
00:01:19,719 --> 00:01:20,920
然后它是一个K乘N

32
00:01:20,920 --> 00:01:22,560
这样子两个矩阵一乘

33
00:01:22,560 --> 00:01:25,280
它还是变成一个M乘N的一个矩阵

34
00:01:27,840 --> 00:01:30,239
OK我们举两个例子

35
00:01:30,239 --> 00:01:31,959
具体是怎么计算的

36
00:01:31,959 --> 00:01:35,280
这几个这其实就是一个我们之后要讲的

37
00:01:35,280 --> 00:01:38,239
线性回归的一个例子

38
00:01:38,239 --> 00:01:42,959
首先假设X和W它都是一个长为N的向量

39
00:01:42,959 --> 00:01:44,719
Y是一个标量

40
00:01:44,719 --> 00:01:48,560
那么的函数Z是说X和W做累积

41
00:01:48,560 --> 00:01:51,159
减去Y然后做平方

42
00:01:51,159 --> 00:01:55,560
那我们要计算Z关于W的一个倒数

43
00:01:55,560 --> 00:01:57,039
那我们怎么做呢

44
00:01:57,040 --> 00:01:58,560
那我们就是先分解吧

45
00:01:58,560 --> 00:02:00,000
我们先把它写开

46
00:02:00,000 --> 00:02:01,480
首先说我记一个A

47
00:02:01,480 --> 00:02:02,719
中间变量A

48
00:02:02,719 --> 00:02:05,040
它是X和W的累积

49
00:02:05,040 --> 00:02:07,000
B是A减去Y

50
00:02:07,000 --> 00:02:08,680
然后Z是等于B的平方

51
00:02:08,680 --> 00:02:11,560
这样子我们把它分解成三个步骤

52
00:02:11,560 --> 00:02:13,360
然后我们用链式法则

53
00:02:13,360 --> 00:02:15,840
那Z关于W的倒数就是说

54
00:02:15,840 --> 00:02:17,159
Z关于B的倒数

55
00:02:17,159 --> 00:02:18,439
B关于A的倒数

56
00:02:18,439 --> 00:02:20,360
A关于W的倒数

57
00:02:20,360 --> 00:02:23,439
然后我们把这个ZBA的定义展开

58
00:02:23,439 --> 00:02:25,439
那就B的平方关于B

59
00:02:25,439 --> 00:02:27,199
A减Y关于A

60
00:02:27,199 --> 00:02:30,759
X和W的累积关于W

61
00:02:30,759 --> 00:02:33,439
那么第一项我们知道就是2B对吧

62
00:02:33,439 --> 00:02:35,680
那么这一块就是因为它是一个

63
00:02:35,680 --> 00:02:37,079
这是1

64
00:02:37,079 --> 00:02:39,039
而这一块我们之前有讲到

65
00:02:39,039 --> 00:02:41,639
它就是X的转制

66
00:02:41,639 --> 00:02:43,680
那么再把B的定义拆开

67
00:02:43,680 --> 00:02:44,759
就B是怎么定义的

68
00:02:44,759 --> 00:02:46,280
然后它展成这个样子

69
00:02:46,280 --> 00:02:47,599
那么就会得到说

70
00:02:47,599 --> 00:02:49,439
Z关于W的倒数

71
00:02:49,439 --> 00:02:52,719
那就是W和X的累积

72
00:02:52,719 --> 00:02:54,960
减去Y然后乘以X的转制

73
00:02:54,960 --> 00:02:56,400
这是一个标量

74
00:02:56,400 --> 00:02:57,520
所以它是说

75
00:02:57,520 --> 00:02:58,719
因为它是一个

76
00:02:58,719 --> 00:03:00,080
它的向量在下面

77
00:03:00,080 --> 00:03:02,879
所以它出来的是一个转制的一个向量

78
00:03:02,879 --> 00:03:03,560
OK

79
00:03:03,560 --> 00:03:05,800
这是我们用刚刚的链式法则

80
00:03:05,800 --> 00:03:07,439
和我们刚刚定义的来

81
00:03:07,439 --> 00:03:10,280
对线性回归做一次求导

82
00:03:11,200 --> 00:03:13,280
那我们可以再往前走一点点

83
00:03:13,280 --> 00:03:16,120
就说我们涉及到矩阵了

84
00:03:16,120 --> 00:03:20,280
X是一个M乘以N的一个矩阵

85
00:03:20,280 --> 00:03:22,520
乘以W一个向量

86
00:03:22,520 --> 00:03:24,000
减去另外一个向量

87
00:03:24,039 --> 00:03:25,240
做L2 long

88
00:03:25,840 --> 00:03:26,520
同样的道理

89
00:03:26,520 --> 00:03:27,960
它是一个标量

90
00:03:27,960 --> 00:03:28,360
对吧

91
00:03:28,360 --> 00:03:30,960
所以我要对标量对向量求导

92
00:03:31,840 --> 00:03:34,280
其实这个跟之前是一样的

93
00:03:34,280 --> 00:03:35,439
我们就是说

94
00:03:35,439 --> 00:03:37,639
首先我就用一个中间变量

95
00:03:37,639 --> 00:03:38,719
A是一个向量

96
00:03:38,719 --> 00:03:40,120
等于X乘以W

97
00:03:40,520 --> 00:03:42,080
B等于A减去Y

98
00:03:42,080 --> 00:03:44,280
Z就等于B的long

99
00:03:44,759 --> 00:03:45,319
同样的话

100
00:03:45,319 --> 00:03:46,400
我们用这个展开

101
00:03:46,400 --> 00:03:47,960
我们就不详细讲了

102
00:03:47,960 --> 00:03:48,960
那么就是说

103
00:03:48,960 --> 00:03:51,920
它的本身是一个2的B的转制

104
00:03:52,080 --> 00:03:53,480
乘以它是一个

105
00:03:53,480 --> 00:03:56,920
就是说一个identity matrix

106
00:03:57,240 --> 00:03:58,560
它我们刚刚讲过

107
00:03:58,560 --> 00:03:59,800
它就是X的本身

108
00:04:00,080 --> 00:04:02,160
那么最后的把B展开的话

109
00:04:02,160 --> 00:04:03,000
那就是

110
00:04:03,680 --> 00:04:05,320
X乘以W减去Y的转制

111
00:04:05,320 --> 00:04:06,360
乘以X

112
00:04:06,360 --> 00:04:07,360
然后乘2

113
00:04:07,960 --> 00:04:08,280
OK

114
00:04:08,280 --> 00:04:08,560
这样子

115
00:04:08,560 --> 00:04:09,120
我们也知道

116
00:04:09,120 --> 00:04:10,960
我们的矩阵是怎么求出来的

117
00:04:12,400 --> 00:04:13,120
好

118
00:04:13,560 --> 00:04:14,800
我们的两个例子

119
00:04:14,800 --> 00:04:16,680
那我们讲到现在是说

120
00:04:16,680 --> 00:04:18,080
我们能够做一个

121
00:04:18,080 --> 00:04:19,560
如果你真的给我一个函数

122
00:04:19,560 --> 00:04:20,840
我能够通过链式法

123
00:04:20,840 --> 00:04:22,240
折和一些很基础的

124
00:04:22,240 --> 00:04:23,000
导数的定义

125
00:04:23,000 --> 00:04:25,440
我可以把你一个一个展开

126
00:04:25,560 --> 00:04:27,600
但是最大的问题是说

127
00:04:27,600 --> 00:04:29,480
神经网络动不动就几百层

128
00:04:29,800 --> 00:04:32,480
几乎是你手写是很难的一件事情

129
00:04:32,920 --> 00:04:35,800
所以我们需要说能够自动求导

130
00:04:36,640 --> 00:04:38,280
自动求导意思是说

131
00:04:38,280 --> 00:04:40,760
一个函数在指定值上

132
00:04:40,760 --> 00:04:41,960
我要做求导数

133
00:04:42,400 --> 00:04:44,280
它其实还有两种不一样的定义

134
00:04:44,280 --> 00:04:45,960
一个叫符号求导

135
00:04:46,200 --> 00:04:48,320
如果大家用过mathematica的话

136
00:04:48,320 --> 00:04:49,840
就是说我给你个函数

137
00:04:50,040 --> 00:04:51,840
我能把你的导数求出来

138
00:04:51,840 --> 00:04:53,640
而且这是一个显示的计算

139
00:04:54,240 --> 00:04:56,560
另外一个是叫数值值求导

140
00:04:56,920 --> 00:04:59,240
就是说我给你任何一个fx

141
00:04:59,240 --> 00:05:00,200
我不需要知道

142
00:05:00,200 --> 00:05:01,760
f本身长什么样子

143
00:05:02,040 --> 00:05:04,400
然后我能通过数值去拟和导数

144
00:05:04,400 --> 00:05:06,560
就是说我用一个很小的h

145
00:05:06,680 --> 00:05:08,720
减去fx除以h

146
00:05:08,960 --> 00:05:10,280
所以我们来讲一下

147
00:05:10,280 --> 00:05:12,440
自动求导是怎么做出来的

148
00:05:12,440 --> 00:05:15,800
这叫涉及到一个叫计算图的概念

149
00:05:16,360 --> 00:05:19,120
就是说虽然我们用的pytorch

150
00:05:19,120 --> 00:05:21,480
不需要去大家去理解计算图

151
00:05:21,480 --> 00:05:25,000
但是我觉得大家有必要去知道一下

152
00:05:25,160 --> 00:05:27,720
它的内部的一个是怎么样工作原理

153
00:05:27,840 --> 00:05:29,000
就是说这样子的话

154
00:05:29,000 --> 00:05:30,079
你如果用Tensorflow

155
00:05:30,079 --> 00:05:31,680
用msnet或用别的话

156
00:05:31,680 --> 00:05:34,280
你大概能够理解计算图是什么样子

157
00:05:34,840 --> 00:05:36,519
就计算图其实本质上

158
00:05:36,519 --> 00:05:38,959
就等价于我们刚刚用链式法则的

159
00:05:38,959 --> 00:05:40,600
一个求导的一个过程

160
00:05:41,360 --> 00:05:45,800
首先我们将代码分解成操作子

161
00:05:46,079 --> 00:05:47,879
就是说一步一步把它展开

162
00:05:48,439 --> 00:05:52,319
然后我们再将计算表示成一个无患的图

163
00:05:52,759 --> 00:05:54,560
就是我们还是用刚刚样例

164
00:05:55,240 --> 00:05:59,360
那就是说z等于x和w累积减去y

165
00:05:59,360 --> 00:06:00,480
然后求平方

166
00:06:00,920 --> 00:06:01,480
我们怎么做

167
00:06:01,639 --> 00:06:04,159
就是按照刚刚的做法

168
00:06:04,159 --> 00:06:06,959
我们把一步一步做成

169
00:06:07,519 --> 00:06:09,639
加入两个中间倍量a和b

170
00:06:09,680 --> 00:06:10,879
把它每一个这样子

171
00:06:10,879 --> 00:06:13,000
做成一个很基本的计算值

172
00:06:13,600 --> 00:06:16,240
所以每一个圈就表示一个操作

173
00:06:16,439 --> 00:06:17,800
但也可以表示一个输入

174
00:06:17,800 --> 00:06:18,120
对吧

175
00:06:18,120 --> 00:06:19,319
这一个圈表示w

176
00:06:19,319 --> 00:06:20,680
这个圈表示x

177
00:06:20,680 --> 00:06:23,319
然后这个圈表示a是在这里做计算了

178
00:06:23,319 --> 00:06:26,960
y再把a和y输入进去

179
00:06:26,960 --> 00:06:27,680
就会得到b

180
00:06:27,680 --> 00:06:29,240
最后输入b得到z

181
00:06:29,920 --> 00:06:31,360
这个就是计算图了

182
00:06:31,360 --> 00:06:33,600
就计算图就是一个无患的一个图

183
00:06:37,319 --> 00:06:38,600
当然是说你计算图

184
00:06:38,600 --> 00:06:40,160
有很多种构造的方式

185
00:06:40,160 --> 00:06:41,439
你可以显示的构造

186
00:06:41,720 --> 00:06:42,879
msnet和Tensorflow

187
00:06:42,879 --> 00:06:44,800
它都可以显示的构造计算图

188
00:06:45,199 --> 00:06:46,280
我定一个symbol

189
00:06:46,600 --> 00:06:47,920
定一个变量叫a

190
00:06:47,920 --> 00:06:48,960
定一个变量叫b

191
00:06:48,960 --> 00:06:50,560
然后定义的是a乘b

192
00:06:50,560 --> 00:06:52,439
2乘a加上b

193
00:06:52,639 --> 00:06:55,000
然后我再说a的值等于多少

194
00:06:55,000 --> 00:06:55,879
b的值等于多少

195
00:06:55,879 --> 00:06:56,639
可以求c

196
00:06:57,000 --> 00:07:00,319
就是说数学上大家都是公式的定义

197
00:07:00,319 --> 00:07:00,920
就是这样子

198
00:07:00,920 --> 00:07:03,240
就是我先定好公式长什么样子

199
00:07:03,240 --> 00:07:04,439
然后我再代入值

200
00:07:04,439 --> 00:07:06,720
所以数学上大家都是用

201
00:07:07,600 --> 00:07:09,560
用的是显示的构造

202
00:07:10,400 --> 00:07:12,040
当然你可以做影视的构造

203
00:07:12,040 --> 00:07:13,199
PyTorch mset

204
00:07:13,199 --> 00:07:15,000
它也是可以用影视的构造

205
00:07:15,039 --> 00:07:15,959
就是说或者

206
00:07:16,560 --> 00:07:19,079
就是说mset它有个ndarray

207
00:07:19,079 --> 00:07:20,039
就是说你可以

208
00:07:20,319 --> 00:07:23,039
放着把它放在一个地方

209
00:07:23,039 --> 00:07:24,319
就是说我一样

210
00:07:24,560 --> 00:07:26,480
之前我们讲到所有的Tensor的计算

211
00:07:26,480 --> 00:07:26,839
一样的

212
00:07:26,839 --> 00:07:28,000
我们把它构造出来

213
00:07:28,000 --> 00:07:30,039
但是告诉系统说你把我记住

214
00:07:30,159 --> 00:07:32,319
就是说我是一步步这么做的

215
00:07:32,360 --> 00:07:35,159
系统把你所有的计算给记住下来

216
00:07:35,159 --> 00:07:37,000
就是影视的构造

217
00:07:38,199 --> 00:07:38,879
OK

218
00:07:39,599 --> 00:07:42,120
然后假设我有了计算图

219
00:07:42,120 --> 00:07:44,759
我有两种自动求导的方式

220
00:07:45,639 --> 00:07:47,240
就是说回忆下链式法则

221
00:07:47,959 --> 00:07:50,879
就假设我Y是关于X的一个函数

222
00:07:51,120 --> 00:07:53,519
然后我有U2到UN

223
00:07:53,519 --> 00:07:55,360
这个N-1个

224
00:07:55,360 --> 00:07:58,360
就U1到UN这N个中间边量的话

225
00:07:58,399 --> 00:08:00,759
那么首先说Y是UN

226
00:08:00,759 --> 00:08:02,040
然后一直到最后

227
00:08:02,040 --> 00:08:04,439
那就U1关于X的一个求导

228
00:08:05,639 --> 00:08:08,480
那么我要计算整个过程的话

229
00:08:08,480 --> 00:08:09,560
我有两种方法

230
00:08:09,560 --> 00:08:10,600
一个是正着计算

231
00:08:10,600 --> 00:08:11,840
或者一个反着计算

232
00:08:12,399 --> 00:08:13,840
正着计算就是说

233
00:08:13,840 --> 00:08:16,959
我先把我从X出发

234
00:08:17,319 --> 00:08:19,519
U1关于X的导数求出来

235
00:08:19,720 --> 00:08:21,480
U2关于X的导数求出来

236
00:08:21,480 --> 00:08:22,319
然后一乘

237
00:08:22,360 --> 00:08:23,720
再往下

238
00:08:23,720 --> 00:08:24,840
最后算出来

239
00:08:25,519 --> 00:08:27,240
Y关于UN的一个导数

240
00:08:28,439 --> 00:08:29,959
我也可以反过来

241
00:08:30,360 --> 00:08:31,879
反过来怎么做呢

242
00:08:32,159 --> 00:08:33,960
就是说我先计算

243
00:08:34,000 --> 00:08:35,879
Y就是最终的函数

244
00:08:35,879 --> 00:08:38,200
关于最后的中间边量的一个导数

245
00:08:38,560 --> 00:08:40,519
再乘以导数第二个

246
00:08:40,560 --> 00:08:41,759
然后再往前

247
00:08:41,759 --> 00:08:43,759
一直算到最前面

248
00:08:43,960 --> 00:08:45,639
这个又叫反向传递

249
00:08:45,639 --> 00:08:47,600
就是说自动求导

250
00:08:47,600 --> 00:08:49,159
是一个很古老的领域

251
00:08:49,200 --> 00:08:51,200
然后它反向累积

252
00:08:51,200 --> 00:08:52,439
在人工智能里面

253
00:08:52,439 --> 00:08:53,639
它是大名鼎鼎

254
00:08:53,639 --> 00:08:55,960
叫做BEP Propagation

255
00:08:55,960 --> 00:08:56,960
叫反向传递

256
00:08:56,960 --> 00:08:58,120
其实是一个东西

257
00:08:59,879 --> 00:09:00,480
OK

258
00:09:01,159 --> 00:09:02,319
就是说我们给个例子

259
00:09:02,319 --> 00:09:03,799
就是说反向传递

260
00:09:03,799 --> 00:09:05,799
是到底是怎么样计算的

261
00:09:07,000 --> 00:09:07,279
好

262
00:09:07,279 --> 00:09:09,439
我们首先说正向是这样子算的

263
00:09:09,439 --> 00:09:11,000
在我们刚刚讲过了正向

264
00:09:11,200 --> 00:09:12,440
我们来看一下反向

265
00:09:13,039 --> 00:09:15,360
反向是说我先来算

266
00:09:15,399 --> 00:09:17,000
Z关于B的导数

267
00:09:18,000 --> 00:09:19,759
那就是说我需要去

268
00:09:20,159 --> 00:09:21,759
它就等于2乘以B

269
00:09:21,759 --> 00:09:22,200
对吧

270
00:09:22,200 --> 00:09:23,120
就根据那个

271
00:09:23,240 --> 00:09:27,000
然后因为B是我们之前的计算的结果

272
00:09:27,039 --> 00:09:29,320
就是说你我需要在

273
00:09:29,320 --> 00:09:31,600
那我需要把之前的B的计算

274
00:09:31,600 --> 00:09:33,080
给你存在那个地方

275
00:09:33,240 --> 00:09:34,519
然后把它读出来

276
00:09:35,200 --> 00:09:37,480
就是说我需要读取之前的结果

277
00:09:38,360 --> 00:09:38,960
同样的话

278
00:09:38,960 --> 00:09:41,320
我可以计算Z关于A的一个导数

279
00:09:41,320 --> 00:09:42,360
它就等于是说

280
00:09:42,360 --> 00:09:43,720
我们之前有这个结果了

281
00:09:43,720 --> 00:09:44,639
可以算出来

282
00:09:46,280 --> 00:09:49,320
最后我们计算Z关于W的一个导数

283
00:09:49,600 --> 00:09:52,160
我需要既需要知道这个值

284
00:09:52,200 --> 00:09:53,960
也需要知道W这个值

285
00:09:54,000 --> 00:09:55,320
就是说从这里过来

286
00:09:55,320 --> 00:09:57,400
就是说我需要两个值都需要知道

287
00:09:59,160 --> 00:09:59,960
然后

288
00:10:00,160 --> 00:10:01,200
所以的话

289
00:10:01,200 --> 00:10:02,759
反向累积的话

290
00:10:02,759 --> 00:10:03,600
就是说

291
00:10:03,639 --> 00:10:05,480
我前向计算的时候

292
00:10:05,480 --> 00:10:07,519
我需要把我所有的中间的值

293
00:10:07,519 --> 00:10:08,720
给你存下来

294
00:10:09,800 --> 00:10:11,280
但反向执行的话

295
00:10:11,280 --> 00:10:12,120
我就是说

296
00:10:12,160 --> 00:10:14,400
我就沿着反方向进行

297
00:10:14,560 --> 00:10:16,560
如果我这两个值的导数不需要的话

298
00:10:16,560 --> 00:10:17,600
我就不计算了

299
00:10:17,759 --> 00:10:20,759
但是我需要把这些中间结果

300
00:10:20,759 --> 00:10:21,720
全部拿过来用

301
00:10:23,320 --> 00:10:24,080
所以的话

302
00:10:24,080 --> 00:10:25,840
它的复杂度是什么样子呢

303
00:10:26,680 --> 00:10:28,400
假设我有N个这样子

304
00:10:28,440 --> 00:10:30,160
操作子的话

305
00:10:30,280 --> 00:10:30,879
就是说比如说

306
00:10:30,879 --> 00:10:32,560
我有我的神经网络有N层

307
00:10:32,920 --> 00:10:35,000
那么正向和反向的代价

308
00:10:35,000 --> 00:10:36,040
其实差不多的

309
00:10:36,040 --> 00:10:37,280
就是你正的跑一遍

310
00:10:37,280 --> 00:10:38,200
反的跑一遍

311
00:10:38,240 --> 00:10:39,280
就是说你到之后

312
00:10:39,280 --> 00:10:41,080
我们看到神经网络的

313
00:10:41,080 --> 00:10:42,280
forward和backward

314
00:10:42,280 --> 00:10:44,000
它的计算复杂度差不多

315
00:10:44,520 --> 00:10:47,040
但是比较重要的是说

316
00:10:47,040 --> 00:10:49,040
它的内存复杂度是ON

317
00:10:49,120 --> 00:10:51,560
就是说你需要把正向计算里面

318
00:10:51,560 --> 00:10:53,600
所谓的中间结果给你存起来

319
00:10:53,879 --> 00:10:56,200
这个是给我们说

320
00:10:56,240 --> 00:10:59,120
深度神经网络特别耗GPU资源

321
00:10:59,120 --> 00:11:01,280
这个是它最大的货源

322
00:11:01,280 --> 00:11:02,320
就是在这个地方

323
00:11:03,920 --> 00:11:06,560
这就是因为做气球梯度的时候

324
00:11:06,560 --> 00:11:07,760
我需要把前面的结果

325
00:11:07,760 --> 00:11:08,960
全给你存下来

326
00:11:09,760 --> 00:11:11,200
就是说它有一个

327
00:11:11,200 --> 00:11:13,400
另外一个我们提到是说正向累积

328
00:11:13,840 --> 00:11:16,000
正向累积的好处是说

329
00:11:16,000 --> 00:11:18,600
它的内存复杂度是ONE

330
00:11:18,720 --> 00:11:21,000
就是说不管我要有多深

331
00:11:21,000 --> 00:11:23,040
我不需要存任何的结果

332
00:11:23,160 --> 00:11:24,640
但它的问题是说

333
00:11:24,680 --> 00:11:26,280
我计算一个边的梯度

334
00:11:26,280 --> 00:11:27,200
需要扫一遍

335
00:11:27,200 --> 00:11:27,800
那个的话

336
00:11:27,800 --> 00:11:29,240
我还是需要再扫一遍

337
00:11:29,240 --> 00:11:31,760
所以是说我们通常在神经网络里面

338
00:11:31,760 --> 00:11:32,200
不会用

339
00:11:32,200 --> 00:11:35,000
因为我们需要对每一层计算梯度

340
00:11:35,039 --> 00:11:36,840
所以计算复杂度太高

341
00:11:36,960 --> 00:11:37,399
OK

342
00:11:37,399 --> 00:11:39,480
所以这个就是大概的结果

