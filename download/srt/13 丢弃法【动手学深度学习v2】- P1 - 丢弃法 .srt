1
00:00:00,000 --> 00:00:02,879
第1个问题就是问题11

2
00:00:02,879 --> 00:00:09,279
就是dropout随机致灵对球梯度和反向梯度的影响是什么

3
00:00:10,200 --> 00:00:15,359
如果你被随机致灵

4
00:00:15,359 --> 00:00:17,039
那么你的梯度也会变向0

5
00:00:20,240 --> 00:00:23,080
所以就是说但是你没有这点地方

6
00:00:23,080 --> 00:00:24,600
你会乘了一个数

7
00:00:24,600 --> 00:00:24,960
对吧

8
00:00:24,960 --> 00:00:27,560
梯度也会在相应的乘一个数

9
00:00:27,920 --> 00:00:30,120
就是说dropout你可以认为这个函数

10
00:00:30,120 --> 00:00:33,679
对于球梯度其实是一个对称的一个函数

11
00:00:36,079 --> 00:00:39,000
而且所以就意味着说

12
00:00:39,000 --> 00:00:42,640
等于就是你dropout的那一些输出

13
00:00:42,640 --> 00:00:44,000
它对应的那些权重

14
00:00:44,000 --> 00:00:45,520
它就这一轮就不会被更新

15
00:00:47,400 --> 00:00:50,079
所以你所以就为什么Hinton在一开始

16
00:00:50,079 --> 00:00:53,040
就是说觉得dropout就是一个

17
00:00:53,040 --> 00:00:56,120
每一次我随机的在里面采样一个小网络

18
00:00:56,160 --> 00:00:57,880
然后拿出来进行更新

19
00:00:57,880 --> 00:01:01,160
没有被对被dropout丢掉的那些残

20
00:01:01,160 --> 00:01:04,320
那些隐藏成对应的那些权重

21
00:01:04,320 --> 00:01:05,240
它就不被更新了

22
00:01:05,240 --> 00:01:06,880
就是说他那就没有拿出来

23
00:01:13,159 --> 00:01:15,439
丢弃法的丢弃依据是什么

24
00:01:15,439 --> 00:01:16,760
如果丢弃不合理

25
00:01:16,760 --> 00:01:19,520
对输出的结果会影响很大

26
00:01:19,520 --> 00:01:22,400
他其实你可以简单认为他就是一个正轴向

27
00:01:22,400 --> 00:01:23,719
就跟我们之前的是一样的

28
00:01:23,719 --> 00:01:25,920
就是说你丢所谓的丢弃不合理

29
00:01:25,920 --> 00:01:27,640
就是说你丢弃率没事

30
00:01:27,640 --> 00:01:29,519
要么太小了

31
00:01:29,519 --> 00:01:29,920
太小了

32
00:01:29,920 --> 00:01:35,079
就是说你对模型的正轴效果不好

33
00:01:35,079 --> 00:01:36,519
不大不够

34
00:01:36,519 --> 00:01:37,840
所以你还是overfeeling

35
00:01:37,840 --> 00:01:39,079
就还是过拧合

36
00:01:39,359 --> 00:01:40,599
就要么就太大了

37
00:01:40,599 --> 00:01:42,120
那就是欠拧合

38
00:01:43,040 --> 00:01:43,480
对吧

39
00:01:43,480 --> 00:01:45,400
就是合适比较好

40
00:01:45,799 --> 00:01:47,040
所以就是一个超三数

41
00:01:47,040 --> 00:01:48,359
你就是你可以调

42
00:01:52,840 --> 00:01:53,960
问题13

43
00:01:54,520 --> 00:01:56,120
dropout的随机丢弃

44
00:01:56,120 --> 00:01:58,880
如何保证结果的正确性和可重复性

45
00:01:58,880 --> 00:02:02,000
这个是个正确性和可重复性

46
00:02:02,000 --> 00:02:02,760
是两个问题

47
00:02:02,760 --> 00:02:05,240
所谓的正确性

48
00:02:05,240 --> 00:02:06,040
这没有正确

49
00:02:06,040 --> 00:02:08,040
这机器学习没有正确性

50
00:02:08,560 --> 00:02:10,319
机器学习效果好不好

51
00:02:10,319 --> 00:02:11,439
哪有正确不正确

52
00:02:11,439 --> 00:02:12,319
就是说

53
00:02:12,920 --> 00:02:15,240
所以机器学习特别是神经网络

54
00:02:15,240 --> 00:02:17,120
因为他那个网络很复杂

55
00:02:17,120 --> 00:02:19,240
用随机提速下降

56
00:02:19,240 --> 00:02:20,240
它的稳定性很好

57
00:02:20,240 --> 00:02:21,040
所以

58
00:02:21,439 --> 00:02:22,439
所以说

59
00:02:22,599 --> 00:02:24,359
假设你代码有bug

60
00:02:24,799 --> 00:02:26,639
很有可能你是看不出来的

61
00:02:26,639 --> 00:02:28,560
很有可能你代码写错了

62
00:02:28,560 --> 00:02:30,319
一个可能是一个很大的bug

63
00:02:31,159 --> 00:02:33,719
比如说我们最早在做msnet的时候

64
00:02:33,719 --> 00:02:34,759
我们的

65
00:02:35,159 --> 00:02:36,399
全AlexNet的时候

66
00:02:36,399 --> 00:02:38,359
我们有一个隐藏层没写好

67
00:02:38,359 --> 00:02:40,079
我们就cuda没写好

68
00:02:40,079 --> 00:02:41,840
就是我们在隐藏层

69
00:02:41,840 --> 00:02:43,280
最后一个隐藏层的有一些

70
00:02:43,280 --> 00:02:44,959
比如说有一个

71
00:02:45,000 --> 00:02:47,399
有一些输出永远是至上点

72
00:02:47,919 --> 00:02:49,199
就根本就没有把

73
00:02:49,240 --> 00:02:51,640
就把全中国输出给丢掉了

74
00:02:51,720 --> 00:02:52,760
就这么大的bug

75
00:02:52,760 --> 00:02:55,040
其实也没有可能对你的accuracy

76
00:02:55,040 --> 00:02:56,240
或者会就是你的精度

77
00:02:56,400 --> 00:02:57,640
可能会丢那么一个点

78
00:02:57,640 --> 00:02:59,120
零点几个点或一个点

79
00:02:59,240 --> 00:03:00,000
所以

80
00:03:01,320 --> 00:03:02,680
所以就是说

81
00:03:02,960 --> 00:03:04,520
你不知道结果

82
00:03:04,520 --> 00:03:06,520
你就算你代码没写错

83
00:03:06,520 --> 00:03:07,320
或者jump out怎么样

84
00:03:07,320 --> 00:03:08,040
你也不知道

85
00:03:08,080 --> 00:03:09,120
结果是不是正确

86
00:03:09,120 --> 00:03:11,160
所以没有正确性可以

87
00:03:12,080 --> 00:03:14,080
就是说只有说精度好不好可以

88
00:03:14,440 --> 00:03:17,000
但我是说从实际上来说

89
00:03:17,360 --> 00:03:18,760
统计上来说就不一样了

90
00:03:18,919 --> 00:03:20,280
但是统计者的东西

91
00:03:20,280 --> 00:03:22,159
你在实际上也验证不了

92
00:03:22,599 --> 00:03:22,919
所以

93
00:03:23,359 --> 00:03:26,879
所以你可能唯一注意的是可重复性

94
00:03:27,120 --> 00:03:28,519
所谓的可重复性

95
00:03:28,519 --> 00:03:29,399
就是说

96
00:03:29,439 --> 00:03:30,919
你下一次运行的时候

97
00:03:30,919 --> 00:03:33,399
因为jump out的每次随机丢值

98
00:03:33,840 --> 00:03:34,959
你下次做的时候

99
00:03:34,959 --> 00:03:35,719
你下次跑的时候

100
00:03:35,719 --> 00:03:36,840
你结果就不一样了

101
00:03:38,479 --> 00:03:40,000
所以这个是一个

102
00:03:40,000 --> 00:03:42,120
整个神经网络的可重复性

103
00:03:42,120 --> 00:03:43,879
一直就是一个非常难的事情

104
00:03:44,359 --> 00:03:46,239
但jump out还好一点

105
00:03:46,360 --> 00:03:48,280
就jump out的它的bug

106
00:03:48,280 --> 00:03:49,320
不是那么明显

107
00:03:49,320 --> 00:03:52,200
是说你只要固定你的随机种子

108
00:03:53,200 --> 00:03:54,600
所谓的随机种子

109
00:03:54,600 --> 00:03:55,360
大家可以试一下

110
00:03:55,439 --> 00:03:57,400
我固定住随机种子的话

111
00:03:57,439 --> 00:03:59,280
我每一次我run10次

112
00:03:59,360 --> 00:04:00,439
就是说我就

113
00:04:00,439 --> 00:04:01,840
我还是就jump out

114
00:04:01,840 --> 00:04:03,840
我就把random seat固定住

115
00:04:03,840 --> 00:04:05,160
就是random的函数的

116
00:04:05,160 --> 00:04:06,640
random seat固定住的话

117
00:04:06,760 --> 00:04:08,600
那么我jump out10次

118
00:04:09,640 --> 00:04:11,120
然后我再重复的run10次

119
00:04:11,120 --> 00:04:12,280
结果应该是一样的

120
00:04:13,040 --> 00:04:14,000
所以就是说

121
00:04:14,040 --> 00:04:15,760
如果你真的想重复性的话

122
00:04:15,760 --> 00:04:18,080
你可以把你的随机种子固定住

123
00:04:18,240 --> 00:04:19,160
就是说你可以保证

124
00:04:19,160 --> 00:04:20,480
下一次应该是差不多了

125
00:04:21,560 --> 00:04:22,319
当然了

126
00:04:23,000 --> 00:04:25,040
但是你整个神经网络的随机性

127
00:04:25,080 --> 00:04:25,879
还是挺大的

128
00:04:25,879 --> 00:04:28,400
你的权重的初始是随机的

129
00:04:28,400 --> 00:04:28,840
对吧

130
00:04:28,879 --> 00:04:31,800
如果你真的要做可重复的训练的话

131
00:04:31,800 --> 00:04:34,640
你的随机种子得重复住

132
00:04:34,680 --> 00:04:37,240
然后你的jump out里面是要重复住

133
00:04:37,240 --> 00:04:40,280
应该这是两大主要的随机数的来源

134
00:04:40,480 --> 00:04:42,920
第三大数就是你很难控制的

135
00:04:42,920 --> 00:04:45,560
就是假设你用的是cuda的话

136
00:04:45,560 --> 00:04:47,959
NVIDIA的cudan这个library

137
00:04:48,519 --> 00:04:50,759
它是用来加速你整个矩阵运算的

138
00:04:50,879 --> 00:04:52,240
你不用cudan的话

139
00:04:52,600 --> 00:04:54,160
基本上所有的主流函数

140
00:04:54,160 --> 00:04:56,480
主流framework都是用了cudan

141
00:04:56,480 --> 00:04:57,560
但你可以禁掉它

142
00:04:57,879 --> 00:04:58,840
就只要用了

143
00:04:58,879 --> 00:05:00,079
用cudan会给你带来

144
00:05:00,079 --> 00:05:02,600
比如说50%或者80%的加速

145
00:05:02,639 --> 00:05:03,720
你不用cudan的话

146
00:05:03,720 --> 00:05:04,639
你会慢一些

147
00:05:05,000 --> 00:05:06,079
就GPU的话

148
00:05:06,120 --> 00:05:08,240
但是cudan它每次算的

149
00:05:08,279 --> 00:05:08,959
矩阵乘法

150
00:05:08,959 --> 00:05:10,319
算计的结果是不一样的

151
00:05:11,160 --> 00:05:12,639
这个是涉及到一个

152
00:05:12,720 --> 00:05:14,480
计算机体系结构的问题

153
00:05:14,520 --> 00:05:17,040
就是说你把N个数相加

154
00:05:18,560 --> 00:05:19,879
加的顺序不一样

155
00:05:19,879 --> 00:05:21,000
结果会不一样

156
00:05:25,640 --> 00:05:26,200
理解吗

157
00:05:26,200 --> 00:05:28,319
就是因为你的精度不够

158
00:05:28,319 --> 00:05:29,400
所以就是说

159
00:05:29,480 --> 00:05:32,280
你在cudan它就是尽量的

160
00:05:32,280 --> 00:05:34,000
要尽量好的做并行

161
00:05:34,000 --> 00:05:35,040
一旦做并行的话

162
00:05:35,040 --> 00:05:36,400
加的一些数相

163
00:05:36,400 --> 00:05:37,319
加的顺序不一样

164
00:05:37,319 --> 00:05:38,240
导致你的结果不一样

165
00:05:38,240 --> 00:05:39,759
所以cudan出来的结果

166
00:05:39,759 --> 00:05:41,080
随机性挺大的

167
00:05:41,640 --> 00:05:43,040
几乎不能重复

168
00:05:43,240 --> 00:05:44,600
所以如果你想可重复的话

169
00:05:44,600 --> 00:05:45,520
要禁掉cudan

170
00:05:45,520 --> 00:05:47,840
然后dropout和random seed

171
00:05:47,840 --> 00:05:50,000
就random就是权重

172
00:05:50,000 --> 00:05:51,200
随机出世化

173
00:05:51,200 --> 00:05:52,040
那个seed要

174
00:05:52,040 --> 00:05:53,640
就这个种子要固定住

175
00:05:53,800 --> 00:05:55,320
应该就是你可以可重复了

176
00:05:55,480 --> 00:05:56,160
但反过来讲

177
00:05:56,160 --> 00:05:57,360
你也没必要可重复

178
00:05:57,360 --> 00:05:58,720
就是说最后的最后

179
00:05:58,720 --> 00:05:59,720
就是说你只要保证

180
00:05:59,720 --> 00:06:01,520
我训练个100个

181
00:06:01,520 --> 00:06:02,840
100轮或者多少

182
00:06:02,840 --> 00:06:03,879
一个epoch之后

183
00:06:03,879 --> 00:06:05,120
我的精度差不多

184
00:06:05,120 --> 00:06:06,680
在那个范围就行了

185
00:06:07,000 --> 00:06:08,439
所以说这就是

186
00:06:08,720 --> 00:06:10,240
通常来说大家的做法

187
00:06:13,040 --> 00:06:15,240
而且我们虽然有随机性

188
00:06:15,240 --> 00:06:16,600
随机性它不是个坏事情

189
00:06:16,600 --> 00:06:18,120
随机性让你更稳定

190
00:06:18,319 --> 00:06:21,000
就是随机它

191
00:06:21,319 --> 00:06:22,920
你可以认为随机这个事情

192
00:06:23,040 --> 00:06:25,720
就是把你整个东西变得很平滑

193
00:06:26,560 --> 00:06:28,680
就是说你一旦随机性一高

194
00:06:28,680 --> 00:06:30,400
你的稳定性也会增加

195
00:06:31,680 --> 00:06:32,720
就是说你要这么想

196
00:06:32,720 --> 00:06:33,800
我整个神经网络

197
00:06:33,800 --> 00:06:35,240
我都能在你给我

198
00:06:35,240 --> 00:06:36,439
把所有东西都给我

199
00:06:36,439 --> 00:06:37,520
里面东西全部置领

200
00:06:37,520 --> 00:06:38,680
各种随机的情况下

201
00:06:38,680 --> 00:06:39,600
我还能收敛

202
00:06:39,600 --> 00:06:40,759
我还能到这个地方

203
00:06:40,759 --> 00:06:41,960
下次我也行

204
00:06:43,720 --> 00:06:50,280
这个问题是说丢弃法

205
00:06:50,280 --> 00:06:51,400
是在训练中

206
00:06:51,400 --> 00:06:54,000
把神经元丢弃后训练

207
00:06:54,080 --> 00:06:55,800
在预测中神经元没有丢弃

208
00:06:55,800 --> 00:06:56,240
是的

209
00:06:56,320 --> 00:06:57,240
就是说丢弃法

210
00:06:57,240 --> 00:06:58,480
只是在训练的时候

211
00:06:58,480 --> 00:07:00,080
随机把一些隐藏成了

212
00:07:00,080 --> 00:07:02,800
或者说或者说你把随机神经元

213
00:07:02,879 --> 00:07:05,280
在这一轮不参加计算

214
00:07:05,320 --> 00:07:06,640
不参加更新

215
00:07:06,840 --> 00:07:07,920
但是在预测的时候

216
00:07:07,920 --> 00:07:08,920
是大家一起的

217
00:07:08,920 --> 00:07:15,000
丢弃法是每次迭代一次

218
00:07:15,000 --> 00:07:16,240
随机丢一次吗

219
00:07:16,240 --> 00:07:16,800
是的

220
00:07:16,840 --> 00:07:17,759
就每

221
00:07:17,800 --> 00:07:21,160
就是说它是每一个层

222
00:07:21,160 --> 00:07:24,480
在调用前项运算的时候

223
00:07:24,920 --> 00:07:26,160
随机丢一次

224
00:07:27,240 --> 00:07:30,080
如果你有三个隐藏层

225
00:07:30,120 --> 00:07:33,800
那你用了三个dropout layer的话

226
00:07:33,840 --> 00:07:36,319
那么它会那就有要扣三次了

227
00:07:36,840 --> 00:07:37,520
就是说

228
00:07:37,560 --> 00:07:39,359
所以我觉得应该是你所谓的

229
00:07:39,359 --> 00:07:40,319
每次迭代一次

230
00:07:40,319 --> 00:07:41,319
就是一个batch

231
00:07:41,319 --> 00:07:42,839
要重新丢一次

232
00:07:45,199 --> 00:07:47,240
请问在用bn的时候

233
00:07:47,279 --> 00:07:49,079
会有必要用dropout吗

234
00:07:49,120 --> 00:07:50,560
我们还没讲bn

235
00:07:50,560 --> 00:07:53,560
bn是之后我们接下来要讲的

236
00:07:54,879 --> 00:07:56,279
也是一个

237
00:07:57,799 --> 00:08:00,680
边有一点点政策的意思在里面

238
00:08:01,159 --> 00:08:05,799
但是边是作用在卷积层上的

239
00:08:06,800 --> 00:08:09,120
就边是给卷积层用的

240
00:08:10,040 --> 00:08:12,360
dropout是给全链接层用的

241
00:08:12,400 --> 00:08:13,040
不一样

242
00:08:13,319 --> 00:08:16,759
所以dropout和边没有太多相关性

243
00:08:17,120 --> 00:08:18,520
所以我们现在没讲cn

244
00:08:18,680 --> 00:08:19,520
所以我们也没讲边

245
00:08:19,520 --> 00:08:21,120
所以dropout你可以认为

246
00:08:21,160 --> 00:08:23,199
它就是在全链接层用的

247
00:08:23,240 --> 00:08:24,680
不会在卷积层

248
00:08:24,720 --> 00:08:25,960
我们是不会用dropout

249
00:08:31,080 --> 00:08:33,000
dropout会不会让训练

250
00:08:33,000 --> 00:08:35,000
nulls的曲线方差变大

251
00:08:35,000 --> 00:08:35,680
不够平滑

252
00:08:35,680 --> 00:08:37,560
其实dropout会让曲线变得

253
00:08:40,720 --> 00:08:41,720
看你怎么说

254
00:08:42,720 --> 00:08:44,200
有可能是你

255
00:08:44,200 --> 00:08:46,280
就是说有可能是不够平滑

256
00:08:46,280 --> 00:08:48,160
你我觉得我大概理解你的意思

257
00:08:48,160 --> 00:08:50,000
就是说看你怎么画曲线了

258
00:08:50,040 --> 00:08:51,840
我们曲线画是

259
00:08:51,880 --> 00:08:53,120
我们曲线画的是

260
00:08:53,120 --> 00:08:56,640
每一次每一个数据扫完之后

261
00:08:56,640 --> 00:08:57,440
我算一个平均

262
00:08:57,440 --> 00:08:58,720
它其实是很平滑的

263
00:08:58,880 --> 00:09:00,680
但是你真正的做硬算的话

264
00:09:00,720 --> 00:09:02,320
你扫一遍数据那么贵

265
00:09:02,360 --> 00:09:05,560
所以你可能就每算个10个batch

266
00:09:05,800 --> 00:09:07,440
我就画一下10个batch画一下

267
00:09:07,480 --> 00:09:09,160
它就可能会让你的

268
00:09:09,160 --> 00:09:10,080
如果你这么画的话

269
00:09:10,080 --> 00:09:12,400
你会发现整个曲线抖动很大

270
00:09:12,520 --> 00:09:13,480
其实你不care

271
00:09:13,480 --> 00:09:14,680
抖动大就抖动大

272
00:09:14,680 --> 00:09:15,640
谁care

273
00:09:15,720 --> 00:09:17,960
所以你不要担心曲线

274
00:09:17,960 --> 00:09:19,880
它有可能让你曲线不平滑

275
00:09:19,880 --> 00:09:20,240
有可能

276
00:09:20,240 --> 00:09:21,760
但我没有真正的去看过

277
00:09:21,760 --> 00:09:23,560
但是我们不care这个事情

278
00:09:23,680 --> 00:09:25,600
就是说曲线平不平滑

279
00:09:25,600 --> 00:09:26,760
对最后的

280
00:09:26,800 --> 00:09:27,720
就是说一开始

281
00:09:27,720 --> 00:09:30,280
就你一开始不平滑

282
00:09:30,280 --> 00:09:31,280
最后都得平滑

283
00:09:31,600 --> 00:09:32,640
如果你最后不平滑

284
00:09:32,640 --> 00:09:34,600
就表示你收敛很麻烦

285
00:09:35,560 --> 00:09:40,400
问题18

286
00:09:40,400 --> 00:09:43,280
推理的dropout是直接返回输入吗

287
00:09:43,280 --> 00:09:43,880
为什么

288
00:09:44,280 --> 00:09:44,960
对

289
00:09:44,960 --> 00:09:47,040
这个是dropout最大的一个

290
00:09:47,080 --> 00:09:49,120
经常大家容易误解的地方

291
00:09:49,120 --> 00:09:51,640
就是在做预测的时候

292
00:09:51,840 --> 00:09:53,520
假设你所谓的预测

293
00:09:53,520 --> 00:09:56,160
就是不对我的权重做更新的时候

294
00:09:56,960 --> 00:09:58,200
dropout是不用的

295
00:09:58,200 --> 00:09:59,240
就不用dropout

296
00:09:59,360 --> 00:10:00,600
为什么是因为

297
00:10:00,840 --> 00:10:03,040
dropout是一个正则项

298
00:10:03,199 --> 00:10:05,839
正则项为的作用是让你在

299
00:10:05,959 --> 00:10:08,120
更新你的权重的时候

300
00:10:08,120 --> 00:10:10,559
让你的模型复杂度变低一点点

301
00:10:11,360 --> 00:10:12,799
但你在做推理的时候

302
00:10:12,799 --> 00:10:14,719
你不会更新你的模型复杂度

303
00:10:14,719 --> 00:10:16,399
而不会更新你的模型

304
00:10:16,919 --> 00:10:18,559
所以你是不需要dropout

305
00:10:19,039 --> 00:10:20,839
你可以用dropout也可以

306
00:10:21,319 --> 00:10:22,559
但是可能对你

307
00:10:22,559 --> 00:10:23,679
如果你用了dropout

308
00:10:23,679 --> 00:10:24,879
那就有随机性

309
00:10:25,399 --> 00:10:28,199
所以你要避免你随机预测的时候

310
00:10:28,199 --> 00:10:28,679
有出问题

311
00:10:28,679 --> 00:10:30,959
那你可能得多算几次

312
00:10:31,759 --> 00:10:32,960
就我给一个样本过来

313
00:10:33,000 --> 00:10:33,840
假设我开了

314
00:10:33,840 --> 00:10:35,240
我在推理可以开dropout

315
00:10:35,240 --> 00:10:35,759
你可以开

316
00:10:36,320 --> 00:10:37,840
就是说反正推理就是说

317
00:10:37,840 --> 00:10:39,400
对权重更新没影响

318
00:10:39,480 --> 00:10:40,480
但是你可以开

319
00:10:40,680 --> 00:10:42,480
开的代价是说

320
00:10:42,519 --> 00:10:44,120
你因为你开了它

321
00:10:44,120 --> 00:10:45,560
所以导致你预测的时候

322
00:10:45,560 --> 00:10:46,920
我给你一只猫的图片

323
00:10:46,960 --> 00:10:47,920
第一次预测是猫

324
00:10:47,920 --> 00:10:49,280
第二次预测是狗

325
00:10:49,280 --> 00:10:49,680
对吧

326
00:10:49,680 --> 00:10:51,160
因为丢掉东西了

327
00:10:51,759 --> 00:10:52,480
就是所以说

328
00:10:52,480 --> 00:10:55,000
你可能要得多算几次推理

329
00:10:55,040 --> 00:10:56,480
做一下平均才能使得

330
00:10:56,480 --> 00:10:58,480
把这个方差给降下来

331
00:10:58,680 --> 00:10:59,560
那训练没问题

332
00:11:00,080 --> 00:11:01,000
训练为什么没问题

333
00:11:01,000 --> 00:11:01,480
因为训练

334
00:11:01,480 --> 00:11:02,880
我要跑很多很多是dropout

335
00:11:02,880 --> 00:11:04,200
我就不断在跑dropout

336
00:11:04,240 --> 00:11:05,240
我可能跑个

337
00:11:05,280 --> 00:11:06,240
我们在这个地方

338
00:11:06,240 --> 00:11:08,320
就是每一次我都跑了

339
00:11:08,320 --> 00:11:08,760
对吧

340
00:11:08,800 --> 00:11:10,240
我也跑个几十万次

341
00:11:10,560 --> 00:11:12,240
所以在几十万次的

342
00:11:12,240 --> 00:11:13,720
随机的丢在里面

343
00:11:13,760 --> 00:11:16,400
对整个系统的稳定性是没问题的

344
00:11:16,520 --> 00:11:17,560
但是在推理的时候

345
00:11:17,560 --> 00:11:19,720
我如果就关心某一个样本

346
00:11:20,120 --> 00:11:21,920
它的结果的话

347
00:11:21,920 --> 00:11:22,920
你可能要做平均

348
00:11:22,960 --> 00:11:24,520
就是在我们部署的时候

349
00:11:25,600 --> 00:11:26,600
你拿张图片过来

350
00:11:26,600 --> 00:11:28,800
我给你出了一个坏结果怎么办

351
00:11:28,840 --> 00:11:29,120
对吧

352
00:11:29,120 --> 00:11:30,840
我尽量不要出坏结果

353
00:11:30,879 --> 00:11:32,639
但是你如果反过来讲

354
00:11:32,679 --> 00:11:33,759
如果你就是关心一个

355
00:11:33,759 --> 00:11:35,679
在一个测试级上的一个金柱的话

356
00:11:35,679 --> 00:11:36,720
可能开dropout

357
00:11:36,720 --> 00:11:38,039
不会给你带来很多影响

358
00:11:38,039 --> 00:11:38,840
你可以试一下

359
00:11:40,039 --> 00:11:40,480
很简单

360
00:11:40,639 --> 00:11:41,759
你就把前面函数

361
00:11:41,759 --> 00:11:43,480
把if圈里的1

362
00:11:43,519 --> 00:11:44,759
使用dropout把它删掉

363
00:11:44,759 --> 00:11:45,159
不就行了

364
00:11:45,200 --> 00:11:46,759
注销掉就行了

365
00:11:46,759 --> 00:11:59,159
dropout函数返回值的表达

366
00:11:59,159 --> 00:12:01,639
是没有被丢弃的输入值

367
00:12:01,639 --> 00:12:03,360
会因为分母的

368
00:12:03,559 --> 00:12:05,240
1-p改变

369
00:12:05,600 --> 00:12:07,360
训练数的标签

370
00:12:07,360 --> 00:12:08,679
还是原来的值

371
00:12:09,200 --> 00:12:10,159
是的

372
00:12:10,200 --> 00:12:10,919
就是说

373
00:12:11,080 --> 00:12:11,679
就dropout的

374
00:12:11,679 --> 00:12:12,039
就是说

375
00:12:12,039 --> 00:12:14,840
你要么就把输出变成0

376
00:12:15,000 --> 00:12:15,800
不然的话

377
00:12:15,800 --> 00:12:17,040
要除一个1-p

378
00:12:17,080 --> 00:12:18,560
那是为了保证我的

379
00:12:18,600 --> 00:12:19,200
就随

380
00:12:19,240 --> 00:12:21,160
因为随机性保证我的期望

381
00:12:21,200 --> 00:12:22,960
就我的均值还是不会变的

382
00:12:23,960 --> 00:12:25,840
但是标签还是原来的值

383
00:12:25,840 --> 00:12:27,200
对标签我不改变

384
00:12:27,200 --> 00:12:30,040
我们dropout的唯一改变的是

385
00:12:30,080 --> 00:12:31,000
我的

386
00:12:31,080 --> 00:12:31,920
啊

387
00:12:32,040 --> 00:12:32,720
drop

388
00:12:32,759 --> 00:12:35,480
我的隐藏层的输出

389
00:12:37,680 --> 00:12:38,600
你可以改标签

390
00:12:38,600 --> 00:12:39,759
改标签是别的算法

391
00:12:39,759 --> 00:12:40,400
就是说

392
00:12:41,120 --> 00:12:43,000
你可以改标签

393
00:12:43,000 --> 00:12:44,919
改标签也是一种政策化

394
00:12:44,919 --> 00:12:47,279
我们我们这个课不一定会讲

395
00:12:47,480 --> 00:12:48,399
就你可以改标签

396
00:12:48,399 --> 00:12:50,039
就是你把

397
00:12:50,080 --> 00:12:50,919
啊

398
00:12:51,360 --> 00:12:53,559
改标签还是一个正常

399
00:12:53,559 --> 00:12:55,440
挺常用的一个政策化

400
00:12:55,440 --> 00:12:57,480
我在想有说不定

401
00:12:57,480 --> 00:12:58,440
我可以给大家讲一讲

402
00:12:58,440 --> 00:13:01,480
就是我可能会在最后讲cv的时候

403
00:13:01,480 --> 00:13:02,720
给大家大概提一下

404
00:13:02,879 --> 00:13:04,120
还真有就是说

405
00:13:05,080 --> 00:13:07,320
我可以把标签也改了

406
00:13:07,360 --> 00:13:08,799
也是一个随机

407
00:13:08,919 --> 00:13:09,559
就是说

408
00:13:10,240 --> 00:13:12,000
我们之所以只讲dropout的

409
00:13:12,000 --> 00:13:13,960
就是说他是最早最早的一个

410
00:13:13,960 --> 00:13:16,600
在神经网络中间引入随机向来做

411
00:13:16,600 --> 00:13:18,120
政策化的一个东西

412
00:13:18,159 --> 00:13:19,399
但实际上在

413
00:13:19,559 --> 00:13:20,279
之后啊

414
00:13:20,279 --> 00:13:21,279
在之后你

415
00:13:21,320 --> 00:13:22,759
大家发现dropout吗

416
00:13:22,759 --> 00:13:24,080
我dropout的隐藏层数

417
00:13:24,080 --> 00:13:25,279
什么都可以dropout

418
00:13:25,919 --> 00:13:27,159
我可以dropout的weight

419
00:13:27,159 --> 00:13:28,200
我可以dropout

420
00:13:28,200 --> 00:13:30,759
我的我我可以把我的输入给变成了

421
00:13:30,759 --> 00:13:30,960
呀

422
00:13:30,960 --> 00:13:32,480
我可以把我的label变成了呀

423
00:13:32,480 --> 00:13:33,480
就是都可以

424
00:13:33,679 --> 00:13:35,000
都可以没关系的

425
00:13:35,200 --> 00:13:35,919
听说

426
00:13:37,360 --> 00:13:40,600
就和在过去的特别dropout出来

427
00:13:40,879 --> 00:13:43,720
的5年或者6年之内吧

428
00:13:43,840 --> 00:13:45,519
就有大量大量的

429
00:13:45,639 --> 00:13:46,720
算法在出来

430
00:13:46,720 --> 00:13:48,960
怎么样把一个东西搞成0搞成0搞成0

431
00:13:48,960 --> 00:13:49,600
啊

432
00:13:49,600 --> 00:13:51,800
所以我们这一单就没空去给你

433
00:13:51,800 --> 00:13:52,879
所有的过一遍了

434
00:13:52,879 --> 00:13:55,639
就是大家如果碰到在实际中啊

435
00:13:55,639 --> 00:13:57,279
就是说用我们的一些

436
00:13:57,759 --> 00:13:58,040
啊

437
00:13:58,040 --> 00:13:59,480
实现你会发现把这个改成你

438
00:13:59,480 --> 00:14:00,159
反正改成0

439
00:14:00,159 --> 00:14:01,600
可能这小trick啊

440
00:14:01,600 --> 00:14:03,120
就是很常见了

441
00:14:03,399 --> 00:14:04,879
就是大家不要惊讶就行了

442
00:14:04,879 --> 00:14:06,759
就当个也不不是个bug

443
00:14:11,320 --> 00:14:14,759
对就是说这也是

444
00:14:14,879 --> 00:14:15,879
第问问题

445
00:14:15,879 --> 00:14:16,920
问题二是就是说

446
00:14:16,960 --> 00:14:19,320
训练时使用dropout的推理是不用

447
00:14:19,320 --> 00:14:21,920
会不会导致推理输出结果翻倍了

448
00:14:21,960 --> 00:14:23,200
比如说dropout的0.5

449
00:14:23,200 --> 00:14:25,320
推理的是输出是训练时

450
00:14:25,320 --> 00:14:26,960
两个神级员叠加翻倍

451
00:14:27,000 --> 00:14:29,560
所以大家记得我们除了个一减批

452
00:14:30,440 --> 00:14:31,000
对吧

453
00:14:31,000 --> 00:14:33,399
就是说假设你在训练的时候

454
00:14:33,440 --> 00:14:34,680
你dropout的0.5

455
00:14:34,680 --> 00:14:37,279
就是说我把一半的神级员丢成0

456
00:14:37,320 --> 00:14:39,360
剩下的我要除以0.5

457
00:14:39,360 --> 00:14:40,320
就成了2

458
00:14:41,279 --> 00:14:42,960
所以你输出和输入

459
00:14:42,960 --> 00:14:43,840
就是说在训练时

460
00:14:43,840 --> 00:14:45,800
你的方差是不会发生变化的

461
00:14:47,080 --> 00:14:48,960
所以这就是为什么要除一个一减批

462
00:14:48,960 --> 00:14:50,639
为什么我给大家算一下

463
00:14:50,639 --> 00:14:52,240
说期望没有发生变化

464
00:14:52,240 --> 00:14:54,840
就是说导致说你的就是要避免

465
00:14:54,840 --> 00:14:56,840
你的输出的结果是训练的

466
00:14:56,840 --> 00:14:58,960
是说翻倍的结果

467
00:15:00,200 --> 00:15:00,680
OK

468
00:15:05,399 --> 00:15:06,480
问题21

469
00:15:07,080 --> 00:15:10,120
dropout的每次随机选几个子网络

470
00:15:10,120 --> 00:15:11,399
最后做平均的做法

471
00:15:11,399 --> 00:15:13,120
是不是类似于随机生灵

472
00:15:13,279 --> 00:15:13,799
多角色

473
00:15:13,799 --> 00:15:15,279
输出投票的思想式的

474
00:15:15,279 --> 00:15:16,480
就最早最早

475
00:15:16,600 --> 00:15:18,399
Hinton大家应该知道

476
00:15:18,519 --> 00:15:19,399
Hinton老爷子

477
00:15:20,240 --> 00:15:21,360
神经网络的

478
00:15:22,120 --> 00:15:22,919
电竞人之一

479
00:15:23,080 --> 00:15:24,759
就是最大的山头

480
00:15:25,120 --> 00:15:26,600
他们做dropout的时候

481
00:15:26,600 --> 00:15:27,919
确实就是这么说的

482
00:15:28,120 --> 00:15:29,360
就是说我dropout干嘛

483
00:15:29,360 --> 00:15:31,120
我就是每一次在训练的时候

484
00:15:31,120 --> 00:15:32,960
随机踩了一个子网络

485
00:15:33,080 --> 00:15:34,000
训练一下

486
00:15:34,200 --> 00:15:35,720
然后我做预测的时候

487
00:15:35,720 --> 00:15:36,759
我也可以做预测的时候

488
00:15:36,759 --> 00:15:38,480
我也可以说那就是

489
00:15:38,480 --> 00:15:39,080
预测的时候

490
00:15:39,080 --> 00:15:40,039
我也可以用dropout

491
00:15:40,159 --> 00:15:41,399
就是做n下

492
00:15:41,560 --> 00:15:42,600
就是每一次

493
00:15:42,960 --> 00:15:43,680
比如说预测时候

494
00:15:43,680 --> 00:15:44,920
我就重复5次

495
00:15:45,680 --> 00:15:47,720
然后每次踩一个子网络

496
00:15:47,720 --> 00:15:48,399
做一下预测

497
00:15:48,399 --> 00:15:49,320
然后做平均

498
00:15:49,879 --> 00:15:51,080
就是你可以这么说

499
00:15:51,080 --> 00:15:51,720
就是说

500
00:15:51,759 --> 00:15:54,279
但是实际上效果没什么用

501
00:15:54,279 --> 00:15:55,039
就是说实际上

502
00:15:55,039 --> 00:15:56,800
你就是说你当然可以这么说了

503
00:15:56,840 --> 00:15:58,000
但是实际上大家发现

504
00:15:58,000 --> 00:15:59,600
它更像一个正轴项

505
00:16:00,039 --> 00:16:00,759
如果有兴趣

506
00:16:00,759 --> 00:16:01,519
我可以把那个

507
00:16:01,560 --> 00:16:02,680
那有几篇paper

508
00:16:02,680 --> 00:16:03,800
就是说去讲

509
00:16:03,800 --> 00:16:05,000
你dropout的是一个

510
00:16:05,039 --> 00:16:06,080
你可以就说dropout

511
00:16:06,080 --> 00:16:07,240
是一个regularization

512
00:16:07,519 --> 00:16:08,519
就是说这个title

513
00:16:08,519 --> 00:16:09,399
应该就长这个样子

514
00:16:09,600 --> 00:16:11,120
应该是2015年

515
00:16:11,159 --> 00:16:12,240
2015年发的

516
00:16:12,759 --> 00:16:15,240
我的具体我不保证是15年

517
00:16:15,759 --> 00:16:16,480
就是说

518
00:16:16,519 --> 00:16:18,240
就是说他们证明说

519
00:16:18,240 --> 00:16:18,960
他不是证明

520
00:16:19,080 --> 00:16:19,680
就是说

521
00:16:19,879 --> 00:16:21,639
给你看了一些实验结果

522
00:16:21,639 --> 00:16:23,240
说更像一个正轴项

523
00:16:23,360 --> 00:16:25,360
不是在做投票

524
00:16:26,080 --> 00:16:28,320
所以hentai老爷子又不是很爽

525
00:16:28,320 --> 00:16:30,279
就是对这个东西一直耿耿于怀

526
00:16:30,279 --> 00:16:31,560
所以在两年前

527
00:16:31,680 --> 00:16:32,799
又搞了一个新的网络

528
00:16:32,799 --> 00:16:34,840
就是capso

529
00:16:35,160 --> 00:16:37,320
反正我不知道念对没有

530
00:16:37,879 --> 00:16:38,720
核心也是一样的

531
00:16:38,720 --> 00:16:39,560
就是更小

532
00:16:39,600 --> 00:16:40,840
就是也是一个

533
00:16:40,840 --> 00:16:42,399
在一个小神经元里面

534
00:16:42,399 --> 00:16:43,399
一个蛟狼里面

535
00:16:43,399 --> 00:16:45,000
蛟狼网络把中文叫做

536
00:16:45,720 --> 00:16:47,280
然后也随机随机搞一搞

537
00:16:47,280 --> 00:16:48,399
也是一个

538
00:16:48,759 --> 00:16:49,440
hentai老爷子

539
00:16:49,440 --> 00:16:50,680
一直想在做这个东西

540
00:16:50,680 --> 00:16:52,040
但是现在我觉得

541
00:16:52,080 --> 00:16:53,240
dropout work

542
00:16:53,280 --> 00:16:55,800
但是大家觉得

543
00:16:55,800 --> 00:16:57,399
大家觉得就是一个正轴项

544
00:16:57,399 --> 00:17:00,320
不是一个做投票的思想

545
00:17:00,360 --> 00:17:01,879
所以hentai又搞了一个别的

546
00:17:01,879 --> 00:17:04,039
那个东西还没有被搞

547
00:17:04,240 --> 00:17:05,039
搞work

548
00:17:05,039 --> 00:17:06,879
就是说效果还一般

549
00:17:07,079 --> 00:17:07,759
但是

550
00:17:09,879 --> 00:17:10,440
OK

551
00:17:12,240 --> 00:17:13,879
在解决过拟核问题上

552
00:17:13,879 --> 00:17:15,159
dropout的regularization的

553
00:17:15,159 --> 00:17:15,960
主要区别是什么

554
00:17:15,960 --> 00:17:16,920
就是你可以认为dropout的

555
00:17:16,920 --> 00:17:18,079
就是一个regularization

556
00:17:18,200 --> 00:17:19,680
就他跟L2的regularization

557
00:17:19,680 --> 00:17:20,519
会不一样一点

558
00:17:20,960 --> 00:17:23,279
就是说他同样的是让你

559
00:17:24,319 --> 00:17:26,240
同样是让你避免过拟核

560
00:17:27,079 --> 00:17:28,839
就是说是两种不一样的过拟核

561
00:17:28,839 --> 00:17:30,039
就是你有很多种办法

562
00:17:30,039 --> 00:17:30,920
可以避免过拟核

563
00:17:30,920 --> 00:17:31,720
dropout是一种

564
00:17:31,920 --> 00:17:32,799
我们之前讲的

565
00:17:33,399 --> 00:17:35,399
权重衰退是一种

566
00:17:35,399 --> 00:17:37,000
你可以一起使用

567
00:17:38,000 --> 00:17:38,879
你可以结合起来使用

568
00:17:38,879 --> 00:17:39,399
就两种

569
00:17:39,399 --> 00:17:40,000
就是说

570
00:17:40,319 --> 00:17:41,240
比如说你治病

571
00:17:41,240 --> 00:17:43,200
你有不同的退休

572
00:17:43,559 --> 00:17:44,119
你头痛

573
00:17:44,839 --> 00:17:46,559
所以我就经常吃各种止痛药

574
00:17:47,039 --> 00:17:48,119
有不同的止痛药

575
00:17:48,119 --> 00:17:49,359
你可以一起吃

576
00:17:49,359 --> 00:17:50,240
你可以混着吃

577
00:17:50,240 --> 00:17:51,839
你可以轮着吃都可以

578
00:17:56,519 --> 00:17:58,399
所以这个是跟之前是一样的

579
00:18:00,799 --> 00:18:02,000
这个问题就不答了

580
00:18:02,799 --> 00:18:05,399
对24是个很好玩的问题

581
00:18:05,399 --> 00:18:06,000
就是说

582
00:18:06,159 --> 00:18:08,839
dropout已经被Google申请了专利

583
00:18:08,839 --> 00:18:10,799
产品开发有替代的方法

584
00:18:11,359 --> 00:18:12,559
你不要说dropout了

585
00:18:12,559 --> 00:18:14,319
基本上所有东西都申请专利了

586
00:18:14,319 --> 00:18:15,720
所以Google

587
00:18:16,240 --> 00:18:17,440
Google就特别

588
00:18:17,440 --> 00:18:18,759
这我有

589
00:18:18,759 --> 00:18:19,319
我

590
00:18:19,319 --> 00:18:20,680
不好说什么

591
00:18:20,680 --> 00:18:21,359
就是说

592
00:18:21,639 --> 00:18:23,440
Google还不仅是申了dropout

593
00:18:23,440 --> 00:18:24,599
RN我觉得也是

594
00:18:24,919 --> 00:18:25,599
反正

595
00:18:25,720 --> 00:18:27,079
transform肯定是申了的

596
00:18:27,079 --> 00:18:27,720
就是说

597
00:18:27,919 --> 00:18:28,559
dropout都是

598
00:18:28,720 --> 00:18:29,720
Google都帮你申了

599
00:18:30,480 --> 00:18:31,279
所以

600
00:18:31,519 --> 00:18:32,279
我觉得没事

601
00:18:32,279 --> 00:18:33,440
你不用

602
00:18:33,720 --> 00:18:34,799
好像

603
00:18:34,839 --> 00:18:36,200
大家好像还

604
00:18:36,240 --> 00:18:38,079
恐慌过一阵子

605
00:18:38,079 --> 00:18:39,440
说你申了之后怎么办

606
00:18:39,559 --> 00:18:41,879
后来发现

607
00:18:41,879 --> 00:18:42,480
也

608
00:18:43,480 --> 00:18:44,279
问题不大

609
00:18:44,639 --> 00:18:45,559
就最后

610
00:18:49,200 --> 00:18:50,399
就你可以不用dropout

611
00:18:50,399 --> 00:18:52,440
你可以dropout

612
00:18:52,440 --> 00:18:54,039
但是我觉得你不要担心这个事情

613
00:18:54,240 --> 00:18:54,839
就是说

614
00:18:55,000 --> 00:18:55,720
虽然我也不

615
00:18:55,720 --> 00:18:56,559
我不是律师

616
00:18:56,559 --> 00:18:57,440
我不敢打包票

617
00:18:57,440 --> 00:18:59,200
但是我在

618
00:18:59,200 --> 00:19:00,559
比如说我在Amazon用dropout

619
00:19:00,559 --> 00:19:01,680
从来没有律师说

620
00:19:01,680 --> 00:19:03,039
跟我说不要用

621
00:19:03,039 --> 00:19:05,160
就是说你估计也不要担心这个事情

622
00:19:05,360 --> 00:19:05,759
我猜

623
00:19:05,759 --> 00:19:10,200
律师没跟我说可以用

624
00:19:10,200 --> 00:19:11,480
也没跟我说不要用

625
00:19:11,960 --> 00:19:12,480
对吧

626
00:19:12,799 --> 00:19:13,720
这就是说

627
00:19:14,200 --> 00:19:15,840
一般来说律师没有说不要用

628
00:19:15,840 --> 00:19:16,840
或者说

629
00:19:16,880 --> 00:19:20,039
他肯定不会跟你说可以用

630
00:19:20,320 --> 00:19:21,160
所以我不是律师

631
00:19:21,160 --> 00:19:22,759
所以我说用吧

632
00:19:22,759 --> 00:19:23,400
没事

633
00:19:24,240 --> 00:19:25,400
但是我不是律师

634
00:19:25,400 --> 00:19:29,039
请问

635
00:19:29,319 --> 00:19:31,000
丢弃的是前一层

636
00:19:31,000 --> 00:19:31,920
还是后一层

637
00:19:32,559 --> 00:19:33,720
是怎么说

638
00:19:33,839 --> 00:19:34,639
他就是

639
00:19:34,920 --> 00:19:35,720
都一样

640
00:19:35,720 --> 00:19:36,319
就是说

641
00:19:36,319 --> 00:19:39,799
你可认为他丢弃前一层的输出

642
00:19:39,799 --> 00:19:42,159
和丢弃后一层的输入

643
00:19:42,680 --> 00:19:44,079
是一个东西

644
00:19:44,079 --> 00:19:44,599
对吧

645
00:19:44,599 --> 00:19:45,079
本质的

646
00:19:45,079 --> 00:19:47,159
就他或者你可以认为

647
00:19:47,159 --> 00:19:47,599
dropout

648
00:19:47,599 --> 00:19:48,480
他其实是一层

649
00:19:48,480 --> 00:19:49,559
就是一个层

650
00:19:49,920 --> 00:19:50,920
输入是一个东西

651
00:19:50,920 --> 00:19:52,519
输出就丢掉一个东西了

652
00:19:52,519 --> 00:20:02,400
dropout的和权重衰退

653
00:20:02,400 --> 00:20:03,559
都属于正则

654
00:20:03,559 --> 00:20:05,519
为什么dropout的效果更好

655
00:20:05,639 --> 00:20:06,559
现在更常用

656
00:20:06,559 --> 00:20:10,279
其实dropout的没有weight decay常用

657
00:20:10,279 --> 00:20:12,519
就是weight decay其实是说

658
00:20:12,879 --> 00:20:13,680
大家都在用

659
00:20:13,680 --> 00:20:16,160
就是说一般都会用开weight decay

660
00:20:16,160 --> 00:20:19,000
就dropout的主要是对全连接层使用

661
00:20:19,000 --> 00:20:21,079
但weight decay这个东西权重衰退

662
00:20:21,119 --> 00:20:23,199
对于卷积层

663
00:20:23,199 --> 00:20:25,519
对于之后的former都可以用

664
00:20:25,519 --> 00:20:26,159
就是说

665
00:20:26,159 --> 00:20:27,559
但是统一用

666
00:20:27,559 --> 00:20:30,439
dropout的为什么效果更好

667
00:20:31,319 --> 00:20:32,960
dropout的其实我觉得

668
00:20:32,960 --> 00:20:33,599
就是

669
00:20:34,079 --> 00:20:35,599
更好调参一点

670
00:20:36,279 --> 00:20:38,079
就权重衰退的跟南部打一样

671
00:20:38,079 --> 00:20:39,399
这不是很好调

672
00:20:39,759 --> 00:20:40,720
dropout好调

673
00:20:40,720 --> 00:20:41,839
dropout的很直观

674
00:20:42,559 --> 00:20:43,919
就我丢多少

675
00:20:44,679 --> 00:20:45,199
我

676
00:20:46,679 --> 00:20:47,519
丢一半

677
00:20:47,960 --> 00:20:49,439
这dropout就三个值

678
00:20:49,439 --> 00:20:49,960
0.5

679
00:20:49,960 --> 00:20:50,439
0.1

680
00:20:50,439 --> 00:20:51,039
0.9

681
00:20:51,039 --> 00:20:51,799
就三个值

682
00:20:52,480 --> 00:20:53,480
丢一半就表示

683
00:20:54,159 --> 00:20:54,799
就是说

684
00:20:56,639 --> 00:20:58,559
就是说你可以这么简单

685
00:20:58,559 --> 00:20:59,240
这么觉得

686
00:20:59,839 --> 00:21:04,279
我假设去年一个单隐藏层的全连接

687
00:21:04,319 --> 00:21:05,359
MLP

688
00:21:05,879 --> 00:21:07,599
然后隐藏层大小是64

689
00:21:07,599 --> 00:21:08,159
我劝了一下

690
00:21:08,159 --> 00:21:09,000
觉得还行

691
00:21:10,559 --> 00:21:12,200
就说不用dropout

692
00:21:12,279 --> 00:21:13,159
问题不大

693
00:21:13,519 --> 00:21:14,920
觉得没那么过瘾

694
00:21:14,960 --> 00:21:16,079
那么接下来你可怎么办

695
00:21:16,519 --> 00:21:18,599
接下来你就把它变成128

696
00:21:18,839 --> 00:21:20,639
开dropout的0.5

697
00:21:22,039 --> 00:21:22,399
对吧

698
00:21:22,399 --> 00:21:22,960
就等效

699
00:21:22,960 --> 00:21:23,920
你感觉上等效

700
00:21:23,920 --> 00:21:25,440
就等于就是反正一半的丢掉了

701
00:21:25,440 --> 00:21:25,920
就等效

702
00:21:25,920 --> 00:21:27,759
就是我的隐藏层被减半

703
00:21:28,559 --> 00:21:30,960
那很有可能128开dropout的0.5

704
00:21:30,960 --> 00:21:33,440
效果比你直接是64要好

705
00:21:35,440 --> 00:21:36,039
就还是一样

706
00:21:36,039 --> 00:21:38,240
深度学习说我可以过礼盒

707
00:21:38,440 --> 00:21:40,079
但是我通过别的来

708
00:21:40,319 --> 00:21:41,920
让你训练的时候不要

709
00:21:42,480 --> 00:21:44,599
就我需要让模型够强

710
00:21:44,879 --> 00:21:48,119
我然后通过我的正责来使得你不要学偏

711
00:21:48,359 --> 00:21:49,960
就是说你跟小孩一样的

712
00:21:49,960 --> 00:21:52,279
就是说我想你智商高一点

713
00:21:52,279 --> 00:21:52,680
没问题

714
00:21:52,840 --> 00:21:53,840
你性格差一点不要紧

715
00:21:54,039 --> 00:21:55,480
我们想办法来

716
00:21:56,079 --> 00:21:57,960
或者说你说大家

717
00:21:58,559 --> 00:21:59,039
就一个人

718
00:21:59,200 --> 00:22:02,319
你就说一个人的能力是你的模型复杂度

719
00:22:02,319 --> 00:22:03,720
一个人的性格是你的

720
00:22:03,720 --> 00:22:07,160
或者说性格这种东西是你的

721
00:22:07,160 --> 00:22:08,680
你容易学歪的一个东西

722
00:22:08,799 --> 00:22:12,200
那么大家是愿意跟性格好的能力不强的人

723
00:22:12,640 --> 00:22:15,640
做事情还是跟性格差的

724
00:22:16,120 --> 00:22:17,519
但是能力很强的人做事情

725
00:22:17,559 --> 00:22:22,200
很有可能是性格好的

726
00:22:22,839 --> 00:22:24,920
不性格差一点的能力强的人

727
00:22:24,920 --> 00:22:26,759
然后你尽量去废掉他的性格

728
00:22:26,759 --> 00:22:27,119
对吧

729
00:22:27,119 --> 00:22:29,160
但最好的是说你能力有强性格也好

730
00:22:29,160 --> 00:22:30,359
当然是极少数了

731
00:22:30,359 --> 00:22:31,519
我们也有这样子的模型

732
00:22:31,519 --> 00:22:34,680
但是MLP不属于那种

733
00:22:34,680 --> 00:22:39,000
MLP属于能力强性格不好的那种

734
00:22:39,000 --> 00:22:42,000
所以现在尽量也不要使用MLP

735
00:22:42,000 --> 00:22:45,000
就是说一般大家用的越来越少

736
00:22:45,599 --> 00:22:47,920
所以就为什么我们之后要讲CEN

737
00:22:47,920 --> 00:22:49,200
讲这次formula

738
00:22:49,200 --> 00:22:50,799
CEN你可认为就是一个

739
00:22:50,799 --> 00:22:53,240
special的一个特别的一个MLP

740
00:22:54,599 --> 00:22:56,880
所以就是说我就回到这个问题

741
00:22:56,880 --> 00:22:59,279
我说我觉得Dropout主要是调起来比较方便

742
00:22:59,279 --> 00:23:02,119
就是说你就可以调个0.9

743
00:23:02,119 --> 00:23:04,519
如果你觉得这个隐藏层特别大

744
00:23:04,559 --> 00:23:05,359
模型特别复杂

745
00:23:05,359 --> 00:23:06,279
我就搞高一点

746
00:23:06,279 --> 00:23:07,079
搞0.9

747
00:23:07,079 --> 00:23:08,640
就90%的丢掉

748
00:23:08,640 --> 00:23:09,799
就是强回归

749
00:23:09,880 --> 00:23:11,920
如果是小一点的话

750
00:23:11,920 --> 00:23:12,839
我去个0.1

751
00:23:12,880 --> 00:23:14,279
就随便回归一点点

752
00:23:16,000 --> 00:23:18,799
改标签是一种mask吗

753
00:23:18,799 --> 00:23:21,400
改标签就是说我们之后会讲怎么改标签

754
00:23:21,400 --> 00:23:23,799
也是一种常用的技巧

755
00:23:23,799 --> 00:23:25,920
但是也不是简简单单的

756
00:23:25,920 --> 00:23:27,079
就把随机改下来

757
00:23:27,079 --> 00:23:27,960
不是这样子改的

758
00:23:27,960 --> 00:23:30,119
会有一点别的技术在里面

759
00:23:31,440 --> 00:23:32,960
如果你强行改的话

760
00:23:32,960 --> 00:23:35,759
可能效果不那么好

761
00:23:35,759 --> 00:23:37,680
但有一些别的技巧可以让你更好

762
00:23:38,400 --> 00:23:40,240
在同样的最后一个问题

763
00:23:40,400 --> 00:23:42,079
在同样的我们时间差不多了

764
00:23:42,440 --> 00:23:44,160
在同样的学习率下

765
00:23:44,160 --> 00:23:47,200
dropout的介入会造成参数收敛更慢

766
00:23:47,200 --> 00:23:49,880
需要比dropout的情况适当调大

767
00:23:49,880 --> 00:23:50,519
learning rate

768
00:23:50,519 --> 00:23:55,800
dropout还有真有可能使得你收敛会变慢

769
00:23:55,840 --> 00:23:57,200
这个是有可能的

770
00:23:57,600 --> 00:24:02,480
因为你等于是说你每次权重就t度更新

771
00:24:02,560 --> 00:24:04,640
就是说你更少的一些在更新t度

772
00:24:04,640 --> 00:24:07,759
但是我好像没有听说过说

773
00:24:07,759 --> 00:24:09,279
你因为有了dropout

774
00:24:09,279 --> 00:24:10,160
learning rate变大

775
00:24:10,160 --> 00:24:12,279
因为从期望上来讲

776
00:24:12,279 --> 00:24:13,240
他们是差不多了

777
00:24:13,279 --> 00:24:15,000
就dropout的不改变期望

778
00:24:15,000 --> 00:24:16,440
learning rate就学习率

779
00:24:16,440 --> 00:24:19,599
主要是对期望和方差敏感一点的

780
00:24:20,880 --> 00:24:23,759
所以我没有听说过要适当调到learning rate

781
00:24:23,759 --> 00:24:24,440
你可以调

782
00:24:24,440 --> 00:24:26,240
但是我没有听说过有

783
00:24:26,400 --> 00:24:28,440
大家说有经验上总结说

784
00:24:28,440 --> 00:24:29,279
dropout的原因

785
00:24:29,279 --> 00:24:30,759
我就把learning rate改到两倍

786
00:24:30,759 --> 00:24:32,000
没有听说过这个事情

787
00:24:32,160 --> 00:24:34,839
但我觉得会收敛变慢是有可能的

788
00:24:36,400 --> 00:24:40,200
transformer可以看作是一种特殊的MLP吗

789
00:24:41,000 --> 00:24:42,160
目前还没有

790
00:24:42,200 --> 00:24:43,360
目前还没有这么看

791
00:24:43,360 --> 00:24:45,480
transformer可以看作是一个kernel machine

792
00:24:45,480 --> 00:24:47,320
就是一个核方法

793
00:24:47,320 --> 00:24:48,279
也就是这个是可以

794
00:24:48,279 --> 00:24:49,320
这个是大家可以看的

795
00:24:49,800 --> 00:24:51,920
还没有看成是特殊的一个MLP

