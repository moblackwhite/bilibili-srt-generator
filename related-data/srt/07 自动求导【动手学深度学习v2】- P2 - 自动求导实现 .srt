1
00:00:00,000 --> 00:00:04,240
首先我们还是import touch

2
00:00:04,240 --> 00:00:10,439
touch array就是我们创建一个x是等于123的一个常位4的一个向量

3
00:00:13,280 --> 00:00:20,320
好我们呢因为大家知道是说我要计算y关于x的t度之前

4
00:00:20,320 --> 00:00:23,280
我们需要一个地方来存我的t度

5
00:00:23,280 --> 00:00:24,400
我怎么存呢

6
00:00:24,400 --> 00:00:25,920
我可以告诉你是说

7
00:00:25,920 --> 00:00:29,960
我告诉你x.requiresgrad等于true

8
00:00:29,960 --> 00:00:33,160
就告诉你说我要把这个t度存在什么地方

9
00:00:33,160 --> 00:00:38,040
这样子的话我就是x.grad我就可以以后就可以访问它的t度了

10
00:00:38,040 --> 00:00:43,320
就是y关于x的导数是放在这个地方的

11
00:00:43,320 --> 00:00:49,920
它等下也是说我在创建这个x的时候requiresgradient等于true

12
00:00:49,920 --> 00:00:53,200
就是我告诉你说我需要这个东西我就不要写两步了

13
00:00:53,200 --> 00:00:53,760
对吧

14
00:00:53,879 --> 00:00:56,679
这是好现在我们来计算y

15
00:00:56,679 --> 00:01:01,439
y我们很简单就是x和x的点累积乘以2

16
00:01:01,439 --> 00:01:03,599
对吧这个就是y的计算

17
00:01:03,599 --> 00:01:06,560
我们知道y就是累积就是一个标量

18
00:01:06,560 --> 00:01:08,879
然后乘以2那就是等于一个标量

19
00:01:08,879 --> 00:01:11,120
但是你可以看到有一个gradient function

20
00:01:11,120 --> 00:01:15,439
就是说因为它是显示显影视的构造的计算图

21
00:01:15,439 --> 00:01:19,039
所以它有一个求t度的函数存在这个地方

22
00:01:19,039 --> 00:01:21,719
告诉你说y是从x计算过来的

23
00:01:24,600 --> 00:01:25,520
OK

24
00:01:25,760 --> 00:01:30,160
接下来我就可以通过调用反向传播函数来计算

25
00:01:30,160 --> 00:01:33,400
y关于x每一个分量的t度

26
00:01:33,719 --> 00:01:37,160
那就是y.backward就是求导

27
00:01:37,359 --> 00:01:39,120
然后我就可以通过求完之后

28
00:01:39,120 --> 00:01:42,200
我就可以通过x.grad来访问我的导数了

29
00:01:42,520 --> 00:01:49,480
记得是说我的y是等于2乘以x乘以x的累积

30
00:01:49,520 --> 00:01:53,640
那就是我们算出来的就是标准答案就是4乘以x对不对

31
00:01:53,640 --> 00:01:55,719
所以我们可以访问一下x的grad

32
00:01:55,719 --> 00:01:57,159
它等于4乘以x

33
00:01:57,359 --> 00:01:58,680
这是应该是等于对的

34
00:01:58,680 --> 00:01:59,760
大家可以验证一下

35
00:02:01,560 --> 00:02:02,200
OK

36
00:02:03,520 --> 00:02:05,120
我们就是说默认情况下

37
00:02:05,120 --> 00:02:08,000
PyTorch会把所有的t度给你累积起来

38
00:02:08,000 --> 00:02:09,920
所以我们在算下一个的时候

39
00:02:09,920 --> 00:02:13,039
我们要把运行zero这个函数

40
00:02:13,039 --> 00:02:14,560
就是说zero函数

41
00:02:14,560 --> 00:02:16,360
就是说PyTorch里面就是说

42
00:02:16,360 --> 00:02:18,960
下滑线就表示这个函数是

43
00:02:19,519 --> 00:02:21,399
就是说重写我的内容

44
00:02:21,519 --> 00:02:25,239
就zero就是一个把0写进我的t度里面

45
00:02:25,239 --> 00:02:26,959
就是把所有的t度清零

46
00:02:27,919 --> 00:02:30,959
然后我说我重新来算另外一个函数

47
00:02:30,959 --> 00:02:33,319
就是xy等于x的sum

48
00:02:33,919 --> 00:02:35,479
然后我们再求backward

49
00:02:35,479 --> 00:02:38,519
我们知道求向量的sum

50
00:02:39,000 --> 00:02:41,120
它的t度是等于全一

51
00:02:41,120 --> 00:02:43,879
所以可以看到是x的grad等于全一

52
00:02:44,599 --> 00:02:44,959
OK

53
00:02:44,959 --> 00:02:46,199
大家如果可以试一下

54
00:02:46,199 --> 00:02:47,599
如果把这一行注释掉

55
00:02:47,599 --> 00:02:48,639
会发生什么变化

56
00:02:48,640 --> 00:02:49,480
大家可以试一下

57
00:02:52,160 --> 00:02:53,480
另外一个是说

58
00:02:54,240 --> 00:02:58,200
假设我的y它不是一个标量会怎么样

59
00:02:58,760 --> 00:03:01,520
就是说假设我的y是等于x乘以x

60
00:03:01,680 --> 00:03:03,320
那么x是一个向量

61
00:03:03,560 --> 00:03:05,080
那么y也是一个向量

62
00:03:05,320 --> 00:03:08,160
那么理论上它的backward function

63
00:03:08,440 --> 00:03:09,880
它应该是一个t度

64
00:03:09,920 --> 00:03:10,400
不

65
00:03:10,400 --> 00:03:11,840
是一个矩阵

66
00:03:12,480 --> 00:03:15,280
但是在深度学习或者机器学习里面

67
00:03:15,280 --> 00:03:17,200
我们很少去做这个事情

68
00:03:17,640 --> 00:03:23,000
我们很少去对一个向量的函数来求导

69
00:03:24,280 --> 00:03:26,640
所以我们在大部分情况是说

70
00:03:26,640 --> 00:03:28,760
我们只是对一个标量来求导

71
00:03:29,320 --> 00:03:31,000
在绝大部分情况

72
00:03:31,000 --> 00:03:33,720
我们会把它y做一个求和

73
00:03:33,920 --> 00:03:34,880
然后再来求导

74
00:03:34,880 --> 00:03:35,520
这样子的话

75
00:03:35,520 --> 00:03:36,560
我就是一个标量

76
00:03:36,560 --> 00:03:37,040
对吧

77
00:03:37,160 --> 00:03:39,720
我的求导还是一个0246

78
00:03:40,120 --> 00:03:41,520
我们会具体会讲

79
00:03:41,520 --> 00:03:42,880
在什么样的情况下

80
00:03:42,880 --> 00:03:44,640
我们会对它求和

81
00:03:47,720 --> 00:03:48,640
这就意味着是说

82
00:03:48,640 --> 00:03:49,760
我们之前讲过了

83
00:03:49,760 --> 00:03:52,320
大量的向量和矩阵

84
00:03:52,320 --> 00:03:53,600
或者矩阵和矩阵的求导

85
00:03:53,600 --> 00:03:56,160
其实是在我们深度学习里面

86
00:03:56,160 --> 00:03:56,960
是用了很少的

87
00:03:58,240 --> 00:03:59,520
另外一块就是说

88
00:03:59,560 --> 00:04:02,760
我可以把一些计算逻辑计算图以外

89
00:04:02,800 --> 00:04:03,560
会怎么样呢

90
00:04:04,120 --> 00:04:06,200
就是说我首先还是清零t度

91
00:04:06,920 --> 00:04:09,200
我定义一个y等于x乘以x

92
00:04:10,080 --> 00:04:11,080
然后我说

93
00:04:11,880 --> 00:04:14,480
我把ydetach掉

94
00:04:14,520 --> 00:04:17,800
就是说我把y当做一个常数

95
00:04:17,800 --> 00:04:20,600
而不是一个关于x的一个函数

96
00:04:21,280 --> 00:04:22,680
把它做成u

97
00:04:22,840 --> 00:04:25,360
那就是说u对系统来讲

98
00:04:25,360 --> 00:04:26,480
它就是一个

99
00:04:26,600 --> 00:04:28,720
不再是一个关于x的一个函数

100
00:04:28,720 --> 00:04:29,680
它就是一个常数

101
00:04:29,680 --> 00:04:31,439
它的直接等于x乘以x

102
00:04:31,800 --> 00:04:33,920
z在等于u乘以x的情况下

103
00:04:33,960 --> 00:04:35,240
那么对系统来讲

104
00:04:35,280 --> 00:04:37,120
那么z关于x的导数

105
00:04:37,120 --> 00:04:39,920
那就是一个常数乘以x

106
00:04:40,400 --> 00:04:41,800
然后对z的sum

107
00:04:41,800 --> 00:04:43,360
就backward function的话

108
00:04:43,360 --> 00:04:44,720
那么它就等于是u

109
00:04:45,120 --> 00:04:45,759
可以看到

110
00:04:47,759 --> 00:04:48,280
对吧

111
00:04:48,560 --> 00:04:49,800
同样道理是说

112
00:04:49,840 --> 00:04:52,920
我y我还是一个x的函数

113
00:04:53,120 --> 00:04:53,639
对吧

114
00:04:53,759 --> 00:04:57,520
我还是可以通过y来求和来求导

115
00:04:57,639 --> 00:04:58,199
同样的话

116
00:04:58,199 --> 00:05:00,439
它的导数是等于二乘以x

117
00:05:00,480 --> 00:05:02,560
就是说y是x的函数

118
00:05:02,560 --> 00:05:03,879
但u不再是了

119
00:05:03,879 --> 00:05:07,160
所以我还是可以通过y来对x求导

120
00:05:07,360 --> 00:05:08,560
z对x求导的话

121
00:05:08,560 --> 00:05:12,080
又是当做一个常数来进行的

122
00:05:12,759 --> 00:05:14,519
这个在我们之后的

123
00:05:14,560 --> 00:05:15,759
我们需要做一些

124
00:05:15,759 --> 00:05:17,919
把一些网络给你的参数

125
00:05:17,919 --> 00:05:18,879
给固定住的时候

126
00:05:18,879 --> 00:05:20,959
这个是很有用的一个功能

127
00:05:24,039 --> 00:05:25,560
然后还有一个是说

128
00:05:25,560 --> 00:05:27,680
当你用一个很复杂的

129
00:05:27,680 --> 00:05:30,079
Python的一个计算控制流的时候

130
00:05:30,079 --> 00:05:31,319
我们还是可以求导

131
00:05:31,680 --> 00:05:32,479
举一个例子

132
00:05:32,639 --> 00:05:35,399
有个fx a的函数

133
00:05:35,759 --> 00:05:37,519
b等于a乘以2

134
00:05:37,759 --> 00:05:39,159
我们根据b的long

135
00:05:39,159 --> 00:05:40,719
是不是小于1000

136
00:05:40,720 --> 00:05:42,840
我们一直把它乘2乘2乘2

137
00:05:43,640 --> 00:05:45,960
如果b的sum大于0的话

138
00:05:45,960 --> 00:05:47,800
我们就是c等于b

139
00:05:47,800 --> 00:05:49,320
不然的话我们就乘100

140
00:05:49,680 --> 00:05:51,240
就是说你发现所谓的

141
00:05:51,240 --> 00:05:53,000
这y与g与e

142
00:05:53,000 --> 00:05:54,840
它都是根据b的值

143
00:05:54,840 --> 00:05:57,520
或者你的输入的函数的值来指定的

144
00:05:58,160 --> 00:05:59,960
所以它的工作原理是说

145
00:06:00,000 --> 00:06:01,800
我每一次在算的时候

146
00:06:02,080 --> 00:06:03,760
torch会把整个计算图

147
00:06:03,760 --> 00:06:05,480
在背后给你存下来

148
00:06:05,520 --> 00:06:07,360
然后再把计算图

149
00:06:07,360 --> 00:06:08,840
往倒的回去做一遍

150
00:06:08,840 --> 00:06:10,280
就可以得到它的矩阵

151
00:06:10,920 --> 00:06:12,200
就是说我说我g

152
00:06:12,200 --> 00:06:15,000
如果是一个随机数

153
00:06:15,000 --> 00:06:16,240
就size等于空的话

154
00:06:16,240 --> 00:06:17,240
就是一个标量

155
00:06:17,280 --> 00:06:18,280
我需要t度

156
00:06:18,280 --> 00:06:19,680
把a放进去的话

157
00:06:19,680 --> 00:06:21,800
我可以不管每次我怎么运行

158
00:06:21,800 --> 00:06:23,120
我的结果应该都是对的

159
00:06:23,800 --> 00:06:24,240
OK

160
00:06:24,240 --> 00:06:26,400
这就是一个控制流的语句

161
00:06:26,440 --> 00:06:28,600
这也是说PyTorch

162
00:06:28,720 --> 00:06:31,720
就是说这种影视执行计算流的

163
00:06:31,720 --> 00:06:32,560
比显示

164
00:06:32,560 --> 00:06:33,960
比那种显示构造

165
00:06:33,960 --> 00:06:36,800
TensorFlow和MSNet的显示构造的好处

166
00:06:36,800 --> 00:06:40,480
就是说我对于这种控制流做得更好一些

167
00:06:40,480 --> 00:06:42,720
但反过来讲它慢一些

