1
00:00:00,000 --> 00:00:01,919
微调意味着说

2
00:00:01,919 --> 00:00:04,679
神经网络进行不同的目标检测前程的

3
00:00:04,679 --> 00:00:06,640
特征区是同样的对的

4
00:00:06,640 --> 00:00:08,720
就是说这的概念是说

5
00:00:08,720 --> 00:00:10,519
你可以认为前面那些层

6
00:00:10,519 --> 00:00:11,599
特别是越底层

7
00:00:11,599 --> 00:00:12,839
它的是越通用

8
00:00:12,839 --> 00:00:16,199
因为它是抽取那些底层的那些纹理信息

9
00:00:16,199 --> 00:00:19,039
就是说你让神经网络尽量去理解

10
00:00:19,120 --> 00:00:20,120
这个数据

11
00:00:20,160 --> 00:00:21,719
这些理解数据就是说

12
00:00:21,719 --> 00:00:22,640
形成那些pattern

13
00:00:22,640 --> 00:00:24,000
我们认为是通用的

14
00:00:26,280 --> 00:00:27,359
数据不平衡问题

15
00:00:27,359 --> 00:00:29,080
对特征提取的分类器影响大

16
00:00:29,080 --> 00:00:31,560
还是对分类器影响大

17
00:00:33,359 --> 00:00:34,600
数据不平衡

18
00:00:34,600 --> 00:00:35,679
其实数据不平衡

19
00:00:35,679 --> 00:00:39,079
对于特征提取倒是不会影响那么大

20
00:00:39,079 --> 00:00:40,280
我觉得主要是对

21
00:00:40,320 --> 00:00:41,960
越上面的层影响大

22
00:00:41,960 --> 00:00:43,480
就是说数据不平衡

23
00:00:43,480 --> 00:00:45,400
很多时候是你的标号的不平衡

24
00:00:45,439 --> 00:00:48,359
你标号是对主要是对上层的影响

25
00:00:48,359 --> 00:00:50,160
对下层基本上还是一些

26
00:00:50,200 --> 00:00:52,280
对图片自己本身的一些东西

27
00:00:52,320 --> 00:00:54,200
最近很多结果是证明这样子

28
00:00:54,600 --> 00:00:56,439
假设有AB两个数据

29
00:00:56,439 --> 00:00:57,240
A是imageNet

30
00:00:57,240 --> 00:00:58,200
B是医学图片

31
00:00:58,200 --> 00:00:59,440
我要识别癌症

32
00:01:00,160 --> 00:01:04,000
然后你说我要我的

33
00:01:04,000 --> 00:01:05,159
就是说我imageNet

34
00:01:05,159 --> 00:01:07,400
是一些猫狗图片

35
00:01:07,400 --> 00:01:08,439
然后你拿一张图片

36
00:01:08,439 --> 00:01:09,400
可能是不一样

37
00:01:09,439 --> 00:01:11,040
我觉得很有可能

38
00:01:11,040 --> 00:01:12,079
你确实

39
00:01:12,079 --> 00:01:13,960
如果你pre-trained data set

40
00:01:13,960 --> 00:01:16,040
和你的差的比较大的话

41
00:01:16,040 --> 00:01:17,000
就比较难

42
00:01:17,200 --> 00:01:18,920
你可以试一下

43
00:01:18,960 --> 00:01:21,079
但我个人觉得可能是

44
00:01:21,800 --> 00:01:23,000
你可能从头训练

45
00:01:23,000 --> 00:01:24,439
可能还会好一些

46
00:01:25,480 --> 00:01:26,040
OK

47
00:01:26,079 --> 00:01:27,079
但是反过来讲

48
00:01:27,079 --> 00:01:29,799
你应该去找在医学图片上的

49
00:01:29,799 --> 00:01:30,719
pre-trained model

50
00:01:30,719 --> 00:01:31,959
不要去找imageNet

51
00:01:31,959 --> 00:01:33,200
imageNet只是一个数据集

52
00:01:34,200 --> 00:01:35,480
实际过程中

53
00:01:35,480 --> 00:01:36,640
imageNet用的不多的

54
00:01:36,640 --> 00:01:38,480
就是说你如果是医学图片

55
00:01:38,480 --> 00:01:40,280
你要去找在医学图片上

56
00:01:40,280 --> 00:01:41,439
pre-trained data set

57
00:01:41,480 --> 00:01:42,599
pre-trained model

58
00:01:44,319 --> 00:01:45,959
FindTony是不是transfer learning

59
00:01:45,959 --> 00:01:46,719
两者有不同

60
00:01:46,719 --> 00:01:48,560
就是了

61
00:01:48,560 --> 00:01:49,799
反正你怎么说

62
00:01:50,400 --> 00:01:50,920
神机网络

63
00:01:50,920 --> 00:01:51,640
反正这帮人

64
00:01:52,239 --> 00:01:54,400
反正什么东西都取一个很好的名字

65
00:01:54,439 --> 00:01:55,599
这样子就可以说

66
00:01:55,759 --> 00:01:57,039
这是我的贡献

67
00:01:57,439 --> 00:01:58,000
换个名字

68
00:01:58,200 --> 00:01:59,359
其实就是transfer learning

69
00:02:03,319 --> 00:02:04,319
重用标号的话

70
00:02:04,319 --> 00:02:06,439
对无关的标号直接删除吗

71
00:02:06,439 --> 00:02:07,200
原始

72
00:02:07,239 --> 00:02:07,640
对的

73
00:02:07,640 --> 00:02:08,800
如果你想重用标号

74
00:02:08,800 --> 00:02:09,400
我们有个

75
00:02:09,400 --> 00:02:10,599
我们你去看那本书

76
00:02:10,719 --> 00:02:12,079
书里面有个练习

77
00:02:12,079 --> 00:02:13,039
就讲这个事情

78
00:02:13,079 --> 00:02:14,960
就是说你要把对应的标号的

79
00:02:14,960 --> 00:02:16,079
权重拎出来

80
00:02:16,079 --> 00:02:17,039
别的都不要

81
00:02:18,520 --> 00:02:20,280
如果原始数据没有的标号

82
00:02:20,280 --> 00:02:21,120
就随机

83
00:02:21,719 --> 00:02:22,960
有的标号拿出来

84
00:02:22,960 --> 00:02:23,960
没有的随机

85
00:02:27,080 --> 00:02:29,600
标号是不是都是label的字符串

86
00:02:29,600 --> 00:02:30,520
对应到数字上

87
00:02:30,520 --> 00:02:31,719
微条有相同

88
00:02:31,719 --> 00:02:32,000
对

89
00:02:32,000 --> 00:02:33,440
你都是要对应的字符串

90
00:02:33,600 --> 00:02:34,800
不要去对应数字

91
00:02:34,800 --> 00:02:35,920
数字是反正是

92
00:02:35,920 --> 00:02:38,640
这个是根据你标号的顺序来的

93
00:02:38,960 --> 00:02:40,160
你要去对应字符串

94
00:02:40,200 --> 00:02:41,280
很多时候你都是

95
00:02:41,840 --> 00:02:42,680
比如说

96
00:02:42,880 --> 00:02:43,560
你的字符串

97
00:02:43,560 --> 00:02:45,560
你还得可能还要得去做一下语音匹配

98
00:02:45,760 --> 00:02:47,040
就是说同样一个东西

99
00:02:47,320 --> 00:02:48,920
在这个数据上叫这个名字

100
00:02:48,920 --> 00:02:50,520
另外数据叫不一样的名字

101
00:02:50,800 --> 00:02:52,040
你得可以去做这个事情

102
00:02:53,920 --> 00:02:55,080
对微条就是transfer

103
00:02:55,120 --> 00:02:56,360
你不是transformer

104
00:02:56,520 --> 00:02:57,200
就transfer

105
00:02:57,240 --> 00:02:57,560
是的

106
00:02:57,600 --> 00:02:58,480
就迁移学习

107
00:02:59,080 --> 00:02:59,760
是的

108
00:03:00,040 --> 00:03:04,160
微条是迁移学习之间里面的一种算法

109
00:03:04,160 --> 00:03:06,760
但是迁移学习可以认为是一大类的算法

110
00:03:06,760 --> 00:03:08,280
它有包括微条很多

111
00:03:10,880 --> 00:03:12,520
imaginate是比较简单的话

112
00:03:12,520 --> 00:03:14,720
有哪些或许更好的用微条的模型

113
00:03:16,200 --> 00:03:17,200
有是有了

114
00:03:18,040 --> 00:03:19,280
别人不一定公开给你

115
00:03:19,400 --> 00:03:22,240
但现在公司都把自己在大数据上

116
00:03:22,240 --> 00:03:24,440
券的模型当做自己的财产

117
00:03:24,440 --> 00:03:26,200
就是可能比数据还重要一点

118
00:03:26,280 --> 00:03:28,440
所以就是说你确实

119
00:03:29,640 --> 00:03:30,480
学术级的话

120
00:03:30,480 --> 00:03:31,360
你学术界的话

121
00:03:31,360 --> 00:03:32,360
你原理比起来是挺好的

122
00:03:32,360 --> 00:03:33,000
你公司的话

123
00:03:33,000 --> 00:03:34,360
你可能自己去找一找

124
00:03:34,720 --> 00:03:36,840
我们可能会release一些模型出来

125
00:03:36,840 --> 00:03:37,720
但我还不能保证

126
00:03:37,720 --> 00:03:41,000
因为这个东西确实是有很多有商业价值的东西

127
00:03:41,000 --> 00:03:42,160
要release出来的话

128
00:03:42,160 --> 00:03:43,280
你得怎么样

129
00:03:43,360 --> 00:03:44,760
你得去说服一下别人

130
00:03:46,720 --> 00:03:48,920
如果原数据和目标数据差异很大

131
00:03:48,920 --> 00:03:50,400
微条效果会下降吗

132
00:03:50,600 --> 00:03:51,040
对的

133
00:03:51,040 --> 00:03:52,280
你这个问题你用过

134
00:03:52,280 --> 00:03:53,400
会

135
00:03:53,400 --> 00:03:56,280
就是说你的原数据及和目标数据

136
00:03:56,280 --> 00:03:58,280
最好是相似

137
00:03:58,640 --> 00:04:00,159
原数据是更大

138
00:04:00,200 --> 00:04:01,080
种类更多

139
00:04:01,080 --> 00:04:03,520
但是最好有点类似

140
00:04:06,840 --> 00:04:07,520
微条的话

141
00:04:07,520 --> 00:04:09,680
原数据是不是要包含目标数据的

142
00:04:09,680 --> 00:04:10,640
那边不需要了

143
00:04:10,640 --> 00:04:11,879
就类似就行了

144
00:04:11,879 --> 00:04:14,319
就是说你都是认认动物

145
00:04:14,480 --> 00:04:15,480
认认什么

146
00:04:15,480 --> 00:04:17,000
就是说你不需要完全一样

147
00:04:20,800 --> 00:04:23,040
微条中的规划保持一直很重要

148
00:04:23,040 --> 00:04:23,840
是因为

149
00:04:24,439 --> 00:04:25,240
就是说

150
00:04:25,879 --> 00:04:27,680
微条就是说对的

151
00:04:27,680 --> 00:04:31,200
就是说你是为了保持数据的分布

152
00:04:31,200 --> 00:04:31,879
就是说

153
00:04:32,000 --> 00:04:33,439
你就微条

154
00:04:33,520 --> 00:04:36,160
你可认为刚刚我们normalization

155
00:04:36,160 --> 00:04:37,480
你可以认为是一个

156
00:04:37,720 --> 00:04:39,600
网络中间的一块

157
00:04:39,920 --> 00:04:41,840
你normalization可以换成一个batch

158
00:04:41,840 --> 00:04:42,480
normalization

159
00:04:42,480 --> 00:04:43,280
那个layer

160
00:04:44,160 --> 00:04:44,720
对吧

161
00:04:44,759 --> 00:04:45,840
就是说你可以认为

162
00:04:45,840 --> 00:04:47,320
to tensor后面的那一块

163
00:04:47,320 --> 00:04:50,000
都应该是我的网络架构的那一块

164
00:04:50,040 --> 00:04:51,840
所以是说只是你的resonate

165
00:04:51,840 --> 00:04:52,760
18copy的时候

166
00:04:52,760 --> 00:04:53,880
没有把normalization

167
00:04:53,880 --> 00:04:54,480
copy过去

168
00:04:54,480 --> 00:04:56,000
所以我们要手动弄过去

169
00:04:56,120 --> 00:04:57,680
但是如果你的resonate18

170
00:04:57,680 --> 00:04:58,880
里面有个batch

171
00:04:58,880 --> 00:05:00,320
normalization的layer的话

172
00:05:00,320 --> 00:05:01,840
你是不需要normalization

173
00:05:01,840 --> 00:05:02,320
layer的

174
00:05:05,440 --> 00:05:08,400
那些参数是从imagenet上算出来的

175
00:05:08,440 --> 00:05:10,160
是imagenet数据集上

176
00:05:10,160 --> 00:05:11,680
RGB三个通道

177
00:05:11,680 --> 00:05:13,000
它的均值和方差

178
00:05:13,600 --> 00:05:15,320
那个是一般你的

179
00:05:15,680 --> 00:05:16,760
是那里搞来的

180
00:05:17,360 --> 00:05:17,600
对

181
00:05:17,600 --> 00:05:18,320
就是基本是这个

182
00:05:18,320 --> 00:05:19,520
所以这个是比较

183
00:05:20,120 --> 00:05:21,040
这个是比较trick

184
00:05:21,360 --> 00:05:23,280
我觉得现在可能我不知道

185
00:05:23,280 --> 00:05:23,960
大家查一下

186
00:05:23,960 --> 00:05:26,400
还需不需要干这个事情

187
00:05:26,439 --> 00:05:28,439
很有可能现在大家做的比较好了

188
00:05:28,439 --> 00:05:30,120
不需要你再去手动的

189
00:05:30,120 --> 00:05:31,520
讲normalize那个东西了

190
00:05:31,520 --> 00:05:32,360
可能是这样

191
00:05:32,960 --> 00:05:34,000
至少你用一个batch

192
00:05:34,000 --> 00:05:35,040
normalization的layer的话

193
00:05:35,040 --> 00:05:36,120
你就不需要这个东西了

194
00:05:36,360 --> 00:05:37,720
Auto-Ground会加入微条

195
00:05:37,720 --> 00:05:38,879
Auto-Ground会加入微条的

196
00:05:38,879 --> 00:05:40,920
Auto-Ground会去帮你做微条

197
00:05:40,920 --> 00:05:42,160
就是Auto-Ground其实

198
00:05:42,200 --> 00:05:43,920
基本上都是主要是用微条

199
00:05:44,120 --> 00:05:45,680
微条总会不会让你变差

200
00:05:45,680 --> 00:05:46,600
说一真话

201
00:05:47,840 --> 00:05:50,080
你用微条通常不会让你变差

202
00:05:50,120 --> 00:05:51,479
就是说很多时候不让你变好

203
00:05:51,479 --> 00:05:52,159
是有可能的

204
00:05:52,159 --> 00:05:53,519
但是变差的概率不大

205
00:05:53,919 --> 00:05:54,639
所以你

206
00:05:57,439 --> 00:05:59,319
比较常用的预训的模型

207
00:05:59,319 --> 00:06:00,039
有哪些

208
00:06:01,919 --> 00:06:03,039
比较常用的

209
00:06:05,360 --> 00:06:06,639
ImageNet上

210
00:06:06,680 --> 00:06:08,319
Projection的模型是常用的

211
00:06:10,719 --> 00:06:11,799
ResNet18

212
00:06:12,079 --> 00:06:13,799
ResNet系列

213
00:06:13,959 --> 00:06:14,839
就是比较常用的

214
00:06:14,840 --> 00:06:21,160
重用分类器权重

215
00:06:21,160 --> 00:06:22,280
对一个80类的数据

216
00:06:22,280 --> 00:06:24,400
如果只想选用其中5个4类

217
00:06:26,200 --> 00:06:27,480
对只能手动提取

218
00:06:27,680 --> 00:06:29,080
没有特别好的方式

219
00:06:29,080 --> 00:06:30,480
就是手动去看你

220
00:06:30,520 --> 00:06:31,040
匹配一下

221
00:06:31,080 --> 00:06:32,520
就你的target的数据机

222
00:06:32,520 --> 00:06:34,640
去看一下你的source的数据机里面

223
00:06:34,640 --> 00:06:35,320
有哪些label

224
00:06:35,320 --> 00:06:36,080
然后比一下

225
00:06:37,360 --> 00:06:38,360
写个python java

226
00:06:38,360 --> 00:06:39,920
看一下哪些是常用的

227
00:06:40,640 --> 00:06:41,800
但是我觉得这个是

228
00:06:41,800 --> 00:06:43,240
你如果现在麻烦

229
00:06:43,240 --> 00:06:44,800
就不要做这事情没关系

230
00:06:44,920 --> 00:06:46,600
我们就是说给大家一个想法

231
00:06:46,600 --> 00:06:48,560
但实际上大家用的不多

232
00:06:48,560 --> 00:06:49,319
就这个东西

233
00:06:51,120 --> 00:06:51,920
训练级的精度

234
00:06:51,920 --> 00:06:52,960
为什么一开始很高

235
00:06:53,160 --> 00:06:54,079
急剧下降的问题

236
00:06:54,360 --> 00:06:56,680
这就是我觉得数据稳定性的问题

237
00:06:56,840 --> 00:06:58,040
就劝到一半抖了一下

238
00:06:58,400 --> 00:06:59,240
你可能下次劝

239
00:06:59,240 --> 00:07:00,199
可能就不会这样子了

240
00:07:01,120 --> 00:07:02,600
这个是随机性的问题

241
00:07:04,920 --> 00:07:07,160
微调是直接把别人的训练模型

242
00:07:07,160 --> 00:07:08,400
做初始化吗

243
00:07:09,840 --> 00:07:11,560
还是每次先用

244
00:07:12,360 --> 00:07:14,319
对是一般微调是把别人用的

245
00:07:15,120 --> 00:07:16,280
就是你有两种做法

246
00:07:16,280 --> 00:07:18,319
一种是你去用别人的模型

247
00:07:18,319 --> 00:07:19,120
第二个方式

248
00:07:19,120 --> 00:07:20,959
你自己project也可以没问题

249
00:07:21,480 --> 00:07:23,160
你自己在mhnet上跑一遍

250
00:07:23,160 --> 00:07:24,120
然后把参数记下来

251
00:07:24,120 --> 00:07:25,079
再跑自己模型也可以

252
00:07:26,160 --> 00:07:26,759
没关系了

253
00:07:31,800 --> 00:07:33,040
计算损失的时候

254
00:07:33,040 --> 00:07:33,600
就是说

255
00:07:34,519 --> 00:07:36,560
计算损失是不是用label对应的标号

256
00:07:36,560 --> 00:07:38,800
对微调对应的标号不太

257
00:07:39,360 --> 00:07:39,840
就是说

258
00:07:40,800 --> 00:07:42,000
你还是说怎么copy

259
00:07:42,160 --> 00:07:43,520
去copy我建议去看一下

260
00:07:43,800 --> 00:07:45,920
我们的练习题

261
00:07:46,000 --> 00:07:47,160
练习题里面有段代码

262
00:07:47,160 --> 00:07:48,520
告诉你怎么做这种事情

263
00:07:48,520 --> 00:07:50,280
大家去看一下是怎么做的就行了

264
00:07:50,800 --> 00:07:53,000
OK你要做标号的映射的

265
00:07:53,000 --> 00:07:55,360
就是说你要对应的那边标号的

266
00:07:55,360 --> 00:07:56,200
这边标号的映射

267
00:07:56,200 --> 00:07:57,680
你是要去弄出来的

268
00:07:58,800 --> 00:08:00,680
微调在学习率上有什么用的技巧吗

269
00:08:00,880 --> 00:08:02,320
微调最好的是说

270
00:08:03,360 --> 00:08:04,440
微调在最好的地方

271
00:08:04,440 --> 00:08:05,600
是说你不要调学习率

272
00:08:05,600 --> 00:08:07,520
就学习率就选一个比较小的就行了

273
00:08:07,520 --> 00:08:08,320
就不重要

274
00:08:08,599 --> 00:08:10,800
所以就是说微调的学习率不敏感

275
00:08:12,599 --> 00:08:12,879
好

276
00:08:12,879 --> 00:08:14,120
迁移学习

277
00:08:14,480 --> 00:08:17,360
固定原数聚种层和目标之间层有

278
00:08:17,360 --> 00:08:18,199
影响吗

279
00:08:20,199 --> 00:08:22,480
就是说你看这个东西

280
00:08:23,560 --> 00:08:25,600
通常你固定是说你

281
00:08:27,360 --> 00:08:29,519
就是说你固定一些层怎么说

282
00:08:29,639 --> 00:08:31,000
就是说你固定一些层

283
00:08:31,039 --> 00:08:32,840
这是一个正则化

284
00:08:33,360 --> 00:08:35,319
因为你让一些层不变了

285
00:08:35,639 --> 00:08:37,399
就把整个模型的幅度降低了

286
00:08:37,439 --> 00:08:38,559
所以它是一个正则化

287
00:08:38,559 --> 00:08:40,360
我觉得这个东西是需要调的

288
00:08:40,360 --> 00:08:42,639
我很难说怎么样最好

289
00:08:42,639 --> 00:08:44,120
就是你得去调一调

290
00:08:44,319 --> 00:08:44,840
OK

291
00:08:44,840 --> 00:08:45,959
就最简单情况下

292
00:08:45,959 --> 00:08:46,759
极端情况

293
00:08:46,759 --> 00:08:47,879
我所有的固定座

294
00:08:47,879 --> 00:08:49,159
只最后一层训练

295
00:08:49,480 --> 00:08:51,199
只最后一个全连接层训练

296
00:08:51,199 --> 00:08:53,679
别的模型下面的卷迹全部不动

297
00:08:54,000 --> 00:08:55,720
但极端情况下大家一起动

298
00:08:55,720 --> 00:08:56,199
对吧

299
00:08:56,559 --> 00:08:57,480
中间的情况就是说

300
00:08:57,480 --> 00:08:58,399
下面一些层不动

301
00:08:58,399 --> 00:08:59,840
50%层不动什么的

302
00:08:59,840 --> 00:09:01,159
你可以自己调

303
00:09:03,199 --> 00:09:06,279
另外问题24也是一个

304
00:09:07,079 --> 00:09:08,879
光学图像上训练好的模型

305
00:09:09,279 --> 00:09:10,000
雷达图片

306
00:09:10,000 --> 00:09:10,720
就是说

307
00:09:13,279 --> 00:09:14,120
就这个东西

308
00:09:14,399 --> 00:09:15,839
我觉得还是说

309
00:09:15,879 --> 00:09:17,279
你雷达图片好

310
00:09:17,480 --> 00:09:18,240
光学图片好

311
00:09:18,279 --> 00:09:19,000
卫星图片好

312
00:09:19,199 --> 00:09:22,679
这种不同的sensor采集的图片

313
00:09:22,679 --> 00:09:24,199
你最好不要跟

314
00:09:24,480 --> 00:09:26,399
最好是你能够去想办法

315
00:09:26,399 --> 00:09:27,519
找一些类似的

316
00:09:27,519 --> 00:09:28,240
比较大数据

317
00:09:28,240 --> 00:09:29,360
pre-trained的一个模型

318
00:09:29,399 --> 00:09:30,399
然后去

319
00:09:30,399 --> 00:09:31,439
你要么去找一找

320
00:09:31,439 --> 00:09:32,879
有没有别人干这个事情

321
00:09:32,879 --> 00:09:33,639
要么很有可能

322
00:09:33,639 --> 00:09:36,079
你真的是自己重新去

323
00:09:36,080 --> 00:09:36,960
from scratch去

324
00:09:36,960 --> 00:09:39,280
fine tuning可能就帮助不大

325
00:09:40,600 --> 00:09:41,840
如果你是一家公司

326
00:09:41,960 --> 00:09:44,360
或者你是一个研究院的话

327
00:09:44,360 --> 00:09:46,080
那么你可以先考虑说

328
00:09:46,080 --> 00:09:47,040
我这一类图片

329
00:09:47,040 --> 00:09:48,720
实在找不到pre-trained模型的话

330
00:09:48,720 --> 00:09:50,080
你自己先去找一点

331
00:09:50,080 --> 00:09:51,320
去搞一个比较大数据

332
00:09:51,320 --> 00:09:52,320
自己做pre-trained

333
00:09:52,840 --> 00:09:54,200
training from scratch

334
00:09:54,200 --> 00:09:56,160
然后你之后再做别的应用的时候

335
00:09:56,160 --> 00:09:57,200
你就可以把这个东西

336
00:09:57,200 --> 00:09:58,320
直接拿过去训练了

337
00:09:58,400 --> 00:09:59,520
而且是说

338
00:09:59,720 --> 00:10:02,000
图片分类的训练模型

339
00:10:02,000 --> 00:10:03,840
会对之后的目标检测

340
00:10:03,960 --> 00:10:05,360
所有东西都是有用的

341
00:10:06,200 --> 00:10:07,840
不仅仅用在图片分类上

342
00:10:07,840 --> 00:10:09,000
做别的应用

343
00:10:09,000 --> 00:10:09,720
一样的有用

344
00:10:09,720 --> 00:10:11,120
就抽特征

345
00:10:11,720 --> 00:10:13,519
只要你任何用到抽特征的东西

346
00:10:13,519 --> 00:10:14,519
你都可以用它

347
00:10:14,560 --> 00:10:16,279
所以这就是它强大的地方

348
00:10:16,480 --> 00:10:17,120
OK

349
00:10:17,320 --> 00:10:18,600
所以我们基本上

350
00:10:19,680 --> 00:10:20,120
看一下

351
00:10:20,240 --> 00:10:21,759
我再最后刷新一下

352
00:10:22,639 --> 00:10:23,240
OK

353
00:10:23,240 --> 00:10:25,000
我们刚好

354
00:10:25,279 --> 00:10:26,200
时间刚刚好

355
00:10:26,440 --> 00:10:27,680
我看到

356
00:10:31,639 --> 00:10:32,560
最后有个问题说

357
00:10:32,560 --> 00:10:33,720
为什么不用微调

358
00:10:33,720 --> 00:10:35,000
一开始测试进度很高

359
00:10:35,039 --> 00:10:35,799
这二分类问题

360
00:10:35,919 --> 00:10:36,919
二分类问题也差不多

361
00:10:37,600 --> 00:10:38,240
二分类问题

362
00:10:38,240 --> 00:10:38,840
就是说

363
00:10:38,840 --> 00:10:39,279
就是说

364
00:10:39,279 --> 00:10:39,960
用微调之后

365
00:10:39,960 --> 00:10:41,440
一下就跳到90%

366
00:10:41,440 --> 00:10:43,440
然后不用微调的话

367
00:10:43,440 --> 00:10:44,879
那就跳到80%

368
00:10:44,879 --> 00:10:45,519
因为是二分类

369
00:10:45,519 --> 00:10:46,440
所以比较简单

370
00:10:47,759 --> 00:10:48,639
另外一个问题

371
00:10:48,639 --> 00:10:49,159
最后一个问题

372
00:10:49,159 --> 00:10:50,000
就是我最后一个问题

373
00:10:50,600 --> 00:10:52,279
因为我们时间有点超了

374
00:10:52,320 --> 00:10:54,000
是不是已经有过

375
00:10:54,000 --> 00:10:55,600
随机选择层的实验了

376
00:10:55,600 --> 00:10:56,200
有

377
00:10:56,919 --> 00:10:57,639
有

378
00:10:57,639 --> 00:10:59,960
就是这个就是hyperparameter tuning

379
00:11:00,120 --> 00:11:01,080
就是写个for loop

380
00:11:01,679 --> 00:11:02,759
最简单对吧

381
00:11:03,159 --> 00:11:04,480
你怎么写

382
00:11:05,240 --> 00:11:07,480
你最假设你有10层卷积层的话

383
00:11:07,480 --> 00:11:08,360
你写个for loop

384
00:11:09,000 --> 00:11:10,360
从01直到10

385
00:11:10,360 --> 00:11:11,879
然后固定

386
00:11:11,879 --> 00:11:12,680
就是说每一次

387
00:11:12,680 --> 00:11:13,480
就是第i次

388
00:11:13,480 --> 00:11:15,639
就是固定最下面的i层

389
00:11:16,600 --> 00:11:17,720
然后调上面的

390
00:11:17,800 --> 00:11:18,920
你就写个for loop就行了

391
00:11:19,639 --> 00:11:20,240
对吧

392
00:11:20,440 --> 00:11:21,720
所以你是可以做这个实验的

393
00:11:21,720 --> 00:11:22,360
我们做过

394
00:11:22,560 --> 00:11:23,320
这个做过

395
00:11:23,320 --> 00:11:24,399
有一点点效果

396
00:11:24,399 --> 00:11:25,320
但是很贵

397
00:11:25,320 --> 00:11:27,279
这种东西等于是跑10次

398
00:11:27,279 --> 00:11:28,920
或者跑很多次时间

399
00:11:29,200 --> 00:11:30,120
你愿意做也可以

400
00:11:30,120 --> 00:11:31,360
但会有一点点效果

401
00:11:31,360 --> 00:11:32,000
OK

