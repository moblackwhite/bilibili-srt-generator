1
00:00:00,000 --> 00:00:05,839
深度学习的损失函数一般是非突的吗

2
00:00:06,560 --> 00:00:12,560
就是说你其实损失函数是其实是突的

3
00:00:12,759 --> 00:00:13,919
但是你没用

4
00:00:13,919 --> 00:00:16,719
因为你的整个审讯网络是一个非突的

5
00:00:16,719 --> 00:00:17,600
一个审讯网络

6
00:00:18,519 --> 00:00:21,000
只要你是不是单层的话

7
00:00:21,000 --> 00:00:22,240
只要加入了隐藏层

8
00:00:22,240 --> 00:00:26,199
它就不再是一个突函数了

9
00:00:26,480 --> 00:00:29,480
所以你整个网络加上你的损失函数

10
00:00:29,480 --> 00:00:30,920
它就是一个非突的

11
00:00:32,200 --> 00:00:35,399
而且我觉得所谓的突函数

12
00:00:35,520 --> 00:00:36,640
突函数对你来说

13
00:00:36,640 --> 00:00:39,240
当然更多的时候是你的理论上的优美性

14
00:00:39,240 --> 00:00:41,280
突函数是说我证明我的优化

15
00:00:41,280 --> 00:00:42,760
一定是可以到最优解的

16
00:00:43,039 --> 00:00:43,679
但没用

17
00:00:43,679 --> 00:00:46,159
因为突函数它的表示能力有限

18
00:00:46,719 --> 00:00:48,679
人大老又不是个突的

19
00:00:48,960 --> 00:00:49,560
对吧

20
00:00:50,000 --> 00:00:50,719
我还没突

21
00:00:51,719 --> 00:00:54,359
所以不要追求突还是非突

22
00:00:54,359 --> 00:00:56,560
就是说如果从实用角度来讲

23
00:00:56,600 --> 00:00:59,120
突函数通常是没什么用的

24
00:00:59,960 --> 00:01:01,080
所以就是说审讯网络

25
00:01:01,080 --> 00:01:02,520
通常大家不会考虑

26
00:01:03,080 --> 00:01:04,400
突或者非突情况

27
00:01:04,400 --> 00:01:05,719
另外反过来讲

28
00:01:06,120 --> 00:01:07,840
你的因为你是非突

29
00:01:08,159 --> 00:01:10,480
所以理论作息难很多

30
00:01:12,600 --> 00:01:13,280
好问题二

31
00:01:13,280 --> 00:01:15,240
我在训练数页分类时

32
00:01:15,240 --> 00:01:16,840
发现训练级的交叉伤

33
00:01:16,840 --> 00:01:18,159
Loss大于验证级

34
00:01:18,200 --> 00:01:21,760
但训练级的ACC也是大于训练级的

35
00:01:21,800 --> 00:01:23,760
这是什么原因引起的呢

36
00:01:23,800 --> 00:01:28,000
我猜你很有可能是你在训练上

37
00:01:28,000 --> 00:01:29,560
加了data augmentation

38
00:01:31,040 --> 00:01:34,040
就是说你加了它的话

39
00:01:34,040 --> 00:01:36,280
你每次看到是一个比较不一样的

40
00:01:36,319 --> 00:01:38,439
所以每次可以认为是看到新的图片

41
00:01:38,960 --> 00:01:41,439
所以你可以看到是在一个新图片上

42
00:01:41,439 --> 00:01:44,159
做的loss和新图片上做的accuracy

43
00:01:44,200 --> 00:01:46,480
你可能是会小于验证级的

44
00:01:46,480 --> 00:01:48,240
其实是很正常的一个现象

45
00:01:48,439 --> 00:01:49,799
我们之前有讲过

46
00:01:51,200 --> 00:01:51,760
问题三

47
00:01:51,760 --> 00:01:52,920
Normalized参数怎么来的

48
00:01:52,920 --> 00:01:53,560
Normalized参数

49
00:01:53,560 --> 00:01:54,200
我没有讲过

50
00:01:54,200 --> 00:01:59,240
是ImageNet RGB出来的

51
00:01:59,240 --> 00:02:00,600
ImageNet的数据

52
00:02:00,600 --> 00:02:03,079
是它的RGB三个channel

53
00:02:03,079 --> 00:02:05,960
它的均值和方差是这么来的

54
00:02:07,240 --> 00:02:08,840
就是说实际上你选一个二

55
00:02:08,879 --> 00:02:11,080
你随便取一个整值也没关系

56
00:02:11,080 --> 00:02:13,640
就是说反正大家习惯了

57
00:02:13,680 --> 00:02:15,920
就是把这个代码copy来copy去了

58
00:02:16,120 --> 00:02:16,680
反过来讲

59
00:02:16,680 --> 00:02:17,920
你不用这个没关系

60
00:02:17,920 --> 00:02:19,000
你就加一个batchnome

61
00:02:19,000 --> 00:02:19,759
加在最前面

62
00:02:19,759 --> 00:02:20,759
把它normalize一下

63
00:02:20,759 --> 00:02:21,840
就你就不要normalize

64
00:02:21,840 --> 00:02:22,719
也是没关系的

65
00:02:24,720 --> 00:02:29,000
TrainValidDataSet起草作用

66
00:02:29,000 --> 00:02:30,200
为什么不把TrainDataSet

67
00:02:30,200 --> 00:02:31,840
和ValidDataSet放在一起做什么

68
00:02:31,840 --> 00:02:32,800
其实说白了

69
00:02:32,800 --> 00:02:33,400
就是

70
00:02:33,600 --> 00:02:34,960
TrainValidDataSet

71
00:02:34,960 --> 00:02:36,360
就是把Train的DataSet

72
00:02:36,360 --> 00:02:37,600
加上ValidDataSet

73
00:02:38,040 --> 00:02:39,240
就是在干这个事情

74
00:02:39,280 --> 00:02:40,360
但我还真不知道

75
00:02:40,360 --> 00:02:42,120
PyTorch的ImageIterator

76
00:02:42,120 --> 00:02:43,160
让你这么加吗

77
00:02:43,160 --> 00:02:44,400
我能够把它加起来

78
00:02:44,400 --> 00:02:45,200
等于不行

79
00:02:45,840 --> 00:02:46,600
反过来讲

80
00:02:46,600 --> 00:02:47,640
当然说我可以说

81
00:02:47,640 --> 00:02:48,920
我forBatch等于它

82
00:02:48,920 --> 00:02:49,840
然后再因它

83
00:02:49,840 --> 00:02:50,960
当然是你可以这么用

84
00:02:51,320 --> 00:02:52,600
在写代码就麻烦一点

85
00:02:53,199 --> 00:02:54,000
其实说

86
00:02:54,079 --> 00:02:55,079
其实就是

87
00:02:55,759 --> 00:02:56,680
语义上来说

88
00:02:56,680 --> 00:02:57,400
就加在一起

89
00:02:57,400 --> 00:02:58,199
但是这么写

90
00:02:58,199 --> 00:02:59,319
就是让你的劝喊数

91
00:02:59,319 --> 00:03:00,319
相对简单一点

92
00:03:03,519 --> 00:03:05,759
DropLast它不补0的

93
00:03:08,000 --> 00:03:10,120
首先我们很少做补0

94
00:03:10,439 --> 00:03:10,840
补0

95
00:03:10,879 --> 00:03:12,400
其实我们在之后的

96
00:03:12,400 --> 00:03:14,439
目标检测会做补0

97
00:03:14,479 --> 00:03:16,479
但是一般做图片分类的话

98
00:03:16,479 --> 00:03:17,680
你有几种做法

99
00:03:18,240 --> 00:03:19,439
常用做法

100
00:03:20,560 --> 00:03:21,400
一种是什么

101
00:03:21,960 --> 00:03:23,120
一种是

102
00:03:23,360 --> 00:03:25,319
最简单是丢掉

103
00:03:25,319 --> 00:03:26,760
假设你最后一个

104
00:03:26,800 --> 00:03:28,560
读到最后发现

105
00:03:28,599 --> 00:03:30,040
不够一个p量大小了

106
00:03:30,080 --> 00:03:30,640
怎么办

107
00:03:30,680 --> 00:03:31,599
最简单的方法丢掉

108
00:03:31,599 --> 00:03:33,040
就DropLast等于2

109
00:03:33,759 --> 00:03:34,840
第二个是说我不管

110
00:03:35,360 --> 00:03:36,120
我就有多少

111
00:03:36,120 --> 00:03:37,000
返回给你多少

112
00:03:37,000 --> 00:03:38,200
就是说你可能最后一个

113
00:03:38,200 --> 00:03:39,120
Batch返回给你的

114
00:03:39,120 --> 00:03:39,840
是一个

115
00:03:40,040 --> 00:03:41,920
你本来要的是32的BatchSize

116
00:03:42,200 --> 00:03:44,280
最后一个可能给你在Batch里面

117
00:03:44,280 --> 00:03:45,439
其实是NumberOfExamples

118
00:03:45,439 --> 00:03:46,360
是等于7

119
00:03:47,200 --> 00:03:48,360
就是说它就给你一个

120
00:03:48,360 --> 00:03:49,120
残一点的

121
00:03:49,120 --> 00:03:50,000
就小一点的

122
00:03:50,680 --> 00:03:51,800
另外还有很多做法

123
00:03:51,800 --> 00:03:55,560
就是说我会去把这一个

124
00:03:55,840 --> 00:03:59,120
再去随机Sample一些

125
00:03:59,120 --> 00:04:00,439
来把它补全

126
00:04:01,319 --> 00:04:02,400
这个是做的

127
00:04:02,520 --> 00:04:04,639
但是它一般来说不补0

128
00:04:04,639 --> 00:04:06,159
因为补0没意义

129
00:04:06,159 --> 00:04:07,159
我就补0干嘛

130
00:04:07,159 --> 00:04:08,280
补0就是说

131
00:04:11,520 --> 00:04:13,319
补0就是让你浪费点计算

132
00:04:13,479 --> 00:04:14,800
就是没什么太多用

133
00:04:14,800 --> 00:04:15,920
所以不补0也没关系

134
00:04:15,960 --> 00:04:16,959
补0就是说

135
00:04:17,360 --> 00:04:18,519
你也许会觉得说

136
00:04:18,519 --> 00:04:19,439
可能性能会好一点

137
00:04:19,439 --> 00:04:20,560
但其实不一定

138
00:04:23,040 --> 00:04:23,800
好 问题6

139
00:04:23,800 --> 00:04:26,240
WaitDecay和LearningRateDecay

140
00:04:26,240 --> 00:04:27,160
作用不一样

141
00:04:27,199 --> 00:04:28,519
有什么区别吗

142
00:04:29,079 --> 00:04:29,719
不一样的

143
00:04:29,719 --> 00:04:30,800
WaitDecay就是说

144
00:04:30,800 --> 00:04:33,040
我的权重

145
00:04:33,639 --> 00:04:34,759
每一次更新的时候

146
00:04:34,759 --> 00:04:36,040
把它乘以某一个值

147
00:04:36,040 --> 00:04:39,079
比如乘一个0.999

148
00:04:39,120 --> 00:04:40,879
再加上再减去我的T度

149
00:04:42,079 --> 00:04:43,120
LearningRateDecay

150
00:04:43,720 --> 00:04:46,040
就是作用在我的学习率上面

151
00:04:46,040 --> 00:04:46,399
就是说

152
00:04:46,400 --> 00:04:47,520
我WaitDecay是

153
00:04:47,560 --> 00:04:48,840
Wait乘以

154
00:04:50,640 --> 00:04:54,600
我没有把iPad的链带上面

155
00:04:54,640 --> 00:04:55,400
所以我写不了

156
00:04:56,040 --> 00:04:57,320
所以WaitDecay

157
00:04:57,320 --> 00:04:57,640
就是说

158
00:04:57,640 --> 00:04:58,960
你的Wait更新的时候

159
00:04:59,160 --> 00:05:01,160
它就是它乘一个Decay

160
00:05:01,200 --> 00:05:02,200
LearningRateDecay

161
00:05:02,320 --> 00:05:03,040
就是说

162
00:05:03,040 --> 00:05:04,240
它不是更新的时候

163
00:05:04,240 --> 00:05:05,360
是Wait减去你的

164
00:05:05,360 --> 00:05:06,520
LearningRate乘以T度

165
00:05:06,800 --> 00:05:08,280
那就是在LearningRate上

166
00:05:08,280 --> 00:05:09,120
再乘一个Decay

167
00:05:09,120 --> 00:05:12,480
它们的作用是不一样的

168
00:05:12,480 --> 00:05:15,160
LearningRateDecay是让你收敛的

169
00:05:15,439 --> 00:05:16,640
就不要跑到最后

170
00:05:16,640 --> 00:05:18,360
你还是一个很大的学习率

171
00:05:18,360 --> 00:05:20,160
就是说你在那边抖动

172
00:05:20,200 --> 00:05:21,920
就是说学习率变低

173
00:05:21,920 --> 00:05:23,720
会让你的后面会变得更平滑

174
00:05:23,720 --> 00:05:24,920
更容易呆站

175
00:05:24,920 --> 00:05:25,840
你不要出去了

176
00:05:26,800 --> 00:05:28,080
WaitDecay就是说

177
00:05:28,080 --> 00:05:30,640
使得你是一个regularization

178
00:05:31,160 --> 00:05:34,000
使得你Wait不要特别大

179
00:05:34,000 --> 00:05:35,280
这是给模型

180
00:05:36,600 --> 00:05:39,160
所以说这是一个统计上的

181
00:05:39,160 --> 00:05:41,120
一个模型上的一个参数

182
00:05:41,160 --> 00:05:43,240
这个是优化算法上的一个参数

183
00:05:43,240 --> 00:05:44,840
这两个东西是不一样的

184
00:05:45,840 --> 00:05:48,920
统计这个东西是说统计模型

185
00:05:48,920 --> 00:05:50,280
我整个模型长什么样子

186
00:05:50,920 --> 00:05:52,360
就是说统计模型

187
00:05:52,360 --> 00:05:56,640
决定了你整个模型的厉害的程度

188
00:05:56,640 --> 00:05:58,040
能够做

189
00:05:58,720 --> 00:06:01,840
你和都怎么样子的函数

190
00:06:02,160 --> 00:06:03,560
然后它的性能怎么样

191
00:06:04,040 --> 00:06:06,120
LRDecay是你的优化算法

192
00:06:06,120 --> 00:06:07,800
是对统计模型

193
00:06:07,800 --> 00:06:10,280
给定损失情况下求解的一个方法

194
00:06:10,400 --> 00:06:12,320
因为你的统计模型是非2的

195
00:06:12,320 --> 00:06:13,080
或者什么

196
00:06:13,080 --> 00:06:14,400
你没有自由解

197
00:06:14,400 --> 00:06:15,040
没有自由解

198
00:06:15,040 --> 00:06:16,680
就是说我只能去通过一个算法

199
00:06:16,680 --> 00:06:18,000
去逼近一个值

200
00:06:18,000 --> 00:06:20,160
所以是一个求解的方法

201
00:06:21,160 --> 00:06:21,800
OK

202
00:06:22,120 --> 00:06:24,600
就是说你或者说你

203
00:06:24,640 --> 00:06:25,560
你可认为

204
00:06:26,080 --> 00:06:27,440
这个是统计模型

205
00:06:27,440 --> 00:06:29,960
给定的是你的上限

206
00:06:29,960 --> 00:06:32,200
就是说我要做一个事情

207
00:06:32,200 --> 00:06:33,400
我设一个多大的目标

208
00:06:33,400 --> 00:06:33,720
对吧

209
00:06:33,720 --> 00:06:34,800
我的vision是什么样子

210
00:06:34,800 --> 00:06:35,760
我的眼光是什么样子

211
00:06:35,760 --> 00:06:37,360
我觉得未来10年长什么样子

212
00:06:37,400 --> 00:06:38,400
我给你画一个大饼

213
00:06:38,400 --> 00:06:40,880
说这个目标能做那么大

214
00:06:41,400 --> 00:06:43,680
那么你的优化算法是你的直行

215
00:06:44,280 --> 00:06:45,680
就是我给你画一个很大的饼

216
00:06:45,680 --> 00:06:46,639
但我执行不行

217
00:06:46,759 --> 00:06:47,800
我做不出来

218
00:06:47,800 --> 00:06:50,480
我就是就执行

219
00:06:50,480 --> 00:06:51,319
就是说

220
00:06:52,120 --> 00:06:52,759
没执行好

221
00:06:52,879 --> 00:06:53,879
什么都没有

222
00:06:54,319 --> 00:06:55,199
执行的最好

223
00:06:55,319 --> 00:06:57,360
也是到你统计模型的上限

224
00:06:58,199 --> 00:06:59,079
所以就是说

225
00:06:59,079 --> 00:07:01,519
你最好是统计模型决定的上限

226
00:07:01,519 --> 00:07:04,120
最好是你做的很上限很高

227
00:07:04,120 --> 00:07:05,280
你的优化算法

228
00:07:05,439 --> 00:07:06,720
就是说你的执行

229
00:07:07,040 --> 00:07:07,639
你执行不行

230
00:07:07,720 --> 00:07:09,199
就是你的下限在这个地方了

231
00:07:09,240 --> 00:07:10,519
但你尽量要执行好

232
00:07:10,519 --> 00:07:11,600
所以尽量能去逼近

233
00:07:11,600 --> 00:07:12,960
你的统计模型的上限

234
00:07:13,200 --> 00:07:13,760
Ok

235
00:07:14,440 --> 00:07:16,520
但是它确实是两个不一样的模型

236
00:07:19,960 --> 00:07:21,360
问题7就是说讲一下

237
00:07:21,360 --> 00:07:21,840
Momentum

238
00:07:22,000 --> 00:07:22,960
我们今天不讲了

239
00:07:23,040 --> 00:07:25,480
我们会有优化

240
00:07:25,480 --> 00:07:28,800
来专门有一节会讲Momentum

241
00:07:28,960 --> 00:07:30,680
Momentum可以认为是说

242
00:07:31,640 --> 00:07:32,960
我在一个

243
00:07:34,600 --> 00:07:37,840
如果我的函数表面比较弱

244
00:07:37,840 --> 00:07:40,000
不光滑的时候

245
00:07:40,000 --> 00:07:40,880
我很容易说

246
00:07:40,880 --> 00:07:42,560
你东跑跑西跑跑

247
00:07:43,199 --> 00:07:45,639
就是沿着它凹凸不平的表面

248
00:07:45,639 --> 00:07:46,560
给你搞过去了

249
00:07:46,560 --> 00:07:48,240
Momentum更多是一个平滑

250
00:07:48,240 --> 00:07:51,439
使得你前一个方向是这么往这边走

251
00:07:51,439 --> 00:07:52,639
下一个方向这么走的话

252
00:07:52,639 --> 00:07:53,639
Momentum是说

253
00:07:53,639 --> 00:07:55,639
你不要上一个方向往这边走的时候

254
00:07:55,639 --> 00:07:57,079
你不要一下就换到下面一个方向

255
00:07:57,079 --> 00:07:57,959
还是有个冲量

256
00:07:57,959 --> 00:08:00,160
使得你慢慢的这样走过去

257
00:08:00,319 --> 00:08:00,959
这样子的话

258
00:08:00,959 --> 00:08:02,800
可以让你就是说把整个

259
00:08:03,160 --> 00:08:05,000
你可能把这个曲线拉平一点

260
00:08:05,399 --> 00:08:09,279
然后使得你不会容易被那些坑坑洼洼的地方陷进去了

261
00:08:09,840 --> 00:08:14,840
问题8什么样的Scheduler是最好的

262
00:08:14,840 --> 00:08:16,280
最优的怎么选择

263
00:08:17,800 --> 00:08:18,880
这是一个很好的问题

264
00:08:20,640 --> 00:08:23,280
LRstep或LRdecay

265
00:08:23,400 --> 00:08:26,200
就是说每个多个epoch

266
00:08:26,200 --> 00:08:27,120
然后乘一个值

267
00:08:27,120 --> 00:08:28,800
是一个很常用的一个办法

268
00:08:28,920 --> 00:08:30,600
但是它的坏处是什么

269
00:08:30,600 --> 00:08:32,720
它的坏处是说你有两个值得设

270
00:08:33,840 --> 00:08:36,879
我们现在好像用的比较多的是cos那个东西

271
00:08:37,679 --> 00:08:40,240
你一开始就是说等于是

272
00:08:41,759 --> 00:08:43,759
你沿着一个cos函数

273
00:08:43,759 --> 00:08:44,879
大家回忆一下cos函数

274
00:08:44,879 --> 00:08:46,519
就是说在0的时候是1

275
00:08:46,759 --> 00:08:48,039
慢慢的会变成0

276
00:08:48,200 --> 00:08:49,240
所以你怎么做呢

277
00:08:49,240 --> 00:08:53,759
就是说你假设你要叠代100个epoch的话

278
00:08:53,879 --> 00:08:56,080
那么你把cos函数从1变成0

279
00:08:56,080 --> 00:08:57,559
切成100下

280
00:08:57,919 --> 00:09:01,960
然后在the i值跟epoch的就乘以cos对的

281
00:09:01,960 --> 00:09:02,639
那个i值

282
00:09:03,320 --> 00:09:04,399
就是说你可以认为

283
00:09:04,720 --> 00:09:05,759
我等会给大家看一下

284
00:09:06,559 --> 00:09:08,039
就是说你有个这样子的东西

285
00:09:09,960 --> 00:09:10,720
这样子一个东西

286
00:09:11,240 --> 00:09:12,439
就是说你的

287
00:09:12,439 --> 00:09:15,159
你可以认为是说你的轴是你的

288
00:09:15,279 --> 00:09:16,279
你的时间的话

289
00:09:16,279 --> 00:09:16,879
epoch的话

290
00:09:16,879 --> 00:09:18,679
你这个是你的能力rate的值

291
00:09:18,840 --> 00:09:20,759
那么你的step就是说

292
00:09:20,759 --> 00:09:23,120
你每一次就是过了一节之后

293
00:09:23,120 --> 00:09:24,600
往下走一下

294
00:09:25,200 --> 00:09:27,039
它的一般它的作用是说

295
00:09:27,039 --> 00:09:32,039
你最好在前期保持比较大的能力rate值到

296
00:09:32,159 --> 00:09:34,319
就是说保证比较大的能力rate

297
00:09:34,319 --> 00:09:34,879
是什么意思

298
00:09:35,000 --> 00:09:37,879
就是说保证你在一个比较大的

299
00:09:37,919 --> 00:09:39,039
速度的情况下

300
00:09:39,279 --> 00:09:40,279
多看一看

301
00:09:40,439 --> 00:09:42,159
就在这各个地方都看一看

302
00:09:42,159 --> 00:09:43,919
就是说不要一开始就去

303
00:09:45,799 --> 00:09:47,720
陷到一个局限的地方去了

304
00:09:48,879 --> 00:09:51,120
有点像你说你前面等你

305
00:09:51,120 --> 00:09:52,080
当你年轻的时候

306
00:09:52,080 --> 00:09:53,279
你应该出去看一看

307
00:09:53,639 --> 00:09:56,360
所以然后后面到后期的话

308
00:09:56,360 --> 00:09:57,720
你就是你可以把它变小一点

309
00:09:57,720 --> 00:09:58,279
就是说

310
00:09:58,559 --> 00:10:00,039
假设你前面已经找到一个

311
00:10:00,039 --> 00:10:02,240
也还不错的一个大的一个山头

312
00:10:02,559 --> 00:10:03,039
山谷

313
00:10:03,279 --> 00:10:04,559
然后再用小的能力

314
00:10:04,559 --> 00:10:05,959
再去在小的地方

315
00:10:05,959 --> 00:10:08,480
再优化

316
00:10:08,559 --> 00:10:09,519
就是这个意思

317
00:10:09,679 --> 00:10:10,559
然后

318
00:10:10,799 --> 00:10:11,639
所以

319
00:10:11,959 --> 00:10:14,159
cos其实这个函数好的是

320
00:10:14,319 --> 00:10:15,719
它cos在前期

321
00:10:15,879 --> 00:10:17,919
就cos它是一个这么下的一个方向

322
00:10:17,959 --> 00:10:20,759
它前期确实是在1附近

323
00:10:20,759 --> 00:10:22,120
会很长一段时间

324
00:10:22,120 --> 00:10:23,799
它都是将近1的

325
00:10:23,839 --> 00:10:26,639
然后在后面就一下就下去了

326
00:10:26,679 --> 00:10:28,240
所以它这个函数的好处是说

327
00:10:28,240 --> 00:10:30,079
让你在用比较大的学习力

328
00:10:30,079 --> 00:10:32,000
做足够时间上的更新

329
00:10:32,080 --> 00:10:33,600
然后可以变得比较小

330
00:10:34,159 --> 00:10:34,600
OK

331
00:10:34,600 --> 00:10:37,200
这就是cos它的好处

332
00:10:37,200 --> 00:10:37,919
但反过来讲

333
00:10:37,919 --> 00:10:40,320
它也还有很多别的schedule

334
00:10:40,320 --> 00:10:41,600
大家可以去看一下

335
00:10:41,600 --> 00:10:42,279
就是说

336
00:10:42,720 --> 00:10:44,840
具体什么年代流行什么东西

337
00:10:44,840 --> 00:10:45,960
我还真说不准

338
00:10:45,960 --> 00:10:47,759
就是看看一段时间

339
00:10:47,759 --> 00:10:48,960
大家喜欢什么样子

340
00:10:48,960 --> 00:10:49,360
吧

341
00:10:50,919 --> 00:10:51,600
问你9

342
00:10:51,600 --> 00:10:52,919
为什么学习力那么小

343
00:10:54,559 --> 00:10:55,919
你可以学大一点没关系

344
00:10:56,440 --> 00:10:57,639
这个就是个demo而已

345
00:10:57,960 --> 00:11:02,360
问题是在完整数据上跑一次的时候

346
00:11:02,360 --> 00:11:03,559
参数还更新吗

347
00:11:03,559 --> 00:11:04,159
如果不行

348
00:11:04,159 --> 00:11:05,439
这一步是不是可以省略

349
00:11:05,480 --> 00:11:07,319
在完整数据上是一样的

350
00:11:07,319 --> 00:11:08,759
完整数据跑的时候

351
00:11:08,759 --> 00:11:10,960
是用那一组hyperparameter

352
00:11:10,960 --> 00:11:11,919
超参数

353
00:11:11,919 --> 00:11:13,159
就是说学习力什么样子

354
00:11:13,559 --> 00:11:15,679
怎么样做decay这样东西

355
00:11:15,679 --> 00:11:17,199
然后从random开始

356
00:11:17,199 --> 00:11:18,399
再重新跑一次

357
00:11:18,759 --> 00:11:21,720
所以它没有用前面那些讯号的东西

358
00:11:21,720 --> 00:11:22,600
是重新跑

359
00:11:22,639 --> 00:11:25,919
所以这一步你可以不用

360
00:11:25,919 --> 00:11:27,240
就是说你用前面channel模型

361
00:11:27,240 --> 00:11:27,759
直接提交

362
00:11:27,759 --> 00:11:28,960
可能也差不多

363
00:11:29,000 --> 00:11:30,000
就是说用百分之数据

364
00:11:30,000 --> 00:11:31,960
channel模型也差不多

365
00:11:34,120 --> 00:11:35,320
问题11

366
00:11:36,000 --> 00:11:38,320
就是说我们在做predict的时候

367
00:11:38,560 --> 00:11:39,919
有copy的CPU

368
00:11:39,919 --> 00:11:40,639
copy的numpy

369
00:11:40,639 --> 00:11:41,560
会造成GPU

370
00:11:41,560 --> 00:11:44,960
CPU之间数据的binding影响效率

371
00:11:45,639 --> 00:11:46,159
还好

372
00:11:46,360 --> 00:11:47,759
就是说predict很小了

373
00:11:47,759 --> 00:11:49,159
predict每一个example

374
00:11:49,159 --> 00:11:50,440
就是有10个

375
00:11:50,720 --> 00:11:51,879
就有10个类

376
00:11:52,159 --> 00:11:52,759
10个类

377
00:11:52,799 --> 00:11:55,759
其实首先prediction这个东西

378
00:11:55,759 --> 00:11:56,519
我们不那么care

379
00:11:56,720 --> 00:11:57,399
就跑一次

380
00:11:58,079 --> 00:12:00,919
其次每一个样本就10个数

381
00:12:00,919 --> 00:12:03,199
你就是5万个

382
00:12:03,199 --> 00:12:05,000
我cfash是几万

383
00:12:05,000 --> 00:12:06,319
6万还是多少个

384
00:12:06,639 --> 00:12:07,039
几万个

385
00:12:07,559 --> 00:12:08,519
几万个乘以10

386
00:12:08,519 --> 00:12:10,199
有就是10万的话

387
00:12:10,199 --> 00:12:10,960
很小了

388
00:12:10,960 --> 00:12:11,439
10万的话

389
00:12:11,439 --> 00:12:13,079
可能就几k一兆

390
00:12:13,199 --> 00:12:14,000
所以copy一下

391
00:12:14,000 --> 00:12:15,960
可能就是基本上可以忽略不计

392
00:12:16,079 --> 00:12:17,120
但它的问题是说

393
00:12:17,120 --> 00:12:18,120
你得copy很多次

394
00:12:18,279 --> 00:12:20,159
每个batch得copy

395
00:12:20,159 --> 00:12:22,639
但我觉得你可以忽略掉不计

396
00:12:24,039 --> 00:12:24,759
这点开销

397
00:12:24,759 --> 00:12:27,360
对你的真正的做influence开销

398
00:12:27,360 --> 00:12:28,840
其实是可以忽略不计的

399
00:12:31,559 --> 00:12:31,960
好

400
00:12:31,960 --> 00:12:32,720
问题12

401
00:12:32,720 --> 00:12:34,480
在比赛中用了resnet的50

402
00:12:34,480 --> 00:12:35,319
确实比red

403
00:12:36,319 --> 00:12:39,120
resnet的50效果比resnet的好

404
00:12:39,120 --> 00:12:39,919
50好

405
00:12:39,919 --> 00:12:40,559
resnet50

406
00:12:40,559 --> 00:12:41,720
我们之前有提过

407
00:12:42,039 --> 00:12:44,759
其实我们之前做了一个论文

408
00:12:46,720 --> 00:12:48,120
这个问题有点超纲了

409
00:12:48,360 --> 00:12:50,759
我们就不在这里特别

410
00:12:54,759 --> 00:12:59,319
这个问题有点超纲

411
00:12:59,319 --> 00:13:00,840
因为我们没有讲过

412
00:13:00,840 --> 00:13:03,399
我们都还没有叫tension这个东西

413
00:13:04,720 --> 00:13:07,039
所以我们就不再去回答这个问题

414
00:13:07,039 --> 00:13:07,799
反过来讲

415
00:13:08,120 --> 00:13:10,000
这个同学就是说

416
00:13:11,080 --> 00:13:12,720
其实说一句真话

417
00:13:13,799 --> 00:13:15,720
我也很难解释这个东西

418
00:13:16,559 --> 00:13:18,559
这个东西很多时候怎么说

419
00:13:20,559 --> 00:13:23,279
现在卷积生就网络做到这个程度

420
00:13:24,240 --> 00:13:25,199
东条条西条条

421
00:13:25,199 --> 00:13:26,279
就是做的越来越复杂

422
00:13:26,279 --> 00:13:27,399
resnet的技术上

423
00:13:27,720 --> 00:13:29,039
做的越来越复杂

424
00:13:30,039 --> 00:13:31,559
我们当然是说我们有解释

425
00:13:31,559 --> 00:13:32,159
这是为什么

426
00:13:32,159 --> 00:13:33,480
但实际上说一真话

427
00:13:33,480 --> 00:13:34,000
不知道

428
00:13:34,600 --> 00:13:35,199
试了一下

429
00:13:35,199 --> 00:13:37,199
发现自己有一点直觉

430
00:13:37,199 --> 00:13:37,879
然后试了一下

431
00:13:37,879 --> 00:13:38,799
效果还可以

432
00:13:38,839 --> 00:13:40,159
然后是不是

433
00:13:40,639 --> 00:13:41,679
效果好的原因

434
00:13:41,679 --> 00:13:42,639
跟你的职业相关

435
00:13:42,639 --> 00:13:43,399
这个是解释

436
00:13:43,399 --> 00:13:45,079
这个是不知道的

437
00:13:45,120 --> 00:13:45,759
其实

438
00:13:47,919 --> 00:13:50,319
用cos的学习率怎么设置

439
00:13:50,440 --> 00:13:53,520
一般就是说

440
00:13:53,520 --> 00:13:54,520
你也不用太设置

441
00:13:54,520 --> 00:13:56,280
就是说你一般说我可以

442
00:13:58,200 --> 00:13:59,200
最简单就不用设

443
00:13:59,760 --> 00:14:01,320
一个cos还是要过去了

444
00:14:01,320 --> 00:14:02,879
就是说没关系

445
00:14:03,200 --> 00:14:04,760
当然你可以稍微好一点

446
00:14:04,760 --> 00:14:06,840
就是说我说我一个

447
00:14:07,120 --> 00:14:08,320
我不要最后降到0

448
00:14:08,360 --> 00:14:11,240
我加到一个0.1-3

449
00:14:11,240 --> 00:14:13,640
就是说我把cos整个值

450
00:14:14,000 --> 00:14:16,920
最后不要真的到0那边

451
00:14:17,240 --> 00:14:19,000
比如到一个小的值就结束了

452
00:14:19,039 --> 00:14:20,120
可能是可以做一点

453
00:14:20,120 --> 00:14:21,799
这样子的优化

454
00:14:22,919 --> 00:14:24,200
问题是4

455
00:14:24,240 --> 00:14:25,240
目标检测

456
00:14:25,240 --> 00:14:26,639
只要识别一个物体就可以

457
00:14:26,639 --> 00:14:27,799
但是制作数据的时候

458
00:14:27,799 --> 00:14:28,759
多标类

459
00:14:28,759 --> 00:14:30,000
其他类是不是要模型范围

460
00:14:30,000 --> 00:14:31,120
就更强

461
00:14:31,559 --> 00:14:33,080
我们可能在数据

462
00:14:33,080 --> 00:14:34,279
我们在目标检测的时候

463
00:14:34,279 --> 00:14:35,440
再给大家讲

464
00:14:36,000 --> 00:14:37,840
就是说我们还没有讲过

465
00:14:37,840 --> 00:14:39,159
多标点其他类

466
00:14:39,159 --> 00:14:42,120
是不是让模型范围更强

467
00:14:42,159 --> 00:14:46,759
其实我觉得你多标一点类

468
00:14:46,759 --> 00:14:49,720
就是说不一定让你更强

469
00:14:49,919 --> 00:14:51,519
当然是说可能有一定好处

470
00:14:51,519 --> 00:14:52,919
但是你多标数据

471
00:14:52,919 --> 00:14:54,120
有你的开销

472
00:14:54,120 --> 00:14:55,200
看你划不划得来

473
00:14:55,200 --> 00:14:58,319
可能你多采点样本

474
00:14:58,319 --> 00:14:59,279
可能更好一点

475
00:15:00,919 --> 00:15:01,240
好

476
00:15:01,240 --> 00:15:02,799
在数据模型的时候

477
00:15:02,799 --> 00:15:04,600
训练跑了1000次左右

478
00:15:04,600 --> 00:15:06,159
精度在0.7就不动了

479
00:15:06,840 --> 00:15:07,960
你跑了1000次

480
00:15:07,960 --> 00:15:09,240
什么1000个epoch

481
00:15:09,360 --> 00:15:11,159
你扫数据扫了1000次吗

482
00:15:11,200 --> 00:15:12,240
还是你跑了1000次

483
00:15:12,240 --> 00:15:12,679
跌大

484
00:15:13,360 --> 00:15:15,039
如果你跑了1000个epoch

485
00:15:15,120 --> 00:15:16,080
你也是挺有钱的

486
00:15:17,120 --> 00:15:17,799
这东西跑起来

487
00:15:17,799 --> 00:15:19,439
可能挺烧钱的

488
00:15:20,879 --> 00:15:23,000
精度在0.7左右就不动了

489
00:15:23,120 --> 00:15:27,159
可能LRDK会有一定的好处

490
00:15:27,159 --> 00:15:27,720
确实有

491
00:15:27,720 --> 00:15:28,439
但是

492
00:15:31,120 --> 00:15:32,200
但是你在0.7

493
00:15:32,200 --> 00:15:34,080
我觉得你可能是别的地方

494
00:15:34,120 --> 00:15:35,000
做得不那么好

495
00:15:35,399 --> 00:15:39,399
LRDK不会让你从0.7变到0.99

496
00:15:39,399 --> 00:15:42,000
LRDK可以让你从0.7变到0.75

497
00:15:42,000 --> 00:15:43,759
或者0.8变到

498
00:15:44,039 --> 00:15:45,480
我不是说0.7到0.75

499
00:15:45,600 --> 00:15:46,840
0.7变到0.8

500
00:15:47,200 --> 00:15:49,080
或者0.9变到0.95

501
00:15:49,080 --> 00:15:50,000
是有可能的

502
00:15:50,039 --> 00:15:52,440
但是它不会从0.7变到0.99

503
00:15:55,320 --> 00:15:57,240
LRDK是不是造成了

504
00:15:57,240 --> 00:15:58,519
weight不要更新太多

505
00:15:58,519 --> 00:16:01,200
能引起类似的weight decay的效果吗

506
00:16:03,320 --> 00:16:04,639
会有一点点

507
00:16:04,800 --> 00:16:05,800
当然是说

508
00:16:05,920 --> 00:16:07,279
就是说你最后的模型

509
00:16:07,279 --> 00:16:08,840
是你的模型

510
00:16:08,879 --> 00:16:10,320
是你的统计模型

511
00:16:10,320 --> 00:16:11,600
加你的优化模型的

512
00:16:11,600 --> 00:16:13,080
共同作用的结果

513
00:16:13,400 --> 00:16:15,560
确实你说我把学习率变低

514
00:16:15,560 --> 00:16:15,920
这样子

515
00:16:15,920 --> 00:16:17,040
每次更新变少

516
00:16:17,040 --> 00:16:18,000
确实会赢

517
00:16:18,000 --> 00:16:19,280
跟你weight decay类似

518
00:16:19,280 --> 00:16:19,759
有点像

519
00:16:19,759 --> 00:16:21,440
就是说限制了我的模型

520
00:16:22,000 --> 00:16:23,480
在同样的迭代情况下

521
00:16:23,480 --> 00:16:24,960
不要跑太远

522
00:16:25,000 --> 00:16:26,440
等价于是我用一个小的学

523
00:16:26,440 --> 00:16:27,840
在后期用一个小的学习率

524
00:16:28,840 --> 00:16:29,639
但是反过来讲

525
00:16:29,639 --> 00:16:31,840
我们通常是还是会区分

526
00:16:33,879 --> 00:16:35,759
会区分这样子的一个效果

527
00:16:35,759 --> 00:16:39,520
因为你的weight decay

528
00:16:39,520 --> 00:16:41,680
是能够换上到你的统计模型

529
00:16:41,720 --> 00:16:43,400
加一个regularization在里面的

530
00:16:43,440 --> 00:16:44,680
LRDK

531
00:16:44,720 --> 00:16:47,680
更多是从优化算法角度来考虑

532
00:16:47,720 --> 00:16:49,840
所以还是两个不一样的途径

533
00:16:50,360 --> 00:16:51,320
确实你反过来讲

534
00:16:51,320 --> 00:16:53,640
是能有类似的效果

535
00:16:54,040 --> 00:16:54,880
所以是说

536
00:16:54,880 --> 00:16:56,560
其实你最后是统计模型

537
00:16:56,560 --> 00:16:57,720
加上优化模型

538
00:16:57,720 --> 00:16:59,000
共同作用的结果

539
00:16:59,240 --> 00:17:00,640
优化模型说一真话

540
00:17:00,640 --> 00:17:01,640
SGD

541
00:17:02,240 --> 00:17:03,560
大家为什么用SGD

542
00:17:03,840 --> 00:17:05,920
SGD其实在很多程度上

543
00:17:05,920 --> 00:17:07,519
会给你做了regularization

544
00:17:08,240 --> 00:17:10,440
SGD它其实是说

545
00:17:11,160 --> 00:17:12,920
它就是说不同的优化算法

546
00:17:12,920 --> 00:17:14,120
SGD是一块

547
00:17:14,120 --> 00:17:15,120
还有很多别的

548
00:17:16,039 --> 00:17:17,720
SGD其实给你做了一个

549
00:17:17,720 --> 00:17:19,360
很强的regularization

550
00:17:19,360 --> 00:17:20,120
就是说这一块

551
00:17:20,160 --> 00:17:21,880
大家有很多论文说

552
00:17:21,920 --> 00:17:23,160
为什么神经网络

553
00:17:23,160 --> 00:17:25,240
我用什么LBFGS效果

554
00:17:25,240 --> 00:17:26,960
其实没有SGD那么好

555
00:17:27,799 --> 00:17:29,240
是因为SGD本身给你

556
00:17:29,240 --> 00:17:29,960
确实做了一个

557
00:17:29,960 --> 00:17:31,559
很强的regularization在里面

558
00:17:31,600 --> 00:17:33,160
它找到的pass

559
00:17:33,160 --> 00:17:33,799
就是说

560
00:17:33,840 --> 00:17:35,360
最后这是一个很复杂的地方

561
00:17:35,360 --> 00:17:39,000
大家去找解不同的优化算法

562
00:17:39,039 --> 00:17:40,599
不同的找解的方法

563
00:17:40,599 --> 00:17:42,200
SGD相对来说比较稳定

564
00:17:42,200 --> 00:17:44,079
因为它有很多噪音在里面

