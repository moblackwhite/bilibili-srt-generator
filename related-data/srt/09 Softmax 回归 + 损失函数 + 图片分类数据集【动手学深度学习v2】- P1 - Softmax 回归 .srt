1
00:00:00,000 --> 00:00:02,000
Softmax回归

2
00:00:02,000 --> 00:00:04,919
它是继续学习另外一个非常经典

3
00:00:04,919 --> 00:00:06,200
而且重要的模型

4
00:00:06,799 --> 00:00:08,599
虽然它的名字叫做回归

5
00:00:08,640 --> 00:00:10,800
但是它其实是一个分类问题

6
00:00:12,120 --> 00:00:15,400
首先我们来解释一下分类和回归的区别

7
00:00:15,720 --> 00:00:18,240
我们之前有学习到线性回归

8
00:00:18,480 --> 00:00:21,559
回归里面我们是估计一个连续值

9
00:00:21,719 --> 00:00:24,039
比如说一个房子它的卖的价格

10
00:00:24,560 --> 00:00:26,000
对分类问题来说

11
00:00:26,039 --> 00:00:28,400
它是预测一个连续的类别

12
00:00:28,560 --> 00:00:30,680
比如说我预测一个图片里面

13
00:00:30,680 --> 00:00:32,200
到底是猫还是狗

14
00:00:32,840 --> 00:00:34,240
我们举几个例子

15
00:00:35,160 --> 00:00:37,640
首先第一个是一个非常经典的数据集

16
00:00:37,679 --> 00:00:38,640
叫做M-List

17
00:00:39,240 --> 00:00:42,280
它是用来识别一个图片里面它的数字

18
00:00:42,320 --> 00:00:45,120
是首写在012还是到7896

19
00:00:46,600 --> 00:00:48,400
那么它就是一个十类问题

20
00:00:48,439 --> 00:00:50,320
因为我们要识别10个数字

21
00:00:51,400 --> 00:00:53,280
另外一个数据集是一个

22
00:00:54,079 --> 00:00:56,320
深度学习里面特别经典的数据集

23
00:00:56,320 --> 00:00:58,240
它也是引起了我们一系列

24
00:00:58,240 --> 00:01:00,719
深度学习突破的一个数据集

25
00:01:00,799 --> 00:01:01,880
叫做ImageNet

26
00:01:02,520 --> 00:01:04,040
它有一百万张图片

27
00:01:04,519 --> 00:01:06,799
每一张图片里面是一个物体

28
00:01:06,840 --> 00:01:07,799
自然物体

29
00:01:08,079 --> 00:01:10,640
它是一个1000类的自然物体之一

30
00:01:10,680 --> 00:01:13,439
比如说里面有大概有一百种不同的狗

31
00:01:13,960 --> 00:01:15,840
那么这个问题就是一个

32
00:01:15,879 --> 00:01:17,640
1000类的分类问题

33
00:01:20,599 --> 00:01:22,400
接下来我们看几个

34
00:01:22,760 --> 00:01:24,159
比如说Kaggle上的

35
00:01:24,200 --> 00:01:26,039
非常典型的分类问题

36
00:01:27,120 --> 00:01:28,240
第一个是说

37
00:01:28,280 --> 00:01:31,440
我们将人类蛋白质的显微镜图片

38
00:01:31,440 --> 00:01:32,480
来进行分类

39
00:01:32,960 --> 00:01:34,600
我们这里有28类

40
00:01:34,640 --> 00:01:36,280
就是对应不同的蛋白质

41
00:01:36,440 --> 00:01:38,200
然后根据我的显微镜图片

42
00:01:38,200 --> 00:01:41,200
来分出这到底是哪一种蛋白质

43
00:01:43,760 --> 00:01:45,240
另外一个问题是说

44
00:01:45,600 --> 00:01:48,000
假设我能知道一个软件的运行

45
00:01:48,000 --> 00:01:49,640
和一些软件的信息

46
00:01:49,880 --> 00:01:51,480
那么我们来判断

47
00:01:51,480 --> 00:01:53,440
它是不是一种恶意软件

48
00:01:53,480 --> 00:01:54,760
以及如果是的话

49
00:01:54,760 --> 00:01:56,920
它属于哪种恶意软件

50
00:02:00,160 --> 00:02:02,040
还有一个经典的问题是说

51
00:02:02,040 --> 00:02:03,760
对文字进行分类

52
00:02:04,000 --> 00:02:05,439
比如说我们把所有的

53
00:02:05,439 --> 00:02:07,120
Wikipedia的评论拿出来

54
00:02:07,240 --> 00:02:12,400
然后来看它是不是比较不雅的评论

55
00:02:12,480 --> 00:02:14,759
而且这里面有几种不一样的

56
00:02:14,800 --> 00:02:17,159
恶意的一个区别

57
00:02:17,200 --> 00:02:18,960
就是说一共有7个类别

58
00:02:21,759 --> 00:02:23,319
所以我们可以看到是说

59
00:02:23,319 --> 00:02:25,959
回归和分类有非常多的相似性

60
00:02:25,959 --> 00:02:27,159
但是它又不一样

61
00:02:27,319 --> 00:02:28,599
对回归来讲

62
00:02:28,639 --> 00:02:31,560
它就是一个单连续数值的输出

63
00:02:31,959 --> 00:02:34,319
它的输出的区间是一个自然区间

64
00:02:34,319 --> 00:02:36,120
比如说房价预测

65
00:02:36,120 --> 00:02:37,919
当然它是你得大于等于0

66
00:02:38,239 --> 00:02:40,439
但是它实际上值可以不同类别

67
00:02:41,039 --> 00:02:43,199
然后我们做损失函数的时候

68
00:02:43,240 --> 00:02:45,919
我们会跟真实值的区别作为损失

69
00:02:45,919 --> 00:02:49,079
这是我们的预测值减去真实值的平方

70
00:02:49,319 --> 00:02:50,719
就我们之前看到的

71
00:02:51,319 --> 00:02:52,680
对于分类而言

72
00:02:52,680 --> 00:02:54,439
它通常要多个输出

73
00:02:54,640 --> 00:02:56,439
我们这里举了个例子

74
00:02:57,560 --> 00:03:00,760
假设我们要做三个类别的分类的话

75
00:03:01,000 --> 00:03:02,920
我们的输出不再是一个O1

76
00:03:02,960 --> 00:03:04,840
而是一个O2 O3

77
00:03:05,400 --> 00:03:08,719
其中输出的DI元素

78
00:03:08,719 --> 00:03:11,719
是用来预测DI类的一个执行度

79
00:03:12,520 --> 00:03:14,520
那么我们可以简单的认为是说

80
00:03:14,560 --> 00:03:15,560
分类问题

81
00:03:15,800 --> 00:03:18,719
从回归的单输出变成了多输出

82
00:03:19,040 --> 00:03:21,400
输出的个数是等于类别的个数

83
00:03:23,400 --> 00:03:24,560
OK

84
00:03:28,080 --> 00:03:29,159
那么我们来看一下

85
00:03:29,400 --> 00:03:32,760
从回归怎么样来过渡到一个分类的问题

86
00:03:34,400 --> 00:03:36,879
首先我们要对类别进行编码

87
00:03:37,240 --> 00:03:38,640
因为类别是一个

88
00:03:40,000 --> 00:03:40,879
不是一个数

89
00:03:40,920 --> 00:03:43,240
它可能是一个比如说一个类别

90
00:03:43,240 --> 00:03:45,200
比如说猫狗是一个字符串

91
00:03:46,040 --> 00:03:48,000
假设我们有n个类别的话

92
00:03:48,040 --> 00:03:50,719
我们可以用最简单的一位有效编码

93
00:03:50,719 --> 00:03:51,680
来进行编码

94
00:03:52,439 --> 00:03:53,719
假设我们n个类别

95
00:03:54,120 --> 00:03:56,159
那么我们的标号

96
00:03:56,200 --> 00:03:57,760
就是一个常为n的向量

97
00:03:57,800 --> 00:03:59,360
从Y1 Y2

98
00:03:59,560 --> 00:04:00,599
一直到Yn

99
00:04:01,120 --> 00:04:03,879
其中假设我的真实的类别是

100
00:04:03,879 --> 00:04:05,319
DI个的话

101
00:04:05,439 --> 00:04:07,640
那么我的Yi就是等于1

102
00:04:07,879 --> 00:04:09,760
其他的元素全部等于0

103
00:04:10,360 --> 00:04:12,000
就是说我们这个向量里面

104
00:04:12,319 --> 00:04:14,240
刚好有一个元素是为1

105
00:04:14,319 --> 00:04:17,399
这个元素它的下标对应的是DI个类别

106
00:04:17,800 --> 00:04:19,560
其他的元素全部为0

107
00:04:19,840 --> 00:04:21,480
所以就是说一位有效

108
00:04:21,800 --> 00:04:23,439
刚好有一个位置是有效的

109
00:04:24,720 --> 00:04:27,640
当然我们有了编码之后

110
00:04:27,640 --> 00:04:30,680
我们其实可以用最简单的回归问题的

111
00:04:30,720 --> 00:04:32,160
军方损失来训练

112
00:04:33,160 --> 00:04:35,200
我们就可以在不改动的情况下

113
00:04:35,800 --> 00:04:39,280
然后我们假设我们有训练出来了

114
00:04:39,280 --> 00:04:40,040
一个模型

115
00:04:40,080 --> 00:04:41,520
我们做预测的时候

116
00:04:41,560 --> 00:04:45,759
那么我们选取I使得最大化OI的

117
00:04:46,240 --> 00:04:48,240
知识度的值作为我的预测

118
00:04:48,759 --> 00:04:51,319
我们ArcMapOI

119
00:04:51,720 --> 00:04:54,439
的I就是我们的预测的一个标号

120
00:04:54,439 --> 00:04:55,640
就说Y hat

121
00:04:57,000 --> 00:04:57,600
OK

122
00:04:58,360 --> 00:05:00,200
接下来我们看一下是说

123
00:05:00,720 --> 00:05:04,920
怎么再一步一步从这一个基础点过渡到

124
00:05:04,920 --> 00:05:07,560
我们真正使用的softmax回归

125
00:05:09,000 --> 00:05:10,560
其中一个问题是说

126
00:05:10,600 --> 00:05:12,160
我们在分类来讲

127
00:05:12,160 --> 00:05:14,280
我们其实不关心

128
00:05:15,240 --> 00:05:17,759
他们之间的实际的值

129
00:05:18,120 --> 00:05:19,640
我们关心的是说

130
00:05:20,039 --> 00:05:24,759
我是不是能够对正确类别的知性度特别大

131
00:05:25,759 --> 00:05:28,719
就是说我们可以将我们的目原函数改成

132
00:05:28,759 --> 00:05:32,599
我们需要使得我们对正确类Y它的

133
00:05:32,599 --> 00:05:34,479
知性度就是OY

134
00:05:34,919 --> 00:05:40,079
要远远的大于其他非正确类的一些OI

135
00:05:40,639 --> 00:05:42,159
那写成数学就是说

136
00:05:42,159 --> 00:05:45,360
我们希望使得OY减去OI要大于

137
00:05:45,360 --> 00:05:46,399
某一个阈值

138
00:05:46,439 --> 00:05:47,479
比如说一个delta

139
00:05:48,480 --> 00:05:50,160
这样子能保证是说

140
00:05:50,160 --> 00:05:53,280
我的模型真正的能够将我的真正的类

141
00:05:53,280 --> 00:05:55,640
和不一样的类能够拉开距离

142
00:05:56,439 --> 00:05:59,319
这是我们最简单的一个想法

143
00:06:00,160 --> 00:06:01,920
另外我们一个想法是说

144
00:06:01,960 --> 00:06:07,879
虽然我们这里没有说你具体OY要什么样的值

145
00:06:07,879 --> 00:06:09,640
面积大一点小一点都没关系

146
00:06:09,640 --> 00:06:11,439
我们关心的是一个相对值

147
00:06:11,640 --> 00:06:14,800
但是如果我们把值放到一个合适的区间

148
00:06:14,800 --> 00:06:17,600
也会让我们后面的变得更加简单

149
00:06:18,560 --> 00:06:25,120
比如说我们希望使得我们的输出能够是一个概率

150
00:06:27,639 --> 00:06:30,600
现在我们的输出是OE到一直到ON

151
00:06:30,639 --> 00:06:32,879
就是一个O的一个向量

152
00:06:33,199 --> 00:06:35,000
那么我们怎么做这个事情呢

153
00:06:35,080 --> 00:06:37,920
我们可以引入一个新的操作子

154
00:06:37,960 --> 00:06:39,120
叫做softmax

155
00:06:39,720 --> 00:06:43,960
我们将softmax作用在O上面得到一个Y hat

156
00:06:44,480 --> 00:06:46,480
它是一个常为N的向量

157
00:06:46,680 --> 00:06:49,400
但是它有我们叫的属性

158
00:06:49,400 --> 00:06:52,040
就是说它每个元素都非负

159
00:06:52,080 --> 00:06:53,440
而且它的和为一

160
00:06:54,440 --> 00:06:56,200
具体我们的操作是说

161
00:06:56,240 --> 00:06:59,320
Y hat里面的I第二个元素

162
00:06:59,440 --> 00:07:02,320
它是等于O里面的第二个元素

163
00:07:02,480 --> 00:07:03,480
做指数

164
00:07:04,000 --> 00:07:05,320
指数的好处是说

165
00:07:05,360 --> 00:07:06,480
我不管是什么值

166
00:07:06,480 --> 00:07:08,000
我都能把它变成非负

167
00:07:08,680 --> 00:07:13,000
然后我们再除以所有的OK它的做指数

168
00:07:13,800 --> 00:07:14,600
这样子的话

169
00:07:14,600 --> 00:07:17,120
使得我们对于Y hat I

170
00:07:17,120 --> 00:07:18,759
对于所有的I加起来

171
00:07:18,759 --> 00:07:19,720
它的和为一

172
00:07:19,759 --> 00:07:21,199
因为我们的分母一样

173
00:07:21,399 --> 00:07:23,279
分子加起来刚好等于分母

174
00:07:24,199 --> 00:07:25,480
这样的好处是说

175
00:07:25,519 --> 00:07:28,199
我们的Y hat它其实就是一个概率了

176
00:07:29,240 --> 00:07:30,279
回忆一下

177
00:07:30,279 --> 00:07:32,079
我们对于真实标号的Y

178
00:07:32,560 --> 00:07:34,000
也是做成一个概率

179
00:07:34,040 --> 00:07:36,240
因为它刚好只有一个元素为一

180
00:07:36,240 --> 00:07:38,040
剩下的元素全部为零

181
00:07:38,160 --> 00:07:41,600
任何满足为非负

182
00:07:41,600 --> 00:07:43,800
而且和为一都可以当做一个概率

183
00:07:44,120 --> 00:07:45,879
那么我们就得到两个概率

184
00:07:45,920 --> 00:07:47,520
一个是真实的Y的概率

185
00:07:47,560 --> 00:07:48,800
一个是预测的概率

186
00:07:49,640 --> 00:07:52,000
我们可以来比较两个概率之间的区别

187
00:07:52,000 --> 00:07:52,960
作为损失

188
00:07:54,760 --> 00:07:57,800
一般来说我们使用交叉商

189
00:07:57,800 --> 00:07:59,000
就是cross entropy

190
00:07:59,000 --> 00:08:00,760
来衡量两个概率的区别

191
00:08:01,320 --> 00:08:02,879
我们假设有两个概率

192
00:08:02,879 --> 00:08:04,560
H P和Q

193
00:08:04,840 --> 00:08:08,080
我们用H P Q来计算交叉商

194
00:08:08,320 --> 00:08:10,160
它的公式是可以认为是

195
00:08:10,280 --> 00:08:13,040
假设P和Q是一个离散概率

196
00:08:13,080 --> 00:08:14,439
它有N个元素的话

197
00:08:14,720 --> 00:08:17,720
我们对每个元素I来

198
00:08:18,280 --> 00:08:21,320
负的P I乘以logQI

199
00:08:21,360 --> 00:08:23,840
然后求和来计算它的交叉商

200
00:08:25,520 --> 00:08:27,640
如果我们把它当做损失的话

201
00:08:27,680 --> 00:08:30,160
那么就是说对于真实标号Y

202
00:08:30,200 --> 00:08:31,960
和预测的标号Y hat

203
00:08:32,120 --> 00:08:34,680
它的损失就L Y和Y hat

204
00:08:34,920 --> 00:08:38,560
它就等于负的对所有的I类别求和

205
00:08:38,720 --> 00:08:41,480
YI乘以logY hatI

206
00:08:43,120 --> 00:08:45,000
回忆一下我们的YI里面

207
00:08:45,000 --> 00:08:47,080
其实就刚好有一个元素为1

208
00:08:47,080 --> 00:08:48,640
剩下的全部为0

209
00:08:48,880 --> 00:08:51,400
那么我们这个求和可以简写成说

210
00:08:51,440 --> 00:08:52,840
就是负的log

211
00:08:53,320 --> 00:08:55,240
对于真实类别Y

212
00:08:55,280 --> 00:08:56,880
它的预测的Y hat

213
00:08:57,600 --> 00:09:00,760
就是对于真实类别我的预测值求log

214
00:09:00,800 --> 00:09:01,680
然后求负数

215
00:09:02,880 --> 00:09:04,560
这就是可以看到是说

216
00:09:04,600 --> 00:09:06,080
对分类问题来讲

217
00:09:06,080 --> 00:09:10,040
我们不关心对于非正确类的一个预测值

218
00:09:10,080 --> 00:09:13,120
我们只关心我们对于正确类的预测值

219
00:09:13,120 --> 00:09:15,200
它要知行度要够大

220
00:09:16,480 --> 00:09:19,440
回忆一下我们的之前讲过的T度

221
00:09:19,879 --> 00:09:21,879
我们损失的T度

222
00:09:21,879 --> 00:09:24,920
它其实就是真实概率和预测概率的区别

223
00:09:25,240 --> 00:09:29,080
比如说损失对DOI进行求导的话

224
00:09:29,240 --> 00:09:31,440
那么它就等于是softmaxO的

225
00:09:31,440 --> 00:09:32,400
第2个元素

226
00:09:32,520 --> 00:09:34,360
减去真实类别Y

227
00:09:34,879 --> 00:09:37,200
那么我们知道T度下降的时候

228
00:09:37,200 --> 00:09:40,720
我们就是不断的做我们的负的导数

229
00:09:40,759 --> 00:09:43,759
那么就是说我们不断的去减去它们之间的区别

230
00:09:43,800 --> 00:09:48,039
最后使得我们预测的softmaxO

231
00:09:48,039 --> 00:09:50,000
和我们真实的Y更相近

232
00:09:50,919 --> 00:09:51,480
好

233
00:09:52,039 --> 00:09:54,240
这样子我们就简单介绍了

234
00:09:54,560 --> 00:09:56,200
什么是softmax回归

235
00:09:56,519 --> 00:09:58,800
首先它虽然叫做回归

236
00:09:58,840 --> 00:10:00,759
它是一个多类的分类问题

237
00:10:02,039 --> 00:10:03,920
然后它的实际操作是说

238
00:10:03,919 --> 00:10:05,679
假设我们n个类别的话

239
00:10:05,719 --> 00:10:07,159
我们使用n个输出

240
00:10:07,559 --> 00:10:09,879
然后使用softmax操作值

241
00:10:10,279 --> 00:10:12,639
使得每个类的知行度

242
00:10:12,639 --> 00:10:13,719
它是一个概率

243
00:10:13,719 --> 00:10:15,079
就是它是非负的

244
00:10:15,159 --> 00:10:16,879
而且它的加起来和等于1

245
00:10:17,759 --> 00:10:19,799
最后我们使用一个交叉商

246
00:10:19,799 --> 00:10:22,919
来衡量我们的预测和标号的区别

247
00:10:22,919 --> 00:10:24,199
作为损失函数

248
00:10:24,599 --> 00:10:24,879
好

249
00:10:24,879 --> 00:10:26,719
这就是我们的softmax回归

