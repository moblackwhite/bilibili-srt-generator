1
00:00:00,000 --> 00:00:04,440
现行回归是机器学习最基础的一个模型

2
00:00:04,440 --> 00:00:08,839
也是我们理解之后所有深度学习模型的基础

3
00:00:08,839 --> 00:00:11,400
所以我们从现行回归开始

4
00:00:12,640 --> 00:00:16,240
在开始之前我们先来讲一个应用

5
00:00:16,879 --> 00:00:19,640
这个应用是说如何在美国买房

6
00:00:20,640 --> 00:00:24,160
在美国买房跟在任何地方买房都是一样

7
00:00:24,160 --> 00:00:26,160
就是说我们先得去看一下那个房

8
00:00:26,480 --> 00:00:30,399
了解一下房子的照片各种信息

9
00:00:31,320 --> 00:00:34,399
它可能跟很多地方不一样的地方在于是说

10
00:00:34,759 --> 00:00:36,399
我看中一个房之后

11
00:00:37,200 --> 00:00:39,880
我要买它的话我是需要竞价

12
00:00:40,439 --> 00:00:41,840
就是说举个例子

13
00:00:43,159 --> 00:00:44,039
这里有一个房

14
00:00:44,039 --> 00:00:46,320
这边显示它的基本信息是说

15
00:00:46,439 --> 00:00:50,280
这个房的出价售价是说500

16
00:00:50,280 --> 00:00:51,840
大概550万美金

17
00:00:52,240 --> 00:00:56,079
它有7个卧室5个卫生间

18
00:00:56,400 --> 00:01:00,320
它的居住面积是4865sqft

19
00:01:00,320 --> 00:01:03,160
整合大概是460平米的样子

20
00:01:04,200 --> 00:01:07,880
但是注意到这个价格它不是你的成交价

21
00:01:07,880 --> 00:01:12,120
它是说我的卖房基金人它的列价

22
00:01:12,640 --> 00:01:14,840
下面你可以看到是有一个redfin

23
00:01:14,840 --> 00:01:18,080
就是一个美国一个比较有名的卖房的网站

24
00:01:18,280 --> 00:01:22,280
它对这个房子的估价是大概是540万的样子

25
00:01:23,159 --> 00:01:25,800
但这两个价格都是给你做参考用的

26
00:01:26,159 --> 00:01:29,000
你最终是说你得去出一个价格

27
00:01:29,159 --> 00:01:31,679
然后再给定一个时间窗里面

28
00:01:31,679 --> 00:01:33,679
很多买家都去出价

29
00:01:33,879 --> 00:01:37,959
最后价高的那个人得到这个房子

30
00:01:39,000 --> 00:01:41,000
所以这是一个预测问题

31
00:01:41,039 --> 00:01:44,039
就是说你得根据现在的行情的预测说

32
00:01:44,039 --> 00:01:46,439
你大概多少钱能买下这个房子

33
00:01:47,599 --> 00:01:48,359
好

34
00:01:48,359 --> 00:01:51,199
所以可以看到是说我给你的是标价

35
00:01:51,199 --> 00:01:52,560
给你的是预计价格

36
00:01:52,560 --> 00:01:55,400
然后我们要出一个价

37
00:01:56,359 --> 00:02:00,679
这里是说你的出价真的很重要

38
00:02:00,679 --> 00:02:01,920
因为是真正的钱

39
00:02:01,920 --> 00:02:04,399
我们再来看两套房子

40
00:02:04,599 --> 00:02:07,000
然后你的x轴是你的年份

41
00:02:07,120 --> 00:02:12,280
然后这个曲线是说根据历史信息

42
00:02:12,280 --> 00:02:15,479
你周围的房子系统对你的评估价格

43
00:02:15,719 --> 00:02:17,719
然后做一个dollar符号是说

44
00:02:17,759 --> 00:02:21,439
你在此你在什么时候出了多少钱

45
00:02:21,439 --> 00:02:22,919
买下了这个房子

46
00:02:23,519 --> 00:02:24,560
所以你的最好的

47
00:02:24,560 --> 00:02:26,000
当然是说你这出的价格

48
00:02:26,000 --> 00:02:28,080
是远低于你的系统的估价

49
00:02:28,080 --> 00:02:29,560
就是说那你就赚到了

50
00:02:30,080 --> 00:02:31,159
这个房子还可以

51
00:02:31,159 --> 00:02:35,680
就是说它的估价跟出价和估价是差不多的

52
00:02:36,120 --> 00:02:38,960
然后这个其实就是我了

53
00:02:39,199 --> 00:02:40,400
就是我们第一次买房

54
00:02:40,400 --> 00:02:41,920
不懂那么多规矩

55
00:02:42,199 --> 00:02:44,080
我们就多出了10万块钱

56
00:02:44,080 --> 00:02:45,439
所以看到是说

57
00:02:46,240 --> 00:02:48,599
这个是在一年之后

58
00:02:48,599 --> 00:02:50,159
系统对这个房子的评估

59
00:02:50,159 --> 00:02:53,159
仍然比我们的出价还要低一些

60
00:02:53,159 --> 00:02:56,039
所以这个大概是10万美金的差价

61
00:02:56,240 --> 00:02:57,840
所以从此以后

62
00:02:57,840 --> 00:03:01,519
我们就开始关心房价预测这个问题

63
00:03:01,639 --> 00:03:06,039
所以也是我们把它作为一个机器学习

64
00:03:06,039 --> 00:03:08,800
入门的一个重要的一个应用

65
00:03:08,840 --> 00:03:11,680
因为它确实跟真金实银是挂钩的

66
00:03:14,519 --> 00:03:16,479
接下来我们通过这个应用

67
00:03:16,479 --> 00:03:18,439
来引出现行回归

68
00:03:19,159 --> 00:03:21,319
这里我们做一个简化的模型

69
00:03:21,719 --> 00:03:23,319
我们有做两个假设

70
00:03:23,840 --> 00:03:25,280
第一个假设是说

71
00:03:25,319 --> 00:03:28,400
影响房价的关键因素是三个

72
00:03:28,719 --> 00:03:29,919
卧室的个数

73
00:03:30,039 --> 00:03:32,639
卫生间的个数和居住面积

74
00:03:32,799 --> 00:03:35,120
我们计为X1 X2 X3

75
00:03:35,280 --> 00:03:37,919
当然是说你其实你还有很多别的因素

76
00:03:38,159 --> 00:03:40,319
我们先做一个最简单的假设

77
00:03:41,120 --> 00:03:42,560
第二个假设是说

78
00:03:42,560 --> 00:03:46,759
我的成交价是我关键因素的加权和

79
00:03:47,039 --> 00:03:49,079
那就是说我的成交价Y

80
00:03:49,200 --> 00:03:51,400
就是W1乘以X1

81
00:03:51,560 --> 00:03:53,520
加上W2乘以X2

82
00:03:53,520 --> 00:03:55,240
加上W3乘以X3

83
00:03:55,600 --> 00:03:56,680
再加上一个B

84
00:03:57,120 --> 00:04:03,560
这里权重W1 W2 W3和偏差B的实际值

85
00:04:03,560 --> 00:04:05,640
我们之后再来决定

86
00:04:05,800 --> 00:04:08,439
但是我们假设我们这些值已经有了

87
00:04:08,480 --> 00:04:12,760
那么我们的成交价就是这四个项加和

88
00:04:13,760 --> 00:04:15,280
这是我们的核心模型

89
00:04:16,279 --> 00:04:21,759
那么把它拓展到一般化的线性模型

90
00:04:21,759 --> 00:04:25,199
就是说我们如果给定一个N位的输入X

91
00:04:25,600 --> 00:04:28,839
它有X1到Xn则N个项

92
00:04:29,480 --> 00:04:33,079
然后我的线性模型就会有一个N位的权重

93
00:04:33,240 --> 00:04:33,879
W

94
00:04:34,279 --> 00:04:37,240
它包括了W1一直到Wn

95
00:04:37,560 --> 00:04:40,360
和一个标量的偏差B

96
00:04:41,319 --> 00:04:45,240
那么我们的输出就是我们的输入的加权和

97
00:04:45,280 --> 00:04:50,400
就是从W1乘以X1一直加到Wn乘以Xn

98
00:04:50,600 --> 00:04:52,240
再加上偏差B

99
00:04:53,120 --> 00:04:55,320
如果我们写成一个项量版本的话

100
00:04:55,320 --> 00:04:58,360
那就是我的输入X一个项量和

101
00:04:58,560 --> 00:05:00,560
权重W的累积

102
00:05:00,680 --> 00:05:03,480
再加上我的标量偏差B

103
00:05:04,840 --> 00:05:05,360
好

104
00:05:05,520 --> 00:05:07,480
这就是我的线性模型

105
00:05:09,600 --> 00:05:12,680
线性模型之所以我们讲这个模型

106
00:05:12,680 --> 00:05:16,439
是因为它可以看作是一个单层的神级网络

107
00:05:17,000 --> 00:05:19,079
假设我们的神级网络

108
00:05:19,079 --> 00:05:21,240
通常我们用这样子的图来表示

109
00:05:21,680 --> 00:05:23,160
我们的输入层

110
00:05:23,160 --> 00:05:26,519
它有这里我们是画了N个输入的元素

111
00:05:26,680 --> 00:05:27,920
跟我们之前的N

112
00:05:27,920 --> 00:05:30,920
第一个输入元素跟我们之前的N是匹配的

113
00:05:31,199 --> 00:05:34,319
那就是说我有输入的维度是D

114
00:05:34,800 --> 00:05:38,240
然后我输出的维度是E

115
00:05:38,240 --> 00:05:39,399
就是一个OE

116
00:05:39,680 --> 00:05:42,360
然后每一个箭头代表了一个权重

117
00:05:42,840 --> 00:05:44,480
这里我们没有画偏差

118
00:05:44,680 --> 00:05:49,800
那就是说我神级网络是一个输入层和一个输出层

119
00:05:50,000 --> 00:05:52,360
它之所以叫做单层神级网络

120
00:05:52,360 --> 00:05:54,800
是因为它带权重的层

121
00:05:54,800 --> 00:06:00,079
就是我们可以把输出层不当作一个层

122
00:06:00,280 --> 00:06:04,120
因为我们把权重跟输入层放在一起

123
00:06:07,439 --> 00:06:09,240
我们第一次提到神级网络

124
00:06:09,480 --> 00:06:10,720
神级网络这个词

125
00:06:11,400 --> 00:06:14,560
起自最早源于神经科学

126
00:06:15,120 --> 00:06:17,720
在60年代50年代

127
00:06:17,920 --> 00:06:20,280
大家对神经科学的一些突破

128
00:06:20,280 --> 00:06:22,560
使得大家来学想

129
00:06:22,760 --> 00:06:25,240
我是不是能够做一个人工的神级网络

130
00:06:25,240 --> 00:06:27,080
来模拟人的大脑

131
00:06:27,440 --> 00:06:31,000
这里显示的是一个真实的神级元的构造

132
00:06:31,360 --> 00:06:33,880
可以看到是说我们的输入

133
00:06:34,200 --> 00:06:36,440
神级元的输入是有挺多的

134
00:06:36,560 --> 00:06:38,400
然后我们的输入之后

135
00:06:38,560 --> 00:06:42,400
会在神级元这个地方发射一些计算

136
00:06:42,600 --> 00:06:48,720
它的输出结果会通过一个输出到下一个神级元

137
00:06:49,240 --> 00:06:50,920
它的计算就表示说

138
00:06:51,240 --> 00:06:54,360
你的输入是不是能够过了我的预指

139
00:06:54,360 --> 00:06:55,480
如果过了我的预指

140
00:06:55,480 --> 00:06:57,760
我就会发射一个神经信号出去

141
00:06:57,760 --> 00:06:58,720
如果没有的话

142
00:06:58,880 --> 00:06:59,800
我就是不发射

143
00:07:00,960 --> 00:07:06,320
这也是为什么我们把这一类的模型叫做神经网络

144
00:07:06,360 --> 00:07:08,639
因为它来自于神经科学

145
00:07:09,000 --> 00:07:10,240
但实际上来说

146
00:07:10,240 --> 00:07:14,040
在过去几十年的50年60年的发展中

147
00:07:14,279 --> 00:07:18,399
神经网络已经远远的超出了神经科学

148
00:07:18,399 --> 00:07:20,159
所以了解的範绸

149
00:07:20,279 --> 00:07:22,639
我们也不那么去追求

150
00:07:22,839 --> 00:07:26,600
对于一个新的网络在神经学上的解释

151
00:07:26,719 --> 00:07:29,399
但是最早确实我们是可以迈过去

152
00:07:29,519 --> 00:07:31,560
而且我们接下来会介绍一些模型

153
00:07:31,560 --> 00:07:35,319
确实有着他自己神经科学的一些背景

154
00:07:36,759 --> 00:07:39,319
好

155
00:07:39,319 --> 00:07:41,719
我们已经有了模型了

156
00:07:41,719 --> 00:07:43,040
我们可以做预测

157
00:07:43,120 --> 00:07:45,639
那么接下来一个很重要的事情是说

158
00:07:45,680 --> 00:07:49,519
我们要衡量我们的预测的质量会怎么样

159
00:07:49,920 --> 00:07:53,079
也就是说我们要比较我们真实的值

160
00:07:53,079 --> 00:07:56,360
就是卖房的成交价和预估值

161
00:07:56,360 --> 00:07:58,240
就是我对这个房子的估计

162
00:07:58,879 --> 00:08:00,639
所以我们要比较它们的区别

163
00:08:00,719 --> 00:08:02,000
当时区别越小

164
00:08:02,000 --> 00:08:03,399
我的模型质量越高

165
00:08:03,399 --> 00:08:04,159
区别越大

166
00:08:04,159 --> 00:08:05,719
我的模型质量就越差

167
00:08:06,360 --> 00:08:09,079
所以我们假设y是我们的真实值

168
00:08:09,360 --> 00:08:11,319
y hat是我们的估计值

169
00:08:11,759 --> 00:08:15,159
一个很常见的一个比较是说

170
00:08:15,319 --> 00:08:20,839
1 2乘以真实值减去估计值的平方

171
00:08:21,360 --> 00:08:22,879
这个叫平方损失

172
00:08:23,039 --> 00:08:24,639
因为它衡量的是说

173
00:08:24,719 --> 00:08:31,719
我们没有完全猜中真实值所带来的损失

174
00:08:31,719 --> 00:08:33,360
或者说是经济损失了

175
00:08:34,360 --> 00:08:36,440
这里之所以有个1 2

176
00:08:36,440 --> 00:08:38,240
是因为我们之后求导的时候

177
00:08:38,240 --> 00:08:40,159
可以很方便的把它消去

178
00:08:42,039 --> 00:08:43,279
定义了模型

179
00:08:43,279 --> 00:08:44,879
定义了损失之后

180
00:08:45,000 --> 00:08:47,480
我们就要来学习我们的参数了

181
00:08:47,480 --> 00:08:49,560
就是我们的权重和我们的偏差

182
00:08:50,159 --> 00:08:51,399
我们怎么学习呢

183
00:08:51,440 --> 00:08:53,200
我们是基于数据来学习的

184
00:08:53,720 --> 00:08:54,720
我们一般来说

185
00:08:54,720 --> 00:08:57,039
会先收集一些数据点

186
00:08:57,159 --> 00:08:58,680
来训练我们的模型

187
00:08:59,039 --> 00:09:00,759
例如说这个例子里面

188
00:09:00,759 --> 00:09:04,120
我们可以去采集过去6个月里面

189
00:09:04,120 --> 00:09:05,519
所有卖的房子

190
00:09:05,679 --> 00:09:08,919
它的房子的信息和它的最终的成交价

191
00:09:09,439 --> 00:09:12,439
这些数据我们一般会称之为训练数据

192
00:09:12,439 --> 00:09:14,319
是用来训练我们模型的数据

193
00:09:14,759 --> 00:09:16,240
通常来说是越多越好

194
00:09:17,080 --> 00:09:18,319
但实际上来说

195
00:09:18,639 --> 00:09:20,200
你会受限于很多事情

196
00:09:20,240 --> 00:09:22,679
比如说这个世界上就那么多房子

197
00:09:22,759 --> 00:09:24,039
每个月卖的房子

198
00:09:24,279 --> 00:09:26,080
它其实是一个缓慢的

199
00:09:26,080 --> 00:09:27,159
可能是在增加

200
00:09:27,159 --> 00:09:28,480
可能是减少的过程

201
00:09:28,559 --> 00:09:30,399
但它不会有无穷多的房子

202
00:09:30,879 --> 00:09:31,720
所以的话

203
00:09:31,759 --> 00:09:35,039
我们就算把所有的房子的信息采集下来

204
00:09:35,319 --> 00:09:36,720
可能就那么多

205
00:09:38,519 --> 00:09:39,399
所以一般来说

206
00:09:39,399 --> 00:09:41,039
我们有很多技术来处理

207
00:09:41,080 --> 00:09:43,799
当你的数据不够的时候会怎么办

208
00:09:43,840 --> 00:09:46,919
我们之后会有非常多的算法

209
00:09:46,919 --> 00:09:48,799
来探讨这个问题

210
00:09:49,120 --> 00:09:51,759
这里我们先不再深入了解

211
00:09:52,200 --> 00:09:54,000
我们先来看一下训练样本

212
00:09:54,439 --> 00:09:56,600
假设我们有n个样本的话

213
00:09:56,919 --> 00:09:58,279
我们可以把它

214
00:09:58,319 --> 00:10:00,039
假设我们一个样本

215
00:10:00,039 --> 00:10:03,039
每一个x是一个列的项量的话

216
00:10:03,079 --> 00:10:04,759
我们一排排排好

217
00:10:04,799 --> 00:10:06,079
然后作为转制

218
00:10:06,439 --> 00:10:08,639
我们的大x的每一行

219
00:10:08,839 --> 00:10:10,120
对应的就是一个样本

220
00:10:10,799 --> 00:10:12,000
的y同样的

221
00:10:12,000 --> 00:10:13,319
它是一个列项量

222
00:10:13,519 --> 00:10:15,079
也是有n个样本

223
00:10:15,079 --> 00:10:17,120
每一个yi就是一个

224
00:10:17,719 --> 00:10:19,000
实数的数值

225
00:10:19,279 --> 00:10:21,679
这就是我们的x和y

226
00:10:21,679 --> 00:10:22,959
也就是我们的数据

227
00:10:26,039 --> 00:10:26,360
好

228
00:10:26,360 --> 00:10:28,199
有了所有之后

229
00:10:28,200 --> 00:10:30,440
那么我们就可以求解我们的模型了

230
00:10:31,160 --> 00:10:34,080
我们怎么根据我们之前的损失

231
00:10:34,560 --> 00:10:36,240
给定我们的数据

232
00:10:36,480 --> 00:10:38,800
那么我们来评估说

233
00:10:38,840 --> 00:10:40,800
对于我们的模型

234
00:10:40,879 --> 00:10:43,960
在每一个数据上的损失求均值

235
00:10:44,200 --> 00:10:46,600
就会得到我们的一个损失函数

236
00:10:46,920 --> 00:10:47,879
我们把它写出来

237
00:10:47,879 --> 00:10:48,759
就是说

238
00:10:48,920 --> 00:10:51,640
它是关于我们的数据x和y

239
00:10:51,759 --> 00:10:53,360
关于我们的权重

240
00:10:53,360 --> 00:10:54,600
关于我们的偏差

241
00:10:55,040 --> 00:10:56,600
它可以写开就是说

242
00:10:56,639 --> 00:10:59,519
1 2这个向来自于我们的损失函数

243
00:10:59,759 --> 00:11:01,879
n 1就是我们要求平均

244
00:11:02,560 --> 00:11:04,800
然后对每一个样本

245
00:11:04,800 --> 00:11:06,480
我们的真实yi

246
00:11:06,560 --> 00:11:08,279
减去后面是我们的预测值

247
00:11:08,279 --> 00:11:08,960
就是

248
00:11:09,879 --> 00:11:13,200
yi跟样本的x和我们的权重的累积

249
00:11:13,399 --> 00:11:14,879
和我们的偏差

250
00:11:16,000 --> 00:11:17,800
当然我们可以写成一个项量版本

251
00:11:18,040 --> 00:11:19,000
就是一个

252
00:11:19,120 --> 00:11:21,639
y 项量版本的y减去

253
00:11:22,160 --> 00:11:24,399
矩阵x乘以项量w

254
00:11:24,440 --> 00:11:25,440
再减去b

255
00:11:25,480 --> 00:11:26,560
b是一个标量

256
00:11:26,600 --> 00:11:29,360
然后对项量求L too long

257
00:11:30,360 --> 00:11:32,040
这就是我们的损失函数

258
00:11:32,519 --> 00:11:33,519
那么我们的目标

259
00:11:33,720 --> 00:11:36,800
就是说我们要找到一个w和一个b

260
00:11:37,040 --> 00:11:40,960
使得我们的整个这一项的值最小

261
00:11:41,279 --> 00:11:42,320
也就是说

262
00:11:42,320 --> 00:11:43,519
我们可以最小

263
00:11:43,519 --> 00:11:46,680
我们的损失函数来学习参数

264
00:11:46,960 --> 00:11:48,800
我们选取一个wb

265
00:11:48,960 --> 00:11:51,000
使得能最小化这一项

266
00:11:51,200 --> 00:11:54,080
把最小化的它作为我们的解

267
00:11:54,080 --> 00:11:57,120
也就是w心和b心

268
00:11:58,440 --> 00:11:58,680
好

269
00:11:58,680 --> 00:12:01,160
整个就是我们的求解过程

270
00:12:01,639 --> 00:12:03,440
当然是因为我们是线性模型了

271
00:12:03,440 --> 00:12:05,920
所以它是有显示节的

272
00:12:06,280 --> 00:12:07,920
我们来看一下它的显示节

273
00:12:07,920 --> 00:12:08,920
长什么样子

274
00:12:09,600 --> 00:12:11,320
我们最简单是说

275
00:12:11,400 --> 00:12:12,920
因为我们有偏差

276
00:12:12,920 --> 00:12:15,200
我们先把偏差加入我们的权重

277
00:12:15,200 --> 00:12:17,280
使得我们的写起来方便一点

278
00:12:17,520 --> 00:12:18,240
具体怎么加

279
00:12:18,920 --> 00:12:21,120
我们加入一列特征

280
00:12:21,120 --> 00:12:22,400
权一的特征

281
00:12:22,600 --> 00:12:24,079
加进我们的x里面

282
00:12:24,319 --> 00:12:27,480
然后我们再把偏差放到权重的最后面

283
00:12:27,480 --> 00:12:28,759
放到我们的w里面

284
00:12:29,279 --> 00:12:32,399
那么我们的预测就是x乘以w了

285
00:12:33,319 --> 00:12:36,840
那我们的损失函数可以写成1 2分之一

286
00:12:37,120 --> 00:12:39,439
然后y减去x是w

287
00:12:39,439 --> 00:12:40,519
它的L too long

288
00:12:41,000 --> 00:12:44,840
回忆一下我们之前介绍的矩阵计算里面

289
00:12:44,919 --> 00:12:48,240
我们可以对它来展开求导

290
00:12:48,439 --> 00:12:50,079
这是大家可以回忆一下

291
00:12:50,079 --> 00:12:52,159
我们是讲过这个例子的

292
00:12:52,480 --> 00:12:54,240
它就是等于1 2分之一

293
00:12:54,639 --> 00:12:57,720
y减去x是w

294
00:12:57,720 --> 00:13:01,519
也就是我们的预计的一个偏差

295
00:13:01,600 --> 00:13:03,759
再转制乘以x

296
00:13:05,559 --> 00:13:06,840
因为这是线性模型

297
00:13:06,840 --> 00:13:08,840
所以它的损失是一个too函数

298
00:13:09,000 --> 00:13:10,919
我们并没有解释什么是too函数

299
00:13:10,919 --> 00:13:14,799
你可以认为就是一个比较简单的这样子的函数

300
00:13:15,279 --> 00:13:17,840
所以too函数它的性质是说

301
00:13:17,879 --> 00:13:20,240
它的最有解是满足于

302
00:13:20,320 --> 00:13:24,120
一定是使得它的t too等于0的地方

303
00:13:24,399 --> 00:13:25,759
所以我们把它代进去

304
00:13:25,799 --> 00:13:27,159
最后可以得到是说

305
00:13:27,200 --> 00:13:29,000
我们的最有解w心

306
00:13:29,120 --> 00:13:32,799
其实可以计算成为x转制乘以x

307
00:13:32,919 --> 00:13:34,279
然后再求逆

308
00:13:34,360 --> 00:13:36,159
再乘以x再乘以y

309
00:13:36,440 --> 00:13:40,279
这是我们最优解的形式

310
00:13:41,080 --> 00:13:44,879
当然是说这个是我们唯一的一个有最优解的模型

311
00:13:44,879 --> 00:13:47,440
我们之后所谓的解都不会有最优解了

312
00:13:48,440 --> 00:13:50,480
好总结一下来说

313
00:13:50,520 --> 00:13:55,520
就是线性回归是对N美输入的一个加权核

314
00:13:55,600 --> 00:13:57,600
再加上一个偏差

315
00:13:57,840 --> 00:14:01,720
这是它的对一个输出值的预估

316
00:14:02,200 --> 00:14:06,680
然后对于它跟预测值和真实值级的差异

317
00:14:06,680 --> 00:14:09,240
通常使用平方损失来衡量

318
00:14:10,040 --> 00:14:13,880
线性回归是有一个是一个非常特别的一个模型

319
00:14:13,880 --> 00:14:15,480
它有显示解

320
00:14:15,680 --> 00:14:19,399
我们这堂课所有别的模型都没有显示解

321
00:14:19,680 --> 00:14:24,200
因为有显示解的模型一般来说过于简单

322
00:14:24,399 --> 00:14:28,720
机器学习通常是用来解决NP complete的问题

323
00:14:28,960 --> 00:14:31,600
如果你一个模型可以很快求解的话

324
00:14:31,600 --> 00:14:33,759
通常来说它的复杂度有限

325
00:14:33,759 --> 00:14:39,000
很难以衡量特别复杂的数据和复杂的应用

326
00:14:39,159 --> 00:14:43,399
所以来说我们一般会再去追求显示解

327
00:14:43,919 --> 00:14:47,039
最后的话我们之所以解释线性回归

328
00:14:47,039 --> 00:14:50,919
是因为线性回归确实是可以看作是一个单层的神经网络

329
00:14:50,919 --> 00:14:53,799
它是最简单的一种神经网络

330
00:14:54,159 --> 00:14:57,159
好我们线性回归就介绍到这里

