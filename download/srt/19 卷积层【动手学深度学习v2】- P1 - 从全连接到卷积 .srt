1
00:00:00,000 --> 00:00:01,000
问题17

2
00:00:01,000 --> 00:00:04,400
100个神级元的单MLP和100维的全连阶层

3
00:00:04,400 --> 00:00:05,120
区别是什么

4
00:00:05,120 --> 00:00:06,879
没有什么区别

5
00:00:06,879 --> 00:00:10,200
就是100个神级元的单隐藏在MLP

6
00:00:10,200 --> 00:00:13,320
就是有一个输出维度为100的全连阶层

7
00:00:14,919 --> 00:00:15,919
为什么

8
00:00:15,919 --> 00:00:16,519
问题18

9
00:00:16,519 --> 00:00:17,679
为什么要全中边形

10
00:00:17,679 --> 00:00:18,760
为什么要重新锁引

11
00:00:18,760 --> 00:00:22,440
这个东西是说我只是给大家介绍说

12
00:00:22,440 --> 00:00:23,519
怎么样从

13
00:00:23,519 --> 00:00:25,839
就是说给大家解释一下

14
00:00:26,280 --> 00:00:29,280
全连阶层和卷积层的关系

15
00:00:29,480 --> 00:00:32,039
就怎么样对全连阶层做变化

16
00:00:32,039 --> 00:00:34,200
做限制能得到卷积层

17
00:00:35,079 --> 00:00:37,960
我就并没有说你要通过做变化

18
00:00:37,960 --> 00:00:39,799
做重新锁引才能得到卷积层

19
00:00:39,799 --> 00:00:40,560
卷积层的定义

20
00:00:40,560 --> 00:00:42,799
就是我们之前讲的公式

21
00:00:43,160 --> 00:00:44,439
只是说我给大家讲一下

22
00:00:44,439 --> 00:00:49,000
为什么卷积就是一个特殊的全连阶层

23
00:00:53,280 --> 00:00:54,200
就问题19

24
00:00:54,200 --> 00:00:56,439
最初的变化是怎么样做的

25
00:00:56,439 --> 00:00:59,120
图片从挨个的线性变成二维集

26
00:00:59,240 --> 00:01:02,440
其实图片进来就是一个二维的东西

27
00:01:03,520 --> 00:01:05,680
只是说大家记得我们之前做

28
00:01:05,680 --> 00:01:08,240
softmax回归的时候

29
00:01:08,240 --> 00:01:11,280
我们把图片reshape成了一个向量

30
00:01:11,480 --> 00:01:13,000
图片本来就是一个矩阵

31
00:01:13,040 --> 00:01:15,080
所以你在做卷积的时候

32
00:01:15,080 --> 00:01:17,640
卷积就是对图片矩阵做运算

33
00:01:17,640 --> 00:01:19,719
不需要做在

34
00:01:20,159 --> 00:01:22,040
要做reshape什么样子

35
00:01:25,080 --> 00:01:27,680
可变形卷积在哪里应用

36
00:01:28,640 --> 00:01:29,960
我其实不是特别知道

37
00:01:29,960 --> 00:01:31,680
可变形卷积什么样子

38
00:01:31,680 --> 00:01:33,680
难道你是说你的kernel才会变吗

39
00:01:33,680 --> 00:01:34,880
我还没听说过这样子

40
00:01:35,520 --> 00:01:36,440
问题21

41
00:01:37,000 --> 00:01:38,440
为什么不应该看那么远

42
00:01:38,440 --> 00:01:41,600
感受一眼就是kernel大小了

43
00:01:41,720 --> 00:01:43,040
不是越大越好吗

44
00:01:45,640 --> 00:01:46,719
这是个很好的问题

45
00:01:48,600 --> 00:01:53,080
就最终你应该要看到所有的是没错的

46
00:01:53,120 --> 00:01:55,200
但你为什么不是越大越好

47
00:01:55,400 --> 00:01:59,920
就是这个问题类似于全面阶层

48
00:01:59,920 --> 00:02:02,640
你为什么不是隐藏层越大越好

49
00:02:02,680 --> 00:02:05,120
我可以做一个很浅的

50
00:02:05,120 --> 00:02:07,040
很宽的一个全面阶层

51
00:02:07,799 --> 00:02:10,640
但很多时候效果没有一个深一点的

52
00:02:10,640 --> 00:02:12,159
窄一点的全面阶层好

53
00:02:12,640 --> 00:02:14,560
所以卷积神经网络是一样的

54
00:02:14,879 --> 00:02:19,080
我做一个卷积层比较少的

55
00:02:19,480 --> 00:02:21,199
就有一层或者两层的

56
00:02:21,240 --> 00:02:23,319
但是我的每个核比较大的

57
00:02:23,799 --> 00:02:26,599
不如我每一层把核的变小一点

58
00:02:26,599 --> 00:02:27,759
但把它做深一点

59
00:02:28,799 --> 00:02:30,799
因为它的最终的事也是一样

60
00:02:30,840 --> 00:02:32,599
就是说每次看一点

61
00:02:32,639 --> 00:02:34,519
然后多看就多几次

62
00:02:34,560 --> 00:02:36,479
最后的东西也可以下来了

63
00:02:36,799 --> 00:02:39,560
所以通常来说小一点的

64
00:02:39,560 --> 00:02:41,599
一般大家主的用三乘三

65
00:02:41,639 --> 00:02:43,079
或者最多用个五乘五

66
00:02:43,079 --> 00:02:45,639
那就说但是做的比较深

67
00:02:45,639 --> 00:02:46,919
所以之后我们会讲

68
00:02:46,919 --> 00:02:53,159
二维剪积层有没有同时使用

69
00:02:53,159 --> 00:02:54,879
两个不同尺寸的科诺进行计算

70
00:02:54,879 --> 00:02:56,359
然后计算是一个合适的科诺

71
00:02:56,359 --> 00:02:58,000
从外提取特征提取的性能

72
00:02:58,280 --> 00:02:59,039
这个idea很好

73
00:02:59,159 --> 00:03:00,960
如果你10年前

74
00:03:01,159 --> 00:03:02,439
5年前有这个idea的话

75
00:03:02,479 --> 00:03:03,639
可能google

76
00:03:03,639 --> 00:03:05,599
google那边paper就是你的了

77
00:03:05,919 --> 00:03:07,120
然后你就出名了

78
00:03:08,759 --> 00:03:11,400
所以我们之后会介绍是有的

79
00:03:11,519 --> 00:03:16,719
这就是inception的设计思路

80
00:03:18,319 --> 00:03:20,039
怎么理解卷积是反过来的

81
00:03:20,319 --> 00:03:22,240
卷积公式的附好原因是

82
00:03:22,360 --> 00:03:23,920
而这就是信号处理

83
00:03:23,920 --> 00:03:25,600
他们就卷积这个东西

84
00:03:25,800 --> 00:03:27,280
是信号处理里面出来了

85
00:03:27,560 --> 00:03:29,360
它的就是一个反的

86
00:03:29,360 --> 00:03:31,600
它就是一个它数学上就这么定义过来

87
00:03:31,600 --> 00:03:34,200
它从fft那边就这么定义过来的

88
00:03:34,200 --> 00:03:35,920
就是复利叶变换那边过来的

89
00:03:35,920 --> 00:03:36,920
就信号处理过来的

90
00:03:36,920 --> 00:03:37,960
所以它是反的

91
00:03:38,320 --> 00:03:41,040
所以不是深度学习

92
00:03:41,040 --> 00:03:42,520
只是说把它拿过来用一用

93
00:03:42,520 --> 00:03:48,719
卷集合的大小体现了局限

94
00:03:48,920 --> 00:03:49,719
局部性

95
00:03:49,760 --> 00:03:51,480
那么什么体现了平易不变性

96
00:03:51,960 --> 00:03:53,600
平易不变性是说

97
00:03:53,600 --> 00:03:56,960
我不管你的输入在哪个位置

98
00:03:56,960 --> 00:03:58,880
我那个和是不变的

99
00:03:59,280 --> 00:04:00,920
和永远是小和

100
00:04:00,920 --> 00:04:02,160
就你不管在图片哪个位置

101
00:04:02,160 --> 00:04:03,760
我都是同一个和在作用它

102
00:04:04,800 --> 00:04:06,840
要不你可以认为这个和就是去识别

103
00:04:06,840 --> 00:04:07,600
那个和的

104
00:04:09,200 --> 00:04:09,600
对吧

105
00:04:09,600 --> 00:04:10,840
这个和是不变的

106
00:04:10,840 --> 00:04:15,640
所以就是说我们在卷积和

107
00:04:15,640 --> 00:04:18,000
就是公式是怎么算的

108
00:04:18,000 --> 00:04:19,439
体现了它的平易不变性

109
00:04:21,520 --> 00:04:24,160
是一二和大小怎么确定的

110
00:04:24,160 --> 00:04:26,759
那就是因为我们是这么构造的

111
00:04:26,759 --> 00:04:28,240
就我们有先言之始

112
00:04:28,240 --> 00:04:28,840
有一和二

113
00:04:29,720 --> 00:04:30,000
对吧

114
00:04:31,720 --> 00:04:33,200
这个是说在实际情况下

115
00:04:33,200 --> 00:04:34,319
你当然不知道一和二了

116
00:04:34,319 --> 00:04:35,879
因为我们是一个很简单的样例

117
00:04:35,879 --> 00:04:37,640
所以我直接给出一和二了

118
00:04:39,920 --> 00:04:41,080
卷积的数学含义

119
00:04:41,240 --> 00:04:41,840
卷积数学含义

120
00:04:42,040 --> 00:04:42,960
卷积是

121
00:04:43,480 --> 00:04:45,720
卷积这是有完整的数学含义的

122
00:04:45,879 --> 00:04:47,040
大家可以去看一下

123
00:04:47,040 --> 00:04:47,439
查一下

124
00:04:47,439 --> 00:04:49,920
这个是在数字信号处理里面

125
00:04:49,920 --> 00:04:50,920
卷积是有

126
00:04:50,920 --> 00:04:52,840
那个是那一套过来的

127
00:04:52,840 --> 00:04:55,960
就是它卷积的数学定义

128
00:04:55,960 --> 00:04:57,480
应该是1967年

129
00:04:57,480 --> 00:04:59,240
1677年就出来了

130
00:04:59,280 --> 00:05:03,720
这个远远早于神经网络

131
00:05:03,720 --> 00:05:06,120
神经网络只是借用了卷积这个概念

132
00:05:06,120 --> 00:05:08,160
它是数学上定义的很好的东西

133
00:05:09,439 --> 00:05:10,360
大家可以去查一下

134
00:05:10,360 --> 00:05:12,200
我们就不做历史上回顾了

135
00:05:17,200 --> 00:05:19,080
在做房价竞赛的时候

136
00:05:19,279 --> 00:05:22,000
画出来损失跌的函数抖动特别厉害

137
00:05:22,439 --> 00:05:23,319
就是说

138
00:05:23,839 --> 00:05:26,279
不像我们的那么平滑

139
00:05:26,279 --> 00:05:27,479
就抖动很厉害

140
00:05:27,479 --> 00:05:28,919
很有可能是你的学习率太大

141
00:05:28,919 --> 00:05:30,759
就抖动很厉害

142
00:05:33,079 --> 00:05:34,279
这抖动很厉害

143
00:05:34,279 --> 00:05:35,399
有两种情况

144
00:05:35,680 --> 00:05:37,639
一种情况是你的数据的

145
00:05:38,319 --> 00:05:39,639
它的多样性比较大

146
00:05:39,639 --> 00:05:42,199
每次你去随机采样的时候

147
00:05:42,240 --> 00:05:45,000
它确实是每次看到很不一样的样本

148
00:05:45,000 --> 00:05:46,279
所以它都会有抖动

149
00:05:46,279 --> 00:05:47,879
这个抖动是没关系的

150
00:05:47,920 --> 00:05:48,800
这个抖动是说

151
00:05:48,800 --> 00:05:51,199
你当你可以把它做些平滑

152
00:05:51,480 --> 00:05:53,399
就把曲线做平滑

153
00:05:53,839 --> 00:05:55,439
就只要看到它的下降就行了

154
00:05:55,439 --> 00:05:55,920
另外一个

155
00:05:55,920 --> 00:05:58,199
当然你可以把P样大小弄大一点点

156
00:05:58,399 --> 00:05:59,079
就会

157
00:05:59,079 --> 00:06:00,920
所以抖动是没关系的

158
00:06:01,560 --> 00:06:04,439
但是就在抖不下降是不行的

159
00:06:06,279 --> 00:06:07,959
就说如果你一直在抖

160
00:06:07,959 --> 00:06:10,480
一个很平的曲线是不行的

161
00:06:10,480 --> 00:06:12,560
但是你在抖动的下降是没关系的

162
00:06:12,680 --> 00:06:14,399
你如果想要不抖的话

163
00:06:14,399 --> 00:06:15,279
你就是把

164
00:06:15,920 --> 00:06:17,480
那学习率可以高一点

165
00:06:17,480 --> 00:06:19,520
或者是说你P样大小给大一点

166
00:06:19,520 --> 00:06:20,879
就是都OK

167
00:06:22,200 --> 00:06:24,360
为什么100的MLP太大放不下

168
00:06:24,360 --> 00:06:25,879
100的全连接层就放得下

169
00:06:26,200 --> 00:06:27,720
其实它不是这个意思

170
00:06:27,720 --> 00:06:28,400
就是说

171
00:06:29,600 --> 00:06:31,800
全连接层最大的问题是什么

172
00:06:31,840 --> 00:06:34,319
最大问题是说你的全重W

173
00:06:34,560 --> 00:06:35,400
它的

174
00:06:36,240 --> 00:06:37,680
它的层

175
00:06:37,720 --> 00:06:41,600
它的高是取决于输入的宽

176
00:06:43,800 --> 00:06:45,040
这是它最大的问题

177
00:06:45,280 --> 00:06:47,319
所以当我的输入给你一个

178
00:06:47,720 --> 00:06:50,560
1200万的像素的图片的时候

179
00:06:50,560 --> 00:06:51,560
你的输入的维度

180
00:06:51,560 --> 00:06:54,400
就变成一个1200万的一个维度了

181
00:06:54,439 --> 00:06:56,240
那么你当然就炸掉了

182
00:06:57,000 --> 00:06:57,360
对吧

183
00:06:57,360 --> 00:06:58,480
这是它的问题

184
00:06:58,480 --> 00:06:59,240
所以是说

185
00:06:59,280 --> 00:07:01,280
100的全连接层是不大的

186
00:07:01,319 --> 00:07:03,879
但我们之前都在用都没关系

187
00:07:03,879 --> 00:07:05,439
因为我们之前图片很小

188
00:07:05,439 --> 00:07:07,360
但这里是说我给大家举个例子

189
00:07:07,400 --> 00:07:09,200
不是现实中不会出现这个情况

190
00:07:09,759 --> 00:07:12,280
假设你给一个全连接一个很大图片

191
00:07:12,400 --> 00:07:14,240
那么你发现MP就不行了

192
00:07:14,319 --> 00:07:16,680
所以卷机就没有这个问题

193
00:07:16,680 --> 00:07:18,879
卷机壳的大小是固定的

194
00:07:18,879 --> 00:07:19,879
不管你输入多大

195
00:07:19,879 --> 00:07:21,720
我的盒总是这么一点

196
00:07:21,720 --> 00:07:22,240
所以

197
00:07:22,560 --> 00:07:24,519
我是说卷机层解决这个问题

198
00:07:24,720 --> 00:07:26,560
但实际中来说

199
00:07:26,560 --> 00:07:29,280
你不会把一个1000万的图片丢进去

200
00:07:29,360 --> 00:07:30,120
实际中的话

201
00:07:30,120 --> 00:07:31,160
你可能用一个

202
00:07:31,759 --> 00:07:34,399
我们之后会讲就是一个200×200的

203
00:07:34,399 --> 00:07:36,120
一个差不多一个图片大小

204
00:07:36,160 --> 00:07:38,199
那其实也是不小的一个东西

205
00:07:38,240 --> 00:07:39,399
所以全连接层

206
00:07:39,439 --> 00:07:41,280
确实在对图片来讲

207
00:07:41,560 --> 00:07:42,600
不是那么的好用

