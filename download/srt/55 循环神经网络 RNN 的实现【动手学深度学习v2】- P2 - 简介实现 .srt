1
00:00:00,000 --> 00:00:04,799
OK 我们先来讲RNN循环神经网络

2
00:00:04,799 --> 00:00:06,040
从零开始实现

3
00:00:06,040 --> 00:00:10,000
就是说直接跟我们之前的讲卷积

4
00:00:10,279 --> 00:00:12,599
讲信息回归一样

5
00:00:12,599 --> 00:00:15,880
从最简单的开始给大家实现看一看

6
00:00:16,160 --> 00:00:17,240
这样子更容易理解

7
00:00:17,240 --> 00:00:18,320
我们刚刚的公式

8
00:00:18,320 --> 00:00:21,199
是如何转换成我们的代码的

9
00:00:21,879 --> 00:00:23,080
首先我们import了一些东西

10
00:00:23,600 --> 00:00:26,320
然后下面这两行

11
00:00:26,320 --> 00:00:27,760
就是我们来load

12
00:00:27,760 --> 00:00:29,359
我们之前的数据

13
00:00:30,960 --> 00:00:31,760
大家应该还记得

14
00:00:32,159 --> 00:00:35,000
我们之前定了一个函数

15
00:00:35,000 --> 00:00:39,280
叫做把时间机器的数据

16
00:00:39,280 --> 00:00:40,280
就load进来

17
00:00:40,600 --> 00:00:43,280
如果大家错过了我们之前课程

18
00:00:43,359 --> 00:00:45,159
可以看一下视频的回放

19
00:00:46,240 --> 00:00:49,799
然后它给定了两个参数

20
00:00:49,799 --> 00:00:51,039
一个是p量大小

21
00:00:51,159 --> 00:00:53,320
p量大小是我们之前一直在给定的

22
00:00:53,320 --> 00:00:54,760
就是我们给定32

23
00:00:55,200 --> 00:00:57,519
另外一个是一个是长度

24
00:00:57,880 --> 00:01:00,920
每一次我们看一个多长的序列

25
00:01:00,920 --> 00:01:03,240
就是一个小p量里面一条样本

26
00:01:03,240 --> 00:01:04,600
它是有多长

27
00:01:04,600 --> 00:01:07,240
长就是时间维度大T

28
00:01:07,560 --> 00:01:10,680
我们这里取了一个35这个东西

29
00:01:11,000 --> 00:01:12,879
其实没有特别的含义

30
00:01:12,879 --> 00:01:14,040
说为什么要取35

31
00:01:14,200 --> 00:01:15,000
其实都无所谓

32
00:01:15,000 --> 00:01:16,480
就是你短一点长一点

33
00:01:16,480 --> 00:01:17,600
都没有太多关系

34
00:01:17,800 --> 00:01:18,879
我们就取了个35

35
00:01:20,120 --> 00:01:21,520
然后你拿到之后

36
00:01:21,520 --> 00:01:23,320
它会返回两个东西

37
00:01:23,320 --> 00:01:26,760
一个是它的training iterator

38
00:01:27,440 --> 00:01:29,880
就是说它每一次说给你一个叠代器

39
00:01:29,880 --> 00:01:31,040
我可以读一个小p量

40
00:01:31,200 --> 00:01:32,439
跟之前是一样的

41
00:01:32,920 --> 00:01:33,960
跟之前不一样的

42
00:01:33,960 --> 00:01:36,920
在于我们返回了一个字典

43
00:01:36,920 --> 00:01:37,719
vocab

44
00:01:37,920 --> 00:01:40,439
vocab它目前来看

45
00:01:40,439 --> 00:01:41,800
主要的用处是说

46
00:01:42,080 --> 00:01:43,480
我们可以把它

47
00:01:43,480 --> 00:01:44,640
把其中的一个

48
00:01:46,280 --> 00:01:48,840
index一个整数的一个

49
00:01:49,439 --> 00:01:52,120
index可以转成对应的词

50
00:01:52,120 --> 00:01:53,400
就是可以相互转换

51
00:01:53,680 --> 00:01:55,080
主要是干这个用处的

52
00:01:57,079 --> 00:01:58,560
我们读取了数据之后

53
00:01:58,560 --> 00:02:00,120
我们主要来看一下

54
00:02:01,560 --> 00:02:02,640
我们的实现

55
00:02:02,880 --> 00:02:05,040
首先我们要看一下

56
00:02:05,560 --> 00:02:07,480
one hot encoding

57
00:02:07,480 --> 00:02:08,680
叫读热编码

58
00:02:09,280 --> 00:02:11,040
这个东西我们之前有讲过

59
00:02:11,040 --> 00:02:13,360
其实给大家再回顾一下

60
00:02:13,360 --> 00:02:14,200
是怎么回事

61
00:02:15,200 --> 00:02:16,200
就这个干什么事情

62
00:02:16,719 --> 00:02:17,520
就是说

63
00:02:17,560 --> 00:02:20,360
我给你一个向量0和2

64
00:02:20,800 --> 00:02:22,719
然后这个地方0和2

65
00:02:22,719 --> 00:02:24,159
进入了one hot的时候

66
00:02:24,159 --> 00:02:25,560
0和2就表示一个下标

67
00:02:25,560 --> 00:02:26,120
就是说

68
00:02:26,159 --> 00:02:28,120
表示你可以是一个物体类别

69
00:02:28,240 --> 00:02:30,039
或者是某一个物体的一个

70
00:02:30,039 --> 00:02:30,960
一个identity

71
00:02:30,960 --> 00:02:34,199
不再是一个feature数值了

72
00:02:35,319 --> 00:02:36,520
然后我告诉你一个

73
00:02:36,520 --> 00:02:38,599
我的编码

74
00:02:38,599 --> 00:02:40,080
vector的长度

75
00:02:40,520 --> 00:02:41,400
长度这里

76
00:02:41,520 --> 00:02:43,120
我们要给定的是

77
00:02:43,439 --> 00:02:44,719
字典的大小

78
00:02:45,199 --> 00:02:46,719
你有多少个词

79
00:02:48,080 --> 00:02:48,680
所以你有

80
00:02:48,680 --> 00:02:50,000
你告诉我有多少个词的话

81
00:02:50,000 --> 00:02:52,120
那么对每一个里面的下标

82
00:02:52,560 --> 00:02:54,319
我都会把它变成一个

83
00:02:54,319 --> 00:02:55,680
那么长的一个向量

84
00:02:56,159 --> 00:02:56,840
我们这个词

85
00:02:56,840 --> 00:02:58,000
我们记得是

86
00:02:58,879 --> 00:02:59,639
是多少来着

87
00:02:59,639 --> 00:03:02,039
28还是多少来着

88
00:03:04,560 --> 00:03:07,319
然后你可以看到是说

89
00:03:09,560 --> 00:03:10,959
第0个词

90
00:03:11,560 --> 00:03:12,439
这0个进去

91
00:03:12,639 --> 00:03:13,799
它就变成这个向量

92
00:03:13,799 --> 00:03:15,000
长为应该是28

93
00:03:15,359 --> 00:03:17,599
然后所有的东西都是0

94
00:03:17,599 --> 00:03:19,479
除了第0个元素之外

95
00:03:19,479 --> 00:03:20,079
是1之外

96
00:03:20,079 --> 00:03:20,879
别的都是0

97
00:03:22,079 --> 00:03:23,599
另外我给一个2进去的话

98
00:03:23,680 --> 00:03:25,400
所以会看到是一个

99
00:03:25,400 --> 00:03:26,079
返回一个

100
00:03:26,079 --> 00:03:27,519
长为28的一个向量

101
00:03:27,639 --> 00:03:29,400
除所有的元素都是0

102
00:03:29,799 --> 00:03:34,199
除了第3个

103
00:03:34,280 --> 00:03:35,560
就是2从0开始下标

104
00:03:35,719 --> 00:03:36,439
012

105
00:03:36,639 --> 00:03:38,799
第3个元素是1

106
00:03:38,959 --> 00:03:39,759
别的都是0

107
00:03:40,079 --> 00:03:41,240
所以这样的话

108
00:03:41,400 --> 00:03:42,759
我给你一个下标

109
00:03:42,759 --> 00:03:46,719
我就可以就返回出一个向量来表示它

110
00:03:48,359 --> 00:03:49,719
这就是one-hot encoding

111
00:03:49,879 --> 00:03:50,560
这样子的话

112
00:03:50,719 --> 00:03:52,000
我就可以把一个下标

113
00:03:52,000 --> 00:03:53,599
变成一个向量

114
00:03:53,599 --> 00:03:55,319
我能够做神之网络处理

115
00:03:56,960 --> 00:03:58,520
然后可以看到是说

116
00:03:58,520 --> 00:04:00,280
你返回的结果是什么样子的

117
00:04:00,400 --> 00:04:01,479
你这里是一个

118
00:04:02,639 --> 00:04:04,080
一个长为2的向量的话

119
00:04:04,599 --> 00:04:05,840
它会给你

120
00:04:05,879 --> 00:04:07,240
贴在一个维度

121
00:04:07,319 --> 00:04:08,599
就贴在这个维度

122
00:04:08,639 --> 00:04:11,120
所以你这个地方就是一个2乘以

123
00:04:11,120 --> 00:04:13,639
它的形状应该是2乘以28

124
00:04:14,960 --> 00:04:15,439
好

125
00:04:15,439 --> 00:04:18,720
假设我们给你一个小批量数据

126
00:04:19,280 --> 00:04:21,280
小批量数据的形状

127
00:04:21,280 --> 00:04:22,920
是批量大小

128
00:04:22,920 --> 00:04:24,680
乘以时间的步数

129
00:04:25,480 --> 00:04:28,440
就是我们的32乘以35

130
00:04:30,480 --> 00:04:31,720
然后我们这当然是说

131
00:04:31,720 --> 00:04:33,640
我们先给你构造一个小

132
00:04:34,040 --> 00:04:35,080
构造一个x出来

133
00:04:35,080 --> 00:04:35,960
x是一个

134
00:04:36,000 --> 00:04:37,720
我告诉你说批量大小是2

135
00:04:38,200 --> 00:04:39,880
时间步数是5

136
00:04:41,000 --> 00:04:42,920
那么我们把小批量

137
00:04:43,080 --> 00:04:45,120
放进one-hot看一下会怎么样

138
00:04:45,440 --> 00:04:46,800
就是一个二维的进去

139
00:04:47,080 --> 00:04:47,680
可以理解

140
00:04:47,680 --> 00:04:49,120
就是说你会变成一个三维出来

141
00:04:49,120 --> 00:04:49,600
对吧

142
00:04:50,720 --> 00:04:52,120
但是这个地方我们可以看到

143
00:04:52,160 --> 00:04:53,600
有一个比较奇怪的操作

144
00:04:53,600 --> 00:04:55,959
是我们对x做转制

145
00:04:57,000 --> 00:04:57,800
作为转制之后

146
00:04:57,800 --> 00:04:59,560
你就变成一个5乘以2的矩阵了

147
00:04:59,840 --> 00:05:02,040
然后你假设你说

148
00:05:02,040 --> 00:05:04,240
我的我cap的大小是28的话

149
00:05:04,240 --> 00:05:07,160
那么它就会变成一个三维的

150
00:05:07,360 --> 00:05:08,319
tensor

151
00:05:08,600 --> 00:05:10,439
5乘以2乘以28

152
00:05:11,560 --> 00:05:13,439
为什么你做这个转制呢

153
00:05:13,600 --> 00:05:16,720
是因为一开始我的小批量里面的

154
00:05:17,280 --> 00:05:19,440
后面一个维度是时间

155
00:05:20,000 --> 00:05:21,160
我做转制之后

156
00:05:21,160 --> 00:05:22,840
我就把时间放到了前面

157
00:05:23,560 --> 00:05:24,880
那么出来的是什么

158
00:05:25,360 --> 00:05:26,040
出来的话

159
00:05:26,160 --> 00:05:28,000
就是第一个维度是时间

160
00:05:28,000 --> 00:05:28,840
然后是批量

161
00:05:28,840 --> 00:05:31,000
然后是你的每一个样本的

162
00:05:31,000 --> 00:05:32,440
那一个特征长度

163
00:05:33,200 --> 00:05:34,240
所以这个地方

164
00:05:34,240 --> 00:05:35,840
你可以简单认为是说

165
00:05:35,880 --> 00:05:37,840
我这里面是有

166
00:05:38,920 --> 00:05:40,760
因为我有5个时间步

167
00:05:40,880 --> 00:05:42,480
就是每一个时间步

168
00:05:42,480 --> 00:05:43,200
你的x

169
00:05:43,200 --> 00:05:44,800
就xt就是这个地方

170
00:05:45,800 --> 00:05:46,800
就是xt

171
00:05:47,439 --> 00:05:51,879
然后t是t可以从0一直取到4

172
00:05:52,280 --> 00:05:53,720
就是5个xt

173
00:05:53,920 --> 00:05:54,759
所以这个样子

174
00:05:54,879 --> 00:05:56,360
我如果做了转制的话

175
00:05:56,360 --> 00:05:57,480
我的好处是说

176
00:05:57,480 --> 00:06:00,199
我每次去访问xt的话

177
00:06:00,199 --> 00:06:01,199
它就是一个连续的

178
00:06:01,199 --> 00:06:03,160
因为它挪到了后面两个维度

179
00:06:03,800 --> 00:06:04,080
OK

180
00:06:04,080 --> 00:06:06,000
这就是为什么我们要做个转制

181
00:06:06,000 --> 00:06:06,680
在这个地方

182
00:06:07,600 --> 00:06:08,120
好

183
00:06:10,759 --> 00:06:11,000
好

184
00:06:11,000 --> 00:06:14,000
接下来我们就来初始一下

185
00:06:14,000 --> 00:06:18,120
我的循环神经网络的模型参数了

186
00:06:18,639 --> 00:06:20,399
这个其实是整个里面

187
00:06:21,160 --> 00:06:22,920
有点像我们之前定义

188
00:06:24,439 --> 00:06:25,079
信息回归

189
00:06:25,240 --> 00:06:27,079
或者是MLP

190
00:06:27,079 --> 00:06:28,399
我们都干了这个事情

191
00:06:28,959 --> 00:06:30,839
get一个parents

192
00:06:30,839 --> 00:06:32,319
就是返回到我需要的

193
00:06:32,319 --> 00:06:34,600
那些可以学习的那些参数

194
00:06:35,279 --> 00:06:36,759
这个地方跟之前不一样

195
00:06:37,160 --> 00:06:40,000
之前我们基本只有个隐藏层大小

196
00:06:40,000 --> 00:06:42,680
就是中间隐藏层的输出是多少

197
00:06:43,120 --> 00:06:43,840
现在我们

198
00:06:46,920 --> 00:06:48,720
加了一个workab的大小

199
00:06:49,319 --> 00:06:50,720
我们等会看workab大小

200
00:06:50,720 --> 00:06:52,280
是怎么变成输入输出了

201
00:06:52,560 --> 00:06:53,519
另外一个是device

202
00:06:53,680 --> 00:06:54,560
因为这个东西

203
00:06:54,560 --> 00:06:57,600
我们之后都是尽量在GPU上来训练

204
00:06:57,600 --> 00:06:59,120
这样子会快一点

205
00:06:59,120 --> 00:07:01,759
CPURM的CPU倒是还是可以训练的

206
00:07:02,920 --> 00:07:04,480
但是GPU会快一点

207
00:07:05,399 --> 00:07:05,879
OK

208
00:07:06,280 --> 00:07:07,199
首先我们来看一下

209
00:07:07,360 --> 00:07:07,840
这个地方

210
00:07:08,720 --> 00:07:10,480
number of inputs output

211
00:07:10,480 --> 00:07:12,000
它等于workab size

212
00:07:12,079 --> 00:07:12,519
为什么

213
00:07:13,279 --> 00:07:14,959
是因为你的输入

214
00:07:15,279 --> 00:07:17,000
输入你本来是一个个词

215
00:07:17,000 --> 00:07:17,600
对吧

216
00:07:17,600 --> 00:07:18,680
一个个词

217
00:07:18,680 --> 00:07:21,240
通过1号的变成一个向量之后

218
00:07:21,240 --> 00:07:22,079
那么你就是一个

219
00:07:22,079 --> 00:07:24,560
成为一个workab size的向量

220
00:07:26,040 --> 00:07:27,560
所以你的输入的维度

221
00:07:27,560 --> 00:07:30,000
就是你MLP或者RN的输入的维度

222
00:07:30,000 --> 00:07:31,199
就是workab的size

223
00:07:32,360 --> 00:07:33,120
然后你的输出

224
00:07:33,839 --> 00:07:35,680
因为你其实就是做一个分类问题

225
00:07:35,680 --> 00:07:36,160
对吧

226
00:07:36,160 --> 00:07:36,879
你要分类

227
00:07:36,879 --> 00:07:38,600
你要去预测下一个词

228
00:07:38,920 --> 00:07:39,600
下一个词

229
00:07:39,600 --> 00:07:40,200
那是说

230
00:07:40,400 --> 00:07:43,080
你就是做一个多类分类问题

231
00:07:43,360 --> 00:07:44,240
类别的个数

232
00:07:44,240 --> 00:07:45,560
就是你的workab size

233
00:07:45,560 --> 00:07:47,080
因为你下一个词是

234
00:07:47,080 --> 00:07:48,640
可以是你的workab里面

235
00:07:48,640 --> 00:07:50,760
你的字典里面的任何一个词

236
00:07:51,439 --> 00:07:52,879
所以这就为什么说

237
00:07:52,879 --> 00:07:54,280
你的输入和输出

238
00:07:54,280 --> 00:07:56,120
它都等于workab的size

239
00:07:57,960 --> 00:08:00,360
然后我们定了一个小辅助函数

240
00:08:00,520 --> 00:08:01,160
叫做normal

241
00:08:01,360 --> 00:08:02,280
其实没干什么事情

242
00:08:02,280 --> 00:08:03,800
就是给我一个shape

243
00:08:03,879 --> 00:08:05,600
然后我生成一个

244
00:08:07,160 --> 00:08:08,240
应该是均值为0

245
00:08:08,360 --> 00:08:09,960
方差为1的一个

246
00:08:10,519 --> 00:08:11,759
一个tensor出来

247
00:08:11,759 --> 00:08:13,000
然后在你的device上面

248
00:08:13,000 --> 00:08:15,079
我们之前一直是

249
00:08:15,120 --> 00:08:16,319
最简单的初始化

250
00:08:16,360 --> 00:08:17,000
就是

251
00:08:18,439 --> 00:08:19,160
均一分布

252
00:08:19,160 --> 00:08:19,800
不是

253
00:08:19,800 --> 00:08:21,319
就是normal distribution

254
00:08:21,319 --> 00:08:23,400
然后是means均值是0

255
00:08:23,519 --> 00:08:24,000
方差为1

256
00:08:24,000 --> 00:08:25,319
然后乘一个0.1

257
00:08:25,319 --> 00:08:26,800
就是把方差变成0.01

258
00:08:26,800 --> 00:08:27,920
就是我们之前

259
00:08:27,960 --> 00:08:30,519
一直用的最简单的一个初始化函数

260
00:08:30,639 --> 00:08:31,800
所以我们把它

261
00:08:31,800 --> 00:08:33,000
因为我们要不断调用它

262
00:08:33,200 --> 00:08:35,879
所以我们就把它给写在这个地方

263
00:08:36,639 --> 00:08:36,960
好

264
00:08:36,960 --> 00:08:38,440
下面接下来看我们所有的

265
00:08:38,440 --> 00:08:40,480
那一些要的东西

266
00:08:41,240 --> 00:08:41,680
首先

267
00:08:42,560 --> 00:08:44,720
Wxh是什么

268
00:08:44,759 --> 00:08:45,600
就是

269
00:08:46,560 --> 00:08:47,960
对输入x

270
00:08:48,080 --> 00:08:49,960
然后把它映射到

271
00:08:50,600 --> 00:08:52,120
隐藏层里面的

272
00:08:52,120 --> 00:08:55,360
就隐藏边上的那一个矩阵

273
00:08:55,800 --> 00:08:57,040
它的一个维度

274
00:08:57,040 --> 00:08:58,519
当然是输入的大小

275
00:08:58,960 --> 00:09:00,200
然后另外一个它的

276
00:09:00,200 --> 00:09:00,920
另一个维度

277
00:09:00,920 --> 00:09:02,639
就是你的隐藏层的大小

278
00:09:03,440 --> 00:09:04,759
那whh是什么

279
00:09:05,480 --> 00:09:07,559
上一个时刻的隐藏边量

280
00:09:07,759 --> 00:09:11,039
变换到下一个隐藏时刻的隐藏边量

281
00:09:11,039 --> 00:09:12,080
就whh

282
00:09:12,159 --> 00:09:13,200
它就是一个

283
00:09:13,559 --> 00:09:14,960
因为你上一个隐藏边

284
00:09:14,960 --> 00:09:15,759
和下一个隐藏边

285
00:09:15,759 --> 00:09:16,759
长度都是一样的

286
00:09:16,759 --> 00:09:18,039
所以你就是从一个number of

287
00:09:18,039 --> 00:09:19,600
hindrance到number of hindrance

288
00:09:21,000 --> 00:09:21,600
bh

289
00:09:22,360 --> 00:09:23,000
就bh

290
00:09:23,120 --> 00:09:25,639
当然是你的每一个隐藏源

291
00:09:25,639 --> 00:09:26,600
它都有一个b

292
00:09:26,600 --> 00:09:28,200
那就是一个常为

293
00:09:28,240 --> 00:09:30,439
number of hindrance的向量

294
00:09:30,439 --> 00:09:31,759
然后初始化是0

295
00:09:32,000 --> 00:09:34,439
我们的偏移都是用的0

296
00:09:35,759 --> 00:09:36,480
这个东西是什么

297
00:09:36,480 --> 00:09:39,720
这个东西是你的隐藏变量

298
00:09:39,720 --> 00:09:41,799
到你输出的那一个

299
00:09:42,000 --> 00:09:43,039
w

300
00:09:43,799 --> 00:09:45,080
你可以看到是说

301
00:09:45,080 --> 00:09:46,039
number of hindrance

302
00:09:46,319 --> 00:09:47,519
到number of output

303
00:09:47,559 --> 00:09:48,039
对吧

304
00:09:48,799 --> 00:09:51,679
然后你的b就是你的bias

305
00:09:52,120 --> 00:09:53,120
所以基本上可以看到

306
00:09:53,319 --> 00:09:55,159
如果你把这一行挪掉之后

307
00:09:55,600 --> 00:09:58,480
它就是一个单隐藏层的mlp

308
00:09:58,480 --> 00:09:59,000
对吧

309
00:09:59,960 --> 00:10:00,200
对

310
00:10:00,240 --> 00:10:01,879
这是隐藏层的w和b

311
00:10:01,919 --> 00:10:04,519
这个是输出层的w和b

312
00:10:05,399 --> 00:10:07,960
然后Rn的唯一的区别

313
00:10:07,960 --> 00:10:09,120
就是加了一个这个地方

314
00:10:09,240 --> 00:10:12,159
就加了一个上一个时刻的

315
00:10:13,399 --> 00:10:14,080
隐藏变量

316
00:10:14,080 --> 00:10:15,679
到下一个时刻的隐藏变量

317
00:10:15,679 --> 00:10:16,960
都转换这个w

318
00:10:17,000 --> 00:10:18,720
这是Rn加的东西

319
00:10:18,919 --> 00:10:19,840
后面东西都是一样

320
00:10:20,200 --> 00:10:22,879
就把这些东西放在一个list里面

321
00:10:22,919 --> 00:10:23,840
然后我告诉你

322
00:10:23,840 --> 00:10:24,960
我需要算t度

323
00:10:24,960 --> 00:10:26,759
因为我们要对它进行更新

324
00:10:26,960 --> 00:10:28,960
最后返回到parents

325
00:10:29,319 --> 00:10:29,960
OK

326
00:10:30,080 --> 00:10:31,399
所以我们就解释清楚了

327
00:10:31,399 --> 00:10:32,000
这一个函数

328
00:10:32,120 --> 00:10:33,360
这也是一个关键函数

329
00:10:33,440 --> 00:10:34,600
主要看到是

330
00:10:35,360 --> 00:10:37,000
加了一些下标在里面

331
00:10:37,000 --> 00:10:38,039
然后多了一行

332
00:10:41,919 --> 00:10:42,200
好

333
00:10:42,200 --> 00:10:43,240
然后另外一个东西

334
00:10:43,360 --> 00:10:44,200
另外一个东西是什么

335
00:10:44,200 --> 00:10:49,200
就是要初始化它的隐藏状态

336
00:10:49,320 --> 00:10:51,080
就是你的隐藏变量

337
00:10:51,200 --> 00:10:51,639
为什么

338
00:10:51,759 --> 00:10:55,039
是因为你在零时刻的时候

339
00:10:55,919 --> 00:10:59,200
你没有上一刻的隐藏状态

340
00:10:59,200 --> 00:11:00,440
或者隐藏变量

341
00:11:00,759 --> 00:11:01,320
你要怎么办

342
00:11:01,480 --> 00:11:03,760
你应该给我一个初始的隐藏状态

343
00:11:04,040 --> 00:11:05,720
这样子我能够一直更新下去

344
00:11:06,440 --> 00:11:07,760
所谓的初始隐藏状态

345
00:11:07,760 --> 00:11:08,840
我们在地方知道

346
00:11:08,840 --> 00:11:11,680
我们的对每一个样本

347
00:11:11,680 --> 00:11:12,640
它的隐藏状态

348
00:11:12,640 --> 00:11:14,879
是一个常为number of hidden的

349
00:11:14,879 --> 00:11:16,160
一个向量

350
00:11:16,879 --> 00:11:18,280
但对小批量来讲

351
00:11:18,440 --> 00:11:22,080
所以它就是一个批量大小

352
00:11:22,120 --> 00:11:25,080
乘以你的隐藏大小的一个

353
00:11:26,840 --> 00:11:27,640
一个矩阵

354
00:11:27,640 --> 00:11:29,920
就是你的初始化隐藏状态

355
00:11:29,919 --> 00:11:31,439
而且你在任何时候

356
00:11:31,439 --> 00:11:32,599
你的隐藏状态的shape

357
00:11:32,599 --> 00:11:33,759
都是这个样子的

358
00:11:34,199 --> 00:11:34,919
这里的话

359
00:11:34,919 --> 00:11:36,479
我们就直接给的是零

360
00:11:36,639 --> 00:11:38,240
初始的值全部是零

361
00:11:38,439 --> 00:11:39,799
当然你也可以换成随机

362
00:11:40,079 --> 00:11:41,000
这也没关系

363
00:11:41,279 --> 00:11:43,079
我们就换成零在这个地方

364
00:11:44,079 --> 00:11:44,399
OK

365
00:11:44,399 --> 00:11:45,519
这就是隐藏状态

366
00:11:45,759 --> 00:11:47,599
另外一个地方可以看到是说

367
00:11:47,599 --> 00:11:48,719
我们把隐藏状态

368
00:11:48,959 --> 00:11:50,799
放在一个tuple里面

369
00:11:51,479 --> 00:11:54,719
因为在之后我们看到是Rn

370
00:11:54,719 --> 00:11:56,599
当然隐藏状态就是一个

371
00:11:57,919 --> 00:11:58,919
张量就没了

372
00:11:58,919 --> 00:12:01,479
但是大于之后的LSTM的话

373
00:12:01,479 --> 00:12:03,240
它有两个东西在里面

374
00:12:03,240 --> 00:12:05,000
所以为了统一化

375
00:12:05,000 --> 00:12:06,599
我们就把隐藏状态

376
00:12:06,599 --> 00:12:08,679
做成了一个tuple

377
00:12:09,000 --> 00:12:10,959
这里虽然只有一个东西

378
00:12:11,120 --> 00:12:12,240
但之后我们会有两个

379
00:12:12,240 --> 00:12:15,439
所以我们就写起来比较方便一点

380
00:12:16,679 --> 00:12:17,240
好

381
00:12:17,679 --> 00:12:18,679
就定了这两个

382
00:12:18,919 --> 00:12:21,000
第一个是初始化你的

383
00:12:22,319 --> 00:12:23,360
科学性的参数

384
00:12:23,360 --> 00:12:25,039
第二个是初始化你的隐藏状态

385
00:12:25,039 --> 00:12:27,519
那么接下来就可以做计算了

386
00:12:28,959 --> 00:12:31,519
做计算就是说Rn在这个地方干嘛

387
00:12:32,679 --> 00:12:35,399
它是这个函数

388
00:12:36,079 --> 00:12:39,079
就计算给一个小批量

389
00:12:39,279 --> 00:12:41,039
然后把你所有的

390
00:12:41,039 --> 00:12:42,719
里面的时间步都算一遍

391
00:12:42,719 --> 00:12:46,120
然后得到你的输出

392
00:12:46,959 --> 00:12:49,159
首先看到是我也给我的输入

393
00:12:49,399 --> 00:12:50,039
就是x

394
00:12:50,039 --> 00:12:51,879
x这里包括了所有的时间步

395
00:12:52,000 --> 00:12:54,039
从x0一直到xt

396
00:12:54,599 --> 00:12:55,719
然后你的state

397
00:12:55,919 --> 00:12:57,599
state就是你的

398
00:12:59,720 --> 00:13:02,920
我给我的初始化的隐藏状态

399
00:13:03,600 --> 00:13:06,040
params就是我的科学性的参数

400
00:13:06,800 --> 00:13:08,760
所以我们首先params可以展开

401
00:13:08,920 --> 00:13:11,600
展成我们之前用的命名方法

402
00:13:12,360 --> 00:13:13,520
state我们也知道

403
00:13:13,520 --> 00:13:14,600
我们是一个tuple

404
00:13:14,760 --> 00:13:16,240
其实里面就是一个h

405
00:13:16,400 --> 00:13:17,680
就没有一个

406
00:13:18,000 --> 00:13:20,240
它就是一个成为一个元素的tuple

407
00:13:21,000 --> 00:13:22,800
output我们先设成一个

408
00:13:22,800 --> 00:13:24,760
初始化成一个空的一个list

409
00:13:25,520 --> 00:13:27,000
首先看一下input

410
00:13:27,280 --> 00:13:28,160
input是什么

411
00:13:29,200 --> 00:13:30,360
input是

412
00:13:31,440 --> 00:13:33,680
时间的序列的长度

413
00:13:33,680 --> 00:13:35,040
或时间的步数

414
00:13:36,360 --> 00:13:38,040
然后批量大小

415
00:13:38,200 --> 00:13:39,960
然后就是vocab size

416
00:13:40,080 --> 00:13:42,920
是一个3D的一个tensor

417
00:13:44,680 --> 00:13:45,680
for x

418
00:13:45,680 --> 00:13:47,200
因这个函数会干嘛

419
00:13:47,440 --> 00:13:50,360
它会沿着第一个维度去变理它

420
00:13:50,560 --> 00:13:53,360
就首先拿到的是时刻0的

421
00:13:53,360 --> 00:13:54,240
那一个

422
00:13:54,560 --> 00:13:55,720
对应的x

423
00:13:55,760 --> 00:13:58,360
它就是批量大小乘以vocab size

424
00:13:59,720 --> 00:14:01,840
然后接下来就拿时刻1

425
00:14:01,840 --> 00:14:03,280
时刻1到时刻t

426
00:14:03,600 --> 00:14:05,240
这也是为什么我们之前

427
00:14:05,440 --> 00:14:06,720
进入one hot的时候

428
00:14:07,120 --> 00:14:09,800
把x做了一个转制

429
00:14:09,960 --> 00:14:11,200
这样子把时间维度

430
00:14:11,200 --> 00:14:12,080
本来是在后面

431
00:14:12,400 --> 00:14:13,800
换到了前面

432
00:14:14,240 --> 00:14:14,840
在这个地方

433
00:14:14,840 --> 00:14:18,280
我们就可以对它直接进行迭代

434
00:14:19,200 --> 00:14:21,280
所以每一步是干嘛

435
00:14:21,400 --> 00:14:22,240
每一步里面

436
00:14:22,240 --> 00:14:24,360
就是算一个特定的时间步

437
00:14:25,880 --> 00:14:26,480
OK

438
00:14:26,480 --> 00:14:27,480
所以我们看一下怎么做

439
00:14:28,440 --> 00:14:29,639
我们来把它换个行

440
00:14:29,639 --> 00:14:31,440
这样子看得清楚一点

441
00:14:34,240 --> 00:14:35,519
可以看到是说

442
00:14:35,680 --> 00:14:37,240
当前时间步

443
00:14:37,560 --> 00:14:40,200
首先我的x输入

444
00:14:40,200 --> 00:14:43,560
是跟我的wxh做矩阵乘法

445
00:14:44,399 --> 00:14:47,399
再加上我的h

446
00:14:47,399 --> 00:14:47,960
h是什么

447
00:14:47,960 --> 00:14:48,960
h是

448
00:14:49,840 --> 00:14:50,840
你可以认为是

449
00:14:50,879 --> 00:14:55,279
h是前一个时间的隐藏状态

450
00:14:55,279 --> 00:14:56,920
因为h在这个地方

451
00:14:57,200 --> 00:14:59,080
一开始是从state传过来的

452
00:14:59,080 --> 00:15:00,840
就是你的初始化隐藏状态

453
00:15:00,840 --> 00:15:01,920
所以这个地方

454
00:15:01,920 --> 00:15:03,840
h还是前一个时刻的

455
00:15:04,320 --> 00:15:07,160
然后再跟whh相乘

456
00:15:07,400 --> 00:15:09,320
再加上我的bios

457
00:15:09,760 --> 00:15:12,800
然后用的timeh作为激活函数

458
00:15:13,120 --> 00:15:17,080
得到当前的h就是ht了

459
00:15:17,280 --> 00:15:20,480
就当前时刻先更新我的隐藏状态

460
00:15:20,840 --> 00:15:23,000
所以它跟MLP唯一的不同

461
00:15:23,000 --> 00:15:24,000
就是这个地方

462
00:15:24,440 --> 00:15:27,200
这一行是轻轻加进来的

463
00:15:27,879 --> 00:15:28,399
好

464
00:15:28,399 --> 00:15:30,039
拿到h之后

465
00:15:30,039 --> 00:15:33,759
我们再跟输出层的w相乘

466
00:15:33,799 --> 00:15:34,720
加上我的偏移

467
00:15:34,720 --> 00:15:35,720
得到我的y

468
00:15:36,360 --> 00:15:36,840
好

469
00:15:36,840 --> 00:15:38,399
这里又一个不一样的地方

470
00:15:38,559 --> 00:15:39,360
就我的y

471
00:15:40,039 --> 00:15:42,840
这是当前时刻的预测

472
00:15:43,919 --> 00:15:46,080
就是说在当前时刻

473
00:15:46,080 --> 00:15:49,320
我预测下一个时刻的词是谁

474
00:15:49,720 --> 00:15:51,080
但是我有一个for loop

475
00:15:51,240 --> 00:15:54,440
所以我就是要把所有时刻的输出

476
00:15:54,440 --> 00:15:57,520
全部放在output里面

477
00:15:57,639 --> 00:15:58,720
就放在一个list里面

478
00:15:58,720 --> 00:15:59,840
就用pan的进去

479
00:16:01,320 --> 00:16:01,600
好

480
00:16:01,600 --> 00:16:02,440
pan的进去之后

481
00:16:02,440 --> 00:16:03,600
我们的输出是什么

482
00:16:03,920 --> 00:16:06,800
我们输出当时首先是你的输出

483
00:16:07,200 --> 00:16:10,400
再加上你的当前的隐藏状态

484
00:16:11,120 --> 00:16:12,879
因为如果你还要之后

485
00:16:12,879 --> 00:16:15,280
你可能还要用隐藏状态

486
00:16:15,280 --> 00:16:16,280
去传递信息的话

487
00:16:16,280 --> 00:16:17,520
我要把它输出出去

488
00:16:18,200 --> 00:16:20,120
就是输出是怎么回事

489
00:16:20,519 --> 00:16:25,080
输出就是对于所有的y之前

490
00:16:25,240 --> 00:16:26,519
每一个时刻的输出

491
00:16:26,560 --> 00:16:27,799
你就是一个批量

492
00:16:27,799 --> 00:16:30,080
大小乘以的workab的

493
00:16:30,960 --> 00:16:32,519
size的一个长度的一个东西

494
00:16:32,720 --> 00:16:33,919
就是对每一个样本

495
00:16:33,919 --> 00:16:35,399
我们预测的向量

496
00:16:35,399 --> 00:16:37,399
就是一个workab长度的向量

497
00:16:37,480 --> 00:16:38,560
这是一个多分类问题

498
00:16:38,919 --> 00:16:41,720
然后你的output全在里面

499
00:16:41,720 --> 00:16:43,039
就时刻t在里面

500
00:16:43,200 --> 00:16:44,840
然后我们在concate

501
00:16:44,840 --> 00:16:46,120
就是在拼起来

502
00:16:47,080 --> 00:16:47,919
拼起来这个东西

503
00:16:47,919 --> 00:16:49,159
有这个地方比较有意思

504
00:16:49,279 --> 00:16:50,120
拼起来是干嘛

505
00:16:50,159 --> 00:16:53,039
拼起来是在0这个维度拼起来

506
00:16:53,839 --> 00:16:54,879
所以你拼出来

507
00:16:54,879 --> 00:16:57,679
会是一个二维的一个矩阵

508
00:16:57,959 --> 00:16:59,759
然后这个矩阵的函数是什么

509
00:17:00,319 --> 00:17:01,839
函数是

510
00:17:02,959 --> 00:17:04,240
你可认为是n个矩阵

511
00:17:04,240 --> 00:17:05,440
按照数方向

512
00:17:05,440 --> 00:17:07,039
垂直方向这么拼起来

513
00:17:07,119 --> 00:17:10,079
所以你的列数没有变

514
00:17:10,079 --> 00:17:10,759
你的列数

515
00:17:10,759 --> 00:17:12,839
还是你的workab的size

516
00:17:13,559 --> 00:17:14,920
然后你的函数变了

517
00:17:14,920 --> 00:17:16,599
你的函数变成了

518
00:17:16,599 --> 00:17:19,559
批量大小乘以10的长度

519
00:17:21,039 --> 00:17:21,639
OK

520
00:17:22,159 --> 00:17:23,199
所以我们记住这一点

521
00:17:23,399 --> 00:17:24,399
我们之后来看

522
00:17:24,399 --> 00:17:25,639
为什么要这么拼

523
00:17:26,240 --> 00:17:28,399
然后当然我们要输出更新

524
00:17:28,399 --> 00:17:30,799
我们的更新后的隐藏状态

525
00:17:31,079 --> 00:17:32,000
就是这个地方

526
00:17:32,399 --> 00:17:32,679
OK

527
00:17:32,679 --> 00:17:35,480
这就是RN的函数干的事情

528
00:17:35,759 --> 00:17:36,559
说白了

529
00:17:36,919 --> 00:17:39,399
有点像我们之前MLP的

530
00:17:39,399 --> 00:17:40,759
那个forward的函数

531
00:17:40,799 --> 00:17:41,879
但是不一样的是说

532
00:17:41,879 --> 00:17:42,719
我这个地方

533
00:17:43,039 --> 00:17:44,039
就几个地方不一样

534
00:17:44,480 --> 00:17:45,679
input里面

535
00:17:45,680 --> 00:17:48,200
不再是一个样本

536
00:17:48,440 --> 00:17:49,480
不再是一个

537
00:17:50,400 --> 00:17:51,840
批量大小乘以样本

538
00:17:52,080 --> 00:17:53,480
这个地方还有个时间维度

539
00:17:53,480 --> 00:17:54,240
其实你认为

540
00:17:54,240 --> 00:17:56,320
如果你的时间不长是t的话

541
00:17:56,320 --> 00:17:58,240
这里面其实有t个样本在里面

542
00:17:59,000 --> 00:17:59,600
第二个是说

543
00:17:59,600 --> 00:18:00,840
我们有个状态

544
00:18:01,039 --> 00:18:01,920
隐藏状态

545
00:18:01,920 --> 00:18:04,000
我们既要输入隐藏状态

546
00:18:04,000 --> 00:18:04,960
我们也要输出

547
00:18:04,960 --> 00:18:06,759
我们最后更新的隐藏状态

548
00:18:07,279 --> 00:18:09,680
这个是当然是我的parameters

549
00:18:09,880 --> 00:18:10,920
所以在这里面的话

550
00:18:10,920 --> 00:18:13,400
我首先要对所有的t个样本

551
00:18:13,400 --> 00:18:14,240
做一次编译

552
00:18:14,559 --> 00:18:16,359
然后在更新h的时候

553
00:18:16,759 --> 00:18:19,200
我们要去使用

554
00:18:19,200 --> 00:18:20,640
前一个时刻的h

555
00:18:20,640 --> 00:18:22,880
来计算我当前的h

556
00:18:23,120 --> 00:18:24,480
然后最后输出的时候

557
00:18:24,519 --> 00:18:29,120
把所有t个样本的输出

558
00:18:29,240 --> 00:18:30,839
全部放在一起

559
00:18:30,839 --> 00:18:33,240
放成一个东西输出出去

560
00:18:33,519 --> 00:18:35,079
然后返回我们的

561
00:18:35,079 --> 00:18:36,200
更新后的隐藏状态

562
00:18:36,599 --> 00:18:39,079
这就是RN跟之前

563
00:18:39,079 --> 00:18:42,359
我们MLP的类对比的区别

564
00:18:43,200 --> 00:18:47,520
然后基本上可以看到

565
00:18:47,520 --> 00:18:49,000
跟我们之前公式

566
00:18:49,640 --> 00:18:50,440
就长得差不多

567
00:18:50,440 --> 00:18:50,960
对吧

568
00:18:51,720 --> 00:18:52,160
OK

569
00:18:52,160 --> 00:18:54,400
就我们刚刚讲的公式

570
00:18:54,400 --> 00:18:57,680
其实是直接映射成代码的样子

571
00:19:00,200 --> 00:19:00,720
好

572
00:19:00,720 --> 00:19:02,560
然后我们来创造一个类

573
00:19:02,760 --> 00:19:05,040
来包装一下这个函数

574
00:19:05,920 --> 00:19:08,120
可以看到是类干嘛

575
00:19:08,280 --> 00:19:09,280
这个类就是

576
00:19:09,400 --> 00:19:11,120
我要存下我的capsize

577
00:19:11,120 --> 00:19:12,120
我的hidden

578
00:19:12,120 --> 00:19:13,880
我的隐藏层的大小

579
00:19:13,920 --> 00:19:17,440
然后把我的parameters拿好

580
00:19:17,480 --> 00:19:19,280
然后另外一个是一个

581
00:19:19,320 --> 00:19:20,640
初始的状态

582
00:19:21,080 --> 00:19:23,440
初始的隐藏状态是谁

583
00:19:23,840 --> 00:19:26,080
然后接下来我说

584
00:19:26,080 --> 00:19:27,920
我的forward的函数是谁

585
00:19:27,920 --> 00:19:28,680
forward的函数

586
00:19:28,680 --> 00:19:30,040
就是刚刚我们的

587
00:19:30,760 --> 00:19:32,320
定义的RN函数

588
00:19:32,560 --> 00:19:33,800
因为之后我们会讲

589
00:19:33,800 --> 00:19:35,560
别的不一样的更新的法则

590
00:19:35,560 --> 00:19:36,840
URL STM

591
00:19:37,200 --> 00:19:38,640
所以我们尽量这个东西

592
00:19:38,640 --> 00:19:40,120
做的比较通用一点

593
00:19:40,120 --> 00:19:41,280
就是说让你传入

594
00:19:41,280 --> 00:19:43,720
你的初始化状态的函数

595
00:19:43,720 --> 00:19:45,720
和你的forward的函数

596
00:19:46,640 --> 00:19:46,920
好

597
00:19:46,920 --> 00:19:49,080
我们这个模型怎么做forward

598
00:19:49,240 --> 00:19:51,800
就是你可以既定

599
00:19:52,200 --> 00:19:53,120
我们之前有讲过

600
00:19:53,200 --> 00:19:54,000
你可以

601
00:19:54,040 --> 00:19:55,480
括一个forward函数

602
00:19:55,480 --> 00:19:56,880
你也可以制定core

603
00:19:56,880 --> 00:19:57,560
就是python里面

604
00:19:57,560 --> 00:19:59,120
那个core函数也可以

605
00:20:00,200 --> 00:20:01,440
然后给定x

606
00:20:02,160 --> 00:20:04,320
这个x跟我们之前的input不一样

607
00:20:04,520 --> 00:20:05,960
这个x就是一个

608
00:20:05,960 --> 00:20:08,280
我们时间机器

609
00:20:08,559 --> 00:20:10,639
数据集漏的进来的x

610
00:20:11,119 --> 00:20:13,480
它就是一个批量大小

611
00:20:13,519 --> 00:20:14,879
乘以时间部署

612
00:20:14,879 --> 00:20:15,480
时间部署

613
00:20:15,480 --> 00:20:16,839
就是你序列的长度

614
00:20:16,960 --> 00:20:18,079
就是一句话的长度

615
00:20:18,079 --> 00:20:18,839
那个东西

616
00:20:19,559 --> 00:20:21,200
然后是你的state

617
00:20:21,799 --> 00:20:22,039
好

618
00:20:22,039 --> 00:20:23,319
我们看一下我们在干嘛

619
00:20:23,519 --> 00:20:26,039
我们把x转制

620
00:20:27,240 --> 00:20:28,639
做到one hot里面

621
00:20:29,079 --> 00:20:30,399
然后你的one hot

622
00:20:30,399 --> 00:20:31,119
你的大小

623
00:20:31,119 --> 00:20:32,519
应该是我capital size

624
00:20:33,319 --> 00:20:34,879
然后我们当然还要

625
00:20:35,200 --> 00:20:36,639
因为它会one hot出来

626
00:20:36,639 --> 00:20:37,480
是一个整形

627
00:20:37,599 --> 00:20:38,880
我们会变成一个

628
00:20:38,920 --> 00:20:40,279
浮点形在这个地方

629
00:20:40,920 --> 00:20:41,759
所以这样子的话

630
00:20:41,759 --> 00:20:42,920
我的x就会拉

631
00:20:42,920 --> 00:20:45,120
变成了一个长度

632
00:20:45,120 --> 00:20:46,839
就是number steps

633
00:20:46,839 --> 00:20:47,720
或者叫t

634
00:20:48,079 --> 00:20:49,599
乘以我的批量大小

635
00:20:49,920 --> 00:20:51,319
乘以我的cap size的

636
00:20:51,319 --> 00:20:52,160
那个input

637
00:20:52,559 --> 00:20:53,519
然后这样子

638
00:20:53,519 --> 00:20:54,440
我们就可以放到

639
00:20:54,440 --> 00:20:55,360
我们的

640
00:20:55,559 --> 00:20:56,440
rn的

641
00:20:56,680 --> 00:20:58,120
刚刚定的函数里面

642
00:20:58,120 --> 00:20:59,079
就可以让你的

643
00:20:59,079 --> 00:21:00,759
我们现在叫做forward function

644
00:21:01,200 --> 00:21:01,880
放在里面

645
00:21:01,880 --> 00:21:03,160
可以得到我们的

646
00:21:03,559 --> 00:21:05,960
输出和更新后的状态

647
00:21:06,960 --> 00:21:07,519
另外一个

648
00:21:07,519 --> 00:21:08,680
它当然有一个

649
00:21:08,960 --> 00:21:10,480
说我的初始状态是什么

650
00:21:10,680 --> 00:21:11,759
初始状态就是

651
00:21:12,519 --> 00:21:13,319
就是调用我们

652
00:21:14,920 --> 00:21:15,600
放进去的

653
00:21:15,799 --> 00:21:17,160
init state的函数

654
00:21:17,160 --> 00:21:18,480
就是我们刚刚定的函数

655
00:21:18,480 --> 00:21:20,000
我们就传过来

656
00:21:20,000 --> 00:21:21,079
然后调用它

657
00:21:21,079 --> 00:21:22,799
会得到我们的输出

658
00:21:22,960 --> 00:21:23,519
我们

659
00:21:23,600 --> 00:21:24,559
虽然我们是这里

660
00:21:24,559 --> 00:21:25,440
有点点遮挡

661
00:21:25,840 --> 00:21:27,079
但实际上就是

662
00:21:27,200 --> 00:21:28,279
没什么太多区别

663
00:21:28,279 --> 00:21:30,160
就是调用刚刚init函数

664
00:21:30,360 --> 00:21:31,319
批量大小

665
00:21:31,640 --> 00:21:32,559
隐藏大小

666
00:21:32,559 --> 00:21:34,160
和你的device就行了

667
00:21:35,240 --> 00:21:35,720
OK

668
00:21:35,720 --> 00:21:36,440
所以这个函数

669
00:21:36,720 --> 00:21:38,519
这个类其实就是包了一下

670
00:21:39,279 --> 00:21:41,720
就把核心是把刚刚那两个

671
00:21:41,720 --> 00:21:42,400
定的两个函数

672
00:21:42,400 --> 00:21:42,880
包进来

673
00:21:42,880 --> 00:21:43,880
包成一个类似的

674
00:21:43,880 --> 00:21:45,200
我们可以做成一个

675
00:21:46,400 --> 00:21:48,600
做成一个module来计算

676
00:21:49,000 --> 00:21:49,559
OK

677
00:21:50,880 --> 00:21:51,600
好

678
00:21:52,120 --> 00:21:52,279
好

679
00:21:52,279 --> 00:21:53,880
我们来看一个样例

680
00:21:54,039 --> 00:21:55,519
看一下我们的输出

681
00:21:55,680 --> 00:21:57,559
是不是符合我们的要求

682
00:21:59,000 --> 00:22:00,720
首先我们就说

683
00:22:00,720 --> 00:22:01,720
你的隐藏大小

684
00:22:01,720 --> 00:22:03,600
我们设了一个512的这个地方

685
00:22:04,600 --> 00:22:06,280
然后我们说定义了

686
00:22:06,280 --> 00:22:06,920
我们的模型

687
00:22:07,920 --> 00:22:09,400
我的模型一个是

688
00:22:10,840 --> 00:22:11,400
Vocab

689
00:22:11,400 --> 00:22:12,160
就是

690
00:22:12,360 --> 00:22:13,520
你的输出

691
00:22:14,080 --> 00:22:16,320
输出的就是你的Vocab的大小

692
00:22:16,480 --> 00:22:17,240
你的hidden

693
00:22:17,360 --> 00:22:18,400
然后我们使用

694
00:22:18,400 --> 00:22:20,200
尽量使用GPU

695
00:22:20,480 --> 00:22:21,440
然后说

696
00:22:22,360 --> 00:22:24,320
你的定义你的

697
00:22:25,880 --> 00:22:28,400
可学习的参数的函数getParents

698
00:22:28,440 --> 00:22:29,840
然后你的初始状态

699
00:22:29,840 --> 00:22:31,360
是init RNN state

700
00:22:31,360 --> 00:22:32,680
因为我们这是RNN记得

701
00:22:32,960 --> 00:22:34,160
因为之后我们会说

702
00:22:34,160 --> 00:22:37,200
会把这个RNN换成grurlstm

703
00:22:38,560 --> 00:22:39,680
另外一个是说

704
00:22:40,039 --> 00:22:42,080
给我输入之后

705
00:22:42,080 --> 00:22:43,320
怎么样计算输出

706
00:22:43,560 --> 00:22:44,279
for的函数

707
00:22:44,320 --> 00:22:45,400
都是这个函数

708
00:22:45,400 --> 00:22:45,960
这三个函数

709
00:22:45,960 --> 00:22:48,000
是我们刚刚讲过的

710
00:22:48,759 --> 00:22:49,440
好

711
00:22:49,799 --> 00:22:52,360
然后我们就可以调用

712
00:22:52,519 --> 00:22:54,200
Net的begin state

713
00:22:54,200 --> 00:22:55,720
就是初始化一个状态出来

714
00:22:55,720 --> 00:22:56,840
就是那个隐藏状态

715
00:22:56,840 --> 00:22:58,080
就是时刻0

716
00:22:59,360 --> 00:23:01,000
之前的那个隐藏状态

717
00:23:01,640 --> 00:23:03,559
然后我们给一个x

718
00:23:04,279 --> 00:23:05,599
x回一下是什么

719
00:23:06,720 --> 00:23:10,359
x是一个2×5的一个向量

720
00:23:10,440 --> 00:23:12,400
我们刚刚2×5的矩阵

721
00:23:12,400 --> 00:23:13,640
我们刚刚定义了

722
00:23:14,559 --> 00:23:16,559
然后我们放到GPU上

723
00:23:16,599 --> 00:23:17,720
然后把state拉进去

724
00:23:17,720 --> 00:23:19,640
会得到y和一个new state

725
00:23:20,200 --> 00:23:20,400
好

726
00:23:20,400 --> 00:23:21,160
我们来看一下

727
00:23:21,160 --> 00:23:22,279
就是我们来看一下

728
00:23:22,279 --> 00:23:22,880
我们这些东西

729
00:23:22,880 --> 00:23:23,960
是不是我们要的

730
00:23:24,359 --> 00:23:25,799
首先y的形状

731
00:23:26,319 --> 00:23:27,799
y的形状是什么

732
00:23:28,079 --> 00:23:29,599
y的形状理论上

733
00:23:29,600 --> 00:23:31,400
是一个二维的一个矩阵

734
00:23:31,560 --> 00:23:32,480
它的第一维度

735
00:23:32,480 --> 00:23:34,040
是你的批量大小

736
00:23:34,040 --> 00:23:36,120
乘以你的时间的部署

737
00:23:36,800 --> 00:23:38,000
就是x是一个2×5

738
00:23:38,120 --> 00:23:40,040
所以你就是

739
00:23:40,200 --> 00:23:41,240
它就会变成一个10

740
00:23:41,360 --> 00:23:42,200
就是2×5

741
00:23:43,760 --> 00:23:45,320
然后你的第二个维度

742
00:23:45,320 --> 00:23:48,880
就是对于x中的每一个词

743
00:23:49,760 --> 00:23:51,520
它的下一个词的预测向量

744
00:23:51,520 --> 00:23:53,280
就是一个长远28的一个向量

745
00:23:53,760 --> 00:23:54,840
说白了你是干嘛的

746
00:23:54,840 --> 00:23:55,200
说白了

747
00:23:55,200 --> 00:23:55,920
就是说

748
00:23:56,840 --> 00:23:58,400
你的x在这个地方

749
00:23:58,560 --> 00:24:01,040
我们对x中间的每一个词

750
00:24:01,080 --> 00:24:01,759
去预测

751
00:24:01,759 --> 00:24:03,280
它下一个词是谁

752
00:24:03,440 --> 00:24:04,720
所以x有20×5

753
00:24:04,720 --> 00:24:06,120
所以它里面有10个词

754
00:24:06,160 --> 00:24:08,400
那么的y就是每一个词

755
00:24:08,480 --> 00:24:10,000
就这10个词

756
00:24:10,000 --> 00:24:10,759
对每一个词

757
00:24:10,759 --> 00:24:11,840
我的预测就是

758
00:24:12,840 --> 00:24:14,480
预测就是28

759
00:24:15,320 --> 00:24:16,519
你在28个里面选

760
00:24:16,640 --> 00:24:18,160
就是一个28类的分类

761
00:24:18,960 --> 00:24:19,519
OK

762
00:24:21,880 --> 00:24:23,320
接下来就是你的state

763
00:24:23,440 --> 00:24:24,360
state它是一个

764
00:24:24,360 --> 00:24:25,720
乘为1的一个tuple

765
00:24:25,840 --> 00:24:27,600
然后里面有一个

766
00:24:27,880 --> 00:24:30,840
state里面有一个东西

767
00:24:30,840 --> 00:24:33,160
它就是一个p让大小

768
00:24:33,160 --> 00:24:35,240
乘以你的隐藏元

769
00:24:35,560 --> 00:24:37,000
隐藏大小的一个

770
00:24:37,640 --> 00:24:38,960
更新后的隐藏状态

771
00:24:39,000 --> 00:24:41,560
它就是一个二维的一个矩阵

772
00:24:42,640 --> 00:24:42,960
OK

773
00:24:42,960 --> 00:24:43,840
就是验证一下

774
00:24:43,840 --> 00:24:45,800
我们的定义的函数

775
00:24:45,800 --> 00:24:46,400
是不是正确

776
00:24:46,560 --> 00:24:48,480
形状上是不是符合要求

777
00:24:49,440 --> 00:24:50,080
好

778
00:24:50,760 --> 00:24:52,960
接下来我们就可以来

779
00:24:53,040 --> 00:24:55,880
定义一个预测函数

780
00:24:55,880 --> 00:24:57,240
我们先讲预测

781
00:24:57,240 --> 00:24:58,160
再讲输出

782
00:24:58,760 --> 00:24:59,960
预测是干嘛呢

783
00:25:00,720 --> 00:25:01,800
预测我们看一下

784
00:25:01,800 --> 00:25:03,040
就是说预测是干嘛

785
00:25:04,000 --> 00:25:05,960
预测是说prefix

786
00:25:05,960 --> 00:25:06,440
是什么东西

787
00:25:06,440 --> 00:25:07,440
就prefix是说

788
00:25:07,440 --> 00:25:10,480
我给你一段句子的开头

789
00:25:10,680 --> 00:25:12,440
我让你根据开头

790
00:25:12,560 --> 00:25:14,400
跟我一直生成输出

791
00:25:14,680 --> 00:25:16,960
接下来的那些词

792
00:25:18,440 --> 00:25:19,560
number of predicts

793
00:25:19,560 --> 00:25:20,960
是说我需要你

794
00:25:21,320 --> 00:25:23,120
给我生成多少个词

795
00:25:24,920 --> 00:25:25,760
当然这个地方

796
00:25:25,759 --> 00:25:27,319
我们的用的是字符

797
00:25:27,440 --> 00:25:28,319
所以词和字符

798
00:25:28,319 --> 00:25:29,519
我们就没有区别了

799
00:25:29,519 --> 00:25:31,039
其实这里面就是一个chunk

800
00:25:31,599 --> 00:25:33,200
然后net就是我们训练的模型

801
00:25:33,519 --> 00:25:34,759
我开不就是我们的

802
00:25:34,759 --> 00:25:35,119
这样子

803
00:25:35,119 --> 00:25:37,200
我们可以把预测值

804
00:25:37,200 --> 00:25:38,920
可以map成我的那一个

805
00:25:39,440 --> 00:25:41,079
真实的字符串词

806
00:25:41,279 --> 00:25:43,359
device就是你的GPU或者CPU

807
00:25:44,200 --> 00:25:44,559
OK

808
00:25:44,559 --> 00:25:45,119
我们看一下

809
00:25:45,119 --> 00:25:45,759
先看一下

810
00:25:45,879 --> 00:25:46,960
predict怎么做

811
00:25:48,079 --> 00:25:49,240
predict怎么做呢

812
00:25:49,240 --> 00:25:50,640
首先我们要生成一个

813
00:25:50,640 --> 00:25:52,200
在任何干什么事情之前

814
00:25:52,200 --> 00:25:53,160
我们得生成一个

815
00:25:53,160 --> 00:25:54,200
我们的初始状态

816
00:25:54,200 --> 00:25:56,039
就初始的隐藏状态

817
00:25:56,840 --> 00:25:57,840
然后这个地方

818
00:25:57,960 --> 00:25:58,759
batch size等于1

819
00:25:58,759 --> 00:26:01,039
因为我们就是对一个字符串做预测

820
00:26:01,720 --> 00:26:02,920
device是GPU

821
00:26:03,840 --> 00:26:04,440
好

822
00:26:04,720 --> 00:26:06,160
接下来的output

823
00:26:08,080 --> 00:26:09,039
output是什么

824
00:26:09,039 --> 00:26:09,960
output的这个地方

825
00:26:10,000 --> 00:26:11,640
我们其实做的是

826
00:26:13,319 --> 00:26:14,680
output的那一些

827
00:26:14,680 --> 00:26:16,240
应该是我们做的是

828
00:26:16,240 --> 00:26:19,080
我们把那些下标存在哪里

829
00:26:19,080 --> 00:26:20,559
就是每个字符串

830
00:26:20,559 --> 00:26:21,440
在我cap里面

831
00:26:21,440 --> 00:26:22,240
对应的下标

832
00:26:22,400 --> 00:26:23,400
所以就是说

833
00:26:23,680 --> 00:26:25,840
把你的prefix

834
00:26:25,960 --> 00:26:26,960
第一个字

835
00:26:27,000 --> 00:26:27,400
词

836
00:26:27,480 --> 00:26:28,640
就第一个字符

837
00:26:29,120 --> 00:26:30,560
放到我cap里面

838
00:26:30,680 --> 00:26:33,160
拿到它对应的整形的下标

839
00:26:33,200 --> 00:26:34,240
放在output里面

840
00:26:34,240 --> 00:26:36,360
所以output一开始就是一个

841
00:26:36,360 --> 00:26:37,200
一个词

842
00:26:39,280 --> 00:26:40,640
get input就是一个小函数

843
00:26:40,640 --> 00:26:41,320
就是说

844
00:26:41,320 --> 00:26:42,720
每一次把你的

845
00:26:43,800 --> 00:26:44,280
把你

846
00:26:44,280 --> 00:26:44,640
就是说

847
00:26:44,640 --> 00:26:45,560
每一次我们怎么get

848
00:26:45,560 --> 00:26:47,400
把output最后那一个词

849
00:26:47,400 --> 00:26:47,960
存下来

850
00:26:47,960 --> 00:26:49,400
就当预测完的

851
00:26:49,920 --> 00:26:51,560
最近预测那一个词

852
00:26:51,559 --> 00:26:53,440
作为下一个时刻的输入

853
00:26:53,519 --> 00:26:53,960
对吧

854
00:26:53,960 --> 00:26:55,119
所以output的-1

855
00:26:55,119 --> 00:26:57,039
就是最近预测那个词

856
00:26:57,159 --> 00:26:58,440
做成一个tensor

857
00:26:58,639 --> 00:27:00,440
然后它的shape是1

858
00:27:00,440 --> 00:27:02,359
就说

859
00:27:02,399 --> 00:27:04,079
批量大小是1

860
00:27:04,079 --> 00:27:05,559
然后你的时间不长是1

861
00:27:05,559 --> 00:27:07,200
所以你的input就是一个

862
00:27:07,200 --> 00:27:08,919
一个1乘1的一个矩阵

863
00:27:09,599 --> 00:27:09,960
好

864
00:27:09,960 --> 00:27:11,279
我们先来看一下

865
00:27:11,480 --> 00:27:14,839
说我们对于prefix里面的

866
00:27:15,159 --> 00:27:16,240
你给我一个prefix

867
00:27:16,240 --> 00:27:17,279
给我一段话

868
00:27:17,399 --> 00:27:19,159
我们其实是

869
00:27:19,599 --> 00:27:21,319
我们只能做一个词预测

870
00:27:21,319 --> 00:27:21,799
对吧

871
00:27:22,079 --> 00:27:23,759
所以我们要首先

872
00:27:23,799 --> 00:27:25,839
我们0已经放在output里面了

873
00:27:25,879 --> 00:27:27,480
然后我们接下来对于

874
00:27:27,519 --> 00:27:28,480
后面的那一些

875
00:27:28,519 --> 00:27:29,480
那一段词

876
00:27:29,519 --> 00:27:30,480
先编译一遍

877
00:27:31,799 --> 00:27:34,519
所以每一次拿到get input

878
00:27:34,839 --> 00:27:35,559
所以这个地方

879
00:27:35,559 --> 00:27:36,359
第一次call

880
00:27:36,359 --> 00:27:37,879
就是调用了prefix0

881
00:27:38,000 --> 00:27:39,079
因为我们把这个东西

882
00:27:39,079 --> 00:27:40,399
就存在这个地方了

883
00:27:40,919 --> 00:27:42,799
然后丢进net里面

884
00:27:43,919 --> 00:27:45,480
但我们不care输出

885
00:27:46,079 --> 00:27:46,480
为什么

886
00:27:46,559 --> 00:27:47,679
因为我们说

887
00:27:47,679 --> 00:27:48,839
你给我一段词

888
00:27:48,839 --> 00:27:50,200
比如说hello world

889
00:27:50,680 --> 00:27:51,440
就是或你好

890
00:27:51,720 --> 00:27:53,319
你好说这两个词

891
00:27:53,519 --> 00:27:55,840
那么在预测

892
00:27:55,840 --> 00:27:57,680
我先用你来预测好的时候

893
00:27:58,120 --> 00:27:59,640
但是你已经告诉我标准答案了

894
00:27:59,640 --> 00:28:01,920
所以我不需要把我的预测存下来

895
00:28:01,920 --> 00:28:03,360
我唯一干的事情就是说

896
00:28:03,360 --> 00:28:04,920
我通过你好这个东西

897
00:28:05,080 --> 00:28:06,920
来初始化我的状态

898
00:28:06,920 --> 00:28:09,160
就是把你的prefix的东西

899
00:28:09,160 --> 00:28:09,920
那些信息

900
00:28:10,279 --> 00:28:12,200
放进我的state里面

901
00:28:13,400 --> 00:28:14,080
OK

902
00:28:14,840 --> 00:28:16,279
然后所以你看我的output

903
00:28:16,480 --> 00:28:18,000
output是不适用的

904
00:28:18,559 --> 00:28:20,319
是用的你的真实的

905
00:28:20,480 --> 00:28:21,759
给我的prefix

906
00:28:22,119 --> 00:28:23,480
output是用的我的预测

907
00:28:23,680 --> 00:28:24,680
因为我有真实值

908
00:28:24,680 --> 00:28:26,160
所以我没必要用我自己的预测

909
00:28:26,160 --> 00:28:28,240
这样子我就不用累积误差了

910
00:28:28,599 --> 00:28:29,240
好

911
00:28:29,599 --> 00:28:31,039
当我把这一行

912
00:28:31,039 --> 00:28:32,319
就把prefix里面

913
00:28:32,319 --> 00:28:33,799
就你告诉我前缀

914
00:28:35,359 --> 00:28:36,640
放存进

915
00:28:36,640 --> 00:28:39,200
把所有的信息存进state里面之后

916
00:28:39,359 --> 00:28:41,039
我们就可以真正的预测了

917
00:28:41,319 --> 00:28:42,680
因为我们要n个词

918
00:28:42,839 --> 00:28:43,720
我们就是说

919
00:28:43,720 --> 00:28:45,359
你需要number of predict的词

920
00:28:45,440 --> 00:28:47,200
我们就是做那么多次

921
00:28:48,000 --> 00:28:50,160
每一次就是把前一个时刻的

922
00:28:51,920 --> 00:28:52,839
预测

923
00:28:53,839 --> 00:28:55,440
做成输入放进来

924
00:28:55,640 --> 00:28:57,599
然后更新我的state

925
00:28:57,640 --> 00:28:59,319
然后拿到我的输出y

926
00:29:00,119 --> 00:29:00,720
y的话

927
00:29:00,720 --> 00:29:01,200
你可以认为

928
00:29:01,200 --> 00:29:01,960
现在就是一个

929
00:29:01,960 --> 00:29:02,759
1乘以一个

930
00:29:02,759 --> 00:29:04,240
我cap size的一个向量

931
00:29:04,440 --> 00:29:06,960
然后我们要去把它

932
00:29:06,960 --> 00:29:08,039
就是做分类

933
00:29:08,160 --> 00:29:09,039
说白了就是个分类

934
00:29:09,240 --> 00:29:11,279
我们就把index拿出来

935
00:29:12,119 --> 00:29:13,960
我们这里挡住了一下

936
00:29:13,960 --> 00:29:14,640
就是

937
00:29:14,799 --> 00:29:17,119
把它最大的坐标拿出来

938
00:29:17,120 --> 00:29:19,200
然后reshape成一个标量

939
00:29:19,600 --> 00:29:20,720
直接转成整形

940
00:29:20,720 --> 00:29:22,040
然后放到output里面

941
00:29:22,560 --> 00:29:23,160
Ok

942
00:29:23,400 --> 00:29:25,040
所以这个东西

943
00:29:25,360 --> 00:29:27,480
就是我们当前

944
00:29:28,000 --> 00:29:30,320
给定当前词和之前看到的东西

945
00:29:30,360 --> 00:29:31,960
预测下一个词是谁

946
00:29:31,960 --> 00:29:33,840
那个下一个词堆的小标

947
00:29:33,880 --> 00:29:34,840
就放到output里面

948
00:29:34,840 --> 00:29:35,800
我们一直做

949
00:29:35,800 --> 00:29:38,760
一直做你的number of predict的词

950
00:29:39,920 --> 00:29:40,640
最后干嘛

951
00:29:40,640 --> 00:29:41,680
最后就是把小标

952
00:29:41,840 --> 00:29:43,360
把index转成token

953
00:29:43,840 --> 00:29:45,040
就是调用我cap的东西

954
00:29:45,040 --> 00:29:47,080
把我那些整形转成我字符串

955
00:29:47,400 --> 00:29:48,440
最后用一个空字符

956
00:29:48,440 --> 00:29:49,160
把它join起来

957
00:29:49,160 --> 00:29:50,160
然后就输出了

958
00:29:50,320 --> 00:29:51,600
你可以看到是说

959
00:29:51,920 --> 00:29:53,520
你看所有的output里面

960
00:29:53,880 --> 00:29:54,720
i在里面

961
00:29:54,720 --> 00:29:56,560
然后转成index转成token

962
00:29:56,600 --> 00:29:57,680
然后join起来

963
00:29:57,880 --> 00:29:58,480
Ok

964
00:29:58,520 --> 00:29:59,720
所以这就是predict

965
00:30:00,040 --> 00:30:01,120
看一下效果

966
00:30:01,320 --> 00:30:02,520
看下效果就是说

967
00:30:02,800 --> 00:30:04,760
我们记得time traveler

968
00:30:04,800 --> 00:30:05,720
就是时间机器

969
00:30:05,720 --> 00:30:06,560
那本书

970
00:30:06,680 --> 00:30:08,040
就是最常见的一个词

971
00:30:08,040 --> 00:30:09,400
就是time traveler

972
00:30:09,440 --> 00:30:10,320
就是时间

973
00:30:10,920 --> 00:30:13,480
然后我们把它作为前缀

974
00:30:13,920 --> 00:30:15,440
让你接着往下写

975
00:30:15,440 --> 00:30:16,840
这就是prefix

976
00:30:17,120 --> 00:30:18,240
所以这个参数干的事情

977
00:30:18,240 --> 00:30:20,920
就是先把它编了一遍

978
00:30:20,960 --> 00:30:22,120
然后把所有的信息

979
00:30:22,120 --> 00:30:23,240
存在state里面

980
00:30:23,240 --> 00:30:26,000
然后再往后面预测10个词

981
00:30:26,720 --> 00:30:28,280
后面这些参数都是填进去的

982
00:30:29,080 --> 00:30:29,920
可以看一下效果

983
00:30:30,080 --> 00:30:30,960
就predict出来

984
00:30:30,960 --> 00:30:31,680
就是

985
00:30:31,720 --> 00:30:32,880
你看到就是

986
00:30:33,080 --> 00:30:34,280
对你还没训练

987
00:30:34,320 --> 00:30:34,560
对吧

988
00:30:34,560 --> 00:30:35,120
我的net

989
00:30:35,120 --> 00:30:36,560
就是一个随机数字化的东西

990
00:30:36,560 --> 00:30:37,680
所以后面的预测词

991
00:30:37,680 --> 00:30:38,440
都是一些乱码

992
00:30:38,880 --> 00:30:40,240
还有unknow的token出来

993
00:30:40,240 --> 00:30:40,680
对吧

994
00:30:40,840 --> 00:30:43,640
所以反正一直在乱猜这个地方

995
00:30:43,800 --> 00:30:44,360
Ok

996
00:30:44,640 --> 00:30:45,680
所以我们接下来就是说

997
00:30:45,680 --> 00:30:46,760
我们要训练模型

998
00:30:46,759 --> 00:30:48,480
然后不断的去用这个函数

999
00:30:48,480 --> 00:30:49,480
来看一下

1000
00:30:49,480 --> 00:30:51,400
我们训练模型的效果

1001
00:30:51,400 --> 00:30:52,799
就是说你可以眼睛看一眼

1002
00:30:52,799 --> 00:30:54,160
说预测的那些

1003
00:30:54,200 --> 00:30:56,400
往下写的东西是不是靠谱

1004
00:30:56,400 --> 00:30:58,240
这就是我们这个函数干的事情

1005
00:30:59,160 --> 00:30:59,559
Ok

1006
00:30:59,559 --> 00:31:00,519
定义了这个函数之后

1007
00:31:01,319 --> 00:31:03,960
我们就可以定义另外一个辅助函数

1008
00:31:04,079 --> 00:31:06,160
就是做t度减材

1009
00:31:07,440 --> 00:31:08,599
我们之前有讲过

1010
00:31:08,680 --> 00:31:09,720
t度减材这个东西

1011
00:31:11,599 --> 00:31:12,440
因为RN

1012
00:31:12,599 --> 00:31:14,519
因为你是可以刚刚看到

1013
00:31:15,119 --> 00:31:16,799
我给你一个小批量进去

1014
00:31:16,799 --> 00:31:18,480
就算是一个批量

1015
00:31:18,480 --> 00:31:19,119
我没有

1016
00:31:19,119 --> 00:31:20,519
我就一个单隐藏词

1017
00:31:20,559 --> 00:31:22,599
但是我会做t步

1018
00:31:22,759 --> 00:31:24,119
就做t步迭代

1019
00:31:24,319 --> 00:31:25,559
那个t就是你那个长度

1020
00:31:25,639 --> 00:31:26,839
就35在这个地方

1021
00:31:27,440 --> 00:31:29,039
所以你就等价于说

1022
00:31:29,039 --> 00:31:31,920
有点像一个长度为35层的一个MLP

1023
00:31:32,000 --> 00:31:35,000
因为你有35个矩阵乘法在里面

1024
00:31:35,079 --> 00:31:36,039
至少是35个

1025
00:31:36,799 --> 00:31:37,920
但其实更多一点

1026
00:31:38,359 --> 00:31:40,879
因为每一次你得做几个矩阵的乘法

1027
00:31:41,359 --> 00:31:44,000
所以它会容易有t度爆炸

1028
00:31:44,599 --> 00:31:45,599
t度爆炸的一个办法

1029
00:31:45,599 --> 00:31:47,720
就是说我把p度映射回去

1030
00:31:48,559 --> 00:31:49,879
这个公式我们之前讲过

1031
00:31:50,000 --> 00:31:51,480
我们就看一下它的实现

1032
00:31:53,039 --> 00:31:56,240
实现我干脆把这个函数先

1033
00:31:56,839 --> 00:31:57,879
把这个公式拿掉

1034
00:31:57,879 --> 00:31:59,519
这样子的话

1035
00:32:01,000 --> 00:32:02,799
就可以往前挪一点了

1036
00:32:03,559 --> 00:32:05,480
首先我给你了我的net

1037
00:32:05,759 --> 00:32:07,000
给我一个θ

1038
00:32:07,119 --> 00:32:08,599
这就是你整个干的事情

1039
00:32:09,200 --> 00:32:10,759
首先我去把你那些parameter

1040
00:32:10,759 --> 00:32:11,440
全部拿出来

1041
00:32:11,960 --> 00:32:13,639
如果你是module的话

1042
00:32:13,840 --> 00:32:14,920
因为我们之后会说

1043
00:32:14,920 --> 00:32:16,200
用module怎么实现

1044
00:32:16,200 --> 00:32:17,720
那就是net一个

1045
00:32:17,720 --> 00:32:19,000
get一个parameters

1046
00:32:19,160 --> 00:32:21,560
然后并且是需要t度的

1047
00:32:21,560 --> 00:32:22,840
然后把它全部拿出来

1048
00:32:23,200 --> 00:32:23,840
不然的话

1049
00:32:24,040 --> 00:32:25,400
我们刚刚实现那个函数

1050
00:32:25,720 --> 00:32:28,160
我们的params就存在这个地方

1051
00:32:28,280 --> 00:32:29,160
就直接拿到了

1052
00:32:30,200 --> 00:32:30,880
注意到这一点

1053
00:32:31,360 --> 00:32:34,080
这个地方是我们把所有的层的

1054
00:32:34,080 --> 00:32:35,600
可以参与训练的

1055
00:32:35,600 --> 00:32:37,040
参数全部拿出来了

1056
00:32:37,440 --> 00:32:39,000
不是一层一层的

1057
00:32:39,640 --> 00:32:40,000
OK

1058
00:32:40,000 --> 00:32:42,080
就所有层加起放在一起

1059
00:32:42,799 --> 00:32:42,960
好

1060
00:32:42,960 --> 00:32:44,119
接下来取弄

1061
00:32:45,079 --> 00:32:46,679
取弄可以看到这个地方

1062
00:32:47,359 --> 00:32:48,240
这也是要注意的

1063
00:32:50,960 --> 00:32:54,199
我们把所有的层里面的p

1064
00:32:54,199 --> 00:32:55,480
然后的t度

1065
00:32:56,000 --> 00:32:57,439
取平方

1066
00:32:57,879 --> 00:32:58,720
求和

1067
00:32:59,359 --> 00:33:01,559
然后对所有的层求和

1068
00:33:01,559 --> 00:33:02,919
再开根号

1069
00:33:02,919 --> 00:33:03,839
等于就是说

1070
00:33:03,839 --> 00:33:05,759
我把所有的层的t度

1071
00:33:06,000 --> 00:33:07,000
然后拉成一个向量

1072
00:33:07,000 --> 00:33:08,679
然后把这些向量全部拼在一起

1073
00:33:08,679 --> 00:33:10,119
拼成一个特别长的向量

1074
00:33:10,480 --> 00:33:11,879
再对它求弄

1075
00:33:12,320 --> 00:33:14,840
求那个LL2弄

1076
00:33:15,640 --> 00:33:18,000
就如果这个弄大于θ

1077
00:33:18,000 --> 00:33:18,840
就是我们值

1078
00:33:18,840 --> 00:33:20,000
比如说5或者10

1079
00:33:20,560 --> 00:33:23,800
那么我们就要把参数面

1080
00:33:23,800 --> 00:33:24,440
所有的t度

1081
00:33:24,440 --> 00:33:26,040
就是in place操作

1082
00:33:26,120 --> 00:33:27,600
就是直接改写了

1083
00:33:28,080 --> 00:33:29,960
乘以等于θ除以弄

1084
00:33:30,880 --> 00:33:32,120
这样子导致说

1085
00:33:32,280 --> 00:33:35,080
假设你的当前弄大于θ的话

1086
00:33:35,200 --> 00:33:36,400
我们把它映射回去

1087
00:33:36,400 --> 00:33:38,160
使得所有这些t度

1088
00:33:38,280 --> 00:33:39,400
把它concate起来之后

1089
00:33:39,400 --> 00:33:41,400
它的弄就等于θ

1090
00:33:41,400 --> 00:33:43,320
就不会超过5或者超过10

1091
00:33:44,200 --> 00:33:45,000
当然反过来讲

1092
00:33:45,000 --> 00:33:46,320
如果你很小

1093
00:33:46,519 --> 00:33:47,320
在正常范围

1094
00:33:47,320 --> 00:33:48,960
我就不做操作了

1095
00:33:48,960 --> 00:33:51,160
就是是预防你的t度变大的时候

1096
00:33:51,160 --> 00:33:52,960
我给你做一次映射投影

1097
00:33:53,280 --> 00:33:55,400
就是grad clipping这个函数

1098
00:33:55,400 --> 00:33:56,080
这种事情

1099
00:33:58,400 --> 00:33:59,040
好

1100
00:33:59,320 --> 00:33:59,519
好

1101
00:33:59,519 --> 00:34:01,040
接下来就是另外一个关键函数

1102
00:34:01,120 --> 00:34:01,880
就是

1103
00:34:01,960 --> 00:34:03,680
劝一个epoch

1104
00:34:04,360 --> 00:34:05,560
我们刚刚已经定了

1105
00:34:05,560 --> 00:34:07,240
劝一个batch怎么做

1106
00:34:07,360 --> 00:34:08,480
劝一个iteration怎么做

1107
00:34:08,480 --> 00:34:09,599
现在劝一个epoch

1108
00:34:10,559 --> 00:34:12,199
epoch就是给我个net

1109
00:34:12,199 --> 00:34:13,719
给我一个train的iterator

1110
00:34:13,759 --> 00:34:15,360
给我的损失函数

1111
00:34:15,360 --> 00:34:16,159
给我的updater

1112
00:34:16,159 --> 00:34:17,000
给我的device

1113
00:34:17,000 --> 00:34:19,759
和是不是用的是

1114
00:34:19,799 --> 00:34:21,119
随机的iterator

1115
00:34:21,400 --> 00:34:22,719
回忆一下随机iterator

1116
00:34:22,719 --> 00:34:23,159
是什么东西

1117
00:34:23,599 --> 00:34:24,440
随机就是说

1118
00:34:24,440 --> 00:34:25,400
如果你用了的话

1119
00:34:25,519 --> 00:34:26,079
就是

1120
00:34:26,599 --> 00:34:27,759
下一个批量

1121
00:34:27,759 --> 00:34:29,159
它的第二个样本

1122
00:34:29,159 --> 00:34:30,799
和上一个批量的第二个样本

1123
00:34:30,799 --> 00:34:32,239
是没有任何关系的

1124
00:34:33,039 --> 00:34:34,119
如果不是的话

1125
00:34:34,239 --> 00:34:35,360
就是sequence iterator

1126
00:34:35,360 --> 00:34:37,199
就是说下一个批量的

1127
00:34:37,199 --> 00:34:38,119
第二个样本

1128
00:34:38,119 --> 00:34:39,400
是接的上一个批量

1129
00:34:39,400 --> 00:34:39,920
第二个样本

1130
00:34:39,920 --> 00:34:42,119
在文本里面是相邻的

1131
00:34:42,320 --> 00:34:42,680
OK

1132
00:34:42,680 --> 00:34:43,760
这是它的区别

1133
00:34:44,000 --> 00:34:44,720
它这个东西

1134
00:34:44,720 --> 00:34:46,800
会导致我的隐藏状态的更新

1135
00:34:46,800 --> 00:34:47,440
不一样

1136
00:34:49,760 --> 00:34:50,519
好

1137
00:34:51,360 --> 00:34:51,960
然后可以看到

1138
00:34:52,440 --> 00:34:55,039
这个东西是说state

1139
00:34:55,039 --> 00:34:56,680
state我们先初始化成none

1140
00:34:56,880 --> 00:34:58,119
timer这东西

1141
00:34:58,160 --> 00:34:58,960
计时器

1142
00:34:59,079 --> 00:34:59,880
然后matrix

1143
00:34:59,880 --> 00:35:01,000
就是我们用来

1144
00:35:01,360 --> 00:35:02,039
用来

1145
00:35:02,079 --> 00:35:03,800
存各种loss信息的

1146
00:35:04,960 --> 00:35:05,440
好

1147
00:35:05,440 --> 00:35:06,320
可以看到

1148
00:35:06,360 --> 00:35:07,840
把state加入进来

1149
00:35:08,120 --> 00:35:09,360
对我们epoch函数

1150
00:35:09,360 --> 00:35:10,200
有一点变化

1151
00:35:11,039 --> 00:35:11,600
首先看到

1152
00:35:12,000 --> 00:35:13,480
我们首先的train iterator

1153
00:35:13,680 --> 00:35:15,320
拿到我们的x和y

1154
00:35:15,680 --> 00:35:15,920
好

1155
00:35:15,920 --> 00:35:16,320
这个地方

1156
00:35:16,320 --> 00:35:18,240
是我们怎么样初始化的state

1157
00:35:19,320 --> 00:35:20,680
如果state是none

1158
00:35:20,680 --> 00:35:22,160
就是你什么都没有

1159
00:35:22,320 --> 00:35:23,360
我当然要初始化一下

1160
00:35:23,360 --> 00:35:23,600
对吧

1161
00:35:23,600 --> 00:35:26,320
我就调用了net.beginstate

1162
00:35:26,360 --> 00:35:27,800
然后这个东西

1163
00:35:27,840 --> 00:35:30,680
把我的隐藏状态给初始化了

1164
00:35:33,120 --> 00:35:34,480
另外一个是说

1165
00:35:34,840 --> 00:35:37,280
如果你是用的random iterator

1166
00:35:38,200 --> 00:35:39,760
在每一个

1167
00:35:40,840 --> 00:35:41,640
iteration

1168
00:35:41,640 --> 00:35:43,440
就是每一个batch的时候

1169
00:35:43,480 --> 00:35:46,920
我会把state重新initialize成0

1170
00:35:47,800 --> 00:35:48,280
为什么

1171
00:35:48,280 --> 00:35:48,960
是因为

1172
00:35:49,480 --> 00:35:52,120
前面时刻的sequence信息

1173
00:35:52,120 --> 00:35:53,880
跟我当前的sequence信息

1174
00:35:53,880 --> 00:35:55,360
是不是一个连续的

1175
00:35:55,400 --> 00:35:57,160
所以上一个p量state

1176
00:35:57,160 --> 00:35:58,160
不应该用到这里来

1177
00:35:58,160 --> 00:36:00,200
因为我们在时序上不连续

1178
00:36:00,360 --> 00:36:02,039
所以我当前的p量

1179
00:36:02,039 --> 00:36:04,120
就是从头开始

1180
00:36:04,120 --> 00:36:05,880
就是我是一个新的sequence

1181
00:36:06,880 --> 00:36:08,039
但反过来讲

1182
00:36:08,200 --> 00:36:10,440
如果你是用的sequence iterator

1183
00:36:10,480 --> 00:36:11,760
而且不是

1184
00:36:12,200 --> 00:36:13,320
第一个

1185
00:36:13,680 --> 00:36:14,880
小p量的话

1186
00:36:15,320 --> 00:36:16,960
那么我会不会对state

1187
00:36:16,960 --> 00:36:18,480
做重新的初始化

1188
00:36:19,039 --> 00:36:21,240
你看到我们不会做else的话

1189
00:36:21,240 --> 00:36:22,480
不做重新初始化

1190
00:36:22,519 --> 00:36:23,840
只做detach

1191
00:36:23,840 --> 00:36:25,160
detach意思是说

1192
00:36:25,160 --> 00:36:27,320
我不把里面的state里面的值

1193
00:36:27,720 --> 00:36:28,320
改掉

1194
00:36:28,320 --> 00:36:29,360
我只是说ok

1195
00:36:29,360 --> 00:36:30,200
把detach一下

1196
00:36:30,200 --> 00:36:32,320
就是说做backward的时候

1197
00:36:33,320 --> 00:36:34,440
前面的计算图

1198
00:36:34,440 --> 00:36:35,360
我就detach掉了

1199
00:36:35,360 --> 00:36:36,640
就不要前面那一块

1200
00:36:36,640 --> 00:36:38,480
就做t2的时候

1201
00:36:38,480 --> 00:36:41,039
只关心现在之后开始的运算

1202
00:36:41,039 --> 00:36:42,320
做for的时候就在

1203
00:36:42,840 --> 00:36:44,280
因为你的backward的时候

1204
00:36:44,280 --> 00:36:45,440
你只能在一个

1205
00:36:45,480 --> 00:36:46,599
iteration里面做

1206
00:36:46,599 --> 00:36:47,039
对吧

1207
00:36:47,200 --> 00:36:48,200
所以就是说

1208
00:36:48,400 --> 00:36:49,920
把之前那些存的东西

1209
00:36:49,920 --> 00:36:51,480
全部跟计算图相关

1210
00:36:51,480 --> 00:36:52,200
做backward的时候

1211
00:36:52,200 --> 00:36:53,599
全部detach掉

1212
00:36:53,599 --> 00:36:54,559
删掉不要

1213
00:36:55,480 --> 00:36:56,559
所以这个地方是为什么

1214
00:36:57,079 --> 00:36:58,800
是因为如果是你用的

1215
00:36:58,800 --> 00:37:00,320
sequence iterator的话

1216
00:37:00,360 --> 00:37:02,000
你的下一个小p量的

1217
00:37:02,000 --> 00:37:02,680
第二个样本

1218
00:37:02,680 --> 00:37:03,519
上一个小p量

1219
00:37:03,519 --> 00:37:04,280
第二个样本

1220
00:37:04,280 --> 00:37:05,480
是连续的

1221
00:37:05,560 --> 00:37:08,160
所以你可以把上一个

1222
00:37:08,360 --> 00:37:11,640
小p量的状态传过来

1223
00:37:11,640 --> 00:37:13,040
因为你有时间性

1224
00:37:13,040 --> 00:37:14,600
你也是时间上是连续的

1225
00:37:14,600 --> 00:37:16,640
你有一个sequence的信息

1226
00:37:16,640 --> 00:37:17,240
在里面

1227
00:37:17,280 --> 00:37:18,600
所以你就把它传过来了

1228
00:37:18,880 --> 00:37:19,440
Ok

1229
00:37:19,440 --> 00:37:20,480
所以就是说

1230
00:37:21,080 --> 00:37:23,120
基本上是如果是随机采用

1231
00:37:23,120 --> 00:37:24,800
每个小p量随机采用的话

1232
00:37:24,800 --> 00:37:25,720
那么每个小p量

1233
00:37:25,720 --> 00:37:27,560
我的状态就要重新初始化

1234
00:37:27,840 --> 00:37:29,120
如果你不是的话

1235
00:37:29,120 --> 00:37:30,720
那么是整个epoch

1236
00:37:30,920 --> 00:37:32,080
在epoch一开始的时候

1237
00:37:32,080 --> 00:37:33,040
初始化一次

1238
00:37:33,199 --> 00:37:33,920
整个epoch

1239
00:37:33,920 --> 00:37:34,440
我会读一个

1240
00:37:34,440 --> 00:37:36,039
特别长的一个序列

1241
00:37:37,159 --> 00:37:37,800
Ok

1242
00:37:38,320 --> 00:37:39,440
所以这个就是初始

1243
00:37:39,559 --> 00:37:40,079
这个状态

1244
00:37:40,199 --> 00:37:41,719
主要逻辑在这个地方

1245
00:37:41,920 --> 00:37:42,719
别的就没什么了

1246
00:37:42,719 --> 00:37:43,599
别的就是说

1247
00:37:43,840 --> 00:37:44,320
我的y

1248
00:37:44,440 --> 00:37:46,440
我就y就直接拉成了一个向量

1249
00:37:47,239 --> 00:37:48,279
做了一个转制

1250
00:37:48,279 --> 00:37:49,519
做一个转制是说

1251
00:37:49,519 --> 00:37:51,519
我要把时间序列

1252
00:37:51,679 --> 00:37:53,559
时间信息拉到前面

1253
00:37:53,559 --> 00:37:55,239
把它拉成一个向量

1254
00:37:55,599 --> 00:37:56,000
然后

1255
00:37:57,199 --> 00:37:58,960
我把它放到我的GPU上

1256
00:37:59,079 --> 00:38:02,440
然后再做我的forward函数

1257
00:38:03,039 --> 00:38:04,320
然后算loss

1258
00:38:05,320 --> 00:38:05,880
Ok

1259
00:38:05,880 --> 00:38:06,840
所以这就回到了

1260
00:38:06,840 --> 00:38:09,880
为什么我们要把所谓的输出

1261
00:38:10,000 --> 00:38:10,759
在RNN里面

1262
00:38:10,759 --> 00:38:11,519
把所有输出

1263
00:38:11,519 --> 00:38:12,679
在第一个维度上

1264
00:38:12,679 --> 00:38:13,759
concate起来

1265
00:38:13,840 --> 00:38:14,880
这是因为

1266
00:38:15,079 --> 00:38:16,880
从loss角度来讲

1267
00:38:16,880 --> 00:38:19,480
你就是一个多分类问题

1268
00:38:20,320 --> 00:38:21,639
你在每一个

1269
00:38:21,679 --> 00:38:22,719
每一个

1270
00:38:23,159 --> 00:38:23,920
序列里面

1271
00:38:23,920 --> 00:38:24,800
任何一个时间点

1272
00:38:24,800 --> 00:38:26,039
它就是一个样板

1273
00:38:26,320 --> 00:38:27,199
我就把你拉长

1274
00:38:27,199 --> 00:38:28,440
拉成一条向量

1275
00:38:28,519 --> 00:38:30,639
所以对它y hat

1276
00:38:30,639 --> 00:38:31,480
对loss来讲

1277
00:38:31,480 --> 00:38:32,519
它就是一个

1278
00:38:32,519 --> 00:38:34,719
你里面有一个批量大小

1279
00:38:34,719 --> 00:38:35,800
乘以时间

1280
00:38:36,199 --> 00:38:38,119
部署的那么多样板

1281
00:38:38,679 --> 00:38:39,239
你的y

1282
00:38:39,519 --> 00:38:41,559
你不是也把它拉成向量了吗

1283
00:38:41,840 --> 00:38:42,679
这y也拉成向量

1284
00:38:43,239 --> 00:38:44,519
那就是一个标准的

1285
00:38:44,519 --> 00:38:45,679
多分类问题

1286
00:38:45,880 --> 00:38:47,079
只是你的批量大小

1287
00:38:47,199 --> 00:38:48,079
能够变成了

1288
00:38:48,119 --> 00:38:49,159
真正的批量大小

1289
00:38:49,159 --> 00:38:50,599
乘以你的时间长度

1290
00:38:51,119 --> 00:38:51,759
Ok

1291
00:38:52,519 --> 00:38:54,280
所以从下面开始

1292
00:38:54,280 --> 00:38:55,360
就是一个标准的

1293
00:38:55,360 --> 00:38:56,119
一个多分类问题

1294
00:38:56,119 --> 00:38:57,519
就是没有什么别的

1295
00:38:57,920 --> 00:38:59,159
太多的区别

1296
00:38:59,159 --> 00:39:00,679
而唯一的区别在这个地方

1297
00:39:01,119 --> 00:39:01,599
算

1298
00:39:01,880 --> 00:39:02,880
这个东西你就不用管了

1299
00:39:02,880 --> 00:39:03,799
这个东西就是说

1300
00:39:03,799 --> 00:39:04,880
如果是你用的

1301
00:39:04,880 --> 00:39:06,799
Torch的Optimizer要怎么样

1302
00:39:06,799 --> 00:39:07,559
不然的话

1303
00:39:07,559 --> 00:39:09,839
是我们的Updater要怎么办

1304
00:39:10,519 --> 00:39:10,799
对吧

1305
00:39:11,159 --> 00:39:11,839
就是唯一的

1306
00:39:11,839 --> 00:39:14,119
就是一个要不要zero grad

1307
00:39:14,519 --> 00:39:16,199
那么另外一个区别是说

1308
00:39:16,239 --> 00:39:18,400
在算完backload的时候

1309
00:39:18,440 --> 00:39:20,719
我会对我的整个的t

1310
00:39:20,719 --> 00:39:22,000
都做一次clipping

1311
00:39:22,199 --> 00:39:22,960
我们用的是1

1312
00:39:23,119 --> 00:39:24,079
我们用的比较狠

1313
00:39:24,079 --> 00:39:26,279
就是假设你长度超过1的话

1314
00:39:26,279 --> 00:39:28,920
我们就把你往下投影

1315
00:39:29,280 --> 00:39:29,639
Ok

1316
00:39:29,639 --> 00:39:31,680
就是说在你的算完t度

1317
00:39:31,680 --> 00:39:32,880
在你更新之前

1318
00:39:32,880 --> 00:39:34,800
我把你的t度做一次检查

1319
00:39:35,079 --> 00:39:37,000
别的就没什么太多区别

1320
00:39:37,039 --> 00:39:38,559
另外一个区别是说

1321
00:39:38,559 --> 00:39:39,519
我们记得

1322
00:39:41,680 --> 00:39:42,760
这个东西算什么

1323
00:39:42,760 --> 00:39:43,800
这个东西你的loss

1324
00:39:43,800 --> 00:39:45,760
乘以你的批量大小

1325
00:39:45,920 --> 00:39:47,440
就是这东西是

1326
00:39:47,440 --> 00:39:49,760
就是你的Cross entropy

1327
00:39:50,440 --> 00:39:50,960
对吧

1328
00:39:51,000 --> 00:39:52,960
就是你的Average Cross entropy

1329
00:39:53,920 --> 00:39:56,159
然后如果你要算Perplexity

1330
00:39:56,159 --> 00:39:56,599
怎么办

1331
00:39:56,599 --> 00:39:57,559
你就是加一个指数

1332
00:39:57,680 --> 00:39:58,159
对吧

1333
00:39:59,159 --> 00:40:01,000
就这个就是我们之前分类

1334
00:40:01,000 --> 00:40:01,599
一直在用的

1335
00:40:01,599 --> 00:40:03,119
就是说你这个是你所谓的

1336
00:40:03,119 --> 00:40:04,320
loss的小皮带里面

1337
00:40:04,320 --> 00:40:05,599
loss的累加

1338
00:40:05,599 --> 00:40:06,880
然后这个就是你的

1339
00:40:06,880 --> 00:40:07,760
有多少个样本

1340
00:40:07,760 --> 00:40:08,599
然后一除对吧

1341
00:40:08,599 --> 00:40:09,559
就得到平均的

1342
00:40:09,559 --> 00:40:11,360
那个Cross entropy loss

1343
00:40:11,400 --> 00:40:12,639
然后再做个指数

1344
00:40:13,079 --> 00:40:13,680
Ok

1345
00:40:13,960 --> 00:40:17,119
所以就得到你的Perplexity了

1346
00:40:17,480 --> 00:40:18,519
那困惑度了

1347
00:40:19,320 --> 00:40:19,680
Ok

1348
00:40:19,720 --> 00:40:22,639
这就是我们的训练函数

1349
00:40:23,039 --> 00:40:23,920
稍微长一点

1350
00:40:23,920 --> 00:40:24,880
主要是因为

1351
00:40:25,519 --> 00:40:27,240
根据你的随机采用的不一样

1352
00:40:27,240 --> 00:40:28,079
所以在stay

1353
00:40:28,079 --> 00:40:30,199
就初始化你隐藏状态

1354
00:40:30,199 --> 00:40:31,400
时候会有一点不一样

1355
00:40:32,799 --> 00:40:35,559
另外一个是在做

1356
00:40:35,719 --> 00:40:38,519
我们加了一个T度检查进去

1357
00:40:38,719 --> 00:40:39,840
最后一个不一样的是

1358
00:40:39,840 --> 00:40:41,679
说我们打印的时候

1359
00:40:41,679 --> 00:40:43,000
我们打印困惑度

1360
00:40:43,039 --> 00:40:44,639
困惑度就是把我们正常打印的

1361
00:40:44,639 --> 00:40:45,119
那个loss

1362
00:40:45,319 --> 00:40:46,799
算一个exp出来

1363
00:40:46,880 --> 00:40:47,480
Ok

1364
00:40:47,559 --> 00:40:49,400
这就是这三点不一样

1365
00:40:51,480 --> 00:40:52,119
好

1366
00:40:52,159 --> 00:40:52,519
好

1367
00:40:52,519 --> 00:40:54,319
那我们就最后就可以开始训练了

1368
00:40:55,119 --> 00:40:56,759
就我们定了一个train

1369
00:40:56,960 --> 00:40:57,679
CH8

1370
00:40:57,719 --> 00:40:58,319
就CH8

1371
00:40:58,319 --> 00:40:59,359
就第8章的意思

1372
00:41:00,079 --> 00:41:02,319
这个东西就是可以支持到

1373
00:41:02,319 --> 00:41:03,440
我们所有的循环

1374
00:41:03,440 --> 00:41:04,239
甚至网络的训练

1375
00:41:04,239 --> 00:41:06,559
所以我们要也稍微讲一下

1376
00:41:06,559 --> 00:41:08,000
因为之后的LSTM

1377
00:41:08,039 --> 00:41:10,039
GRU就可以重用这个函数了

1378
00:41:11,000 --> 00:41:13,039
所以然后可以看一下

1379
00:41:14,480 --> 00:41:15,919
前面都没什么区别

1380
00:41:16,039 --> 00:41:16,839
给我个网络

1381
00:41:16,839 --> 00:41:18,319
给我的train你的iterator

1382
00:41:18,319 --> 00:41:20,039
但我们这里没有validation iterator

1383
00:41:20,239 --> 00:41:21,039
我们就偷懒

1384
00:41:21,039 --> 00:41:21,919
没有写这个东西

1385
00:41:21,919 --> 00:41:23,239
就是说你就看一下就行了

1386
00:41:23,480 --> 00:41:24,519
因为我们会predict

1387
00:41:24,639 --> 00:41:25,799
你就看一下效果就行了

1388
00:41:25,960 --> 00:41:27,199
然后给你一个vocab

1389
00:41:27,719 --> 00:41:28,719
这个是新的

1390
00:41:29,079 --> 00:41:29,960
learning rate

1391
00:41:30,239 --> 00:41:31,119
跑多少轮

1392
00:41:31,159 --> 00:41:32,159
device是谁

1393
00:41:32,199 --> 00:41:33,159
另外一个新的是说

1394
00:41:33,159 --> 00:41:35,359
你是不是要用render iterator

1395
00:41:36,639 --> 00:41:38,000
然后 loss

1396
00:41:38,039 --> 00:41:39,879
loss是因为这是其实是一个

1397
00:41:39,879 --> 00:41:40,960
虽然是语言模型

1398
00:41:41,079 --> 00:41:42,639
其实就是一个标准的多分类

1399
00:41:42,879 --> 00:41:44,599
所以用的是cross entropy loss

1400
00:41:45,279 --> 00:41:47,000
这个是我们的用来做动画的

1401
00:41:47,000 --> 00:41:47,719
那个东西

1402
00:41:48,119 --> 00:41:50,480
如果是用的是module的话

1403
00:41:50,599 --> 00:41:52,319
我的updater

1404
00:41:52,639 --> 00:41:55,000
我就用的是optin里面的sgd

1405
00:41:55,279 --> 00:41:55,919
不然的话

1406
00:41:56,360 --> 00:41:58,000
我们就调用的是我们的sgd

1407
00:41:58,519 --> 00:41:59,720
我们之前实现过的

1408
00:42:00,119 --> 00:42:01,159
predict是什么

1409
00:42:01,159 --> 00:42:02,159
我们predict的函数

1410
00:42:02,159 --> 00:42:02,800
就是说

1411
00:42:04,760 --> 00:42:06,000
就把前面predict

1412
00:42:06,000 --> 00:42:07,119
ch8包装一下

1413
00:42:07,440 --> 00:42:08,079
封装一下

1414
00:42:08,079 --> 00:42:09,280
给我一个prefix

1415
00:42:09,880 --> 00:42:12,880
然后我去往后预测50个chart

1416
00:42:13,440 --> 00:42:14,000
OK

1417
00:42:14,440 --> 00:42:16,760
所以接下来就是for loop

1418
00:42:16,960 --> 00:42:18,880
for loop就是对每一个epoch

1419
00:42:19,559 --> 00:42:22,360
然后我们调用一下我们的train

1420
00:42:22,400 --> 00:42:23,760
刚刚定的train

1421
00:42:23,760 --> 00:42:24,920
epoch的函数

1422
00:42:25,519 --> 00:42:26,680
后面的参数我就不用管了

1423
00:42:27,440 --> 00:42:27,840
然后

1424
00:42:29,480 --> 00:42:31,000
perplexity ppl存在

1425
00:42:31,000 --> 00:42:32,200
speed也存在

1426
00:42:33,079 --> 00:42:34,360
另外一个不一样的是说

1427
00:42:34,360 --> 00:42:35,960
我们之后是画这个东西

1428
00:42:36,079 --> 00:42:36,800
就是画

1429
00:42:37,039 --> 00:42:37,800
不一样的是说

1430
00:42:37,800 --> 00:42:39,000
我们对每个epoch

1431
00:42:39,200 --> 00:42:40,159
会去看一下

1432
00:42:40,159 --> 00:42:42,400
就是说我给你一个time traveler

1433
00:42:42,400 --> 00:42:44,480
让你去predict看一看

1434
00:42:45,519 --> 00:42:46,840
然后最后我们打一下

1435
00:42:46,840 --> 00:42:47,599
你的困惑度

1436
00:42:47,800 --> 00:42:48,039
打

1437
00:42:48,039 --> 00:42:49,880
最后我们打一两个

1438
00:42:50,000 --> 00:42:52,000
就是给一个time traveler

1439
00:42:52,000 --> 00:42:52,920
然后给一个traveler

1440
00:42:52,960 --> 00:42:54,119
让你作为前缀

1441
00:42:54,119 --> 00:42:55,839
你去预测后面50个字

1442
00:42:56,239 --> 00:42:57,199
会长什么样子

1443
00:42:57,480 --> 00:42:59,839
这就是我们整个的训练函数

1444
00:43:00,880 --> 00:43:02,519
也是相对来说比较直观

1445
00:43:03,039 --> 00:43:05,239
我觉得跟前面也没什么太多

1446
00:43:05,639 --> 00:43:06,519
其实没什么区别

1447
00:43:06,639 --> 00:43:09,319
跟我们标准的分类

1448
00:43:09,679 --> 00:43:11,799
跟之前所有的那些分类train

1449
00:43:11,799 --> 00:43:12,960
没什么本质区别

1450
00:43:13,279 --> 00:43:14,839
因为唯一的区别

1451
00:43:14,839 --> 00:43:17,119
可能是我们现在每次predict

1452
00:43:17,119 --> 00:43:18,199
看一看效果

1453
00:43:18,719 --> 00:43:19,199
OK

1454
00:43:19,920 --> 00:43:20,199
好

1455
00:43:20,199 --> 00:43:21,839
我们就可以开始训练了

1456
00:43:22,119 --> 00:43:22,480
训练

1457
00:43:22,480 --> 00:43:26,039
我们就是我这个是在之前

1458
00:43:26,480 --> 00:43:28,480
我在刚刚直播开始之前

1459
00:43:28,480 --> 00:43:29,400
给大家run的

1460
00:43:30,960 --> 00:43:31,960
我没有在这里run

1461
00:43:31,960 --> 00:43:34,240
因为跑一下大概要一分钟的样子

1462
00:43:35,679 --> 00:43:37,519
我们就跑了500个epoch

1463
00:43:37,519 --> 00:43:39,719
epoch跑的不算少

1464
00:43:40,559 --> 00:43:43,079
因为这个样本比较就是一本书

1465
00:43:43,240 --> 00:43:44,039
就是很小

1466
00:43:44,039 --> 00:43:45,960
所以跑一跑也可以

1467
00:43:46,240 --> 00:43:48,039
然后你的learning rate去了个1

1468
00:43:48,360 --> 00:43:49,320
然后你的能力

1469
00:43:50,159 --> 00:43:51,000
然后我们看一下

1470
00:43:51,000 --> 00:43:51,719
就是说

1471
00:43:52,320 --> 00:43:53,360
首先空和度

1472
00:43:53,559 --> 00:43:55,639
空和度我们训练到了1.0

1473
00:43:57,079 --> 00:43:58,000
这个地方是

1474
00:44:00,480 --> 00:44:02,119
首先我们空和度训练的1.0

1475
00:44:02,119 --> 00:44:02,559
是什么意思

1476
00:44:02,599 --> 00:44:04,519
1.0就是我们说白了

1477
00:44:04,760 --> 00:44:06,320
我们知道空和度最好的情况

1478
00:44:06,320 --> 00:44:06,840
就是1.0

1479
00:44:06,920 --> 00:44:08,199
就对应的你的

1480
00:44:10,960 --> 00:44:12,320
你的loss已经很低了

1481
00:44:12,880 --> 00:44:15,440
所以1.0就是完美的

1482
00:44:15,559 --> 00:44:17,719
把整个文本给你

1483
00:44:17,760 --> 00:44:19,360
基本上是记住了

1484
00:44:21,000 --> 00:44:21,320
为什么

1485
00:44:21,320 --> 00:44:22,000
是因为

1486
00:44:22,199 --> 00:44:23,920
你就一个一本书

1487
00:44:23,920 --> 00:44:24,519
我叠带了

1488
00:44:24,519 --> 00:44:25,480
我扫了500遍

1489
00:44:25,519 --> 00:44:27,000
我当然就基本上把你记住了

1490
00:44:27,039 --> 00:44:27,480
你可以看到

1491
00:44:27,480 --> 00:44:28,800
perplexity是这么下降的

1492
00:44:28,800 --> 00:44:30,559
到第300的那时候

1493
00:44:30,559 --> 00:44:31,960
我就基本上给你记住了

1494
00:44:32,800 --> 00:44:34,000
然后我们当然是说

1495
00:44:34,000 --> 00:44:37,039
跑了大概6万8000个token

1496
00:44:37,199 --> 00:44:38,800
就是每次能跑6万

1497
00:44:39,440 --> 00:44:40,639
每秒就能跑多少次

1498
00:44:40,639 --> 00:44:41,840
所以还是挺快的

1499
00:44:43,119 --> 00:44:45,320
然后可以看到我们的预测效果

1500
00:44:46,119 --> 00:44:47,280
time traveler

1501
00:44:47,440 --> 00:44:49,159
you can show black is white

1502
00:44:49,159 --> 00:44:50,559
by argument set

1503
00:44:50,559 --> 00:44:51,239
什么

1504
00:44:51,239 --> 00:44:52,599
基本上其实

1505
00:44:53,079 --> 00:44:53,759
就是说

1506
00:44:55,000 --> 00:44:56,239
这基本上你看traveler

1507
00:44:56,239 --> 00:44:56,880
也是基本上

1508
00:44:56,880 --> 00:44:58,039
就是说你可以看到

1509
00:44:58,119 --> 00:45:00,239
我们每一次是用一个char

1510
00:45:00,239 --> 00:45:02,119
用一个字符来预测下一个字符

1511
00:45:02,559 --> 00:45:03,880
所以基本上你可以看到

1512
00:45:03,880 --> 00:45:05,719
说从词的角度来讲

1513
00:45:05,799 --> 00:45:06,679
还是靠谱的

1514
00:45:06,679 --> 00:45:08,199
就是说你还是一些正常词

1515
00:45:08,199 --> 00:45:09,880
不是给你一些乱七八糟的

1516
00:45:10,000 --> 00:45:11,639
词都不是词的东西

1517
00:45:12,480 --> 00:45:14,039
但是从句子角度来看

1518
00:45:14,039 --> 00:45:14,679
就是

1519
00:45:15,840 --> 00:45:16,639
不是make sense

1520
00:45:16,639 --> 00:45:17,000
是吧

1521
00:45:17,000 --> 00:45:17,639
就是说

1522
00:45:17,679 --> 00:45:18,719
这个应该是一句话

1523
00:45:18,840 --> 00:45:19,440
这个是一句话

1524
00:45:19,440 --> 00:45:20,240
在跟我之前

1525
00:45:20,760 --> 00:45:21,920
没什么太多区别

1526
00:45:22,440 --> 00:45:23,840
跟我之前没什么通过联系

1527
00:45:24,000 --> 00:45:24,560
就是

1528
00:45:25,560 --> 00:45:26,680
给我个traveler

1529
00:45:26,680 --> 00:45:27,200
就你

1530
00:45:27,200 --> 00:45:28,560
因为他都是一个traveler

1531
00:45:28,720 --> 00:45:30,680
就是说他都是预测都是一样的

1532
00:45:31,080 --> 00:45:32,200
所以就是说

1533
00:45:32,240 --> 00:45:32,880
你可以看到

1534
00:45:33,560 --> 00:45:34,560
基本上我们RNN

1535
00:45:34,680 --> 00:45:37,520
把我们文本给记住了

1536
00:45:37,560 --> 00:45:40,120
所以就算是字符的

1537
00:45:40,760 --> 00:45:43,320
也词角度来看是靠谱的

1538
00:45:43,320 --> 00:45:44,440
句子角度是靠谱的

1539
00:45:44,440 --> 00:45:47,080
但是放在一起就plumbock靠谱了

1540
00:45:47,079 --> 00:45:48,759
这也是语言模型

1541
00:45:48,759 --> 00:45:50,639
最常见的问题

1542
00:45:50,639 --> 00:45:51,319
就是

1543
00:45:52,599 --> 00:45:53,719
乍一眼看

1544
00:45:54,119 --> 00:45:55,519
感觉还挺靠谱的

1545
00:45:55,559 --> 00:45:56,719
但仔细看

1546
00:45:56,719 --> 00:45:58,279
就是就不知道是什么东西

1547
00:45:58,920 --> 00:45:59,519
OK

1548
00:46:00,719 --> 00:46:01,880
我们接下来再看一下

1549
00:46:01,880 --> 00:46:02,759
另外一个

1550
00:46:06,039 --> 00:46:07,319
另外一个就是

1551
00:46:07,759 --> 00:46:08,319
不一样的

1552
00:46:08,319 --> 00:46:08,920
跟之前不一样

1553
00:46:08,920 --> 00:46:10,440
是use random iterator

1554
00:46:10,440 --> 00:46:10,960
等于true

1555
00:46:11,199 --> 00:46:12,679
每一次采样的时候

1556
00:46:13,119 --> 00:46:16,599
我就是去随机去取一个sequence

1557
00:46:16,759 --> 00:46:18,039
所以跟之前

1558
00:46:19,279 --> 00:46:20,920
上一个平安是没什么关系

1559
00:46:21,279 --> 00:46:23,639
有更多的随机性在里面出现

1560
00:46:24,199 --> 00:46:25,319
可以看到是说

1561
00:46:25,319 --> 00:46:26,719
那么空合度就会高一点

1562
00:46:26,719 --> 00:46:28,719
就是同样500个一迭代的话

1563
00:46:28,719 --> 00:46:29,199
你可以看到

1564
00:46:29,199 --> 00:46:30,360
它的下载会慢一点

1565
00:46:30,360 --> 00:46:31,159
这是因为

1566
00:46:31,759 --> 00:46:34,599
每一次我的state是重新打乱了

1567
00:46:34,599 --> 00:46:36,440
重新每一个小批量

1568
00:46:36,839 --> 00:46:38,719
就是被出发向零

1569
00:46:38,799 --> 00:46:40,799
然后再做运算

1570
00:46:44,079 --> 00:46:45,960
每次只看一个比较短的sequence

1571
00:46:46,119 --> 00:46:47,679
random性更高一点

1572
00:46:47,960 --> 00:46:51,119
所以在训练上来说更加难一点

1573
00:46:51,480 --> 00:46:52,000
但

1574
00:46:53,519 --> 00:46:54,519
另外看一下效果

1575
00:46:54,519 --> 00:46:55,240
就是说

1576
00:46:56,480 --> 00:46:57,599
就基本上你

1577
00:46:57,800 --> 00:46:58,559
就是

1578
00:46:58,559 --> 00:47:02,159
你其实也看不出太多效果

1579
00:47:02,199 --> 00:47:04,360
因为我们文本实在是太小

1580
00:47:04,360 --> 00:47:06,039
我们的训练样本实在是太小

1581
00:47:06,079 --> 00:47:07,960
它基本上给我记住了很多东西

1582
00:47:08,000 --> 00:47:08,920
所以还是一样的

1583
00:47:08,920 --> 00:47:09,639
就是说你看

1584
00:47:09,639 --> 00:47:13,000
holding his hands

1585
00:47:13,000 --> 00:47:14,280
with什么什么东西

1586
00:47:14,320 --> 00:47:15,240
这个词是不对的

1587
00:47:15,240 --> 00:47:17,200
这个词反正是乱出来的

1588
00:47:17,400 --> 00:47:18,880
后面两个词应该是不对的

1589
00:47:19,880 --> 00:47:20,840
所以基本上可以看到

1590
00:47:20,960 --> 00:47:23,960
之前我们每一个epoch

1591
00:47:24,160 --> 00:47:26,680
就是读一个特别长的一条序列

1592
00:47:27,040 --> 00:47:28,560
现在是我们每一个iteration

1593
00:47:28,560 --> 00:47:29,160
是读一个序列

1594
00:47:29,160 --> 00:47:31,200
但是iteration之间是随机的

1595
00:47:31,240 --> 00:47:33,120
所以这里随机性

1596
00:47:33,120 --> 00:47:34,160
使得你的

1597
00:47:34,640 --> 00:47:36,480
没有那么完全的记住了

1598
00:47:36,480 --> 00:47:38,120
我的数据

1599
00:47:38,120 --> 00:47:39,720
因为数据的随机性更强了

1600
00:47:39,720 --> 00:47:41,280
所以同样的模型

1601
00:47:41,920 --> 00:47:43,080
就没有完全记住

1602
00:47:43,320 --> 00:47:44,480
所以预测效果来看

1603
00:47:44,480 --> 00:47:47,199
就会出现了一些奇怪的词在里面

1604
00:47:48,079 --> 00:47:48,800
OK

1605
00:47:48,840 --> 00:47:50,079
所以这个就是

1606
00:47:51,159 --> 00:47:53,079
基本上这就是RNN的实现了

1607
00:47:53,079 --> 00:47:54,760
就是从零开始的实现

1608
00:47:55,000 --> 00:47:56,960
基本上我们就全部给大家讲一遍

1609
00:47:57,240 --> 00:47:58,039
最后看一下效果

1610
00:47:58,039 --> 00:47:59,719
效果也不差了

1611
00:47:59,719 --> 00:48:00,440
其实不差了

1612
00:48:00,440 --> 00:48:02,400
就是说我是作为每一个字符

1613
00:48:02,960 --> 00:48:04,639
一个字符过去

1614
00:48:04,639 --> 00:48:06,039
你RNN至少能知道

1615
00:48:06,039 --> 00:48:06,599
大概知道

1616
00:48:06,599 --> 00:48:07,960
我这个词是怎么组合的

1617
00:48:07,960 --> 00:48:08,400
对吧

1618
00:48:09,000 --> 00:48:10,119
另外一个是说

1619
00:48:10,519 --> 00:48:12,519
我们这个训练实在是特别小

1620
00:48:12,559 --> 00:48:14,240
如果大家感兴趣的话

1621
00:48:14,320 --> 00:48:16,080
你可以去尝试

1622
00:48:16,840 --> 00:48:18,520
把样本搞多一点

1623
00:48:19,040 --> 00:48:21,080
就搞个一兆

1624
00:48:21,280 --> 00:48:24,360
至少我们这个就几十K在这个地方

1625
00:48:25,120 --> 00:48:25,720
OK

1626
00:48:26,320 --> 00:48:27,800
这就是RNN的

1627
00:48:27,840 --> 00:48:29,080
从零开始实现

1628
00:48:29,320 --> 00:48:32,640
我们接下来直接跳到简洁实现

1629
00:48:32,640 --> 00:48:34,200
我们再来回答问题

