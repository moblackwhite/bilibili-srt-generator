1
00:00:00,000 --> 00:00:03,839
所以今天我们在开始之前先跟大家回顾一下

2
00:00:03,839 --> 00:00:07,280
resnet是怎么来处理它的梯度消失

3
00:00:07,280 --> 00:00:10,040
使得能够训练个1000层的样子

4
00:00:10,519 --> 00:00:15,120
其实我们在在讲梯度消失的时候讲过一次

5
00:00:15,120 --> 00:00:18,240
就是说一个办法是说你避免梯度消失

6
00:00:18,240 --> 00:00:21,440
就是将你的乘法变加法

7
00:00:21,440 --> 00:00:22,879
其实resnet就是这么做的

8
00:00:22,879 --> 00:00:24,640
特别是那个recedure connection

9
00:00:25,320 --> 00:00:28,000
所以我们今天给大家稍微讲

10
00:00:28,000 --> 00:00:29,280
花几分钟给讲一下

11
00:00:29,280 --> 00:00:33,679
说recedure到底是怎么样去处理梯度消失的

12
00:00:35,240 --> 00:00:37,520
我们就用手写给大家讲一下

13
00:00:37,520 --> 00:00:42,160
首先假设我有一个网络在这个地方

14
00:00:42,160 --> 00:00:46,159
假设我说我预测的y等于fx

15
00:00:46,159 --> 00:00:47,400
x是输入

16
00:00:47,400 --> 00:00:48,960
你的y是你的输出

17
00:00:48,960 --> 00:00:52,760
你的f就是你一个比如说10个卷集材

18
00:00:52,760 --> 00:00:53,760
或者是怎么样子的

19
00:00:53,760 --> 00:00:57,799
然后你要去比如说我在里面有个w

20
00:00:57,840 --> 00:00:59,120
是我的全重要更新

21
00:00:59,120 --> 00:01:00,920
我就算梯度的话

22
00:01:00,920 --> 00:01:03,200
那就是说关于y

23
00:01:03,200 --> 00:01:05,719
假设我们的loss已经省略掉了

24
00:01:05,800 --> 00:01:07,159
我就说y在这里

25
00:01:07,840 --> 00:01:10,040
关于w

26
00:01:10,040 --> 00:01:11,760
w就是我的某一个层

27
00:01:12,040 --> 00:01:14,359
就某一个靠底层的一个参数

28
00:01:14,480 --> 00:01:15,960
他的t都是这样子算

29
00:01:17,120 --> 00:01:20,400
所以我们得保证说你这个东西不会特别小

30
00:01:20,680 --> 00:01:23,640
这是因为你每一次做更新的时候

31
00:01:23,640 --> 00:01:27,920
你的是w等于w减去你的能力rate

32
00:01:27,920 --> 00:01:29,159
乘以你的

33
00:01:30,319 --> 00:01:33,159
假设我们还y里面有loss了

34
00:01:33,159 --> 00:01:35,960
我们就没有把损失函数写进去了

35
00:01:36,519 --> 00:01:39,799
所以你不希望这个东西太小

36
00:01:39,799 --> 00:01:40,879
小的等于零的时候

37
00:01:40,879 --> 00:01:42,439
你不管调怎么样调学习率

38
00:01:42,439 --> 00:01:43,359
你也很麻烦

39
00:01:43,519 --> 00:01:46,200
因为你的数值等到特别小的时候

40
00:01:46,200 --> 00:01:47,840
你的数值稳定性会发生问题

41
00:01:48,040 --> 00:01:49,840
所以你希望这个东西不要变得很小

42
00:01:51,120 --> 00:01:51,400
好

43
00:01:51,480 --> 00:01:52,880
我们接下来看说

44
00:01:52,920 --> 00:01:55,640
假设我在这一层网络上面

45
00:01:56,040 --> 00:01:57,840
再加一些层会怎么样

46
00:01:58,719 --> 00:01:59,880
加一些层的话

47
00:01:59,880 --> 00:02:01,200
你那么写出来的话

48
00:02:01,200 --> 00:02:02,400
我用一个颜色

49
00:02:02,440 --> 00:02:04,320
假设我是y一撇

50
00:02:04,840 --> 00:02:06,960
我在上面再加一个g函数

51
00:02:11,560 --> 00:02:13,159
你这个g就等于是我在

52
00:02:13,159 --> 00:02:14,520
比如说我是10层卷积层

53
00:02:14,520 --> 00:02:16,000
我在上面再加10层

54
00:02:17,400 --> 00:02:19,159
我们现在看一下它会发生什么问题

55
00:02:20,159 --> 00:02:22,560
还是我们去看那个w跟之前一样的

56
00:02:22,560 --> 00:02:25,280
w就是比较靠近数据端的

57
00:02:25,280 --> 00:02:26,800
那些层的w

58
00:02:26,800 --> 00:02:29,280
因为经常是他们会发生问题

59
00:02:29,800 --> 00:02:31,400
如果我要写t度的话

60
00:02:31,400 --> 00:02:32,280
那就是说

61
00:02:32,439 --> 00:02:34,840
它关于这个wt度

62
00:02:34,879 --> 00:02:38,120
那就等于是其实是我们知道

63
00:02:39,120 --> 00:02:44,319
卷积是t度是怎么样展开的

64
00:02:44,479 --> 00:02:45,840
就是链式罚折

65
00:02:46,840 --> 00:02:47,800
可以写成这样子

66
00:02:47,800 --> 00:02:48,240
对吧

67
00:02:50,079 --> 00:02:51,680
然后你当然是等于

68
00:02:57,319 --> 00:02:58,039
乘以

69
00:03:00,719 --> 00:03:01,000
好

70
00:03:01,000 --> 00:03:02,240
我们来看一下这个东西

71
00:03:02,439 --> 00:03:03,400
它是个什么东西

72
00:03:04,280 --> 00:03:07,120
这一块跟之前是一样的

73
00:03:07,120 --> 00:03:07,439
对吧

74
00:03:07,439 --> 00:03:08,680
跟这个是一样的

75
00:03:09,519 --> 00:03:10,599
没有发生变化

76
00:03:11,400 --> 00:03:13,159
那么问题出在这个地方

77
00:03:13,960 --> 00:03:15,840
这个地方是你的

78
00:03:16,840 --> 00:03:19,879
g y关于y的导数

79
00:03:20,599 --> 00:03:23,360
等于是说你新加的层

80
00:03:23,360 --> 00:03:26,520
对我的对它求的导数

81
00:03:26,520 --> 00:03:28,719
就它的输入和输出求的导数

82
00:03:29,800 --> 00:03:31,640
假设你加的层

83
00:03:31,680 --> 00:03:33,879
它的理和能力比较强的话

84
00:03:34,319 --> 00:03:35,800
假设你是AlexNet

85
00:03:35,800 --> 00:03:37,400
让你加那种全面阶层

86
00:03:38,400 --> 00:03:41,640
那么你这个东西会很快变得特别小

87
00:03:41,759 --> 00:03:44,800
这是因为你可以简单认为是说

88
00:03:45,439 --> 00:03:47,760
它的导数其实等于是说

89
00:03:47,760 --> 00:03:51,760
你的预测值和你的真实值之间的

90
00:03:51,760 --> 00:03:54,080
那个差别是有一定的关系的

91
00:03:54,240 --> 00:03:55,800
假设你预测的比较好了

92
00:03:55,800 --> 00:03:57,880
这一块已经做的比较好了的情况下

93
00:03:58,160 --> 00:04:00,920
那么它的值出来就会比较小

94
00:04:01,680 --> 00:04:03,960
就是说它的值就会很小

95
00:04:05,439 --> 00:04:07,200
如果它的值很小的话

96
00:04:07,920 --> 00:04:10,760
一个很小的值乘以一个之前的

97
00:04:10,760 --> 00:04:12,000
我们在这里的t度

98
00:04:12,039 --> 00:04:15,000
那么它的t度就比你之前会小很多

99
00:04:15,680 --> 00:04:17,399
那么你的t度小很多的话

100
00:04:17,439 --> 00:04:20,720
那么你要么就增大学习率

101
00:04:20,920 --> 00:04:24,120
如果很有可能你增大也没有太多用

102
00:04:24,319 --> 00:04:25,560
因为你不能真的太大

103
00:04:25,560 --> 00:04:27,000
因为你真的太大的话

104
00:04:27,000 --> 00:04:31,199
它这是对于靠近数据底部层的更新

105
00:04:31,240 --> 00:04:32,279
如果我们讲过

106
00:04:32,279 --> 00:04:33,439
如果你真的太大的话

107
00:04:33,439 --> 00:04:36,959
可能它的这个新加的G里面的W

108
00:04:37,000 --> 00:04:38,360
它的t度已经很大了

109
00:04:38,519 --> 00:04:40,279
你把它的t度变得特别大

110
00:04:40,279 --> 00:04:41,639
那你整个就不稳定了

111
00:04:42,319 --> 00:04:45,279
这就是说当你变深的时候会有问题

112
00:04:45,480 --> 00:04:46,800
我们之前讲过这个问题

113
00:04:46,800 --> 00:04:48,319
就是因为乘法对吧

114
00:04:48,480 --> 00:04:51,439
就乘法你是做一直做乘乘乘

115
00:04:51,560 --> 00:04:53,199
如果从今天一个比较小的话

116
00:04:53,199 --> 00:04:55,480
或者n个小的数相乘的话

117
00:04:55,480 --> 00:04:58,519
你会越到底层你的t度越小

118
00:04:59,680 --> 00:05:00,639
那么我们来看一下

119
00:05:00,639 --> 00:05:04,279
就是说resizle ResNet

120
00:05:04,279 --> 00:05:05,519
是怎么解决的问题的

121
00:05:06,120 --> 00:05:09,160
它的解决方法是说我的输出

122
00:05:09,279 --> 00:05:11,120
假设是Y两撇的话

123
00:05:11,240 --> 00:05:15,480
它其实是等于我本来来自于下一层

124
00:05:15,480 --> 00:05:17,120
一个网络的输入

125
00:05:17,160 --> 00:05:20,319
再加上我的GFX

126
00:05:21,759 --> 00:05:22,120
对吧

127
00:05:22,399 --> 00:05:25,680
你可以认为这个其实就是Y

128
00:05:25,680 --> 00:05:26,840
这个就是Y一撇

129
00:05:28,879 --> 00:05:31,680
记得是就是说我这个resizle

130
00:05:31,680 --> 00:05:33,759
是作用在G上面

131
00:05:34,680 --> 00:05:36,319
那么对它求导数的话

132
00:05:36,319 --> 00:05:37,079
你会怎么样

133
00:05:37,800 --> 00:05:41,599
那就是因为加法原则

134
00:05:41,599 --> 00:05:43,000
加法原则你求导的话

135
00:05:43,000 --> 00:05:47,560
它其实就是它和加上它Y一撇

136
00:05:47,560 --> 00:05:48,519
关于它的导数

137
00:05:49,839 --> 00:05:52,319
所以我们这两个都之前看过

138
00:05:52,680 --> 00:05:54,240
就这一个东西确实就是这个

139
00:05:54,240 --> 00:05:58,519
那么它的导数就是它

140
00:05:58,919 --> 00:06:01,439
所以就算你发生这个情况

141
00:06:01,439 --> 00:06:02,759
它比较小的话

142
00:06:02,759 --> 00:06:03,959
就这一块比较小的话

143
00:06:04,000 --> 00:06:05,199
但是你没关系

144
00:06:05,199 --> 00:06:06,439
你还有这一块

145
00:06:07,199 --> 00:06:09,279
就当做是不存在的时候

146
00:06:09,279 --> 00:06:12,560
你去你和小FX的时候

147
00:06:12,560 --> 00:06:14,680
它的梯度是这一块在这个地方

148
00:06:15,519 --> 00:06:16,560
因为是加法

149
00:06:16,599 --> 00:06:19,199
就是大数加小数还是一个大数

150
00:06:19,399 --> 00:06:20,759
但是大数乘以小数

151
00:06:20,759 --> 00:06:21,919
它可能会变成一个小数

152
00:06:22,240 --> 00:06:24,439
所以这就是加法在

153
00:06:24,439 --> 00:06:25,799
就是resizle connection

154
00:06:25,799 --> 00:06:26,919
在这起到作用

155
00:06:27,159 --> 00:06:30,360
就是说对于特别是靠底部的层来讲

156
00:06:30,360 --> 00:06:33,879
你跳转使得它的梯度不会特别小

157
00:06:34,000 --> 00:06:35,000
就不会比

158
00:06:35,040 --> 00:06:39,000
几乎不会比在没有加机之前比较小

159
00:06:40,000 --> 00:06:41,680
你直观上来说可以怎么看

160
00:06:41,879 --> 00:06:43,000
就直观上来看

161
00:06:43,000 --> 00:06:45,040
其实你可以这么看

162
00:06:45,159 --> 00:06:47,000
就是记得resnet有什么样的架构

163
00:06:47,000 --> 00:06:49,399
就是说假设这是我的输出的话

164
00:06:49,680 --> 00:06:50,680
我的Y的话

165
00:06:51,279 --> 00:06:53,000
这是我的一个卷积层

166
00:06:53,319 --> 00:06:54,439
一个卷积块

167
00:06:55,360 --> 00:06:57,439
这也是另外我的一个卷积块

168
00:06:58,680 --> 00:07:00,040
那么它这里会有一个跳转

169
00:07:00,040 --> 00:07:00,279
对吧

170
00:07:00,279 --> 00:07:01,719
有个加号在这个地方

171
00:07:02,480 --> 00:07:03,240
同样道理的话

172
00:07:03,240 --> 00:07:04,400
你这里会有个跳转

173
00:07:04,960 --> 00:07:07,199
然后最下的是一个卷积层

174
00:07:07,199 --> 00:07:08,400
是这是你的data

175
00:07:10,160 --> 00:07:11,600
就data我们知道这一块

176
00:07:11,600 --> 00:07:14,000
其实是一两个卷积层

177
00:07:14,439 --> 00:07:16,199
这些都是一些resizle的block

178
00:07:17,160 --> 00:07:19,920
那么难点就是在靠近数据端

179
00:07:19,920 --> 00:07:20,879
这些东西的W

180
00:07:21,319 --> 00:07:23,120
它的W是比较难以训练的

181
00:07:24,240 --> 00:07:27,600
那么现在因为你加入了跳转

182
00:07:27,600 --> 00:07:29,800
就是说你有一条这样子的路过来

183
00:07:30,680 --> 00:07:33,040
所以在算梯度的时候

184
00:07:33,400 --> 00:07:34,360
他得到梯度

185
00:07:34,360 --> 00:07:37,120
他可以是上面直接这么过来

186
00:07:37,120 --> 00:07:39,439
就直接从高速公路过来

187
00:07:40,199 --> 00:07:42,439
就不需要一定要等到他这边走完

188
00:07:43,000 --> 00:07:45,160
所以说在一开始的时候

189
00:07:45,480 --> 00:07:48,360
我最下面的层也会拿到比较大的梯度

190
00:07:48,360 --> 00:07:49,680
就是因为我的跳转

191
00:07:50,240 --> 00:07:50,960
同样道理的话

192
00:07:50,960 --> 00:07:53,199
对于他来讲也是一样的

193
00:07:53,519 --> 00:07:54,920
因为你这一块能过来

194
00:07:55,840 --> 00:07:56,240
OK

195
00:07:56,240 --> 00:08:00,600
这就是从梯度大小的角度来解释

196
00:08:01,240 --> 00:08:02,480
这个resizle connection

197
00:08:02,600 --> 00:08:05,720
使得你特别靠近数据层的那一些W

198
00:08:05,720 --> 00:08:07,319
也会拿到比较大的梯度

199
00:08:07,680 --> 00:08:08,879
从而你不管你有多深

200
00:08:09,240 --> 00:08:12,000
我下面的层都是可以拿到足够大的梯度

201
00:08:12,000 --> 00:08:14,720
使得我能够做比较高效的更新

202
00:08:15,640 --> 00:08:15,840
OK

203
00:08:15,840 --> 00:08:19,319
这就是一个我们对resnet

204
00:08:19,319 --> 00:08:21,480
从梯度角度来讲的一个分析

