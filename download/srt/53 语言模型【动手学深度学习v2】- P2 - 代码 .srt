1
00:00:00,000 --> 00:00:01,480
语言模型

2
00:00:01,480 --> 00:00:05,719
这个图片是一个猴子去打字

3
00:00:05,719 --> 00:00:10,759
这有个笑话就是说猴子要多么聪明

4
00:00:10,759 --> 00:00:15,679
就是说要尝试多少次才能够把整个莎士比亚的著作打出来

5
00:00:15,679 --> 00:00:17,760
概率是非常低的

6
00:00:17,760 --> 00:00:24,000
然后大家算过要一只猴子打字打到宇宙都要快

7
00:00:24,000 --> 00:00:27,199
就是说灭了它也打不出莎士比亚那篇文章

8
00:00:28,199 --> 00:00:33,759
所以语言模型就是说这是NLP里面最经典的模型

9
00:00:33,759 --> 00:00:35,200
就是说它是干个什么事情

10
00:00:35,320 --> 00:00:36,759
它其实干的事情很简单

11
00:00:36,799 --> 00:00:39,039
我们之前讲序列模型我们讲过

12
00:00:39,079 --> 00:00:40,879
就是说给你一个文本序列

13
00:00:41,000 --> 00:00:43,679
这里是序列具体制的是文本序列

14
00:00:43,840 --> 00:00:46,079
就是我们给一段文章

15
00:00:46,640 --> 00:00:49,120
所以里面一个x i

16
00:00:49,159 --> 00:00:50,719
你可能是一个词

17
00:00:50,719 --> 00:00:52,920
也可能是一个char就是一个字符

18
00:00:52,960 --> 00:00:54,799
当然是取决于你的token什么样子

19
00:00:55,800 --> 00:00:57,480
所以给你一个序列

20
00:00:57,480 --> 00:00:59,760
x1到一直到xt

21
00:01:00,320 --> 00:01:01,640
所以语言模型干嘛

22
00:01:01,800 --> 00:01:06,359
它就是去估计整个联合概率

23
00:01:06,760 --> 00:01:09,680
Px1一直到给定xt

24
00:01:10,760 --> 00:01:11,159
OK

25
00:01:11,159 --> 00:01:14,280
所以我们在讲序列模型的时候

26
00:01:14,280 --> 00:01:17,480
其实序列模型核心也就是去预测

27
00:01:17,520 --> 00:01:20,159
整个文本序列出现的概率

28
00:01:21,719 --> 00:01:23,480
然后它的应用当然包括了说

29
00:01:24,359 --> 00:01:26,760
大家知道最近刷屏刷的比较凶的

30
00:01:26,880 --> 00:01:30,719
BERT GPT-3这种大的语言模型

31
00:01:30,719 --> 00:01:32,520
它本质上就是一个语言模型

32
00:01:32,680 --> 00:01:34,200
它就给你很多的文本

33
00:01:34,200 --> 00:01:35,760
然后去训练模型

34
00:01:35,760 --> 00:01:38,000
去预测它整个文本出现的概率

35
00:01:39,240 --> 00:01:40,040
另外一个

36
00:01:40,040 --> 00:01:41,359
所以它这样子的话

37
00:01:41,359 --> 00:01:43,159
它能够得到很多的训练数据

38
00:01:43,159 --> 00:01:44,920
然后能够做比较大的模型

39
00:01:44,920 --> 00:01:46,719
因为文本你几乎是

40
00:01:46,760 --> 00:01:48,159
你就是很便宜了

41
00:01:48,159 --> 00:01:49,359
就不用标注对吧

42
00:01:49,359 --> 00:01:52,120
就是说你就拿很多文本来就行了

43
00:01:53,120 --> 00:01:55,359
然后你做完之后

44
00:01:55,359 --> 00:01:56,680
你可以做预训练模型

45
00:01:56,719 --> 00:01:59,120
可以去做fine tune别的模型

46
00:01:59,359 --> 00:02:02,760
就有点像我们在图片处理的时候

47
00:02:02,760 --> 00:02:03,240
讲

48
00:02:03,240 --> 00:02:06,040
我在imginet上做一个预训练模型

49
00:02:06,200 --> 00:02:07,800
然后拿好之后

50
00:02:07,920 --> 00:02:10,680
然后去再fine tune别的任务

51
00:02:11,040 --> 00:02:13,560
所以它就是一个目前来说

52
00:02:13,680 --> 00:02:15,120
语言模型做的

53
00:02:16,560 --> 00:02:17,560
最为有用的

54
00:02:17,560 --> 00:02:19,000
就是作为预训练模型

55
00:02:19,759 --> 00:02:21,319
而第二个

56
00:02:21,519 --> 00:02:23,599
它就是说可以用来生成文本

57
00:02:23,840 --> 00:02:25,079
就是说GPT-3

58
00:02:25,079 --> 00:02:27,120
其实也是给大家演示的是说

59
00:02:27,120 --> 00:02:29,199
我这个模型怎么用来生成文本

60
00:02:29,239 --> 00:02:30,560
生成文本就是说

61
00:02:30,919 --> 00:02:32,519
你给定前面几个词

62
00:02:33,039 --> 00:02:34,199
然后不断的说

63
00:02:34,199 --> 00:02:36,479
我去给定这几个词

64
00:02:36,479 --> 00:02:38,120
去采样下一个词

65
00:02:38,560 --> 00:02:39,120
采样好之后

66
00:02:39,120 --> 00:02:40,359
再去采样下一个词

67
00:02:40,439 --> 00:02:42,359
就有点像我们就

68
00:02:42,359 --> 00:02:43,079
不是有点像

69
00:02:43,079 --> 00:02:44,840
就是我们之前说的

70
00:02:45,039 --> 00:02:46,079
训练模型里面

71
00:02:46,079 --> 00:02:47,959
我们预测sign函数

72
00:02:48,400 --> 00:02:49,400
正确函数

73
00:02:49,439 --> 00:02:50,439
每次给定前面

74
00:02:50,439 --> 00:02:52,039
这个只是预测下一个值

75
00:02:52,039 --> 00:02:53,319
就一直预测下去

76
00:02:53,920 --> 00:02:55,240
就当然你的模型得非常好

77
00:02:55,719 --> 00:02:57,000
如果你模型不好的话

78
00:02:57,000 --> 00:02:59,240
我们看到是你的误差会累积

79
00:02:59,520 --> 00:03:03,560
然后放到后面就没戏了

80
00:03:03,560 --> 00:03:04,040
对吧

81
00:03:06,360 --> 00:03:07,680
第三个应用

82
00:03:07,879 --> 00:03:09,000
这个其实也是一个

83
00:03:09,000 --> 00:03:10,520
非常常见的应用

84
00:03:10,560 --> 00:03:12,680
就是说我给你多个序列

85
00:03:12,680 --> 00:03:15,759
我来判断哪个序列最常见

86
00:03:16,000 --> 00:03:17,319
比如这个例子是说

87
00:03:17,560 --> 00:03:18,200
第一个例

88
00:03:18,320 --> 00:03:20,920
一句话叫to recognize a speech

89
00:03:20,920 --> 00:03:22,480
就是说去识别语言

90
00:03:23,000 --> 00:03:24,440
第二个句子是去

91
00:03:24,760 --> 00:03:25,760
有点音有点像

92
00:03:26,160 --> 00:03:27,800
to recognize speech

93
00:03:28,640 --> 00:03:29,320
就是说

94
00:03:30,040 --> 00:03:31,680
基本上你给人可以判断出来

95
00:03:32,000 --> 00:03:33,120
假设我说一句话

96
00:03:33,120 --> 00:03:34,680
我说的如果不标准的话

97
00:03:34,680 --> 00:03:36,560
你可能说我的模型出来

98
00:03:36,560 --> 00:03:38,360
我的语音模型识别出来

99
00:03:38,360 --> 00:03:40,240
可能会识别出我两个

100
00:03:40,320 --> 00:03:41,440
有两个可能性

101
00:03:41,480 --> 00:03:42,960
我识别出两个句

102
00:03:43,000 --> 00:03:44,600
就是说从语音上来说

103
00:03:44,600 --> 00:03:45,640
这两个句子差不多

104
00:03:45,640 --> 00:03:46,080
对吧

105
00:03:46,719 --> 00:03:48,520
所以说我的语音识别模型

106
00:03:48,520 --> 00:03:49,200
可能告诉你说

107
00:03:49,400 --> 00:03:51,120
这两个模型的概率都非常高

108
00:03:51,360 --> 00:03:52,720
那么接下来就是说

109
00:03:53,000 --> 00:03:54,680
我可以用一个语言模型

110
00:03:54,680 --> 00:03:56,160
来判断哪一个句子

111
00:03:56,160 --> 00:03:58,880
出现的概率更正常一点

112
00:03:59,120 --> 00:04:00,040
当然是第一句话

113
00:04:00,200 --> 00:04:02,640
去识别一个语言

114
00:04:02,920 --> 00:04:04,080
去识别一个语音

115
00:04:04,480 --> 00:04:06,760
第二个你说beach海滩

116
00:04:07,640 --> 00:04:10,040
当然就不那么常见

117
00:04:10,040 --> 00:04:11,640
所以基本上你可以认为说

118
00:04:11,640 --> 00:04:12,080
OK

119
00:04:12,080 --> 00:04:14,040
我觉得第一个句子

120
00:04:14,080 --> 00:04:15,600
出现的更频繁

121
00:04:15,599 --> 00:04:16,480
所以你这句话

122
00:04:16,480 --> 00:04:17,800
应该是讲的是第一个

123
00:04:18,639 --> 00:04:20,040
另外一个最简单

124
00:04:20,040 --> 00:04:20,839
也是说

125
00:04:20,839 --> 00:04:22,480
你说做打字的时候

126
00:04:23,120 --> 00:04:24,360
你中文书法的时候

127
00:04:24,360 --> 00:04:25,199
他要帮你

128
00:04:25,399 --> 00:04:26,920
你给一个拼音过去

129
00:04:26,920 --> 00:04:27,319
对吧

130
00:04:27,319 --> 00:04:28,839
他给你补全的时候

131
00:04:28,839 --> 00:04:31,040
他也是根据你的语言模型来看

132
00:04:31,079 --> 00:04:31,759
你这个句子

133
00:04:31,759 --> 00:04:33,279
大概出现的概率是多高

134
00:04:33,600 --> 00:04:35,079
他们甚至还可以

135
00:04:35,079 --> 00:04:36,519
他们大家都可以做

136
00:04:37,159 --> 00:04:38,519
你针对个性化

137
00:04:38,680 --> 00:04:41,600
就是针对你打字的习惯

138
00:04:41,600 --> 00:04:42,680
来生成你

139
00:04:43,000 --> 00:04:44,120
这样子你的

140
00:04:44,120 --> 00:04:45,000
根据你打字

141
00:04:45,000 --> 00:04:46,079
前面打字的习惯

142
00:04:46,079 --> 00:04:47,000
来判断说

143
00:04:47,399 --> 00:04:48,800
拼音一样的句子里面

144
00:04:49,000 --> 00:04:49,879
甚至可以纠错

145
00:04:50,120 --> 00:04:52,399
看看哪个是更符合你的使用习惯

146
00:04:52,840 --> 00:04:55,600
这都是属于语言模型的范畴

147
00:04:57,600 --> 00:04:59,120
首先接下来我们来看一个

148
00:05:00,519 --> 00:05:04,319
用一个最简单的模型来建模

149
00:05:04,319 --> 00:05:05,160
会怎么样子

150
00:05:05,959 --> 00:05:06,840
就是说语言模型

151
00:05:06,840 --> 00:05:08,120
我们可以用一个

152
00:05:09,280 --> 00:05:11,480
技术来进行建模

153
00:05:12,480 --> 00:05:15,920
就假设我们整个序列长度是为2的话

154
00:05:16,480 --> 00:05:17,680
那么我们要判断

155
00:05:17,680 --> 00:05:19,640
我一个常为2的序列怎么做

156
00:05:19,800 --> 00:05:21,280
我们昨天有讲过

157
00:05:21,280 --> 00:05:25,879
就是说你就是说我一个px和x1撇

158
00:05:26,160 --> 00:05:27,319
那么它可以写开

159
00:05:27,319 --> 00:05:30,319
就是说等于px和px1撇

160
00:05:30,319 --> 00:05:31,280
给定x

161
00:05:32,680 --> 00:05:34,200
那么如果你要算的话

162
00:05:34,200 --> 00:05:36,520
如果我能知道整个我的

163
00:05:36,560 --> 00:05:39,160
所谓的序列长度是为2的话

164
00:05:40,160 --> 00:05:42,439
那么我能去算的是说

165
00:05:42,960 --> 00:05:44,439
我第一个是一个

166
00:05:44,960 --> 00:05:45,640
我来个笔

167
00:05:47,800 --> 00:05:48,840
这第一个是

168
00:05:51,640 --> 00:05:55,040
n就是说你是整个词的个数

169
00:05:55,040 --> 00:05:57,360
就是说我就是那个cap的大小

170
00:05:58,080 --> 00:05:59,879
那么nx就表示说

171
00:05:59,920 --> 00:06:03,840
我这个词x在我们整个里面出现的

172
00:06:03,879 --> 00:06:04,480
个数

173
00:06:04,680 --> 00:06:05,760
那就是px

174
00:06:05,760 --> 00:06:07,560
这x这个东西出现了概率

175
00:06:10,160 --> 00:06:13,680
不好意思

176
00:06:13,680 --> 00:06:16,640
这个n是总词数是corpus

177
00:06:16,640 --> 00:06:20,640
就是我们采集到的所有的样本

178
00:06:21,400 --> 00:06:22,200
这个就不是

179
00:06:22,200 --> 00:06:25,280
就不是词

180
00:06:25,280 --> 00:06:26,160
vocabulary

181
00:06:26,160 --> 00:06:28,760
就是n就是我们可以看到整个文本

182
00:06:28,960 --> 00:06:29,640
整个文本

183
00:06:29,640 --> 00:06:31,000
我们采集到所有的文本

184
00:06:31,000 --> 00:06:33,400
里面n是里面的总词数

185
00:06:33,400 --> 00:06:35,720
或者说所谓的token的个数

186
00:06:36,240 --> 00:06:37,440
那么x就是说

187
00:06:37,439 --> 00:06:38,800
整个token的x

188
00:06:38,800 --> 00:06:40,639
在我们整个里面出现的概率

189
00:06:41,920 --> 00:06:42,879
出现的次数

190
00:06:42,879 --> 00:06:44,560
那除一下词数

191
00:06:44,719 --> 00:06:47,959
就是整个px出现的概率

192
00:06:49,000 --> 00:06:50,360
那么接下来就是说

193
00:06:51,680 --> 00:06:53,120
给定x

194
00:06:53,120 --> 00:06:57,079
然后再去算px一撇的话

195
00:06:57,079 --> 00:06:58,800
那就说首先看一下

196
00:06:58,839 --> 00:06:59,839
说

197
00:07:00,480 --> 00:07:01,839
x出现多少次

198
00:07:02,439 --> 00:07:03,600
另外一个是说

199
00:07:04,680 --> 00:07:07,079
在x后面再跟上x一撇

200
00:07:07,079 --> 00:07:09,439
这两个长面二的序列出现多少次

201
00:07:10,639 --> 00:07:10,919
OK

202
00:07:10,919 --> 00:07:11,919
就是他们一起

203
00:07:11,919 --> 00:07:13,279
以前以后出现的概率

204
00:07:13,279 --> 00:07:14,279
就是这个序列

205
00:07:14,279 --> 00:07:15,680
子序列出现的概率

206
00:07:16,199 --> 00:07:19,120
然后你一除最后一乘

207
00:07:19,479 --> 00:07:21,120
就可以得到你的结果

208
00:07:21,360 --> 00:07:23,079
当然你当然可以跟

209
00:07:23,079 --> 00:07:23,759
很简单

210
00:07:23,759 --> 00:07:24,039
就是说

211
00:07:24,039 --> 00:07:24,919
其实你说白了

212
00:07:24,919 --> 00:07:25,639
也就是

213
00:07:26,240 --> 00:07:27,120
你可把这个划掉

214
00:07:27,759 --> 00:07:28,439
说白了

215
00:07:28,439 --> 00:07:31,879
就是nx x一撇

216
00:07:31,879 --> 00:07:32,599
除以n

217
00:07:32,639 --> 00:07:33,879
就是说你这个

218
00:07:34,839 --> 00:07:36,959
序列x和x一撇的概率

219
00:07:37,079 --> 00:07:37,959
就是说整个序列

220
00:07:37,959 --> 00:07:40,680
在我们整个文本里面出现的次数

221
00:07:40,959 --> 00:07:41,920
除以你的次数

222
00:07:41,920 --> 00:07:42,879
当然你可以

223
00:07:43,560 --> 00:07:44,439
这么算

224
00:07:44,959 --> 00:07:45,959
但我们这么写开

225
00:07:46,120 --> 00:07:47,079
主要是从

226
00:07:47,920 --> 00:07:50,439
可以拓展到更通用的情况

227
00:07:52,079 --> 00:07:53,959
然后当然你很容易拓展到

228
00:07:53,959 --> 00:07:55,759
长为三的情况

229
00:07:56,000 --> 00:07:56,879
就三的情况

230
00:07:56,879 --> 00:07:57,600
就是

231
00:07:59,040 --> 00:08:00,159
px

232
00:08:00,439 --> 00:08:02,240
px一撇给定x

233
00:08:02,280 --> 00:08:03,519
那就是x一撇一撇

234
00:08:03,519 --> 00:08:04,400
给定前面

235
00:08:04,879 --> 00:08:06,120
然后你这么看的话

236
00:08:06,920 --> 00:08:08,519
第一个就是px

237
00:08:08,759 --> 00:08:10,600
第二个是px一撇给定x

238
00:08:10,600 --> 00:08:11,680
第三个就是说

239
00:08:11,680 --> 00:08:12,840
px一撇一撇

240
00:08:12,840 --> 00:08:15,079
给定x和x一撇

241
00:08:15,280 --> 00:08:15,680
OK

242
00:08:15,680 --> 00:08:16,800
当然你可以这么算

243
00:08:17,639 --> 00:08:18,160
OK

244
00:08:18,199 --> 00:08:19,439
这就是说

245
00:08:20,240 --> 00:08:20,800
这样的话

246
00:08:20,800 --> 00:08:22,639
我们能够算出

247
00:08:22,920 --> 00:08:25,160
给我一个任何长为二的序列

248
00:08:25,160 --> 00:08:26,079
就是说我给

249
00:08:26,079 --> 00:08:28,360
看这个序列出现的概率多少

250
00:08:28,560 --> 00:08:31,079
就是去扫一遍整个文本

251
00:08:31,079 --> 00:08:31,919
然后去统计一下

252
00:08:31,919 --> 00:08:32,559
它出现概率

253
00:08:32,559 --> 00:08:33,079
对吧

254
00:08:33,639 --> 00:08:34,759
这样子的话说

255
00:08:35,079 --> 00:08:36,399
语言模型我怎么实现

256
00:08:38,639 --> 00:08:39,519
就是说

257
00:08:39,960 --> 00:08:40,919
给我一个

258
00:08:40,919 --> 00:08:42,799
比如说现在还是做二和三的

259
00:08:43,000 --> 00:08:45,199
给我一个二和三长的序列

260
00:08:45,240 --> 00:08:46,840
我要判断这个序列出现的概率

261
00:08:46,840 --> 00:08:48,039
就是去看这个序列

262
00:08:48,039 --> 00:08:50,240
到底在文本中间出现多少次

263
00:08:50,519 --> 00:08:51,600
然后算出来之后

264
00:08:51,600 --> 00:08:54,039
然后一除你整个文本有多长

265
00:08:54,039 --> 00:08:54,720
那就知道了

266
00:08:54,759 --> 00:08:55,279
对吧

267
00:08:57,840 --> 00:09:00,480
所以但是这个东西会有个问题

268
00:09:00,600 --> 00:09:02,759
这个东西的问题是我们之前有讲过

269
00:09:02,759 --> 00:09:04,399
如果你很长怎么办

270
00:09:05,759 --> 00:09:06,840
很长的话

271
00:09:07,519 --> 00:09:08,120
一般来说

272
00:09:08,120 --> 00:09:09,360
我们有讲过说

273
00:09:09,360 --> 00:09:11,080
你可以用马尔科夫假设

274
00:09:12,680 --> 00:09:13,600
就是说

275
00:09:16,680 --> 00:09:17,840
马尔科夫假设是说

276
00:09:17,840 --> 00:09:19,920
假设你用一个

277
00:09:20,159 --> 00:09:21,159
这叫unground

278
00:09:21,360 --> 00:09:22,639
就是说一元语法

279
00:09:22,639 --> 00:09:23,600
或者二元语法

280
00:09:23,600 --> 00:09:24,120
三元语法

281
00:09:24,120 --> 00:09:27,039
就是或者经常叫background

282
00:09:31,480 --> 00:09:33,320
然后就是说

283
00:09:35,000 --> 00:09:35,759
一元语法

284
00:09:35,759 --> 00:09:37,720
就是我们看一下一元语法是什么意思

285
00:09:38,039 --> 00:09:39,560
一元语法意味就是说

286
00:09:39,560 --> 00:09:40,560
马尔科夫假设

287
00:09:41,360 --> 00:09:42,800
我们之前讲序列模型

288
00:09:42,800 --> 00:09:44,080
那个tau等于1

289
00:09:44,519 --> 00:09:46,519
也就是说我们算

290
00:09:46,720 --> 00:09:48,279
不是让个tau是等于0

291
00:09:48,440 --> 00:09:49,120
不是等于1

292
00:09:49,120 --> 00:09:50,639
就是说每次算

293
00:09:51,320 --> 00:09:52,680
一个x出现的概率

294
00:09:52,680 --> 00:09:54,360
我们不用管前面的东西

295
00:09:55,399 --> 00:09:56,279
具体来说怎么算

296
00:09:56,759 --> 00:09:58,000
假设我们要算一个

297
00:09:58,000 --> 00:10:00,440
常为4的一个序列的话

298
00:10:01,679 --> 00:10:02,039
那么

299
00:10:03,799 --> 00:10:06,440
x1 x2 x3 x4

300
00:10:06,639 --> 00:10:07,600
我们之前写法

301
00:10:07,600 --> 00:10:09,159
当时说你得全部展开

302
00:10:09,200 --> 00:10:11,720
假设我们只用一元语法的话

303
00:10:11,720 --> 00:10:12,200
那就是说

304
00:10:12,200 --> 00:10:13,840
我们基本上是认为

305
00:10:13,840 --> 00:10:15,279
每个词是独立的

306
00:10:15,960 --> 00:10:17,559
那就是说它写开之后

307
00:10:17,559 --> 00:10:20,360
就等于是px1 px2 px3 x4

308
00:10:20,399 --> 00:10:21,240
就是说x2

309
00:10:21,240 --> 00:10:23,200
我们因为一元语法

310
00:10:23,200 --> 00:10:26,360
就是马尔科夫假设里面tau等于0

311
00:10:26,440 --> 00:10:27,240
所以就是说

312
00:10:27,240 --> 00:10:29,800
算x2的概率的时候

313
00:10:29,960 --> 00:10:31,480
就是说它跟x1是独立的

314
00:10:31,480 --> 00:10:32,080
我们认为

315
00:10:32,159 --> 00:10:33,600
所以我们就可以一直写下去

316
00:10:34,159 --> 00:10:36,000
那么算起来就是等于是说

317
00:10:36,279 --> 00:10:38,279
你每一个

318
00:10:39,039 --> 00:10:40,560
你词的总个数

319
00:10:40,560 --> 00:10:43,639
除以你特定的词出现的概率

320
00:10:43,680 --> 00:10:45,399
然后把它一乘就行了

321
00:10:47,320 --> 00:10:48,840
然后第二个是二元语法的话

322
00:10:48,840 --> 00:10:50,519
二元语法就是说tau等于1

323
00:10:51,080 --> 00:10:52,720
就认为是我要算

324
00:10:53,600 --> 00:10:54,120
这个地方

325
00:10:54,320 --> 00:10:55,120
x2的时候

326
00:10:55,120 --> 00:10:56,720
它只依赖于x1

327
00:10:57,080 --> 00:10:57,919
x3的时候

328
00:10:57,919 --> 00:10:59,600
我只依赖于x2

329
00:10:59,720 --> 00:11:00,639
x4的时候

330
00:11:00,639 --> 00:11:02,039
只依赖于x3

331
00:11:02,720 --> 00:11:04,200
然后当你这么写下来

332
00:11:04,200 --> 00:11:04,799
就是说

333
00:11:05,840 --> 00:11:07,159
按照刚刚我们那个写法

334
00:11:07,279 --> 00:11:09,440
就是去数数把它数出来

335
00:11:11,840 --> 00:11:12,759
三元语法

336
00:11:12,960 --> 00:11:13,639
三元语法

337
00:11:13,639 --> 00:11:14,440
就是tau等于2

338
00:11:14,440 --> 00:11:15,440
就是每一个

339
00:11:16,879 --> 00:11:20,440
x3它就是只跟前面x1和x2相关

340
00:11:20,639 --> 00:11:22,759
我们x4的话

341
00:11:23,039 --> 00:11:24,919
就是跟x2和x3相关

342
00:11:24,960 --> 00:11:26,319
它就不跟x1相关

343
00:11:26,360 --> 00:11:27,639
就是说每一个词

344
00:11:27,679 --> 00:11:30,039
跟前面两个词是有关系的

345
00:11:30,039 --> 00:11:30,519
认为

346
00:11:32,679 --> 00:11:33,240
OK

347
00:11:33,279 --> 00:11:34,840
所以这个东西好处是什么样子

348
00:11:35,879 --> 00:11:37,360
好处就是说

349
00:11:38,200 --> 00:11:39,600
第一个好处是说

350
00:11:41,919 --> 00:11:42,879
这也不是第一个好处

351
00:11:43,080 --> 00:11:44,759
最大的好处是说

352
00:11:44,799 --> 00:11:47,200
你可以处理比较长的序列

353
00:11:48,639 --> 00:11:48,919
就是说

354
00:11:48,919 --> 00:11:50,519
如果你有个很长的序列的话

355
00:11:50,519 --> 00:11:51,360
如果我给你一个

356
00:11:51,360 --> 00:11:53,159
比如说长为10

357
00:11:53,320 --> 00:11:55,039
或长为20的序列的话

358
00:11:55,320 --> 00:11:58,000
那么你的做法是说

359
00:11:58,320 --> 00:12:00,919
我很难把它存下来

360
00:12:01,120 --> 00:12:01,360
对吧

361
00:12:01,399 --> 00:12:02,879
我不可能把整个文本里面

362
00:12:02,879 --> 00:12:03,639
长为

363
00:12:04,639 --> 00:12:05,919
任何长度的序列

364
00:12:05,919 --> 00:12:06,680
都给你存下来

365
00:12:06,680 --> 00:12:09,360
因为这个是一个指数级的复杂度

366
00:12:10,800 --> 00:12:13,320
所以我能够做的

367
00:12:13,320 --> 00:12:13,560
就是说

368
00:12:13,560 --> 00:12:14,680
给你一个任意长的序列

369
00:12:14,680 --> 00:12:17,519
那么我去看你这个文本里面

370
00:12:17,519 --> 00:12:19,399
有真的出现过多少次

371
00:12:19,600 --> 00:12:20,639
很多时候就扫一遍

372
00:12:20,639 --> 00:12:21,120
对吧

373
00:12:21,360 --> 00:12:21,759
扫一遍

374
00:12:21,759 --> 00:12:22,039
就是说

375
00:12:22,039 --> 00:12:24,639
任何查询自我的开销是ON

376
00:12:24,639 --> 00:12:25,759
N是你的长度

377
00:12:26,800 --> 00:12:28,480
那么做到N远余法的话

378
00:12:28,480 --> 00:12:31,200
那么我给一个任意长的东西

379
00:12:31,240 --> 00:12:34,480
其实我要看的子序列是固定的

380
00:12:34,480 --> 00:12:35,600
就是比如说二远余法

381
00:12:36,519 --> 00:12:37,200
二远余法

382
00:12:37,320 --> 00:12:38,919
我每次只看

383
00:12:39,480 --> 00:12:40,800
长为2的序列

384
00:12:42,399 --> 00:12:43,039
我可以怎么做

385
00:12:43,759 --> 00:12:46,320
我能够把所有的N

386
00:12:46,759 --> 00:12:47,639
X1 X2

387
00:12:47,639 --> 00:12:50,159
就X1 X2是任何一个词代表

388
00:12:50,319 --> 00:12:51,679
全部给你存下来

389
00:12:52,639 --> 00:12:53,839
就怎么算

390
00:12:53,919 --> 00:12:54,639
就是说

391
00:12:56,039 --> 00:12:59,000
假设你词有1000个词的话

392
00:12:59,000 --> 00:13:01,480
假设你的字典里面有1000个词

393
00:13:01,639 --> 00:13:03,519
我们的词和token是混着用的

394
00:13:03,679 --> 00:13:04,360
我们这里

395
00:13:05,319 --> 00:13:06,799
就假设你1000个词

396
00:13:07,079 --> 00:13:10,360
然后你长为2的可能性多少

397
00:13:10,360 --> 00:13:11,279
就1000乘1000

398
00:13:11,319 --> 00:13:11,839
对吧

399
00:13:11,959 --> 00:13:13,039
那就是100万

400
00:13:13,199 --> 00:13:16,159
那就存100万个可能性

401
00:13:16,159 --> 00:13:19,039
然后把每一个词组合另外一个词

402
00:13:19,039 --> 00:13:20,799
它在文本中出现的概率

403
00:13:20,799 --> 00:13:22,000
全部存起来

404
00:13:22,959 --> 00:13:24,480
那么就是存100万个数

405
00:13:24,919 --> 00:13:27,000
然后你把一个词出现的概率

406
00:13:27,000 --> 00:13:27,519
也存下来

407
00:13:27,519 --> 00:13:28,360
就是NX1

408
00:13:29,120 --> 00:13:30,399
存下来就1000个数

409
00:13:30,399 --> 00:13:31,360
然后存个N

410
00:13:31,439 --> 00:13:32,439
就是说你这样存

411
00:13:32,439 --> 00:13:33,839
大概是100万个数的话

412
00:13:33,879 --> 00:13:34,879
那么你下一次

413
00:13:34,879 --> 00:13:37,599
我让你查询任何一个长

414
00:13:37,639 --> 00:13:38,799
任意长的序列

415
00:13:39,159 --> 00:13:42,279
我的时间复杂度

416
00:13:42,559 --> 00:13:43,719
那就是OL

417
00:13:43,719 --> 00:13:45,319
L是你的序列的长度的话

418
00:13:45,319 --> 00:13:45,879
叫OT

419
00:13:46,039 --> 00:13:47,599
我们用的是t叫OT

420
00:13:48,600 --> 00:13:50,040
三元语法的话

421
00:13:50,040 --> 00:13:50,639
你是一样的

422
00:13:50,759 --> 00:13:52,279
你可以把它存起来

423
00:13:52,399 --> 00:13:54,759
就是所有长度为3的序列

424
00:13:54,800 --> 00:13:55,399
这样子

425
00:13:55,399 --> 00:13:57,440
你在给你一个语言的时候

426
00:13:57,440 --> 00:13:58,040
你就查询

427
00:13:58,040 --> 00:13:58,879
那就OT

428
00:13:59,960 --> 00:14:02,159
但是这个东西的问题是说一样的

429
00:14:02,279 --> 00:14:04,040
这个是根据你的N元语法

430
00:14:04,399 --> 00:14:05,960
跟N是一个指数关系

431
00:14:06,000 --> 00:14:07,480
当你的N变得很长的时候

432
00:14:07,480 --> 00:14:09,600
我要存的东西就会变得很大

433
00:14:10,480 --> 00:14:12,639
所以这个也是会带来一个问题

434
00:14:13,040 --> 00:14:15,759
所以基本上大家一元语法用的不多

435
00:14:16,559 --> 00:14:17,159
一元语法

436
00:14:17,159 --> 00:14:19,799
因为就是说完全忽略掉了

437
00:14:19,799 --> 00:14:20,840
任何时序信息

438
00:14:21,399 --> 00:14:22,080
二元语法

439
00:14:22,240 --> 00:14:24,200
三元语法用的还是挺多的

440
00:14:26,360 --> 00:14:27,480
这个是

441
00:14:28,519 --> 00:14:29,679
Mark就是说

442
00:14:29,679 --> 00:14:32,319
使用Marker附加式的N元语法的好处

443
00:14:32,759 --> 00:14:34,639
使得你的复杂度用

444
00:14:34,919 --> 00:14:36,639
如果你把这个东西存起来的话

445
00:14:36,639 --> 00:14:38,519
那么你在计算复杂

446
00:14:38,519 --> 00:14:40,679
计算一个序列的复杂度

447
00:14:40,679 --> 00:14:41,600
变成OT

448
00:14:42,000 --> 00:14:45,360
而不是说最之前的用ON这个复杂度

449
00:14:45,680 --> 00:14:47,080
因为OT这个东西很重要

450
00:14:47,080 --> 00:14:48,519
因为就是说

451
00:14:48,519 --> 00:14:49,320
我要真

452
00:14:49,320 --> 00:14:51,800
我的语量库真的会通常会很大

453
00:14:51,800 --> 00:14:53,039
就是说可能是几十G

454
00:14:53,200 --> 00:14:55,240
或者现在可以做到几百G

455
00:14:55,279 --> 00:14:56,399
或者一个T都有可能

456
00:14:57,480 --> 00:14:59,360
所以这样的情况下

457
00:14:59,399 --> 00:15:02,240
所以我判断一个句子的概率的话

458
00:15:02,480 --> 00:15:03,159
这个东西

459
00:15:03,159 --> 00:15:05,480
我可能每秒钟需要做个100万次的样子

460
00:15:05,720 --> 00:15:07,600
因为我不但做语音识别

461
00:15:07,840 --> 00:15:09,480
我做打字的时候

462
00:15:09,480 --> 00:15:11,639
我得去实时的算这个东西

463
00:15:11,639 --> 00:15:13,519
所以计算复杂度非常关键

464
00:15:13,919 --> 00:15:14,319
这样子

465
00:15:14,319 --> 00:15:15,279
你在n元语法

466
00:15:15,279 --> 00:15:16,600
在他考虑的时候

467
00:15:16,600 --> 00:15:19,919
确实会去考虑你的精度

468
00:15:19,919 --> 00:15:21,120
就是你n越大

469
00:15:21,120 --> 00:15:22,159
当然你精度越高

470
00:15:22,159 --> 00:15:23,519
但是n越大

471
00:15:23,519 --> 00:15:26,799
你的算复杂度还行

472
00:15:26,960 --> 00:15:29,159
主要是你的空间复杂度变得特别大

473
00:15:29,480 --> 00:15:30,000
OK

474
00:15:30,039 --> 00:15:31,399
但即使这样子

475
00:15:31,600 --> 00:15:32,279
二元语法

476
00:15:32,279 --> 00:15:32,840
三元语法

477
00:15:32,840 --> 00:15:35,799
也是非常常见的一个模型

478
00:15:37,559 --> 00:15:37,919
OK

479
00:15:37,919 --> 00:15:38,519
总结一下

480
00:15:38,519 --> 00:15:39,120
就是说

481
00:15:40,000 --> 00:15:40,960
语言模型

482
00:15:41,200 --> 00:15:44,519
它其实就是估计一个文本序列出现的

483
00:15:44,519 --> 00:15:45,560
联合概率

484
00:15:47,040 --> 00:15:48,200
所以它就是

485
00:15:49,080 --> 00:15:50,320
叫做language model

486
00:15:50,320 --> 00:15:53,560
也是NLP领域最常见的一个应用

487
00:15:53,960 --> 00:15:55,639
如果你要统计算法的话

488
00:15:55,639 --> 00:15:55,960
就是说

489
00:15:55,960 --> 00:15:58,200
最常见的用的是n元语法

490
00:15:58,360 --> 00:15:59,160
就n元语法

491
00:15:59,160 --> 00:15:59,840
就是说白了

492
00:15:59,840 --> 00:16:03,160
就是看每次看一个常为n的一个子序列

493
00:16:03,200 --> 00:16:04,639
去数数

494
00:16:04,840 --> 00:16:06,519
然后给一个长的序列的话

495
00:16:06,519 --> 00:16:09,639
我就把它拆成很多个连续的n的

496
00:16:10,960 --> 00:16:13,720
序列把它可以就可以算出概率来

497
00:16:14,200 --> 00:16:17,040
这个就是一个基于马可夫的一个统计模型

