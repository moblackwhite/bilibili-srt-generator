1
00:00:00,000 --> 00:00:05,560
微调是我觉得是计算机视觉里面最重

2
00:00:05,560 --> 00:00:08,720
特别是对深度学习来讲最重要的一个基础

3
00:00:09,480 --> 00:00:14,120
你可以发现整个深度学习为什么能够work

4
00:00:14,679 --> 00:00:16,480
这是因为有微调在

5
00:00:16,519 --> 00:00:19,080
所谓的微调也叫transfer learning

6
00:00:19,080 --> 00:00:20,760
就是迁移学习

7
00:00:21,160 --> 00:00:27,199
就是说这个是改变了整个计算机视觉的一个方法

8
00:00:28,199 --> 00:00:31,480
假设你们前面东西都可以忘的话

9
00:00:31,480 --> 00:00:34,320
我觉得这一点是最图片分类

10
00:00:34,439 --> 00:00:36,079
或者说计算机视觉任务

11
00:00:36,119 --> 00:00:40,159
现在越来越多NLP在原处理也是靠微调

12
00:00:40,200 --> 00:00:44,000
所以是说这个可能是最重要的一个基础

13
00:00:44,039 --> 00:00:46,120
大家一定是要搞清楚的

14
00:00:48,640 --> 00:00:49,719
好 我们来看一下

15
00:00:49,759 --> 00:00:51,679
我们要微调是什么意思

16
00:00:52,600 --> 00:00:54,920
就是说首先我们可以看一下

17
00:00:55,120 --> 00:00:57,480
ImageNet当年标ImageNet

18
00:00:57,480 --> 00:00:58,800
他们花了几百万美金

19
00:00:59,800 --> 00:01:01,120
就那个是很多年前

20
00:01:01,120 --> 00:01:05,000
所以大家当然现在标可能就很便宜了

21
00:01:05,240 --> 00:01:06,920
在当年标了100

22
00:01:06,960 --> 00:01:09,120
其实ImageNet标了1200万张

23
00:01:09,280 --> 00:01:10,240
1000多万张

24
00:01:10,280 --> 00:01:13,079
实际用的是120万张图片

25
00:01:13,120 --> 00:01:14,040
用的比较多的

26
00:01:14,079 --> 00:01:15,480
然后类别数是1000

27
00:01:15,960 --> 00:01:17,719
这是比较大的数据集了算

28
00:01:17,960 --> 00:01:21,359
在Fashion M list就是6万张数样本

29
00:01:21,400 --> 00:01:23,400
然后10类就是很小

30
00:01:24,240 --> 00:01:26,280
这个是80年代标的数据

31
00:01:26,840 --> 00:01:28,280
所以但是真正的就是说

32
00:01:28,280 --> 00:01:30,880
你一般要用的数据是什么样子

33
00:01:31,000 --> 00:01:33,040
你不会像ImageNet那么大

34
00:01:33,040 --> 00:01:33,560
标一个

35
00:01:33,560 --> 00:01:34,880
现在标也标

36
00:01:34,880 --> 00:01:36,400
重新标ImageNet也要

37
00:01:36,440 --> 00:01:38,080
可能十几万人民币也要

38
00:01:39,360 --> 00:01:40,040
你的数据集

39
00:01:40,040 --> 00:01:41,480
通常来说是说

40
00:01:41,520 --> 00:01:46,880
我可能就图片可能就5万张图片

41
00:01:46,880 --> 00:01:48,600
比如说我需要识别所有的车

42
00:01:49,280 --> 00:01:50,719
类别一般是100

43
00:01:51,040 --> 00:01:51,880
就为什么这么说

44
00:01:51,959 --> 00:01:53,599
就是说你一般做什么

45
00:01:53,599 --> 00:01:55,280
一般说我要识别一些物体

46
00:01:55,319 --> 00:01:56,719
我特别感兴趣的物体

47
00:01:56,719 --> 00:01:59,679
比如说我是卖商品的公司

48
00:01:59,719 --> 00:02:01,679
或者我说我自己家的产品

49
00:02:01,679 --> 00:02:02,599
可能你家的产品

50
00:02:02,599 --> 00:02:03,679
可能就100类差不多

51
00:02:03,840 --> 00:02:04,319
这个类别

52
00:02:04,519 --> 00:02:06,920
你家有一不同型号的

53
00:02:06,920 --> 00:02:08,479
可能就100类个东西

54
00:02:09,479 --> 00:02:10,800
每一个东西大概是说

55
00:02:10,800 --> 00:02:12,479
你标个500张图片也不错了

56
00:02:12,960 --> 00:02:14,759
一个东西我对他一身猛

57
00:02:14,759 --> 00:02:16,079
拍500张也不容易

58
00:02:16,159 --> 00:02:17,280
各个角度各个框罩

59
00:02:17,280 --> 00:02:17,719
对吧

60
00:02:17,960 --> 00:02:18,919
所以就是说

61
00:02:18,919 --> 00:02:20,599
你可能你自己的东西

62
00:02:21,319 --> 00:02:23,400
一个5万个样本

63
00:02:23,400 --> 00:02:24,840
然后100类的样子

64
00:02:25,719 --> 00:02:26,879
这个是你一般的

65
00:02:26,879 --> 00:02:30,079
你自己的数准备数据的尺寸

66
00:02:31,159 --> 00:02:32,079
但是有个问题

67
00:02:32,560 --> 00:02:33,439
有问题是什么

68
00:02:33,479 --> 00:02:35,639
我们知道数据级越大越好

69
00:02:36,199 --> 00:02:37,159
你那么数据级

70
00:02:37,159 --> 00:02:39,879
你感觉没有比Fashion Amnesty好多少

71
00:02:40,000 --> 00:02:41,479
Fashion Amnesty还有6万张了

72
00:02:41,599 --> 00:02:41,879
对吧

73
00:02:41,879 --> 00:02:42,799
这也6万张

74
00:02:43,560 --> 00:02:44,519
所以怎么办

75
00:02:45,079 --> 00:02:45,799
通常来说

76
00:02:45,840 --> 00:02:50,479
就是说你希望能在很大的数据上

77
00:02:50,479 --> 00:02:51,519
训练好的东西

78
00:02:51,519 --> 00:02:52,639
训练好的模型

79
00:02:52,679 --> 00:02:55,799
能够帮助你提升你的数据级上的进度

80
00:02:56,239 --> 00:02:58,519
反正你就是认一认一些小细节

81
00:02:59,000 --> 00:02:59,679
就认一认

82
00:02:59,679 --> 00:03:00,439
比如说

83
00:03:01,079 --> 00:03:01,679
就是一个

84
00:03:01,679 --> 00:03:03,479
比如一个对一个人来讲

85
00:03:04,199 --> 00:03:06,560
我识别了一些东西之后

86
00:03:06,560 --> 00:03:08,199
碰到一个识别一个新的物体

87
00:03:08,199 --> 00:03:09,039
去认一个新的物体

88
00:03:09,039 --> 00:03:09,879
给我看一次

89
00:03:09,879 --> 00:03:10,399
我就记住了

90
00:03:10,399 --> 00:03:10,759
对吧

91
00:03:10,759 --> 00:03:11,799
谁都可以记住

92
00:03:12,000 --> 00:03:12,879
所以就是说

93
00:03:12,919 --> 00:03:13,560
你希望说

94
00:03:13,560 --> 00:03:16,599
我假设已经对整个物体识别

95
00:03:16,599 --> 00:03:18,560
有一定的基础的情况下

96
00:03:18,800 --> 00:03:20,240
我真的不需要

97
00:03:20,240 --> 00:03:21,520
在它的基础上

98
00:03:21,520 --> 00:03:22,920
我真的不需要很多数据

99
00:03:22,920 --> 00:03:23,800
就能做好

100
00:03:23,840 --> 00:03:26,280
这其实就是一个核心的思想

101
00:03:26,280 --> 00:03:28,680
这也是人工智能所追求的一个目标

102
00:03:28,719 --> 00:03:29,280
就是说

103
00:03:29,280 --> 00:03:30,680
我给你教了一点点东西

104
00:03:30,680 --> 00:03:32,199
以后你可自己是

105
00:03:32,759 --> 00:03:35,199
几乎再不用我来再教你

106
00:03:35,199 --> 00:03:37,039
或者说你就教一点点时候

107
00:03:37,039 --> 00:03:38,840
你可以拓展到别的地方去

108
00:03:38,960 --> 00:03:41,039
这也是人工智能最重要的一个

109
00:03:42,199 --> 00:03:42,920
一个基石

110
00:03:42,960 --> 00:03:45,000
这也是人类学习的时候

111
00:03:45,480 --> 00:03:47,199
基本上大家都掌握了一个技能

112
00:03:49,159 --> 00:03:49,680
好

113
00:03:49,680 --> 00:03:50,039
然后

114
00:03:51,120 --> 00:03:52,560
就是说我们仔细看一下

115
00:03:52,560 --> 00:03:53,719
我们可以怎么做

116
00:03:55,080 --> 00:03:55,960
一般来说

117
00:03:55,960 --> 00:03:58,520
一个神经网络可以分成两块

118
00:03:59,759 --> 00:04:01,080
就一块是说

119
00:04:01,080 --> 00:04:03,520
我们一个图片过来

120
00:04:03,520 --> 00:04:05,039
就说我这是一只猫

121
00:04:05,400 --> 00:04:07,159
然后你可以认为

122
00:04:07,280 --> 00:04:09,319
我最下的那一块

123
00:04:09,319 --> 00:04:11,319
是在做特征的提取

124
00:04:12,640 --> 00:04:13,439
就特征提取

125
00:04:13,439 --> 00:04:13,759
就是说

126
00:04:13,759 --> 00:04:16,600
我将原始的像素变换成

127
00:04:16,680 --> 00:04:19,120
最后能够进行

128
00:04:19,120 --> 00:04:21,600
很容易做线性分割的一些特征

129
00:04:23,040 --> 00:04:25,240
最后一层就是有一个全连接层

130
00:04:25,240 --> 00:04:27,720
然后再加一个softmax做分类

131
00:04:27,720 --> 00:04:28,040
就是说

132
00:04:28,040 --> 00:04:28,760
你可以认为

133
00:04:28,800 --> 00:04:31,400
它这里就是一个很简单的

134
00:04:31,400 --> 00:04:32,520
线性分类器

135
00:04:32,840 --> 00:04:34,680
也就是一个softmax regression

136
00:04:34,680 --> 00:04:35,920
softmax回归

137
00:04:36,840 --> 00:04:37,720
就你几乎认为

138
00:04:37,720 --> 00:04:39,840
所有做分类的那一些神经网络

139
00:04:39,840 --> 00:04:40,960
都是长成这样子

140
00:04:41,000 --> 00:04:42,840
下面你先不管长什么样子

141
00:04:42,879 --> 00:04:43,680
最后的最后

142
00:04:43,680 --> 00:04:45,760
你可能真的就是一个softmax

143
00:04:45,759 --> 00:04:46,800
回归在那个地方

144
00:04:47,039 --> 00:04:48,120
所以你可以简单认为

145
00:04:48,120 --> 00:04:48,599
是说

146
00:04:48,599 --> 00:04:49,519
你的神经网络

147
00:04:49,519 --> 00:04:50,800
是包含了两部分

148
00:04:51,639 --> 00:04:52,560
一部分

149
00:04:52,680 --> 00:04:54,680
就是做的是特征抽取

150
00:04:55,719 --> 00:04:56,399
另外一部分

151
00:04:56,399 --> 00:04:58,800
做的是你的信息分类

152
00:04:59,680 --> 00:05:00,759
所以很多时候

153
00:05:00,759 --> 00:05:01,599
你就是说

154
00:05:01,639 --> 00:05:03,599
大家为什么把神经网络

155
00:05:03,599 --> 00:05:05,039
做成是一个end to end

156
00:05:05,039 --> 00:05:05,399
就是说

157
00:05:05,399 --> 00:05:06,560
从原始的pixel

158
00:05:06,560 --> 00:05:08,120
到最后的余意信息

159
00:05:08,279 --> 00:05:10,639
这是而且它使得

160
00:05:10,680 --> 00:05:13,079
所以说深度神经网络

161
00:05:13,079 --> 00:05:14,480
对于整个

162
00:05:14,480 --> 00:05:15,520
技术人际视觉

163
00:05:15,520 --> 00:05:16,280
不同领域的

164
00:05:16,280 --> 00:05:17,319
它的突破性的进展

165
00:05:17,319 --> 00:05:17,920
就是说

166
00:05:17,960 --> 00:05:18,680
这一块

167
00:05:18,680 --> 00:05:21,080
就是让你的特征提取

168
00:05:21,080 --> 00:05:22,680
变得可以学习

169
00:05:23,000 --> 00:05:25,120
而不是说人去想怎么提取特征

170
00:05:26,160 --> 00:05:26,759
OK

171
00:05:27,040 --> 00:05:27,960
所以就是说

172
00:05:27,960 --> 00:05:29,040
你可以简单认为

173
00:05:29,360 --> 00:05:31,160
除了最后一块

174
00:05:31,360 --> 00:05:32,280
剩下所有东西

175
00:05:32,280 --> 00:05:33,439
都在做特征提取

176
00:05:33,480 --> 00:05:35,160
通过学习的手段

177
00:05:35,200 --> 00:05:36,640
把你的图片信息

178
00:05:36,640 --> 00:05:38,600
能够把原始的数据

179
00:05:38,600 --> 00:05:39,800
能够转换到一个

180
00:05:39,800 --> 00:05:41,600
可以线性分类的一个空间里面

181
00:05:41,600 --> 00:05:42,920
一个语义空间里面

182
00:05:45,480 --> 00:05:46,319
好

183
00:05:46,319 --> 00:05:47,360
微调的意思

184
00:05:47,360 --> 00:05:47,720
就是说

185
00:05:48,720 --> 00:05:51,800
假设我在一个元数据集上

186
00:05:52,640 --> 00:05:54,160
这里的元数据集

187
00:05:54,200 --> 00:05:55,240
那就是一个

188
00:05:55,280 --> 00:05:57,160
我们说是imageNet

189
00:05:57,759 --> 00:05:59,720
在一个比较大的数据集上

190
00:05:59,759 --> 00:06:01,520
已经训练好了一个模型

191
00:06:02,560 --> 00:06:03,480
同样的道理是说

192
00:06:03,480 --> 00:06:04,040
那么这一块

193
00:06:04,040 --> 00:06:06,560
就是对整个图片

194
00:06:06,560 --> 00:06:07,400
imageNet图片

195
00:06:07,400 --> 00:06:09,680
做语义的抽取

196
00:06:09,960 --> 00:06:11,720
最后得到一个信息分类器

197
00:06:12,319 --> 00:06:13,879
那么我可以认为是说

198
00:06:14,199 --> 00:06:15,279
假设这个模型

199
00:06:15,279 --> 00:06:17,079
确实是按照我们想的方式

200
00:06:17,079 --> 00:06:17,879
来做的话

201
00:06:17,920 --> 00:06:19,959
那么这一块东西

202
00:06:20,240 --> 00:06:21,959
对图片的特征提取

203
00:06:22,159 --> 00:06:23,759
你对imageNet行

204
00:06:23,800 --> 00:06:25,959
对我的图片数据应该也行

205
00:06:26,399 --> 00:06:27,159
就应该是说

206
00:06:27,159 --> 00:06:28,439
你学好了这一块东西

207
00:06:28,439 --> 00:06:29,079
对我来讲

208
00:06:29,079 --> 00:06:30,759
应该也是可以做比较好

209
00:06:30,800 --> 00:06:32,480
是一个很不错的

210
00:06:32,480 --> 00:06:33,680
作为特征提取的

211
00:06:33,680 --> 00:06:34,480
一个

212
00:06:34,639 --> 00:06:36,639
一个一开始的做法

213
00:06:36,680 --> 00:06:37,480
总比随机好

214
00:06:37,480 --> 00:06:38,040
对吧

215
00:06:38,519 --> 00:06:40,480
我随机给我生成东西

216
00:06:40,480 --> 00:06:41,800
我随机我都不知道在干嘛

217
00:06:41,839 --> 00:06:43,120
你的东西肯定比我随机

218
00:06:43,120 --> 00:06:43,920
好那么一点点

219
00:06:45,439 --> 00:06:47,120
但是我最后一层

220
00:06:47,120 --> 00:06:48,280
可能就不能直接用了

221
00:06:48,280 --> 00:06:49,079
是因为

222
00:06:49,920 --> 00:06:51,360
我的标号变了

223
00:06:51,360 --> 00:06:53,959
所以我很难重用你最后一层

224
00:06:55,280 --> 00:06:57,399
所以核心思想的微调

225
00:06:57,399 --> 00:06:58,079
就是说

226
00:06:58,120 --> 00:07:00,000
在一个原数据上

227
00:07:00,000 --> 00:07:01,600
叫source data set上

228
00:07:01,639 --> 00:07:03,040
通常是比较大的数据上

229
00:07:03,040 --> 00:07:04,160
训练的模型

230
00:07:04,879 --> 00:07:08,600
我们觉得可以把它做特征提取

231
00:07:08,600 --> 00:07:08,959
那一块

232
00:07:08,959 --> 00:07:10,439
可以拿来重新用一用

233
00:07:10,560 --> 00:07:12,840
在我的目标数据上重新用一用

234
00:07:13,040 --> 00:07:13,640
当然这一块

235
00:07:13,640 --> 00:07:15,440
我们就可能就没有太多用了

236
00:07:17,720 --> 00:07:19,440
实际上来说是怎么做的

237
00:07:20,320 --> 00:07:22,920
就假设我在原数据上

238
00:07:22,920 --> 00:07:24,520
训练好了一个模型

239
00:07:24,600 --> 00:07:26,720
这种模型一般叫做pre-trained

240
00:07:28,000 --> 00:07:28,560
就pre-trained

241
00:07:28,560 --> 00:07:29,800
就是在我的

242
00:07:29,800 --> 00:07:31,320
就在我的training之前

243
00:07:31,320 --> 00:07:32,320
pre-trained model

244
00:07:34,040 --> 00:07:35,960
然后训练好之后

245
00:07:37,040 --> 00:07:39,000
我把它来

246
00:07:39,160 --> 00:07:41,240
我在我重新训练

247
00:07:41,240 --> 00:07:43,199
我自己的数据上

248
00:07:43,199 --> 00:07:44,800
重新训练模型的时候

249
00:07:45,439 --> 00:07:48,319
我使用一个跟pre-trained

250
00:07:48,319 --> 00:07:50,120
是一样的架构的模型

251
00:07:50,439 --> 00:07:52,720
如果你用的是resnet18的话

252
00:07:52,759 --> 00:07:54,759
那么我也用resnet18

253
00:07:56,079 --> 00:07:57,280
我做

254
00:07:57,680 --> 00:07:59,720
所有除了最后一层的

255
00:07:59,720 --> 00:08:01,840
模型的初始化的时候

256
00:08:02,400 --> 00:08:03,680
我的模型初始化

257
00:08:03,680 --> 00:08:05,800
不再是随机的初始化

258
00:08:07,000 --> 00:08:08,680
而是从你那边的

259
00:08:08,680 --> 00:08:10,120
训练好的模型的weight

260
00:08:10,120 --> 00:08:11,000
copy过来

261
00:08:11,360 --> 00:08:13,000
等价于是我把你的特征

262
00:08:13,000 --> 00:08:15,439
提取模块复制过来

263
00:08:15,439 --> 00:08:17,759
作为我初始化的模型

264
00:08:17,879 --> 00:08:20,199
使得我一开始就能做到

265
00:08:20,199 --> 00:08:21,960
还不错的一些特征的

266
00:08:21,960 --> 00:08:23,800
一个表达

267
00:08:24,079 --> 00:08:24,920
但最后一层

268
00:08:24,920 --> 00:08:26,960
因为我们标号可能不一样

269
00:08:26,960 --> 00:08:28,040
所以我们就不管了

270
00:08:28,040 --> 00:08:28,639
所以最后一层

271
00:08:28,639 --> 00:08:30,120
我们就可以随机初始化

272
00:08:31,319 --> 00:08:32,360
那么就是说

273
00:08:33,120 --> 00:08:34,320
那么在我

274
00:08:34,320 --> 00:08:36,399
对我自己的训练机的一开始

275
00:08:36,399 --> 00:08:36,919
那么就是说

276
00:08:36,919 --> 00:08:38,560
我的特征提取还不错

277
00:08:38,720 --> 00:08:40,000
我这一块就反正也不知道

278
00:08:40,000 --> 00:08:40,799
反正重新去

279
00:08:40,799 --> 00:08:41,360
重新劝

280
00:08:41,799 --> 00:08:42,960
反正他也劝的比较快

281
00:08:42,960 --> 00:08:43,200
对吧

282
00:08:43,200 --> 00:08:44,960
因为你的loss是从下面下来

283
00:08:44,960 --> 00:08:45,559
所以最后一次

284
00:08:45,559 --> 00:08:46,759
永远是劝的比较快的

285
00:08:47,120 --> 00:08:47,840
所以就是说

286
00:08:47,840 --> 00:08:48,600
然后下面一层

287
00:08:49,360 --> 00:08:50,399
我再慢慢的调

288
00:08:50,559 --> 00:08:51,519
就是叫微调

289
00:08:52,080 --> 00:08:53,519
下面层已经是不错的了

290
00:08:53,519 --> 00:08:55,000
但是还会根据我的数据

291
00:08:55,000 --> 00:08:56,200
分布可能不一样

292
00:08:56,360 --> 00:08:57,000
什么不一样

293
00:08:57,000 --> 00:08:59,600
重新再稍微学习一下

294
00:08:59,600 --> 00:09:00,799
但是很有可能

295
00:09:01,320 --> 00:09:03,440
已经是初始化的时候

296
00:09:03,440 --> 00:09:04,679
已经跟我最终的结果

297
00:09:04,679 --> 00:09:05,559
已经很像了

298
00:09:05,720 --> 00:09:08,519
所以就不需要太去训练

299
00:09:09,519 --> 00:09:09,840
OK

300
00:09:09,840 --> 00:09:14,039
这就是微调的核心的一个想法

301
00:09:18,519 --> 00:09:18,799
好

302
00:09:18,799 --> 00:09:19,639
我们来看一下

303
00:09:19,639 --> 00:09:20,360
就是说

304
00:09:21,000 --> 00:09:22,240
他的训练

305
00:09:23,120 --> 00:09:23,919
训练就是说

306
00:09:23,919 --> 00:09:25,559
我是一个目标数据以上的

307
00:09:25,559 --> 00:09:26,639
一个正常训练任务

308
00:09:26,840 --> 00:09:29,240
就是在

309
00:09:29,360 --> 00:09:31,720
但是就跟训练没有什么区别

310
00:09:31,720 --> 00:09:34,240
但是使用了更强的政策化

311
00:09:35,679 --> 00:09:36,159
为什么

312
00:09:36,159 --> 00:09:37,840
是因为我们通常会使用

313
00:09:37,840 --> 00:09:38,840
更小的学习率

314
00:09:38,840 --> 00:09:41,680
因为你的已经比较好了

315
00:09:41,680 --> 00:09:43,360
已经跟我的最优解比较近了

316
00:09:43,360 --> 00:09:45,000
我不需要特别强的学习率

317
00:09:45,160 --> 00:09:47,519
而且我会减少我的数据迭代

318
00:09:47,519 --> 00:09:49,000
比如说本来我要劝个10个

319
00:09:49,000 --> 00:09:49,519
或100个

320
00:09:49,519 --> 00:09:50,879
一泡可能现在劝个一个

321
00:09:50,879 --> 00:09:52,320
或者5个就差不多了

322
00:09:53,320 --> 00:09:54,160
所以就是说

323
00:09:54,160 --> 00:09:55,759
在fine tune的任务

324
00:09:56,120 --> 00:09:58,320
通常会使用这两个技术

325
00:09:58,320 --> 00:10:00,720
来使用更强的政策化

326
00:10:02,519 --> 00:10:03,840
所谓的更强政策化

327
00:10:03,840 --> 00:10:04,160
就是说

328
00:10:04,159 --> 00:10:06,959
不要把那边一个比较好的

329
00:10:07,360 --> 00:10:08,559
东西在你

330
00:10:08,559 --> 00:10:11,480
因为如果你正常训练的话

331
00:10:12,159 --> 00:10:13,799
我还是可以跟你

332
00:10:14,159 --> 00:10:15,759
从随机数据化是一样的

333
00:10:15,799 --> 00:10:16,959
我只要给你足够的时间

334
00:10:16,959 --> 00:10:18,079
总是可以训练成

335
00:10:18,079 --> 00:10:20,159
完全fit到我这个数据上

336
00:10:20,159 --> 00:10:21,079
但是我原本觉得

337
00:10:21,079 --> 00:10:22,839
可是说你没必要完全fit

338
00:10:22,839 --> 00:10:24,399
就是说你完全是fit

339
00:10:24,399 --> 00:10:25,519
可能是over fitting的

340
00:10:25,639 --> 00:10:26,519
还不如说

341
00:10:26,519 --> 00:10:28,480
从那边拿的好的东西

342
00:10:28,480 --> 00:10:29,480
稍微改一改就行了

343
00:10:29,519 --> 00:10:30,439
不要改太大

344
00:10:31,480 --> 00:10:32,879
通常来说是说

345
00:10:32,960 --> 00:10:34,759
我们假设是你的原数据

346
00:10:34,759 --> 00:10:35,559
比较远远的

347
00:10:35,559 --> 00:10:37,279
复杂于目标数据

348
00:10:37,439 --> 00:10:39,159
就是原数据级的类别数

349
00:10:39,600 --> 00:10:40,439
图片数量

350
00:10:40,559 --> 00:10:41,439
样本个数

351
00:10:41,679 --> 00:10:43,439
通常要10倍或者100倍的

352
00:10:43,439 --> 00:10:45,000
大于你的目标数据

353
00:10:45,000 --> 00:10:46,759
你才微调效果很好

354
00:10:46,960 --> 00:10:47,399
不然的话

355
00:10:47,399 --> 00:10:49,240
你就重新训练就行了

356
00:10:49,279 --> 00:10:51,360
因为别的也不见得比你好

357
00:10:51,600 --> 00:10:53,639
所以现在整个工业界

358
00:10:53,639 --> 00:10:55,000
学术界都在追求什么

359
00:10:55,000 --> 00:10:55,919
用更大的数据

360
00:10:55,919 --> 00:10:56,960
去劝更大的模型

361
00:10:56,960 --> 00:10:58,519
就是军备竞赛一样

362
00:11:00,080 --> 00:11:01,399
另外一个就是说

363
00:11:01,399 --> 00:11:03,319
一个常用的技术

364
00:11:03,600 --> 00:11:04,480
几个常用技术

365
00:11:05,079 --> 00:11:06,399
因为你的原数据级

366
00:11:06,399 --> 00:11:07,039
很有可能

367
00:11:07,039 --> 00:11:09,120
也有目标数据级中的标号

368
00:11:10,679 --> 00:11:11,159
就是说

369
00:11:11,159 --> 00:11:11,480
比如说

370
00:11:11,480 --> 00:11:12,959
ImageNet里面也有车

371
00:11:13,679 --> 00:11:14,919
所以你如果有车的话

372
00:11:14,919 --> 00:11:15,840
你可以去

373
00:11:15,840 --> 00:11:17,840
如果我们还要做车分类的话

374
00:11:17,840 --> 00:11:19,279
你可以把你原数据级

375
00:11:19,279 --> 00:11:20,079
那些标号

376
00:11:20,079 --> 00:11:21,199
它的分类器

377
00:11:21,360 --> 00:11:22,879
对应的那些项量

378
00:11:22,879 --> 00:11:23,840
重新拿过来

379
00:11:24,039 --> 00:11:25,199
初始化你的对应

380
00:11:25,960 --> 00:11:27,079
最后一个分类层

381
00:11:28,199 --> 00:11:28,679
OK

382
00:11:29,680 --> 00:11:31,400
第二个是说

383
00:11:32,360 --> 00:11:33,360
我们知道

384
00:11:33,840 --> 00:11:34,440
一般来说

385
00:11:34,440 --> 00:11:35,960
你一个神经网来讲

386
00:11:35,960 --> 00:11:36,760
越下的层

387
00:11:36,760 --> 00:11:38,120
你学习的东西

388
00:11:38,120 --> 00:11:39,960
是一些底层的一些细节

389
00:11:39,960 --> 00:11:40,840
越上面的话

390
00:11:40,840 --> 00:11:42,880
它可能就是更加语意化一些

391
00:11:42,880 --> 00:11:43,280
比如说

392
00:11:43,720 --> 00:11:45,040
这是第一层

393
00:11:45,040 --> 00:11:45,920
你神经网

394
00:11:45,920 --> 00:11:46,600
可能学了一些

395
00:11:46,600 --> 00:11:47,920
这边边角角的东西

396
00:11:48,040 --> 00:11:48,960
到后面的话

397
00:11:48,960 --> 00:11:50,720
可能是真的去识别那些狗

398
00:11:50,880 --> 00:11:51,560
什么样的

399
00:11:52,480 --> 00:11:53,720
我们可以认为说

400
00:11:53,760 --> 00:11:54,560
越到后面

401
00:11:54,560 --> 00:11:56,360
你跟你的标号越相关

402
00:11:57,360 --> 00:11:58,520
但越到前面

403
00:11:58,519 --> 00:11:59,439
越是底层

404
00:11:59,879 --> 00:12:00,879
所以底层的特征

405
00:12:00,879 --> 00:12:01,799
更加通用

406
00:12:02,360 --> 00:12:04,679
高层的跟数据更加相关

407
00:12:05,120 --> 00:12:06,360
就是说一个做法

408
00:12:06,360 --> 00:12:06,600
是说

409
00:12:06,600 --> 00:12:07,919
你可以把底层的一些类

410
00:12:07,919 --> 00:12:08,679
给固定住

411
00:12:09,240 --> 00:12:10,120
不要优化

412
00:12:11,159 --> 00:12:12,759
在fine tuning的时候

413
00:12:12,759 --> 00:12:14,439
不去改变底层的

414
00:12:14,439 --> 00:12:15,960
一些类别的权种

415
00:12:16,079 --> 00:12:17,000
就固定住它

416
00:12:17,360 --> 00:12:18,600
固定这意思是什么

417
00:12:18,679 --> 00:12:20,759
你的模型的复杂度变低了

418
00:12:20,759 --> 00:12:22,000
因为你的不再变化

419
00:12:22,000 --> 00:12:23,000
那些模型的参数

420
00:12:23,000 --> 00:12:24,720
但是你的模型变小了

421
00:12:24,759 --> 00:12:25,319
对吧

422
00:12:25,600 --> 00:12:26,439
所以他可以认为

423
00:12:26,439 --> 00:12:27,399
是一个更强的

424
00:12:27,399 --> 00:12:29,000
振奢的一个效果

425
00:12:29,439 --> 00:12:30,360
通常是说

426
00:12:30,360 --> 00:12:31,519
假设你的数据

427
00:12:31,519 --> 00:12:32,959
真的很小的情况下

428
00:12:33,559 --> 00:12:35,600
你觉得全部开始训练的话

429
00:12:35,600 --> 00:12:37,199
可能会很容易over fitting的

430
00:12:37,199 --> 00:12:37,759
情况下

431
00:12:37,759 --> 00:12:38,600
你可以固定住

432
00:12:38,600 --> 00:12:40,159
一些底部的一些层的参数

433
00:12:40,159 --> 00:12:41,199
不参与更新

434
00:12:41,639 --> 00:12:41,959
OK

435
00:12:41,959 --> 00:12:43,199
这也是一个常用的技巧

436
00:12:44,439 --> 00:12:45,439
再总结一下

437
00:12:45,840 --> 00:12:47,480
就微调通过使用

438
00:12:47,480 --> 00:12:49,480
在Dash的数据上

439
00:12:49,480 --> 00:12:51,279
得到好的预训练的模型

440
00:12:51,279 --> 00:12:52,360
pre-trained的模型

441
00:12:52,399 --> 00:12:53,639
来初始化

442
00:12:53,639 --> 00:12:56,399
我在我们目标数据上的模型

443
00:12:56,399 --> 00:12:57,759
来提升精度

444
00:12:58,720 --> 00:13:00,319
预训练模型的质量很重要

445
00:13:00,319 --> 00:13:01,399
所以大家都是拼的说

446
00:13:01,399 --> 00:13:02,480
你是在哪里训练的

447
00:13:02,480 --> 00:13:03,279
ImageNet也好

448
00:13:03,279 --> 00:13:04,600
ImageNet是个起点

449
00:13:04,600 --> 00:13:06,319
大家一般会在更大的数据上

450
00:13:06,319 --> 00:13:07,000
做训练

451
00:13:08,240 --> 00:13:09,840
所以预训练模型的质量

452
00:13:09,840 --> 00:13:10,639
非常重要

453
00:13:10,919 --> 00:13:11,559
第二个是说

454
00:13:11,559 --> 00:13:13,279
微调通常会速度更快

455
00:13:13,279 --> 00:13:14,120
精度更高

456
00:13:14,120 --> 00:13:15,000
所以就是说

457
00:13:15,000 --> 00:13:16,480
意味着就是说

458
00:13:17,319 --> 00:13:20,519
你通过你精度提升了

459
00:13:20,519 --> 00:13:21,799
训练变快了

460
00:13:22,000 --> 00:13:23,039
那不就是特别好

461
00:13:23,039 --> 00:13:23,519
对吧

462
00:13:24,319 --> 00:13:25,279
而且就是说

463
00:13:25,960 --> 00:13:28,319
你可以借用别人的鲜艳知识

464
00:13:28,319 --> 00:13:30,159
就是说从大数据上的鲜艳知识

465
00:13:30,159 --> 00:13:31,079
能够帮助你

466
00:13:31,120 --> 00:13:32,199
这也就是说

467
00:13:32,199 --> 00:13:33,120
在深度学习

468
00:13:33,120 --> 00:13:34,120
在计算机视觉

469
00:13:34,120 --> 00:13:35,679
迅速的在工业界被使用

470
00:13:35,679 --> 00:13:36,439
就是因为

471
00:13:36,919 --> 00:13:37,720
你有

472
00:13:38,039 --> 00:13:39,879
你有可以有预训练模型

473
00:13:40,000 --> 00:13:41,039
你可以拿过来

474
00:13:41,039 --> 00:13:42,399
直接在数据

475
00:13:42,399 --> 00:13:43,759
自己数据上fine tune一下

476
00:13:43,759 --> 00:13:44,600
效果非常好

477
00:13:44,919 --> 00:13:46,519
这也是深度学习

478
00:13:46,519 --> 00:13:49,000
迅速的在工业界取得了

479
00:13:49,600 --> 00:13:51,919
应用的一个很前提的一个条件

