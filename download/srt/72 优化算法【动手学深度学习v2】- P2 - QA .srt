1
00:00:00,000 --> 00:00:02,160
接下来我们来讲一下

2
00:00:02,160 --> 00:00:04,960
就是说很快速的回顾一下优化算法

3
00:00:04,960 --> 00:00:08,599
以及讲两个我们确实没有讲的东西

4
00:00:10,240 --> 00:00:11,480
一个是说优化算法

5
00:00:11,560 --> 00:00:13,200
优化问题一般的形式

6
00:00:13,200 --> 00:00:14,560
它是一个这样子的形式

7
00:00:14,640 --> 00:00:17,120
就是说你要最小化 fx

8
00:00:17,839 --> 00:00:23,120
你的x可能是在一个c的集合里面

9
00:00:23,760 --> 00:00:25,359
你的目标函数就是一个

10
00:00:25,359 --> 00:00:27,440
从一个x是一个长为n的一个

11
00:00:27,440 --> 00:00:28,240
实数的空间

12
00:00:28,440 --> 00:00:29,519
到一个r

13
00:00:29,519 --> 00:00:31,239
就是你的一个目标值

14
00:00:31,280 --> 00:00:32,320
你要最小化它

15
00:00:33,200 --> 00:00:35,679
然后你这个东西叫做object function

16
00:00:35,880 --> 00:00:37,280
在我们的deep learning里面

17
00:00:37,399 --> 00:00:39,359
f就是你的损失函数

18
00:00:39,359 --> 00:00:42,079
就是f是你的损失

19
00:00:42,560 --> 00:00:46,920
你的预测值给定一个输入

20
00:00:47,039 --> 00:00:49,320
你的预测和你的真实的标号之间的

21
00:00:49,320 --> 00:00:50,480
损失是你的f

22
00:00:50,840 --> 00:00:52,120
然后你的x当然是说

23
00:00:52,120 --> 00:00:54,000
你是所有你那些超限数

24
00:00:54,159 --> 00:00:56,239
你把那些超限数全部抽成

25
00:00:56,320 --> 00:00:57,439
搞成一条向量

26
00:00:57,480 --> 00:00:58,480
从优化来讲

27
00:00:58,480 --> 00:00:59,440
哪代都是一条向量

28
00:00:59,480 --> 00:00:59,920
对吧

29
00:01:00,320 --> 00:01:01,360
所以你不管你是矩阵

30
00:01:01,560 --> 00:01:02,160
也是tensor

31
00:01:02,560 --> 00:01:04,280
它都把你最后变成一条向量

32
00:01:05,439 --> 00:01:06,200
而你的c

33
00:01:06,600 --> 00:01:07,920
就c你是可以限制的

34
00:01:08,280 --> 00:01:09,079
就是说c的话

35
00:01:09,079 --> 00:01:10,800
你可以说我c要满足

36
00:01:10,800 --> 00:01:13,000
说你要这些东西等于0

37
00:01:13,120 --> 00:01:14,159
那些东西小于等于0

38
00:01:14,480 --> 00:01:17,040
这个在通用优化里面常见

39
00:01:17,080 --> 00:01:18,560
比如说我说我的权重

40
00:01:18,760 --> 00:01:20,240
要全部要大于等于0的

41
00:01:20,719 --> 00:01:22,880
或者是说你在具体的优化里面

42
00:01:23,159 --> 00:01:24,480
有很多这样子的假设

43
00:01:24,560 --> 00:01:25,920
但是在我们的里面

44
00:01:26,040 --> 00:01:27,480
我们尽量把它变成一个

45
00:01:27,480 --> 00:01:28,439
不受限的版本

46
00:01:28,439 --> 00:01:30,640
就是c本身就是Rn

47
00:01:31,920 --> 00:01:33,359
这是deep learning常教

48
00:01:33,359 --> 00:01:35,880
或者机器学习也是常用这样

49
00:01:36,000 --> 00:01:36,920
因为不受限的话

50
00:01:36,920 --> 00:01:39,200
优化起来会相对来说会快一点点

51
00:01:41,120 --> 00:01:43,960
然后另外一个就是说有两个解

52
00:01:44,120 --> 00:01:46,359
一个叫做全局最小

53
00:01:46,960 --> 00:01:49,080
一个x星叫做全局最小

54
00:01:49,200 --> 00:01:51,159
就是说我这个x星

55
00:01:51,600 --> 00:01:55,680
比你求解区间里面所有的x

56
00:01:55,800 --> 00:01:57,439
我的目标含用值都要低

57
00:01:58,480 --> 00:02:00,160
这个当然是最好了

58
00:02:00,160 --> 00:02:01,840
就是说你说我是全局的

59
00:02:02,640 --> 00:02:03,800
局部最小的意思

60
00:02:03,920 --> 00:02:05,880
就是说我一个x星

61
00:02:05,880 --> 00:02:07,520
是一个局部最小星

62
00:02:07,520 --> 00:02:10,120
是说存在一个epsilon

63
00:02:11,080 --> 00:02:16,360
使得任何那些x跟我的解

64
00:02:16,719 --> 00:02:20,080
的距离都小于球的半径的时候

65
00:02:20,879 --> 00:02:23,319
在这个地方你这些值都比较大

66
00:02:24,159 --> 00:02:24,759
举个例子

67
00:02:24,759 --> 00:02:26,199
就是说你看到这个线

68
00:02:26,400 --> 00:02:28,240
这个线是一个x乘以cos

69
00:02:28,720 --> 00:02:29,960
一个x的值

70
00:02:30,560 --> 00:02:32,120
这个地方是一个局部最小

71
00:02:32,160 --> 00:02:34,560
是因为只有在这一块

72
00:02:34,920 --> 00:02:36,200
就在这一块的时候

73
00:02:36,200 --> 00:02:39,440
才能说所有的点的值都比它要大

74
00:02:40,240 --> 00:02:41,840
但是你跑到这一块就不行了

75
00:02:41,840 --> 00:02:42,320
对吧

76
00:02:42,600 --> 00:02:45,000
这个点是你的全局最小

77
00:02:45,000 --> 00:02:46,880
是因为你所有整个函数

78
00:02:46,880 --> 00:02:48,040
就在这个点以上

79
00:02:49,600 --> 00:02:52,480
我们知道我们一般是用迭代算法来

80
00:02:52,480 --> 00:02:52,800
求解

81
00:02:52,800 --> 00:02:54,880
因为我们机器学习也好

82
00:02:54,920 --> 00:02:56,040
深度学习也好

83
00:02:56,159 --> 00:02:57,639
那些目标问题

84
00:02:57,639 --> 00:02:59,959
都不能用显示解来求

85
00:02:59,959 --> 00:03:03,280
我们都很难找到显示解

86
00:03:03,280 --> 00:03:04,239
所以我们要迭代

87
00:03:04,239 --> 00:03:06,079
每次去迭代来找

88
00:03:06,120 --> 00:03:08,319
一般来说迭代算法

89
00:03:08,599 --> 00:03:11,599
只能保证找到局部最小值

90
00:03:12,639 --> 00:03:14,519
就是说我一般是说迭代算法

91
00:03:14,519 --> 00:03:15,479
掉到一个坑里面

92
00:03:15,479 --> 00:03:17,280
然后说我的t

93
00:03:17,280 --> 00:03:18,759
这个地方t度已经变成0了

94
00:03:19,079 --> 00:03:19,719
可以看到

95
00:03:20,120 --> 00:03:22,000
因为这个地方的t度是0的话

96
00:03:22,000 --> 00:03:23,280
因为我们是用t度下降

97
00:03:23,280 --> 00:03:24,840
所以到了这个点的时候

98
00:03:24,879 --> 00:03:25,919
t度变成0

99
00:03:25,920 --> 00:03:27,280
不管你学习率是多大

100
00:03:27,280 --> 00:03:28,840
我也是不会再动了

101
00:03:28,880 --> 00:03:29,680
所以就是说

102
00:03:29,680 --> 00:03:32,480
你只能保证找到局部最小值

103
00:03:33,320 --> 00:03:34,360
当然有一些特例

104
00:03:35,440 --> 00:03:38,000
特例是一个突优化是一个特例

105
00:03:38,880 --> 00:03:39,760
讲突优化的时候

106
00:03:39,760 --> 00:03:41,640
我们稍微讲一下这东西

107
00:03:41,880 --> 00:03:43,880
就是说一个东西是个突击

108
00:03:43,880 --> 00:03:44,920
是说你

109
00:03:45,400 --> 00:03:46,680
你就别看公式的话

110
00:03:46,680 --> 00:03:48,480
就是说一个东西是个突击

111
00:03:48,480 --> 00:03:50,480
是说你在里面找任意两个点

112
00:03:50,520 --> 00:03:51,760
你给它连个线

113
00:03:51,760 --> 00:03:54,480
这个线都在这一个区域里面

114
00:03:55,240 --> 00:03:56,320
这个就不是个突击

115
00:03:56,320 --> 00:03:56,600
对吧

116
00:03:56,600 --> 00:03:57,200
这个地方

117
00:03:57,400 --> 00:03:59,200
因为为什么是说

118
00:03:59,200 --> 00:04:00,360
我这里取一个点

119
00:04:00,480 --> 00:04:01,400
这个取一个点

120
00:04:01,400 --> 00:04:02,080
连个线的话

121
00:04:02,080 --> 00:04:02,920
整个这一段

122
00:04:03,480 --> 00:04:05,040
这一段就不在集合里面

123
00:04:05,800 --> 00:04:06,720
这个也不是个突击

124
00:04:06,880 --> 00:04:07,880
这个东西也不是突击

125
00:04:07,880 --> 00:04:09,600
就是说我这一段

126
00:04:09,760 --> 00:04:11,120
也不在集合里面

127
00:04:11,920 --> 00:04:13,320
左边两个都是突击

128
00:04:13,680 --> 00:04:14,760
就当你的

129
00:04:16,439 --> 00:04:17,680
就是说你公式上来解

130
00:04:17,680 --> 00:04:19,840
就是给定任何xy

131
00:04:19,840 --> 00:04:22,400
你的alpha加上x

132
00:04:22,399 --> 00:04:25,519
alphax加上e加alphay

133
00:04:25,519 --> 00:04:26,759
都是在你的集合里面的

134
00:04:28,399 --> 00:04:29,159
突函数

135
00:04:30,319 --> 00:04:30,879
突函数

136
00:04:30,879 --> 00:04:31,959
你也别看公式的话

137
00:04:31,959 --> 00:04:32,799
就是说

138
00:04:33,479 --> 00:04:33,959
最简单

139
00:04:33,959 --> 00:04:36,079
就是说我在函数上画两个点

140
00:04:36,079 --> 00:04:37,319
比如说画一个点

141
00:04:37,519 --> 00:04:38,439
随便取两个点

142
00:04:39,039 --> 00:04:40,479
我两个点连根线

143
00:04:41,519 --> 00:04:43,599
保证说我整个函数

144
00:04:44,239 --> 00:04:46,079
都在我连线的下面

145
00:04:46,759 --> 00:04:48,159
这就是一个突函数

146
00:04:49,279 --> 00:04:49,879
OK

147
00:04:49,919 --> 00:04:51,000
所以这个地方你看到

148
00:04:51,000 --> 00:04:51,679
就是说

149
00:04:53,400 --> 00:04:54,280
这是一个点

150
00:04:54,320 --> 00:04:55,120
这是另外一个点

151
00:04:55,120 --> 00:04:57,360
这个就是这两个点连的一个线

152
00:04:57,480 --> 00:04:59,200
这个就是说在中间

153
00:04:59,360 --> 00:05:00,200
中间这个点

154
00:05:00,200 --> 00:05:02,920
它对应的x值取f

155
00:05:02,960 --> 00:05:04,480
f永远是小于这个线

156
00:05:04,480 --> 00:05:05,760
小于等于这个线的值

157
00:05:07,000 --> 00:05:09,600
就如果你说我当你的x不等于y

158
00:05:09,800 --> 00:05:10,360
alpha

159
00:05:10,360 --> 00:05:11,000
就是说

160
00:05:11,520 --> 00:05:12,960
在0和1之间的时候

161
00:05:13,280 --> 00:05:15,560
这个也不等于是严格成立的话

162
00:05:15,560 --> 00:05:17,320
那么叫做严格的突函数

163
00:05:17,760 --> 00:05:18,640
你可以理解说

164
00:05:18,640 --> 00:05:20,280
这个函数就是严格突函数

165
00:05:20,280 --> 00:05:20,840
因为

166
00:05:22,400 --> 00:05:23,360
在这个线

167
00:05:23,400 --> 00:05:24,920
就不看两头的点

168
00:05:25,520 --> 00:05:27,840
在不看两头那个点的情况下

169
00:05:27,840 --> 00:05:30,040
所谓的下面这些函数的线

170
00:05:30,120 --> 00:05:32,200
都是在线的之下

171
00:05:32,400 --> 00:05:34,200
不是跟线没有交集

172
00:05:34,400 --> 00:05:34,680
OK

173
00:05:34,680 --> 00:05:36,600
所以就是一个严格的突函数

174
00:05:39,400 --> 00:05:41,840
所以如果一个代价函数

175
00:05:42,560 --> 00:05:43,240
它是一个

176
00:05:43,920 --> 00:05:45,520
f是一个突的

177
00:05:45,520 --> 00:05:47,520
就是你的f是一个突函数

178
00:05:47,520 --> 00:05:50,320
而且你的限制集和c也是一个突

179
00:05:50,320 --> 00:05:51,080
集的话

180
00:05:51,079 --> 00:05:53,120
那么就是一个突优化问题

181
00:05:54,319 --> 00:05:55,639
突优化问题最好的

182
00:05:55,639 --> 00:05:56,039
就是说

183
00:05:56,039 --> 00:05:57,279
你的局部最小

184
00:05:57,279 --> 00:05:58,839
一定是全局最小

185
00:05:59,719 --> 00:06:00,039
OK

186
00:06:00,039 --> 00:06:00,959
这就是说

187
00:06:01,319 --> 00:06:03,719
你是一个非常强的理论的保证

188
00:06:03,919 --> 00:06:04,680
所以就是说

189
00:06:04,680 --> 00:06:06,800
你的优化算法找到的局部最小点

190
00:06:06,800 --> 00:06:08,719
就一定是全局的最优点

191
00:06:09,399 --> 00:06:11,279
如果是严格的突优化的话

192
00:06:11,439 --> 00:06:13,039
你有一个全局的

193
00:06:13,039 --> 00:06:15,439
你的全局最优点是唯一的

194
00:06:15,839 --> 00:06:16,959
举个例子是说

195
00:06:16,959 --> 00:06:17,639
你像这个东西

196
00:06:17,639 --> 00:06:19,120
它就不是一个严格的突函数

197
00:06:19,120 --> 00:06:19,399
对吧

198
00:06:19,399 --> 00:06:19,799
为什么

199
00:06:19,879 --> 00:06:22,319
是因为你在这个地方连个线

200
00:06:22,319 --> 00:06:23,319
这个地方连个线

201
00:06:23,319 --> 00:06:25,040
就是说这两个点连个线的话

202
00:06:25,639 --> 00:06:27,520
这个函数在线上面

203
00:06:27,520 --> 00:06:28,439
而不在线下面

204
00:06:28,439 --> 00:06:29,840
存在有点在线上

205
00:06:29,840 --> 00:06:31,759
所以它不是一个严格的突函数

206
00:06:31,879 --> 00:06:32,600
所以导致说

207
00:06:32,600 --> 00:06:33,840
你有很多个

208
00:06:34,400 --> 00:06:35,480
全局的最优点

209
00:06:35,759 --> 00:06:37,080
但这个函数就是一个

210
00:06:37,080 --> 00:06:38,240
全局的

211
00:06:38,280 --> 00:06:40,040
就是一个严格的突函数

212
00:06:40,040 --> 00:06:41,800
所以它只有一个唯一的

213
00:06:42,439 --> 00:06:43,120
最小值

214
00:06:43,960 --> 00:06:45,080
所以这个就是说

215
00:06:45,600 --> 00:06:46,960
大家都喜欢做突函数

216
00:06:46,960 --> 00:06:48,199
是因为它有这样子的

217
00:06:48,199 --> 00:06:49,400
好的理论的保证

218
00:06:50,800 --> 00:06:53,040
但是很遗憾的是

219
00:06:53,600 --> 00:06:55,439
我们积极学习

220
00:06:55,600 --> 00:06:57,040
绝大部分都不是突的

221
00:06:57,079 --> 00:06:58,639
我们目前为止

222
00:06:58,639 --> 00:07:00,120
就两个东西是一个突的

223
00:07:00,120 --> 00:07:01,879
就是一个是线性回归

224
00:07:02,480 --> 00:07:03,879
一个是softmax回归

225
00:07:03,879 --> 00:07:05,480
softmax就是一个线性

226
00:07:05,480 --> 00:07:06,199
也是一个线性的

227
00:07:06,199 --> 00:07:06,639
对吧

228
00:07:07,360 --> 00:07:09,120
它只是说在最后有个softmax

229
00:07:09,120 --> 00:07:10,519
可以做多类的分类

230
00:07:11,439 --> 00:07:14,319
剩下的东西都是非突的

231
00:07:14,960 --> 00:07:16,160
包括了一个隐藏

232
00:07:16,199 --> 00:07:18,120
有一个隐藏层的MLP

233
00:07:18,280 --> 00:07:19,759
因为我激活函数

234
00:07:19,759 --> 00:07:20,519
是一个

235
00:07:20,840 --> 00:07:21,720
不是一个线性的

236
00:07:21,720 --> 00:07:23,519
导致它是一个非线性的

237
00:07:23,519 --> 00:07:24,079
非线性

238
00:07:24,079 --> 00:07:25,280
它是一个非突的

239
00:07:25,720 --> 00:07:27,480
所以的CN RN

240
00:07:27,560 --> 00:07:28,360
CN因为有

241
00:07:29,240 --> 00:07:30,720
卷积本身是一个线性

242
00:07:31,040 --> 00:07:33,040
但是卷积加了激活函数

243
00:07:33,040 --> 00:07:33,840
它就不是了

244
00:07:34,040 --> 00:07:34,959
RN也是一样的

245
00:07:34,959 --> 00:07:36,920
RN就是说跟MLP是

246
00:07:37,000 --> 00:07:38,759
就是有一个state的MLP

247
00:07:38,759 --> 00:07:39,199
对吧

248
00:07:39,240 --> 00:07:40,120
Attention那一块

249
00:07:40,120 --> 00:07:41,680
Attention里面有softmax

250
00:07:41,840 --> 00:07:44,759
所以所有的我们做过的模型

251
00:07:44,879 --> 00:07:46,560
它都是一个非突的

252
00:07:47,199 --> 00:07:48,199
反过来讲也很正常

253
00:07:48,199 --> 00:07:48,720
对吧

254
00:07:49,319 --> 00:07:50,920
因为突函数

255
00:07:51,040 --> 00:07:52,920
它的表达能力是非常有限的

256
00:07:53,480 --> 00:07:55,879
所以我们其实不关心

257
00:07:55,879 --> 00:07:57,000
那些表达能力

258
00:07:57,000 --> 00:07:58,120
特别有限的东西

259
00:07:58,120 --> 00:08:00,040
对于机器学习来好

260
00:08:00,040 --> 00:08:01,160
我们说过机器学习

261
00:08:01,160 --> 00:08:03,399
是可以认为是

262
00:08:04,720 --> 00:08:06,000
是一个很计算机的

263
00:08:06,000 --> 00:08:06,279
一个

264
00:08:06,279 --> 00:08:08,199
就是解决实际问题的一块

265
00:08:08,519 --> 00:08:09,720
所以他觉得

266
00:08:10,240 --> 00:08:11,879
特别在深度学这一块

267
00:08:12,120 --> 00:08:13,800
实用才是排先的

268
00:08:13,800 --> 00:08:15,439
理论是靠后的

269
00:08:15,719 --> 00:08:17,240
如果你要更研究理论的话

270
00:08:17,240 --> 00:08:18,040
你就是从一个

271
00:08:18,040 --> 00:08:19,439
更统计的一个角度

272
00:08:19,439 --> 00:08:20,360
来看这个问题

273
00:08:20,399 --> 00:08:21,399
所以统计上来说

274
00:08:21,399 --> 00:08:22,839
我们会考虑很多突的

275
00:08:22,839 --> 00:08:23,639
突优化也好

276
00:08:23,839 --> 00:08:24,680
突函数也好

277
00:08:24,680 --> 00:08:26,079
会考虑很多突的东西

278
00:08:26,639 --> 00:08:28,040
比如说证明各种东西

279
00:08:28,079 --> 00:08:30,000
都是在突的上面会比较好

280
00:08:30,040 --> 00:08:31,600
但是从深度学习来讲

281
00:08:31,600 --> 00:08:35,480
我们更从计算机的角度

282
00:08:35,480 --> 00:08:36,519
来考虑它的事情

283
00:08:36,519 --> 00:08:38,679
就是说我们希望的是效果

284
00:08:39,080 --> 00:08:40,639
不要太考虑理论

285
00:08:40,879 --> 00:08:42,440
所以导致我们就基本上

286
00:08:42,440 --> 00:08:44,000
都做一些非突的东西

287
00:08:44,639 --> 00:08:45,200
所以导致

288
00:08:45,320 --> 00:08:45,879
所以为什么

289
00:08:45,879 --> 00:08:47,440
我们也没有讲特别多优化

290
00:08:47,440 --> 00:08:49,240
是因为优化的很多理论

291
00:08:49,640 --> 00:08:51,080
基本是在突上面的

292
00:08:51,920 --> 00:08:52,680
最近一些年

293
00:08:52,720 --> 00:08:54,000
有很多理论研究

294
00:08:54,000 --> 00:08:55,600
是在做一些非突的东西

295
00:08:55,600 --> 00:08:56,560
但是整体来讲

296
00:08:56,600 --> 00:08:58,520
大块是在对突函数

297
00:08:58,640 --> 00:09:00,280
所以它有很多漂亮的理论

298
00:09:00,840 --> 00:09:01,360
这些理论

299
00:09:01,480 --> 00:09:03,320
给你一些直观的上的东西

300
00:09:03,480 --> 00:09:05,120
但是它很难说

301
00:09:05,120 --> 00:09:07,440
对我们这些非突的一些模型

302
00:09:07,440 --> 00:09:09,160
有特别多的指导意义

303
00:09:09,600 --> 00:09:10,160
所以就是说

304
00:09:10,160 --> 00:09:10,960
我们时间有限

305
00:09:10,960 --> 00:09:12,920
就不给大家讲太多深入了

306
00:09:13,919 --> 00:09:15,159
好

307
00:09:15,159 --> 00:09:17,479
我们再讲一下一点点理论

308
00:09:17,959 --> 00:09:18,719
接下来看一下

309
00:09:18,719 --> 00:09:20,279
就是说回顾一下我们的东西

310
00:09:20,639 --> 00:09:22,159
首先第一个是说

311
00:09:23,319 --> 00:09:24,919
梯度下降这个算法

312
00:09:25,519 --> 00:09:27,679
这是最简单的迭代求解算法

313
00:09:28,759 --> 00:09:30,759
就是说每一次你选取一个

314
00:09:30,759 --> 00:09:32,360
开始点x0

315
00:09:32,599 --> 00:09:33,919
然后对每一个时间t

316
00:09:35,199 --> 00:09:38,319
对在当前点求它的梯度

317
00:09:38,319 --> 00:09:40,000
就对你的目标函数求梯度

318
00:09:40,159 --> 00:09:42,000
然后在当前点

319
00:09:42,000 --> 00:09:43,279
减掉你的学习率

320
00:09:43,279 --> 00:09:44,200
乘以你的梯度

321
00:09:44,279 --> 00:09:46,159
就ata这个东西叫做学习率

322
00:09:46,440 --> 00:09:47,519
但我没有讲过这个东西

323
00:09:48,879 --> 00:09:49,679
但是

324
00:09:50,759 --> 00:09:51,480
比如说这个点

325
00:09:51,480 --> 00:09:52,759
我们这是一个例子

326
00:09:53,120 --> 00:09:54,559
就是说你蓝色线是什么

327
00:09:54,559 --> 00:09:56,679
蓝色线是你的目标函数的等高线

328
00:09:56,840 --> 00:09:58,559
那么这个点当然是你的最小值了

329
00:09:58,559 --> 00:09:59,480
这个是比较大的

330
00:09:59,480 --> 00:10:00,960
这些地方是值比较大

331
00:10:01,240 --> 00:10:03,399
所以假设你从这里开始的话

332
00:10:04,279 --> 00:10:05,000
你的梯度下降

333
00:10:05,039 --> 00:10:06,399
就会一直走过去

334
00:10:06,399 --> 00:10:07,679
走到最后那个点

335
00:10:07,879 --> 00:10:08,480
OK

336
00:10:09,039 --> 00:10:12,560
随机梯度下降

337
00:10:12,560 --> 00:10:13,840
就随机梯度下降

338
00:10:13,840 --> 00:10:15,600
我们用的都是随机梯度下降

339
00:10:15,600 --> 00:10:17,000
我们没有用梯度下降

340
00:10:17,399 --> 00:10:18,039
就是说为什么

341
00:10:18,039 --> 00:10:20,560
是说当你有n个样本的时候

342
00:10:20,960 --> 00:10:22,120
你的fx是什么

343
00:10:22,120 --> 00:10:26,720
你的fx是你对在所有的样本上的

344
00:10:26,720 --> 00:10:28,120
损失的平均

345
00:10:29,639 --> 00:10:32,680
就当你有你的样本特别多的时候

346
00:10:32,680 --> 00:10:34,920
而且计算一个样本比较贵的时候

347
00:10:35,320 --> 00:10:37,840
求它的导数是非常贵的

348
00:10:37,920 --> 00:10:39,320
刚刚的梯度下降

349
00:10:39,320 --> 00:10:41,399
是要在整个完整的

350
00:10:42,080 --> 00:10:43,759
整个样本上求一次导

351
00:10:44,240 --> 00:10:45,240
这个东西太贵了

352
00:10:45,480 --> 00:10:48,360
所以我们都通常用的是随机梯度下降

353
00:10:48,639 --> 00:10:50,560
随机梯度下降的意思是说

354
00:10:50,600 --> 00:10:52,160
在时间t的时候

355
00:10:52,480 --> 00:10:56,280
我随机的选一个样本ti

356
00:10:57,120 --> 00:10:58,759
在样本里面随机选一个

357
00:10:59,000 --> 00:11:02,120
然后在单个样本上的梯度

358
00:11:02,120 --> 00:11:03,040
就这个东西

359
00:11:04,759 --> 00:11:05,320
就这个东西

360
00:11:05,320 --> 00:11:07,440
就是指在ti上面

361
00:11:08,120 --> 00:11:10,440
样本它的损失计算梯度

362
00:11:10,440 --> 00:11:13,320
用它来净伺你整个fx

363
00:11:14,639 --> 00:11:16,120
为什么你们那么做

364
00:11:16,120 --> 00:11:19,240
是因为导数是线性可以

365
00:11:19,240 --> 00:11:21,120
就是求导是线性可加的

366
00:11:21,120 --> 00:11:23,240
就是说它的导数是谁

367
00:11:23,720 --> 00:11:26,519
它的导数就是它等于你的

368
00:11:27,920 --> 00:11:31,120
它的导数是等于你的n分之一求和

369
00:11:31,120 --> 00:11:34,200
然后对所有的li对吧

370
00:11:34,800 --> 00:11:36,639
然后你对xi

371
00:11:37,639 --> 00:11:40,000
然后因为你这个东西

372
00:11:40,199 --> 00:11:42,080
这个东西是在里面随机取的

373
00:11:42,080 --> 00:11:45,480
所以它的期望就是它这个东西的均值

374
00:11:46,279 --> 00:11:48,720
这就是说虽然我是随机取的

375
00:11:48,759 --> 00:11:51,519
但我的期望就是我的平均值

376
00:11:51,960 --> 00:11:55,159
跟我的用整个来求是差不多的

377
00:11:55,159 --> 00:11:56,199
就虽然有噪音

378
00:11:56,639 --> 00:11:58,039
就是说因为我每次采样

379
00:11:58,080 --> 00:12:00,600
就是说你噪音等于是有方差

380
00:12:00,639 --> 00:12:02,039
但是我均值是ok的

381
00:12:02,080 --> 00:12:04,559
所以我整体大方向是对的

382
00:12:05,240 --> 00:12:06,600
所以但是好处就是说

383
00:12:06,600 --> 00:12:07,480
我每次时间

384
00:12:07,480 --> 00:12:09,600
只要算一个样本的梯度

385
00:12:09,760 --> 00:12:11,800
所以不用算全部

386
00:12:11,800 --> 00:12:14,640
算全部就会有大量的重复性在里面

387
00:12:15,160 --> 00:12:16,920
所以这个地方就是说能看到

388
00:12:17,160 --> 00:12:18,480
这个是随机梯度下降

389
00:12:18,720 --> 00:12:19,720
这个是梯度下降

390
00:12:19,800 --> 00:12:20,640
随机梯度下降

391
00:12:20,640 --> 00:12:21,240
可以看到是说

392
00:12:21,240 --> 00:12:23,440
它整个方向走的会不那么平坏一点

393
00:12:23,560 --> 00:12:25,600
特别是在最后的时候

394
00:12:25,880 --> 00:12:28,320
它整个走的方向比较歪

395
00:12:28,440 --> 00:12:31,400
但整体来看它的方向性还是对的

396
00:12:31,400 --> 00:12:33,960
因为它的均值是没有太发生变化的

397
00:12:34,680 --> 00:12:36,120
因为它每一次

398
00:12:40,280 --> 00:12:41,280
我们是不是

399
00:12:43,600 --> 00:12:44,640
我们卡了一下

400
00:12:46,800 --> 00:12:47,600
我们卡了一下

401
00:12:47,600 --> 00:12:48,000
ok

402
00:12:48,160 --> 00:12:49,520
然后就是说

403
00:12:49,560 --> 00:12:50,840
因为我每次计算

404
00:12:51,000 --> 00:12:52,440
它只要算一个样本梯度

405
00:12:52,440 --> 00:12:53,640
所以整体来看

406
00:12:53,680 --> 00:12:55,040
就算你每次走的

407
00:12:56,040 --> 00:12:57,080
会走一点点弯路

408
00:12:57,080 --> 00:12:58,720
但是整体来看是划得来的

409
00:12:58,720 --> 00:13:00,160
这就是随机梯度下降

410
00:13:00,680 --> 00:13:01,880
当然是随机梯度下降

411
00:13:01,880 --> 00:13:03,680
是在机械骑士

412
00:13:03,680 --> 00:13:06,000
我记得是03年的时候

413
00:13:06,000 --> 00:13:06,840
大家有提出来

414
00:13:06,960 --> 00:13:08,800
之前大家都是做梯度下降

415
00:13:09,640 --> 00:13:12,240
但是在深度学习随机梯度下降

416
00:13:12,240 --> 00:13:15,240
成为了整个是深度学习的基础

417
00:13:15,720 --> 00:13:17,000
其实在别的地方

418
00:13:17,000 --> 00:13:18,800
到别的机械学习模型

419
00:13:18,920 --> 00:13:20,440
不一定要用随机梯度下降

420
00:13:22,480 --> 00:13:24,680
其实我们用的还真不是

421
00:13:24,680 --> 00:13:26,200
完全是随机梯度下降

422
00:13:26,200 --> 00:13:27,560
我们真正的用的东西

423
00:13:27,560 --> 00:13:30,040
叫做小批量随机梯度下降

424
00:13:31,200 --> 00:13:33,440
就是说为什么需要做这个事情

425
00:13:33,440 --> 00:13:35,880
是因为不是统计的原因

426
00:13:35,880 --> 00:13:37,080
是计算的原因

427
00:13:37,160 --> 00:13:39,840
是因为随机梯度下降

428
00:13:39,840 --> 00:13:41,640
对单样本来算梯度

429
00:13:42,040 --> 00:13:43,320
但单样本算梯度

430
00:13:43,320 --> 00:13:45,840
很难完全利用硬件资源

431
00:13:45,840 --> 00:13:46,640
因为我们的硬件

432
00:13:46,640 --> 00:13:48,560
我们讲过CPU也好

433
00:13:48,560 --> 00:13:49,120
GPU也好

434
00:13:49,120 --> 00:13:50,720
它也都是多线传的

435
00:13:50,880 --> 00:13:52,360
假设你是单样本的话

436
00:13:52,360 --> 00:13:53,640
你计算量

437
00:13:53,880 --> 00:13:56,080
不足以能够并行到

438
00:13:56,080 --> 00:13:59,560
能占满我整个硬件的资源

439
00:14:00,600 --> 00:14:02,360
所以假设我可以用

440
00:14:03,520 --> 00:14:05,480
多个样本算梯度的话

441
00:14:05,480 --> 00:14:07,240
因为每个样本算梯度

442
00:14:07,240 --> 00:14:08,160
你可以并行算

443
00:14:08,160 --> 00:14:09,520
它是完美能并行的

444
00:14:09,520 --> 00:14:11,440
所以假设我有

445
00:14:11,440 --> 00:14:13,320
比如说我显卡有1000个核

446
00:14:13,320 --> 00:14:14,480
假设你的硬件

447
00:14:14,480 --> 00:14:15,520
我给你一个

448
00:14:15,960 --> 00:14:17,840
每次算1000个样本的梯度的话

449
00:14:17,840 --> 00:14:18,680
那么每个线程

450
00:14:18,680 --> 00:14:19,800
可以算一个样本梯度

451
00:14:19,800 --> 00:14:20,240
对吧

452
00:14:20,240 --> 00:14:21,040
那么这样子的话

453
00:14:21,040 --> 00:14:22,320
我的并行度就很高了

454
00:14:22,920 --> 00:14:24,480
所以通常来说

455
00:14:24,480 --> 00:14:25,560
在实际上来说

456
00:14:25,560 --> 00:14:28,320
我们会用小批量随机梯度下降

457
00:14:28,800 --> 00:14:30,440
小批量的意思是说

458
00:14:30,440 --> 00:14:31,360
在时间T

459
00:14:31,759 --> 00:14:33,639
我们采样一个随机的子机

460
00:14:33,639 --> 00:14:35,399
就采样一个样本的子机

461
00:14:35,399 --> 00:14:38,320
使得样本子机大小是等于b

462
00:14:38,320 --> 00:14:39,960
这b叫做批量大小

463
00:14:41,360 --> 00:14:42,680
然后在算梯度的时候

464
00:14:42,800 --> 00:14:46,159
我们对于b个样本

465
00:14:46,399 --> 00:14:48,200
采样到的b个样本都算梯度

466
00:14:48,200 --> 00:14:49,480
然后除以b

467
00:14:49,600 --> 00:14:51,279
做了一个平均值

468
00:14:51,279 --> 00:14:52,680
这个东西作为我的

469
00:14:53,279 --> 00:14:55,120
我们用它来近似

470
00:14:55,120 --> 00:14:56,759
我们整个目标函数的梯度

471
00:14:57,480 --> 00:14:58,399
同样道理的话

472
00:14:58,399 --> 00:15:00,560
因为我们是随机采样的

473
00:15:00,560 --> 00:15:02,040
然后我们又除了一个b

474
00:15:02,040 --> 00:15:06,160
所以它的均值

475
00:15:06,280 --> 00:15:07,800
就是它的期望是没变的

476
00:15:07,800 --> 00:15:09,560
就是一个无偏的近似

477
00:15:10,080 --> 00:15:10,840
就跟之前一样

478
00:15:10,920 --> 00:15:12,280
我们就大方向不变

479
00:15:12,440 --> 00:15:13,800
但它有一点点好处

480
00:15:13,920 --> 00:15:15,640
它降低了方差

481
00:15:16,480 --> 00:15:18,800
是因为我这是一个b个样本的

482
00:15:18,800 --> 00:15:19,600
一个平均

483
00:15:19,600 --> 00:15:22,360
所以整个在方向上的抖动性

484
00:15:22,560 --> 00:15:23,960
它会好一点

485
00:15:23,960 --> 00:15:24,960
就没那么多噪音

486
00:15:24,960 --> 00:15:27,400
而且整个方向上更平滑一点

487
00:15:28,120 --> 00:15:30,720
这个地方可以看到这个图是什么意思

488
00:15:30,720 --> 00:15:32,480
这个图是说在实际中

489
00:15:32,679 --> 00:15:33,879
就我们代码就不讲了

490
00:15:33,879 --> 00:15:34,679
你可以去看一下书

491
00:15:34,679 --> 00:15:35,679
书上面是有代码的

492
00:15:35,679 --> 00:15:37,079
这个东西怎么跑出来的

493
00:15:37,360 --> 00:15:38,840
这个地方是你的

494
00:15:39,360 --> 00:15:40,279
这个是你的损失

495
00:15:40,519 --> 00:15:42,480
这个地方是你的时间

496
00:15:42,519 --> 00:15:44,439
x是你的时间秒

497
00:15:45,279 --> 00:15:46,840
而且是要lock单位的

498
00:15:47,360 --> 00:15:48,120
可以看到是说

499
00:15:48,120 --> 00:15:50,319
你的gradient descent在这个地方

500
00:15:50,319 --> 00:15:52,079
这个南线是个梯度下降

501
00:15:52,480 --> 00:15:53,000
梯度下降

502
00:15:53,000 --> 00:15:54,279
就是说一开始就很好

503
00:15:54,480 --> 00:15:56,519
就是说就跑这个地方

504
00:15:56,919 --> 00:15:59,600
然后这个是你的随机梯度下降

505
00:15:59,600 --> 00:16:00,519
随机梯度下降

506
00:16:00,519 --> 00:16:02,399
比梯度下降要慢的原因

507
00:16:02,399 --> 00:16:04,559
是因为每次算一个样本

508
00:16:04,559 --> 00:16:06,319
根本就用不了你的硬件的

509
00:16:06,319 --> 00:16:07,120
并行度

510
00:16:07,120 --> 00:16:08,240
所以导致说

511
00:16:08,240 --> 00:16:09,799
如果你存从时间

512
00:16:09,840 --> 00:16:10,799
物理时间来看

513
00:16:11,360 --> 00:16:12,399
你是花不来的

514
00:16:12,600 --> 00:16:13,639
所以你要干的事情

515
00:16:13,639 --> 00:16:16,799
就是说你需要加比较大的批量

516
00:16:17,240 --> 00:16:18,480
这批量等于10的时候

517
00:16:18,480 --> 00:16:19,279
还花不来

518
00:16:19,439 --> 00:16:21,720
但是加到批量等于100的时候

519
00:16:21,879 --> 00:16:22,600
这个地方

520
00:16:23,480 --> 00:16:24,840
当然有一定偶然性

521
00:16:25,040 --> 00:16:26,759
就是说这个地方看得不是特别清楚

522
00:16:27,080 --> 00:16:27,800
就是说

523
00:16:28,519 --> 00:16:29,720
基本上来说

524
00:16:29,759 --> 00:16:30,639
等到

525
00:16:30,840 --> 00:16:31,639
一般来说

526
00:16:31,639 --> 00:16:33,360
在一个批量比较合适的时候

527
00:16:33,680 --> 00:16:36,360
它比批量很小的时候

528
00:16:36,360 --> 00:16:38,800
批量很小的时候收敛会快

529
00:16:38,800 --> 00:16:39,840
但是计算慢

530
00:16:40,360 --> 00:16:41,879
批量很大的时候

531
00:16:41,879 --> 00:16:43,200
就是整个梯度下降

532
00:16:43,320 --> 00:16:43,960
它收敛

533
00:16:43,960 --> 00:16:47,680
就是它的每次计算代价太大

534
00:16:47,920 --> 00:16:48,519
导致

535
00:16:48,519 --> 00:16:50,840
所以批量大小在一个合适的区间

536
00:16:50,840 --> 00:16:51,360
是比较好的

537
00:16:51,360 --> 00:16:52,160
不能太小

538
00:16:52,160 --> 00:16:53,000
也不能太大

539
00:16:54,000 --> 00:16:55,879
Ok 这个都是我们讲过的

540
00:16:56,000 --> 00:16:58,039
所以我们就快速的过一遍

541
00:16:58,799 --> 00:16:59,159
好

542
00:16:59,159 --> 00:17:00,679
接下来讲两个东西

543
00:17:00,679 --> 00:17:02,200
我们是没有讲过的

544
00:17:02,399 --> 00:17:04,440
但是我们多多少少用过的东西

545
00:17:05,119 --> 00:17:06,480
一个叫做充量法

546
00:17:06,480 --> 00:17:07,480
叫做momentum

547
00:17:09,240 --> 00:17:11,079
就充量法是一个非常

548
00:17:11,079 --> 00:17:13,720
我们在实际中

549
00:17:13,720 --> 00:17:15,200
也用的特别多的一个算法

550
00:17:15,759 --> 00:17:17,000
充量法的

551
00:17:17,400 --> 00:17:18,759
一个意思是说

552
00:17:18,759 --> 00:17:21,440
我使用一个平滑过的梯度

553
00:17:21,440 --> 00:17:22,880
对全中进行更新

554
00:17:23,480 --> 00:17:24,559
就刚刚看到是说

555
00:17:24,559 --> 00:17:25,480
你小批量

556
00:17:25,680 --> 00:17:27,079
就是说小批量

557
00:17:27,079 --> 00:17:28,279
随机梯度下降的时候

558
00:17:28,680 --> 00:17:30,880
还是会梯度会有比较大的抖动

559
00:17:30,920 --> 00:17:33,599
特别是当你整个目标函数

560
00:17:33,599 --> 00:17:34,920
比较复杂的时候

561
00:17:34,920 --> 00:17:36,559
整个你平面不是

562
00:17:36,559 --> 00:17:37,799
就特别抖的时候

563
00:17:37,799 --> 00:17:38,720
里面就是说

564
00:17:38,920 --> 00:17:40,519
在实际中的loss函数

565
00:17:40,680 --> 00:17:41,759
就是说你真实数据

566
00:17:41,759 --> 00:17:43,400
loss函数是比较不平滑的

567
00:17:43,400 --> 00:17:44,599
我们这都是人工数据

568
00:17:44,680 --> 00:17:45,720
所以看上去很好

569
00:17:45,759 --> 00:17:48,640
但实际中就是跟我们最早之前

570
00:17:48,640 --> 00:17:49,160
有个图

571
00:17:49,160 --> 00:17:51,759
没就是坑坑洼洼的样子

572
00:17:52,680 --> 00:17:53,400
所以就是说

573
00:17:53,400 --> 00:17:54,960
在你一个很不平滑的

574
00:17:54,960 --> 00:17:56,160
一个平面作用化的时候

575
00:17:56,519 --> 00:17:58,839
你每一次就是会一下往东

576
00:17:58,839 --> 00:17:59,279
一下往西

577
00:17:59,279 --> 00:17:59,640
一下往东

578
00:17:59,640 --> 00:18:00,039
一下往西

579
00:18:00,039 --> 00:18:01,279
就得像随机运动

580
00:18:02,000 --> 00:18:03,119
所以就是说

581
00:18:03,119 --> 00:18:07,240
说你这个东西没必要那么去

582
00:18:07,519 --> 00:18:08,960
这些噪音可能会带你

583
00:18:08,960 --> 00:18:11,200
给你带来很多没必要的结果

584
00:18:11,240 --> 00:18:12,960
所以充量法的意思是说

585
00:18:12,960 --> 00:18:14,440
我维护一个充量

586
00:18:14,839 --> 00:18:16,160
就维护个惯性

587
00:18:17,279 --> 00:18:19,920
就是说我在一个下山的时候

588
00:18:20,320 --> 00:18:23,080
我不要说看见这边往这边

589
00:18:23,120 --> 00:18:23,600
抖一点

590
00:18:23,600 --> 00:18:24,240
我就往这边去

591
00:18:24,240 --> 00:18:24,680
往那边去

592
00:18:24,680 --> 00:18:26,720
我要有一个重力给你一个惯性

593
00:18:26,720 --> 00:18:27,279
就是说

594
00:18:27,960 --> 00:18:31,000
不要一下方向改变的过快

595
00:18:31,000 --> 00:18:32,279
就是维护一个

596
00:18:32,279 --> 00:18:33,880
所以我要改变方向是没错

597
00:18:33,880 --> 00:18:36,080
但是是我要比较平滑的

598
00:18:36,080 --> 00:18:37,000
改变我的方向

599
00:18:37,840 --> 00:18:38,680
所以他怎么办

600
00:18:39,200 --> 00:18:39,880
就是说

601
00:18:41,440 --> 00:18:43,039
假设我的时间t

602
00:18:43,039 --> 00:18:45,759
我算在样本上算的t都是gt

603
00:18:46,440 --> 00:18:51,440
我的Vt是什么的

604
00:18:51,440 --> 00:18:52,799
我的Vt是我的充量

605
00:18:52,799 --> 00:18:54,119
就Vt是等于

606
00:18:54,400 --> 00:18:56,759
beta乘以Vt-1

607
00:18:56,759 --> 00:18:57,799
加上gt

608
00:18:59,319 --> 00:19:03,160
然后我在更新我的权重的时候

609
00:19:03,279 --> 00:19:05,759
我不再是减去学习率

610
00:19:05,759 --> 00:19:06,640
乘以gt了

611
00:19:06,640 --> 00:19:07,759
而是减去学习率

612
00:19:07,759 --> 00:19:08,519
乘以Vt

613
00:19:08,559 --> 00:19:10,319
用它来替代我的t

614
00:19:11,720 --> 00:19:12,599
所以看一下这个东西

615
00:19:12,599 --> 00:19:13,759
是为什么长这个样子

616
00:19:14,039 --> 00:19:15,480
这东西你把它展开的话

617
00:19:15,480 --> 00:19:16,480
你可以看到是一个

618
00:19:16,519 --> 00:19:17,200
它的Vt

619
00:19:17,200 --> 00:19:18,440
它其实等于gt

620
00:19:18,799 --> 00:19:22,519
加上beta乘以gt-1

621
00:19:22,519 --> 00:19:25,079
就是当前时刻的t度

622
00:19:25,200 --> 00:19:29,000
加上beta乘以上一个时刻的t度

623
00:19:29,319 --> 00:19:32,680
再加上beta平方乘以上上时刻的t度

624
00:19:33,319 --> 00:19:34,799
就beta是一个小于1的值

625
00:19:35,160 --> 00:19:38,039
所以它随着时间的往下降

626
00:19:38,039 --> 00:19:40,200
是一个指数的一个退减

627
00:19:40,759 --> 00:19:42,559
它会到后面就会变得特别小

628
00:19:42,759 --> 00:19:44,360
但是说所以你可以看到是说

629
00:19:44,360 --> 00:19:46,240
当前的t度乘以一个小一点的值

630
00:19:46,240 --> 00:19:48,160
乘以上上一个时刻的t度

631
00:19:48,200 --> 00:19:49,600
再是上上时刻的t度

632
00:19:49,640 --> 00:19:51,360
所以就是说这样子的话

633
00:19:51,360 --> 00:19:52,680
因为后面则有一块

634
00:19:53,080 --> 00:19:55,000
使得你不会

635
00:19:55,000 --> 00:19:57,920
你的对整个权重的更新的方向

636
00:19:58,800 --> 00:20:00,600
不是完全取决gt

637
00:20:00,600 --> 00:20:02,520
还是说也是看一下

638
00:20:02,520 --> 00:20:04,240
过去时间那些t度

639
00:20:04,960 --> 00:20:07,320
如果他们两个长得特别不一样的话

640
00:20:07,320 --> 00:20:09,520
那么假设gt-1和gt

641
00:20:09,520 --> 00:20:10,760
长得特别不一样的话

642
00:20:10,760 --> 00:20:12,920
它会抵消掉一点它的东西

643
00:20:12,960 --> 00:20:14,800
使得你的更新就不那么剧烈

644
00:20:15,720 --> 00:20:17,160
当你可以选择beta

645
00:20:17,440 --> 00:20:18,680
选择beta来

646
00:20:18,680 --> 00:20:20,600
假设你选beta的0.5的话

647
00:20:20,960 --> 00:20:23,160
然后当然是说这个衰减特别快了

648
00:20:23,200 --> 00:20:25,160
如果你是取的是0.99的话

649
00:20:25,160 --> 00:20:26,160
那么很有可能

650
00:20:26,160 --> 00:20:26,920
基本上你可以认为

651
00:20:26,920 --> 00:20:28,039
它会看过去

652
00:20:28,039 --> 00:20:31,920
比如说几十个t度的那些平均

653
00:20:32,600 --> 00:20:34,759
一般来说beta取值是0.5

654
00:20:34,759 --> 00:20:35,279
0.9

655
00:20:35,279 --> 00:20:35,800
0.95

656
00:20:35,800 --> 00:20:36,400
0.99

657
00:20:37,080 --> 00:20:39,400
取一般如果你的样本比较大

658
00:20:39,800 --> 00:20:41,720
你可以去个0.99也挺正常的

659
00:20:42,519 --> 00:20:43,160
0.99

660
00:20:43,160 --> 00:20:44,480
我记得大概算下来

661
00:20:44,480 --> 00:20:46,880
就是说你大概会看过去50个时刻的

662
00:20:46,880 --> 00:20:47,799
t度做平均

663
00:20:48,120 --> 00:20:49,319
超过50个时刻的话

664
00:20:49,319 --> 00:20:50,200
那些值就很小了

665
00:20:50,200 --> 00:20:51,319
就可以忽略不计了

666
00:20:51,319 --> 00:20:52,240
0.5的话

667
00:20:52,240 --> 00:20:54,720
可能就是看过去的两三个t度的样子

668
00:20:55,200 --> 00:20:56,079
你可以去算一下

669
00:20:56,079 --> 00:20:58,600
就是说看一看它的衰减是什么样子的

670
00:20:59,880 --> 00:21:00,240
OK

671
00:21:00,240 --> 00:21:01,600
所以这充量法

672
00:21:02,039 --> 00:21:03,279
你可以看到就是说

673
00:21:03,799 --> 00:21:06,120
这个地方是使用随机t度下降

674
00:21:06,279 --> 00:21:09,000
就是说在一个这个地方

675
00:21:09,000 --> 00:21:11,120
就是说你因为它比较比

676
00:21:11,280 --> 00:21:12,240
这个目标函数

677
00:21:12,240 --> 00:21:13,840
所以它这地方会震动

678
00:21:14,280 --> 00:21:15,480
就是跑过来跑过去

679
00:21:15,480 --> 00:21:17,120
就是说这个地方算t度的时候

680
00:21:17,120 --> 00:21:18,560
它最大方向是这个方向

681
00:21:18,560 --> 00:21:20,520
它一不小心走过头了

682
00:21:20,520 --> 00:21:20,960
对吧

683
00:21:20,960 --> 00:21:22,200
又走回来走过去走回来

684
00:21:22,200 --> 00:21:22,960
走回来震荡

685
00:21:23,520 --> 00:21:26,360
假设你用了充量法的话

686
00:21:27,000 --> 00:21:30,000
第一时间反正v0是等于0的

687
00:21:30,000 --> 00:21:31,400
所以第一时间没用

688
00:21:31,720 --> 00:21:32,680
第二时间也没用

689
00:21:32,680 --> 00:21:33,840
但第三时间的话

690
00:21:33,840 --> 00:21:35,560
他就是说他就不会去真的

691
00:21:35,560 --> 00:21:37,120
就是完全往这个方向了

692
00:21:37,120 --> 00:21:38,280
就会有一个充量

693
00:21:38,280 --> 00:21:38,920
在慢慢的

694
00:21:38,920 --> 00:21:41,039
就是说慢慢的让你这个方向

695
00:21:41,400 --> 00:21:43,200
往这边整个大方向拼

696
00:21:43,360 --> 00:21:44,880
就是说因为你在这个地方

697
00:21:44,880 --> 00:21:46,320
就要么就往这边

698
00:21:46,320 --> 00:21:47,160
要么就往这边

699
00:21:47,160 --> 00:21:47,600
对吧

700
00:21:47,640 --> 00:21:50,000
就两个相互冲突的方向

701
00:21:50,000 --> 00:21:51,759
两个t度在相互冲突

702
00:21:51,800 --> 00:21:53,759
但我如果把它一平化的话

703
00:21:53,759 --> 00:21:54,920
它两个冲突的方向

704
00:21:55,039 --> 00:21:57,600
就会慢慢的就抵消掉一些地方

705
00:21:57,600 --> 00:21:59,200
使得它尽量的往下走

706
00:21:59,920 --> 00:22:01,720
就是说直观上来讲

707
00:22:01,720 --> 00:22:03,080
充量法的作用

708
00:22:04,120 --> 00:22:04,840
充量法的话

709
00:22:04,840 --> 00:22:07,800
在基本上在于所有的

710
00:22:08,920 --> 00:22:10,160
框架里面都实现了

711
00:22:10,320 --> 00:22:11,920
它就会通过一个叫

712
00:22:11,920 --> 00:22:14,400
Momentum的一个超参数来指令

713
00:22:14,600 --> 00:22:16,800
那你可以基本上所有的实现都有

714
00:22:16,800 --> 00:22:17,680
所有的SGD

715
00:22:18,120 --> 00:22:20,039
就最简单的SGD

716
00:22:20,039 --> 00:22:22,160
它可能都有一个叫Momentum的选项

717
00:22:22,160 --> 00:22:25,400
你就把Momentum设成要的值就行了

718
00:22:25,400 --> 00:22:26,880
这就用上了充量法

719
00:22:29,560 --> 00:22:30,560
那边是Adam

720
00:22:30,600 --> 00:22:32,400
我们用的最多的还是Adam

721
00:22:33,279 --> 00:22:34,279
Adam这个算法

722
00:22:34,640 --> 00:22:36,720
就是说给大家讲一下

723
00:22:36,720 --> 00:22:37,680
它是怎么回事

724
00:22:37,799 --> 00:22:38,880
我们之所以用Adam

725
00:22:39,000 --> 00:22:41,039
不是说它真的比SGD好

726
00:22:41,200 --> 00:22:43,120
SGD加Momentum效果非常好了

727
00:22:43,120 --> 00:22:45,480
其实你可以不用Adam都没关系

728
00:22:46,600 --> 00:22:48,560
就是Adam不一定会优化的效果

729
00:22:48,560 --> 00:22:50,519
比SGD加Momentum要好

730
00:22:50,560 --> 00:22:52,200
但是Adam最好的地方

731
00:22:52,200 --> 00:22:54,240
就是说它对学习率没那么敏感

732
00:22:54,279 --> 00:22:56,000
它做了非常多的平化

733
00:22:56,039 --> 00:22:56,640
它就是一个

734
00:22:56,640 --> 00:22:58,519
你可以认为是一个非常平化的一个

735
00:22:58,519 --> 00:22:59,120
SGD

736
00:22:59,160 --> 00:23:00,240
使得它对

737
00:23:00,279 --> 00:23:01,400
你一旦很平化的话

738
00:23:01,400 --> 00:23:02,240
那么就是说

739
00:23:02,240 --> 00:23:04,080
对整个学习率就没那么敏感了

740
00:23:04,120 --> 00:23:05,080
所以导致说

741
00:23:05,679 --> 00:23:07,679
如果你不是说

742
00:23:07,679 --> 00:23:09,599
真的有时间去狂调餐的话

743
00:23:09,919 --> 00:23:11,839
用Adam是一个不错的选项

744
00:23:12,519 --> 00:23:15,879
当然如果你真的会想去调餐的话

745
00:23:16,199 --> 00:23:17,720
你用SGD加Momentum

746
00:23:17,720 --> 00:23:18,879
或者别的一些算法

747
00:23:18,879 --> 00:23:20,399
可能效果可能会比Adam

748
00:23:20,399 --> 00:23:21,359
会好那么一点点

749
00:23:22,279 --> 00:23:22,599
OK

750
00:23:22,599 --> 00:23:24,039
但是因为它简单

751
00:23:24,240 --> 00:23:24,720
对吧

752
00:23:25,079 --> 00:23:26,919
所以大家用的还是挺多的

753
00:23:26,960 --> 00:23:28,159
所以我们就给大家讲一下

754
00:23:28,159 --> 00:23:29,079
Adam这个东西

755
00:23:31,319 --> 00:23:32,839
Adam就是说

756
00:23:32,879 --> 00:23:33,839
它跟之前一样

757
00:23:33,959 --> 00:23:35,759
它会记录一个VT的东西

758
00:23:35,919 --> 00:23:37,679
但它跟之前不一样的是说

759
00:23:37,720 --> 00:23:39,279
我的VT

760
00:23:39,839 --> 00:23:40,759
这个地方是没变

761
00:23:40,759 --> 00:23:42,119
但是它加β1了

762
00:23:42,399 --> 00:23:44,199
β1×VT-1

763
00:23:44,240 --> 00:23:45,159
但是这个地方

764
00:23:45,159 --> 00:23:47,399
它加了一个1-β1

765
00:23:48,240 --> 00:23:49,599
就我们之前的Momentum

766
00:23:49,599 --> 00:23:50,480
是没有这一项的

767
00:23:50,919 --> 00:23:52,279
这个地方我们加了这一项

768
00:23:52,639 --> 00:23:54,720
β1通常会去0.9

769
00:23:55,359 --> 00:23:56,759
所以β1是不需要调的

770
00:23:56,759 --> 00:23:58,199
Momentum那个β是要调的

771
00:23:58,240 --> 00:24:00,480
这个地方你不调没关系

772
00:24:01,720 --> 00:24:02,519
为什么要这么做

773
00:24:02,519 --> 00:24:04,599
是因为假设我把它展开的话

774
00:24:04,599 --> 00:24:05,160
你会发现

775
00:24:05,160 --> 00:24:07,279
它就等于VT等于1-β1

776
00:24:07,279 --> 00:24:08,519
乘以GT加上

777
00:24:08,519 --> 00:24:09,799
一直一路加下去

778
00:24:10,519 --> 00:24:11,879
所以这些权重

779
00:24:12,319 --> 00:24:15,319
就是1加上β1

780
00:24:15,319 --> 00:24:16,599
加上β1的平方

781
00:24:16,599 --> 00:24:17,200
一直加下去

782
00:24:17,200 --> 00:24:18,160
就等于你写下来

783
00:24:18,160 --> 00:24:18,639
就是这个东西

784
00:24:18,639 --> 00:24:19,079
对吧

785
00:24:19,119 --> 00:24:21,119
就是β1的T次方

786
00:24:21,119 --> 00:24:22,599
然后所以这个地方

787
00:24:22,839 --> 00:24:24,000
这个地方我写错了

788
00:24:24,039 --> 00:24:24,920
这个地方是I

789
00:24:26,279 --> 00:24:27,119
I次方

790
00:24:27,119 --> 00:24:28,160
I从0到5

791
00:24:28,160 --> 00:24:29,160
从那的时候

792
00:24:29,279 --> 00:24:31,400
它的序列

793
00:24:31,680 --> 00:24:35,320
它是等于1-β1的

794
00:24:36,720 --> 00:24:38,800
所以我这个地方

795
00:24:38,920 --> 00:24:40,480
因为这些权重加起来

796
00:24:40,480 --> 00:24:41,600
是等于这个值

797
00:24:41,720 --> 00:24:42,880
所以我这地方

798
00:24:42,880 --> 00:24:44,440
乘以了1-β1

799
00:24:45,080 --> 00:24:46,160
就导致说

800
00:24:46,160 --> 00:24:50,040
我的VT是这些所有的东西的

801
00:24:50,040 --> 00:24:53,280
加权和过去的T度的加权和

802
00:24:53,320 --> 00:24:55,720
这些权重的和是等于1的

803
00:24:57,280 --> 00:24:57,519
OK

804
00:24:57,519 --> 00:24:58,640
这就是为什么这个地方

805
00:24:58,640 --> 00:24:59,720
有个1-β1

806
00:25:00,240 --> 00:25:02,000
当然这个东西不是Adam独创

807
00:25:02,440 --> 00:25:04,920
我们是跳过了前面的算法

808
00:25:04,920 --> 00:25:05,360
直接讲

809
00:25:05,360 --> 00:25:07,120
就用了别人的东西

810
00:25:07,400 --> 00:25:10,200
所以你这个是用的

811
00:25:10,200 --> 00:25:11,440
但是它不是他的贡献

812
00:25:11,600 --> 00:25:13,000
所以这个东西是用的

813
00:25:13,200 --> 00:25:14,360
这个东西要调和平均

814
00:25:14,400 --> 00:25:16,400
就用的挺多的

815
00:25:18,360 --> 00:25:19,400
另外一个是说

816
00:25:19,400 --> 00:25:21,240
因为V0是等于0的

817
00:25:21,279 --> 00:25:23,120
所以在一开始的时候

818
00:25:23,759 --> 00:25:25,400
所以就导致大家会偏小

819
00:25:25,400 --> 00:25:26,000
因为G

820
00:25:26,000 --> 00:25:29,559
就T度一开始

821
00:25:29,559 --> 00:25:30,519
当时不等于0

822
00:25:30,519 --> 00:25:31,839
但是你的有一个V0

823
00:25:31,839 --> 00:25:32,519
V0是等于0

824
00:25:32,519 --> 00:25:34,359
所以在一开始的时候

825
00:25:34,359 --> 00:25:37,559
大家会偏的向小一点点

826
00:25:38,440 --> 00:25:41,599
然后因为对小的T的时候

827
00:25:42,119 --> 00:25:43,000
你这个东西

828
00:25:44,400 --> 00:25:45,079
这个地方也是等于

829
00:25:45,519 --> 00:25:46,920
我这里写错了

830
00:25:48,119 --> 00:25:49,160
它是等于

831
00:25:49,640 --> 00:25:51,160
T当你不是五重大的时候

832
00:25:51,160 --> 00:25:52,839
它就等于上面的分子

833
00:25:52,839 --> 00:25:55,960
是1-β1T四方

834
00:25:57,000 --> 00:25:59,319
所以它就会做了一个修正

835
00:25:59,359 --> 00:25:59,960
就是说

836
00:25:59,960 --> 00:26:02,000
当你对T比较小的

837
00:26:02,000 --> 00:26:03,480
对T比较小的时候有用

838
00:26:03,480 --> 00:26:04,480
T大的就没用了

839
00:26:04,920 --> 00:26:06,279
对T比较小的时候

840
00:26:06,480 --> 00:26:08,000
它一个VT帽子

841
00:26:08,000 --> 00:26:09,240
就hatVT

842
00:26:09,279 --> 00:26:13,240
它等于VT除以1-β1T的T四方

843
00:26:14,160 --> 00:26:15,240
这是因为

844
00:26:15,799 --> 00:26:17,200
你当T比较小的时候

845
00:26:17,319 --> 00:26:18,839
因为这个地方是等于五重大的时候

846
00:26:18,839 --> 00:26:19,680
它是全都等于一

847
00:26:19,799 --> 00:26:20,960
但是T比较小的时候

848
00:26:20,960 --> 00:26:22,079
它当然不等于

849
00:26:22,319 --> 00:26:23,599
所以它就是

850
00:26:23,599 --> 00:26:25,720
因为它比较小的时候

851
00:26:25,720 --> 00:26:26,519
就等于这个东西

852
00:26:26,519 --> 00:26:27,960
所以它又除了这个东西

853
00:26:27,960 --> 00:26:30,440
来让你修正一下

854
00:26:30,440 --> 00:26:31,559
当你比较小的时候

855
00:26:31,559 --> 00:26:33,000
偏零的一个趋势

856
00:26:33,759 --> 00:26:35,519
当然T比较大的时候就无所谓了

857
00:26:35,519 --> 00:26:36,920
因为T比较大的时候

858
00:26:36,920 --> 00:26:38,799
这个东西就变成了零了

859
00:26:38,839 --> 00:26:40,600
因为β1是小于1的东西

860
00:26:40,600 --> 00:26:41,200
T很大的时候

861
00:26:41,200 --> 00:26:41,920
都有变成零

862
00:26:41,920 --> 00:26:43,000
所以就没关系了

863
00:26:43,400 --> 00:26:44,319
这个地方修正

864
00:26:44,319 --> 00:26:46,400
是针对比较小的T的时候做的

865
00:26:47,960 --> 00:26:48,640
OK

866
00:26:48,640 --> 00:26:49,200
另外一块

867
00:26:49,319 --> 00:26:50,279
它又记录了一个

868
00:26:50,279 --> 00:26:51,000
另外的东西

869
00:26:51,000 --> 00:26:52,799
就是说ST

870
00:26:52,919 --> 00:26:56,799
就它跟ST跟VT有点类似

871
00:26:56,799 --> 00:26:58,240
但是唯一的区别是

872
00:26:58,240 --> 00:27:00,799
这个地方记的是一个GT的平方

873
00:27:00,879 --> 00:27:01,440
什么意思

874
00:27:01,559 --> 00:27:03,240
就是说每一个元素

875
00:27:03,799 --> 00:27:05,399
G是一个向量耳机的平方

876
00:27:05,399 --> 00:27:07,480
是写成了每个元素的平方

877
00:27:07,720 --> 00:27:12,159
就是说它对每个元素的平方

878
00:27:12,559 --> 00:27:13,879
也做了一个这样子的

879
00:27:13,879 --> 00:27:15,000
一个smooth的版本

880
00:27:15,000 --> 00:27:16,399
存在ST里面

881
00:27:16,680 --> 00:27:19,079
通常这个地方取的特别平滑

882
00:27:19,079 --> 00:27:20,240
就是0.99

883
00:27:20,240 --> 00:27:22,079
就是一个比较大的窗口的平滑

884
00:27:23,480 --> 00:27:26,839
然后也要修正一次

885
00:27:28,039 --> 00:27:28,639
好

886
00:27:28,639 --> 00:27:30,240
有了这两个东西之后

887
00:27:30,759 --> 00:27:33,680
它就会重新调整你的T度

888
00:27:35,399 --> 00:27:36,399
它调整怎么调整

889
00:27:36,720 --> 00:27:38,000
上面用的是VT

890
00:27:38,000 --> 00:27:39,720
就跟我们之前的Momentum有点像

891
00:27:39,720 --> 00:27:39,960
对吧

892
00:27:39,960 --> 00:27:43,079
VT你可认为是一个向量

893
00:27:43,079 --> 00:27:45,720
就整个T度的一个平滑的版本

894
00:27:46,440 --> 00:27:46,720
对吧

895
00:27:46,720 --> 00:27:49,039
在过去一些时间里面

896
00:27:49,039 --> 00:27:51,200
所谓的T度的一个加全平

897
00:27:51,480 --> 00:27:53,480
加全和做一个平滑的版本

898
00:27:54,200 --> 00:27:55,600
所以这个地方是没错

899
00:27:55,600 --> 00:27:56,240
但这个地方

900
00:27:56,240 --> 00:27:57,759
它又加了一个新的东西

901
00:27:58,400 --> 00:28:00,960
它要除以除是按元素除

902
00:28:01,519 --> 00:28:02,799
它除这个东西

903
00:28:03,480 --> 00:28:05,480
开根号加上一个Epsilon

904
00:28:06,080 --> 00:28:07,160
它是干什么呢

905
00:28:08,400 --> 00:28:09,160
它这个东西

906
00:28:09,160 --> 00:28:12,880
它其实是从AdaGrad那一套过来

907
00:28:12,880 --> 00:28:13,600
就是说

908
00:28:13,799 --> 00:28:15,160
它处理了一个

909
00:28:15,160 --> 00:28:17,080
这个地方处理的情况是说

910
00:28:17,120 --> 00:28:19,559
有时候你每一个

911
00:28:21,400 --> 00:28:22,120
特征

912
00:28:22,120 --> 00:28:22,960
就所谓的特征

913
00:28:22,960 --> 00:28:24,000
就表示你做

914
00:28:24,519 --> 00:28:26,080
你做信息回归的时候

915
00:28:26,080 --> 00:28:26,680
每个特征

916
00:28:26,840 --> 00:28:27,960
或者说你做

917
00:28:29,840 --> 00:28:30,720
每一个维度

918
00:28:31,600 --> 00:28:32,440
每一个维度

919
00:28:32,559 --> 00:28:35,120
它值有可能是不一样的

920
00:28:35,759 --> 00:28:36,960
就有些值可能比较大

921
00:28:36,960 --> 00:28:37,799
有些比较小

922
00:28:37,840 --> 00:28:39,360
这个在哪个地方会常用

923
00:28:39,519 --> 00:28:41,600
在NLP里面会出现多一点

924
00:28:41,600 --> 00:28:42,400
有些词

925
00:28:42,400 --> 00:28:45,640
比如说有些词出现的比较频繁

926
00:28:45,680 --> 00:28:47,480
有些词出现比较小

927
00:28:47,480 --> 00:28:50,319
所以那些比较频繁出现的东西

928
00:28:50,319 --> 00:28:51,559
它梯度就会比较大

929
00:28:51,559 --> 00:28:53,000
而且出现的比较小的值

930
00:28:53,159 --> 00:28:54,519
它的梯度会比较小

931
00:28:55,799 --> 00:28:57,000
有时候你要做

932
00:28:57,639 --> 00:29:00,200
如果是做一个特征抽取的话

933
00:29:00,200 --> 00:29:02,240
如果你没有做好normalization的话

934
00:29:02,519 --> 00:29:03,639
有些特征

935
00:29:03,639 --> 00:29:04,759
它的值会比较大

936
00:29:04,799 --> 00:29:05,759
有些比较小

937
00:29:05,960 --> 00:29:06,759
它为什么问题

938
00:29:07,200 --> 00:29:08,599
特征值比较大的

939
00:29:08,599 --> 00:29:09,960
那些梯度会比较大

940
00:29:09,960 --> 00:29:10,960
值会比较大

941
00:29:11,000 --> 00:29:13,879
特征比较小的出现比较小的

942
00:29:13,879 --> 00:29:14,279
那些值

943
00:29:14,399 --> 00:29:16,399
它特征它的梯度会比较小

944
00:29:16,720 --> 00:29:18,960
所以导致你取nulling rate就麻烦了

945
00:29:19,519 --> 00:29:20,880
nulling rate不能取太大

946
00:29:20,880 --> 00:29:21,640
取太大的话

947
00:29:21,840 --> 00:29:24,840
乘以你值比较大的那些东西

948
00:29:24,960 --> 00:29:26,640
就导致那一块更新的太大

949
00:29:26,640 --> 00:29:28,319
容易炸掉了

950
00:29:29,960 --> 00:29:31,759
如果你想取得比较小

951
00:29:31,759 --> 00:29:34,319
使得那些值比较大的一些梯度

952
00:29:35,160 --> 00:29:35,519
它的

953
00:29:40,279 --> 00:29:40,960
怎么说

954
00:29:41,319 --> 00:29:42,680
就是说使得它不会炸的话

955
00:29:43,039 --> 00:29:45,559
但是那些梯度比较小的梯度

956
00:29:45,600 --> 00:29:46,680
乘以学习率

957
00:29:46,680 --> 00:29:48,039
它的更新就不够

958
00:29:48,119 --> 00:29:49,399
就导致跑得比较慢

959
00:29:50,960 --> 00:29:51,559
OK

960
00:29:51,680 --> 00:29:54,440
所以就是说导致说你就很尴尬

961
00:29:54,799 --> 00:29:56,039
我们知道有很多技术

962
00:29:56,200 --> 00:29:57,359
就batch normalization

963
00:29:57,359 --> 00:29:58,480
多多少少干这个事情

964
00:29:58,480 --> 00:29:58,920
对吧

965
00:29:58,960 --> 00:30:02,240
然后我们之前做最早最早的时候

966
00:30:02,240 --> 00:30:05,359
做房价预测的时候

967
00:30:05,359 --> 00:30:07,359
我们对每个特征做了normalization

968
00:30:07,799 --> 00:30:09,480
所以这个地方多多少少有点像

969
00:30:09,480 --> 00:30:10,240
normalization

970
00:30:10,399 --> 00:30:13,279
就是说对每一个你维度

971
00:30:13,279 --> 00:30:14,519
就是你权重

972
00:30:15,519 --> 00:30:18,799
它除以你梯度的平方的和

973
00:30:19,319 --> 00:30:21,680
一个平滑板的平方和

974
00:30:21,920 --> 00:30:25,400
就是过去所有的那些东西加起来

975
00:30:25,400 --> 00:30:26,680
所以这个东西是说

976
00:30:26,680 --> 00:30:29,799
假设你维度的值特别大的话

977
00:30:29,799 --> 00:30:30,879
这个特别大的话

978
00:30:30,920 --> 00:30:32,799
除以它的一个平方的话

979
00:30:32,799 --> 00:30:36,000
那么当然是它也会变得比较小

980
00:30:36,000 --> 00:30:37,599
如果你是这个值比较小的话

981
00:30:37,720 --> 00:30:38,839
你小除以它的话

982
00:30:38,839 --> 00:30:39,799
也会就是说

983
00:30:39,799 --> 00:30:42,160
把大家每个值都拉到一个

984
00:30:42,200 --> 00:30:44,240
差不多的scale上面

985
00:30:45,359 --> 00:30:46,839
另外一个是它是个累加的

986
00:30:46,960 --> 00:30:48,079
这个东西是一个

987
00:30:49,279 --> 00:30:50,480
多多少少有点累加

988
00:30:50,799 --> 00:30:52,240
所以它是有多多少少

989
00:30:52,240 --> 00:30:53,639
有一点点decay的效果

990
00:30:53,639 --> 00:30:55,680
就是说会变得偏大一点

991
00:30:55,879 --> 00:30:56,599
整体来讲

992
00:30:56,599 --> 00:30:58,519
就是说这个东西是用来

993
00:30:59,000 --> 00:31:00,119
使得这个东西

994
00:31:00,599 --> 00:31:01,319
上面这个东西

995
00:31:01,319 --> 00:31:04,319
是让过去的梯度方向比较平滑

996
00:31:04,519 --> 00:31:06,480
下面这个东西是使得每个维度

997
00:31:06,559 --> 00:31:07,400
它那个值

998
00:31:07,839 --> 00:31:09,720
都在比较合适的范围里面

999
00:31:09,720 --> 00:31:11,879
做每个维度的调整

1000
00:31:12,400 --> 00:31:13,320
Epsilon是表示

1001
00:31:13,320 --> 00:31:14,880
你不要除0的一个东西

1002
00:31:15,880 --> 00:31:16,360
OK

1003
00:31:16,360 --> 00:31:17,880
所以这就是最后

1004
00:31:17,880 --> 00:31:18,800
就是说Wt

1005
00:31:18,800 --> 00:31:19,960
等于Wt-1

1006
00:31:19,960 --> 00:31:20,760
减去学习率

1007
00:31:20,760 --> 00:31:21,560
乘以这个东西

1008
00:31:22,120 --> 00:31:22,480
OK

1009
00:31:22,480 --> 00:31:23,440
所以这个就是核心

1010
00:31:24,040 --> 00:31:24,880
所以基本上Adam

1011
00:31:24,880 --> 00:31:25,680
就是一个

1012
00:31:26,000 --> 00:31:27,080
对上面平滑过

1013
00:31:27,080 --> 00:31:27,560
对下面

1014
00:31:27,560 --> 00:31:28,520
然后对每个值

1015
00:31:28,520 --> 00:31:29,400
也做一个

1016
00:31:29,800 --> 00:31:31,440
重新调整的一个算法

1017
00:31:32,120 --> 00:31:33,200
因为它

1018
00:31:34,680 --> 00:31:35,880
就做那么多平滑

1019
00:31:36,120 --> 00:31:37,440
而且这个平滑是比较大的

1020
00:31:37,440 --> 00:31:38,560
0.99到0.

1021
00:31:38,560 --> 00:31:39,840
前一个值是0.9

1022
00:31:39,840 --> 00:31:41,440
这也是0.99

1023
00:31:41,519 --> 00:31:43,279
所以它是相对说比较平滑的

1024
00:31:43,279 --> 00:31:44,279
导致说

1025
00:31:44,519 --> 00:31:45,600
整个Gt

1026
00:31:46,000 --> 00:31:47,039
它出来的Gt

1027
00:31:47,039 --> 00:31:47,799
写出来的东西

1028
00:31:47,880 --> 00:31:49,640
它一些值都是比较均匀的

1029
00:31:49,960 --> 00:31:52,000
而且是值区间比较好

1030
00:31:52,000 --> 00:31:53,440
之间比较均匀

1031
00:31:53,480 --> 00:31:54,880
所以导致说

1032
00:31:55,240 --> 00:31:57,519
你对学习率就没那么敏感一些

1033
00:31:58,440 --> 00:31:58,759
OK

1034
00:31:58,759 --> 00:31:59,720
这就是Adam

1035
00:32:01,440 --> 00:32:01,680
好

1036
00:32:01,680 --> 00:32:02,279
总结一下

1037
00:32:03,440 --> 00:32:05,200
我们就是很快的过了一条

1038
00:32:05,200 --> 00:32:05,799
用后算法

1039
00:32:06,279 --> 00:32:07,279
深度学习的模型

1040
00:32:08,200 --> 00:32:09,519
大多数是非突的

1041
00:32:09,519 --> 00:32:10,559
所以导致说

1042
00:32:10,559 --> 00:32:11,440
用后算法里面

1043
00:32:11,639 --> 00:32:12,519
大量的理论

1044
00:32:12,639 --> 00:32:14,000
关于是突的算法

1045
00:32:14,119 --> 00:32:16,119
就基本上用不过来了

1046
00:32:17,039 --> 00:32:18,559
在深度学习里面

1047
00:32:18,559 --> 00:32:20,680
小批量随Gt度下降

1048
00:32:20,680 --> 00:32:22,519
是最常见的用后算法

1049
00:32:22,680 --> 00:32:25,960
在基学习好像也慢慢的

1050
00:32:25,960 --> 00:32:26,720
大于往往多一点

1051
00:32:26,720 --> 00:32:28,319
但是基学习里面

1052
00:32:28,759 --> 00:32:30,599
没有那么是dominant

1053
00:32:30,599 --> 00:32:31,720
基本上在深度学习

1054
00:32:31,720 --> 00:32:33,000
我还很少看见

1055
00:32:33,000 --> 00:32:35,000
谁不用小批量随Gt度下降

1056
00:32:35,000 --> 00:32:37,319
在早期大家还调研过的算法

1057
00:32:37,319 --> 00:32:39,160
但现在基本上大家都是

1058
00:32:39,200 --> 00:32:40,000
用这个东西了

1059
00:32:41,240 --> 00:32:43,880
然后通常我们

1060
00:32:43,880 --> 00:32:45,680
当我们在最早时讲过

1061
00:32:45,680 --> 00:32:48,040
小批量随Gt度下降的算法的实现

1062
00:32:48,400 --> 00:32:49,600
然后在上面指向

1063
00:32:50,160 --> 00:32:52,240
一般大家常见的是用一个

1064
00:32:52,240 --> 00:32:52,840
momentum

1065
00:32:52,840 --> 00:32:55,519
就是让T度做一个平滑

1066
00:32:56,440 --> 00:32:58,040
这也是一个最简单的

1067
00:32:58,600 --> 00:33:01,720
让你优化稳定一点的算法

1068
00:33:02,080 --> 00:33:02,560
另外一个

1069
00:33:02,560 --> 00:33:04,400
现在大家比较常见的

1070
00:33:04,759 --> 00:33:05,600
是用Adam

1071
00:33:05,600 --> 00:33:07,080
Adam是对T度做平滑

1072
00:33:07,080 --> 00:33:09,280
而且对T度的各个维度的值

1073
00:33:09,280 --> 00:33:10,520
做重新调整

1074
00:33:10,520 --> 00:33:12,320
是他就是double平滑

1075
00:33:13,240 --> 00:33:15,760
所以整体看下来

1076
00:33:15,760 --> 00:33:18,680
就是说我们自己试用下来

1077
00:33:18,800 --> 00:33:20,720
和别人的一些结果发现说

1078
00:33:20,720 --> 00:33:23,520
Adam不一定比冲量法效果

1079
00:33:23,520 --> 00:33:26,120
就最后的accuracy

1080
00:33:26,120 --> 00:33:26,960
会什么东西最高

1081
00:33:27,080 --> 00:33:28,080
收敛会更快

1082
00:33:28,120 --> 00:33:29,360
但是整体来讲

1083
00:33:29,640 --> 00:33:30,920
Adam是比较稳定的

1084
00:33:31,040 --> 00:33:33,320
而且就是说在整体来讲

1085
00:33:33,320 --> 00:33:34,840
就是说对学习率比较稳定

1086
00:33:35,480 --> 00:33:36,200
所谓的稳定

1087
00:33:36,240 --> 00:33:37,880
就是说你学习率再取一个

1088
00:33:37,880 --> 00:33:39,000
就SCD加momentum

1089
00:33:39,000 --> 00:33:39,640
可能学习率

1090
00:33:39,640 --> 00:33:41,080
在这个区间里面效果好

1091
00:33:41,080 --> 00:33:42,319
除了这个区间也不行了

1092
00:33:42,360 --> 00:33:44,480
Adam就是说我的区间可能会大一点

1093
00:33:44,840 --> 00:33:47,559
就虽然每个值都不见得比你好

1094
00:33:47,559 --> 00:33:48,960
但是我这个值会大一点

1095
00:33:48,960 --> 00:33:50,200
使得大家对调参

1096
00:33:50,519 --> 00:33:51,920
相对来说比较容易一点

1097
00:33:52,200 --> 00:33:54,880
我也是觉得这个是非常值得的

1098
00:33:54,880 --> 00:33:57,120
就是说如果你没有特别好的想法

1099
00:33:57,120 --> 00:33:58,440
你新模型一上

1100
00:33:58,440 --> 00:33:59,680
你可以试一下Adam

1101
00:33:59,759 --> 00:34:02,600
这是应该是目前看起来是没错的

