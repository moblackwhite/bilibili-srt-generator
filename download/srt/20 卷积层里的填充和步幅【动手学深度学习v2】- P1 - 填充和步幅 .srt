1
00:00:00,000 --> 00:00:03,879
我们的第一个问题是说是不是有错误

2
00:00:03,879 --> 00:00:06,839
224变成4×4

3
00:00:06,839 --> 00:00:13,439
用5×5的和适用55层来补适用44层

4
00:00:13,439 --> 00:00:16,240
OK这是个很好的问题

5
00:00:16,240 --> 00:00:17,120
我们可以算一下

6
00:00:17,120 --> 00:00:19,240
就是说你用了5×5的和的话

7
00:00:19,240 --> 00:00:20,800
每一次是-4对不对

8
00:00:20,800 --> 00:00:23,240
那么224除以4的话

9
00:00:23,240 --> 00:00:24,760
4524对的

10
00:00:24,760 --> 00:00:25,400
你说的对

11
00:00:25,400 --> 00:00:26,640
应该是55层

12
00:00:26,640 --> 00:00:27,560
你挺厉害的

13
00:00:27,560 --> 00:00:28,320
是的

14
00:00:28,320 --> 00:00:29,080
是55层

15
00:00:29,880 --> 00:00:31,960
我上次在Berkeley讲课的时候

16
00:00:31,960 --> 00:00:32,960
没有人给我们指出来

17
00:00:35,600 --> 00:00:36,480
问题二

18
00:00:37,200 --> 00:00:39,359
这几个超参数的影响

19
00:00:39,359 --> 00:00:41,320
重要程度是怎么得到的

20
00:00:41,400 --> 00:00:43,640
和的大小填充和不符

21
00:00:45,760 --> 00:00:47,240
这是一个很好的问题

22
00:00:49,240 --> 00:00:51,040
就是说我们有讲说

23
00:00:51,040 --> 00:00:54,000
我们有现在有三个超参数了

24
00:00:54,000 --> 00:00:55,600
我们其实之后还有一个

25
00:00:55,600 --> 00:00:56,680
今天还会讲另外一个

26
00:00:56,680 --> 00:00:57,600
最后一个

27
00:00:58,320 --> 00:00:59,160
也不是最后一个

28
00:00:59,160 --> 00:01:01,359
其实我们还有更奇怪的

29
00:01:01,359 --> 00:01:06,000
我们先说这三个超参数的重要程度

30
00:01:07,000 --> 00:01:08,799
一般来说

31
00:01:08,799 --> 00:01:14,840
填充大家会使得输入和输出的高宽是一样

32
00:01:14,960 --> 00:01:19,359
就是你的填充通常是取你的和减移

33
00:01:20,359 --> 00:01:22,599
就使得你的跟我们刚刚一样

34
00:01:22,640 --> 00:01:25,920
是你的输出和输入不要变

35
00:01:26,079 --> 00:01:28,280
他没有特别说为什么你要这么做

36
00:01:28,280 --> 00:01:30,400
纯粹是说这样子算起来比较方便

37
00:01:30,439 --> 00:01:32,240
因为对卷积省去网络的话

38
00:01:32,280 --> 00:01:33,840
你要去一直去想

39
00:01:34,040 --> 00:01:35,560
我放了那么多层的话

40
00:01:35,560 --> 00:01:39,159
我的输入和输出之间是变成什么关系

41
00:01:39,879 --> 00:01:41,519
所以填充通常是

42
00:01:42,400 --> 00:01:45,599
设成我们和减移的情况

43
00:01:46,240 --> 00:01:47,520
另外一个是不符

44
00:01:48,599 --> 00:01:50,120
不符这个东西

45
00:01:50,480 --> 00:01:52,040
你可以认为是说

46
00:01:53,359 --> 00:01:54,640
通常来讲

47
00:01:55,320 --> 00:01:57,079
不符等于一是最好的

48
00:01:57,079 --> 00:02:01,120
就是说能够每一次往外挪一点的

49
00:02:01,120 --> 00:02:02,519
就是说看到东西更多

50
00:02:03,920 --> 00:02:06,159
但是通常我们如果不选不符

51
00:02:06,159 --> 00:02:07,280
唯一的是什么情况

52
00:02:07,680 --> 00:02:10,439
就是说我们觉得计算量太大了

53
00:02:11,400 --> 00:02:13,039
就跟我们刚刚的情况

54
00:02:13,400 --> 00:02:16,560
我们觉得你需要很多层

55
00:02:16,599 --> 00:02:18,759
才能使得我的图片变得越来越小

56
00:02:18,759 --> 00:02:22,560
因为最后其实你是想把一个图片降成一个

57
00:02:22,759 --> 00:02:24,520
33 4x4或者5x5

58
00:02:24,520 --> 00:02:26,199
或最多7x7的一个大小

59
00:02:26,479 --> 00:02:28,319
最后你要降成那么一个大小的图片

60
00:02:29,360 --> 00:02:32,080
然后但是你的输入图片很大的情况下

61
00:02:32,479 --> 00:02:36,640
你不想让真的用很多层去做这个事情的话

62
00:02:36,680 --> 00:02:38,280
那么你通常会选用不符

63
00:02:39,080 --> 00:02:40,759
通常来说不符是取2

64
00:02:41,039 --> 00:02:42,039
每次减半

65
00:02:43,039 --> 00:02:44,920
所以不符等于什么时候不符取2

66
00:02:44,920 --> 00:02:46,640
纯粹是你去看说

67
00:02:47,000 --> 00:02:50,759
我要我的计算复杂度大概是什么需求

68
00:02:50,879 --> 00:02:52,560
然后我可以在中间一些地方

69
00:02:53,000 --> 00:02:54,959
假设一般的做法

70
00:02:54,959 --> 00:02:57,159
我们之后当然会讲具体大概怎么用了

71
00:02:57,159 --> 00:02:58,399
一般的情况下

72
00:02:58,399 --> 00:03:00,039
就是说假设我构造一个

73
00:03:01,639 --> 00:03:05,039
成为100层的一个神经网络

74
00:03:05,679 --> 00:03:09,639
那么我就算说我224要除以多少次2

75
00:03:10,199 --> 00:03:11,639
一般除个5次或6次吧

76
00:03:11,639 --> 00:03:12,199
我记得

77
00:03:12,479 --> 00:03:16,199
所以就是说那就是把5个不符为2的层

78
00:03:16,560 --> 00:03:19,159
均匀的插在你的神经网络的中间

79
00:03:19,519 --> 00:03:20,639
这是我们通常的做法

80
00:03:21,759 --> 00:03:22,239
OK

81
00:03:22,239 --> 00:03:25,120
所以核大小通常是最关键的

82
00:03:25,120 --> 00:03:27,319
填充是一般取默认的

83
00:03:27,319 --> 00:03:28,439
然后不符的话

84
00:03:28,439 --> 00:03:31,159
真的就是取决你要将你的模型

85
00:03:31,719 --> 00:03:33,239
复杂度控制在什么程度

86
00:03:35,519 --> 00:03:36,560
问题3

87
00:03:37,120 --> 00:03:39,359
卷机盒的边长一般选基数

88
00:03:39,359 --> 00:03:40,479
对的

89
00:03:40,479 --> 00:03:43,319
卷机的边长一般取基数

90
00:03:45,000 --> 00:03:47,399
你可以去偶数其实问题不大的

91
00:03:47,399 --> 00:03:50,039
就是说我记得有一篇论文

92
00:03:50,199 --> 00:03:53,159
虽然我不记得论文具体的标题是什么样子

93
00:03:53,439 --> 00:03:56,159
就是说你发现你用2或者4的

94
00:03:56,599 --> 00:03:57,280
核的

95
00:03:57,560 --> 00:04:00,599
核的卷机层其实效果也差不多

96
00:04:01,399 --> 00:04:03,560
之所以为什么取基数

97
00:04:03,560 --> 00:04:04,959
我觉得就是好对称一点

98
00:04:04,959 --> 00:04:06,680
就是你在填充的时候

99
00:04:06,959 --> 00:04:07,759
你取基数的话

100
00:04:07,759 --> 00:04:10,719
上下填充是一个对称的一个情况

101
00:04:10,719 --> 00:04:11,759
如果是偶数的话

102
00:04:11,759 --> 00:04:13,560
你上下填充不对称了

103
00:04:13,560 --> 00:04:14,719
就比较奇怪一点

104
00:04:15,079 --> 00:04:17,519
而且是一般来说

105
00:04:17,519 --> 00:04:18,360
你就是填充

106
00:04:18,639 --> 00:04:19,759
我们现在一般来说

107
00:04:19,759 --> 00:04:21,199
你的卷机盒就是3×3

108
00:04:21,840 --> 00:04:23,960
所以你当然你可以说

109
00:04:23,960 --> 00:04:25,680
我要取2×2的卷机盒

110
00:04:25,680 --> 00:04:27,199
或者4×4都OK了

111
00:04:27,439 --> 00:04:28,960
所以没什么太多区别

112
00:04:29,160 --> 00:04:32,439
所以大家就是取3×3

113
00:04:32,439 --> 00:04:33,040
取多

114
00:04:33,040 --> 00:04:34,720
这是一个常见的一个做法而已

115
00:04:35,400 --> 00:04:36,080
效果上来说

116
00:04:36,080 --> 00:04:37,199
其实大家都差不多了

117
00:04:41,000 --> 00:04:41,720
问题4

118
00:04:41,720 --> 00:04:43,280
一般卷机处理完

119
00:04:43,280 --> 00:04:45,520
我的输出维度都要减半

120
00:04:45,799 --> 00:04:48,199
为什么这里提出输入和输出

121
00:04:48,199 --> 00:04:49,000
保持不变

122
00:04:49,439 --> 00:04:50,240
而不是这样子

123
00:04:50,240 --> 00:04:51,799
你不能每次都减半

124
00:04:51,799 --> 00:04:53,319
如果每次减半的话

125
00:04:53,599 --> 00:04:56,199
你是一个224的一个输入的话

126
00:04:56,199 --> 00:04:57,479
你加个5层

127
00:04:57,479 --> 00:04:58,479
你就没了

128
00:04:58,879 --> 00:05:00,240
加个5层应该就变成7了吧

129
00:05:00,279 --> 00:05:03,000
我觉得224是7×1多来着

130
00:05:04,399 --> 00:05:06,479
就是说你不能每次都减半了

131
00:05:06,479 --> 00:05:07,639
你如果每次减半

132
00:05:07,639 --> 00:05:09,639
你就做不了深的神级网络了

133
00:05:09,799 --> 00:05:12,719
所以减半是很少的情况

134
00:05:12,879 --> 00:05:14,319
绝大部分情况是不变的

135
00:05:16,199 --> 00:05:20,839
不符合填充这两个超参数

136
00:05:20,839 --> 00:05:22,639
为什么实际上不怎么调节

137
00:05:22,959 --> 00:05:24,359
其实跟之前是一样的

138
00:05:24,359 --> 00:05:26,839
就是说他其实不是说不调节

139
00:05:26,839 --> 00:05:28,799
就不符填充核大小

140
00:05:28,799 --> 00:05:30,199
和之后的通道数

141
00:05:30,199 --> 00:05:33,079
都是一个神级网络架构的一部分

142
00:05:33,240 --> 00:05:36,120
是整个说我整个网络怎么设计的一部分

143
00:05:36,120 --> 00:05:38,799
他是其中确实不那么敏感一点

144
00:05:38,879 --> 00:05:40,680
一般来说我们说我们用resnet

145
00:05:40,680 --> 00:05:42,680
我们用AlexNet用什么东西的话

146
00:05:42,680 --> 00:05:43,919
他就告诉你说

147
00:05:43,920 --> 00:05:46,160
我应该去怎么想设置的参数

148
00:05:46,160 --> 00:05:47,840
就是网络架构的一部分

149
00:05:52,319 --> 00:05:53,120
问题六

150
00:05:53,360 --> 00:05:55,759
现在已经有很多经典网络结构了

151
00:05:55,800 --> 00:05:58,160
对于各种应用有各种结构

152
00:05:58,200 --> 00:05:59,640
我们平常使用的时候

153
00:05:59,680 --> 00:06:03,160
自己设置设计卷迹和大小的情况多吗

154
00:06:03,160 --> 00:06:05,240
还是直接他用经典网络结构

155
00:06:05,439 --> 00:06:06,480
这是一个很好的问题

156
00:06:06,480 --> 00:06:08,639
就是说我们当然是说了

157
00:06:08,639 --> 00:06:11,240
你可以总是可以自己弄

158
00:06:11,840 --> 00:06:12,920
但实际上来讲

159
00:06:13,240 --> 00:06:14,040
你不会的

160
00:06:14,040 --> 00:06:14,680
一般来说

161
00:06:14,680 --> 00:06:15,600
你用经典情况

162
00:06:16,640 --> 00:06:18,199
或者更简单来说

163
00:06:18,199 --> 00:06:19,240
你就用resnet

164
00:06:20,759 --> 00:06:21,720
从resnet

165
00:06:21,720 --> 00:06:23,680
我们之后介绍他有一个系列

166
00:06:23,680 --> 00:06:24,720
你可以用18层

167
00:06:24,720 --> 00:06:26,200
34层

168
00:06:26,200 --> 00:06:26,680
50层

169
00:06:26,680 --> 00:06:29,240
或者甚至更多的152层都OK

170
00:06:30,080 --> 00:06:31,759
也就是说一般来说

171
00:06:31,759 --> 00:06:34,759
大家不会去真的去手写神级网络

172
00:06:34,800 --> 00:06:36,280
除非你的

173
00:06:37,120 --> 00:06:41,080
除非你的输入是一个非常不一样的

174
00:06:41,080 --> 00:06:41,800
一个情况

175
00:06:42,720 --> 00:06:43,160
举个例子

176
00:06:43,160 --> 00:06:46,680
假设你的输入图片是一个20×1000的

177
00:06:46,680 --> 00:06:47,439
就是说你

178
00:06:47,600 --> 00:06:49,639
你这个很扁很扁的一个图片

179
00:06:49,680 --> 00:06:51,439
那么经典的结构

180
00:06:51,439 --> 00:06:53,000
你就不好再用了

181
00:06:53,160 --> 00:06:54,560
所以你得改一点东西

182
00:06:54,759 --> 00:06:57,439
但如果你的图片都是差不多的情况下

183
00:06:57,720 --> 00:06:59,680
大家一般是用经典的套路

184
00:06:59,720 --> 00:07:02,120
或者是说你在经典的架构上

185
00:07:02,120 --> 00:07:03,519
稍微做一点调整

186
00:07:04,280 --> 00:07:06,800
那你resnet也可以调整一些东西

187
00:07:06,960 --> 00:07:09,000
使得你可以达到好的效果

188
00:07:09,040 --> 00:07:11,480
但是反过来讲

189
00:07:11,480 --> 00:07:12,920
当然你可以自己设计了

190
00:07:12,960 --> 00:07:16,280
但是很有可能是你自己设计的

191
00:07:16,280 --> 00:07:18,400
不效果不如人家那么好

192
00:07:18,960 --> 00:07:21,319
人家其实我们就会大概会给大家讲一下

193
00:07:21,319 --> 00:07:23,759
是设计里面是怎么做一些平衡的

194
00:07:24,640 --> 00:07:27,879
而且很多时候网络结构

195
00:07:27,920 --> 00:07:30,319
没你想象的那么关键

196
00:07:30,720 --> 00:07:31,640
说一句真话

197
00:07:32,319 --> 00:07:35,000
比如说现在大家都用resnet居多

198
00:07:35,000 --> 00:07:37,319
或者resnet系列的那一些变种

199
00:07:37,640 --> 00:07:38,400
实际上来讲

200
00:07:39,360 --> 00:07:44,040
网络只是我们很多整个一块

201
00:07:44,520 --> 00:07:48,400
就是说具体说它有多少用

202
00:07:48,400 --> 00:07:51,120
大家并没有做特别仔细的比较

203
00:07:51,200 --> 00:07:53,240
因为它取决于你的数据是怎么做

204
00:07:53,240 --> 00:07:53,960
预处理的

205
00:07:54,000 --> 00:07:54,840
你的训练的

206
00:07:54,840 --> 00:07:56,640
你那些细节是怎么做的

207
00:07:56,680 --> 00:07:59,080
那些东西都会对你产生很大的影响

208
00:07:59,840 --> 00:08:01,160
所以通常这里来说

209
00:08:01,200 --> 00:08:03,480
我们在实际情况中

210
00:08:03,520 --> 00:08:06,600
一定是参照经典的网络结构做设计

211
00:08:06,880 --> 00:08:07,520
所谓的经典

212
00:08:07,519 --> 00:08:10,359
说白了就是一堆33的一些网络

213
00:08:10,359 --> 00:08:12,519
然后把它搞成一个两层的结构

214
00:08:13,399 --> 00:08:15,959
然后你再加一个residual connection

215
00:08:15,959 --> 00:08:19,359
我们之后讲不太提前往下讲太多

216
00:08:22,839 --> 00:08:23,759
问题七

217
00:08:24,680 --> 00:08:28,599
一般要用卷积层的时候

218
00:08:28,599 --> 00:08:30,879
只有一个padding参数和一个strat参数

219
00:08:31,560 --> 00:08:36,079
那么应用分别高度填充和是怎么

220
00:08:36,639 --> 00:08:38,200
就是说你调用的时候

221
00:08:38,200 --> 00:08:41,080
你当然可以传入一个等于1

222
00:08:41,280 --> 00:08:41,960
padding等于1

223
00:08:41,960 --> 00:08:42,759
strat等于1

224
00:08:43,040 --> 00:08:45,480
如果你要想分别做高宽的话

225
00:08:45,480 --> 00:08:46,920
那你就用一个列表

226
00:08:46,960 --> 00:08:48,080
就是一个tuple

227
00:08:48,120 --> 00:08:49,040
在python里面的

228
00:08:49,080 --> 00:08:50,360
就一个花括号

229
00:08:50,600 --> 00:08:51,280
12

230
00:08:51,600 --> 00:08:53,560
1应该是表示你的行

231
00:08:53,560 --> 00:08:55,000
2就表示你的列

232
00:08:56,040 --> 00:08:57,840
所以我们之前一样有讲

233
00:08:58,200 --> 00:08:59,080
大家可以看一下

234
00:09:03,040 --> 00:09:03,680
就问题8

235
00:09:03,680 --> 00:09:05,120
问题8其实我们刚刚有讲过

236
00:09:05,679 --> 00:09:06,679
就padding它的

237
00:09:06,679 --> 00:09:08,279
就是说框架的padding

238
00:09:08,279 --> 00:09:10,600
其实是说你在左边

239
00:09:11,360 --> 00:09:13,360
就是说这个padding的意思是这样子的

240
00:09:13,759 --> 00:09:16,679
就是说1是说我左边和右边

241
00:09:16,679 --> 00:09:19,679
同时是padding一行和一列

242
00:09:20,200 --> 00:09:21,039
因为这样子的话

243
00:09:21,039 --> 00:09:21,799
你就比较好说

244
00:09:21,799 --> 00:09:22,960
我要padding不一样的时候

245
00:09:22,960 --> 00:09:24,799
我可以说方括号12

246
00:09:25,200 --> 00:09:27,200
就1就表示说方括号

247
00:09:27,200 --> 00:09:28,639
不是方括

248
00:09:28,639 --> 00:09:30,399
花括号1和1的意思

249
00:09:30,600 --> 00:09:31,560
1和1p

250
00:09:33,360 --> 00:09:33,840
OK

251
00:09:33,840 --> 00:09:34,879
所以这个是意思

252
00:09:35,159 --> 00:09:38,120
我们是讲我们在slice讲的padding

253
00:09:38,120 --> 00:09:40,559
是说你就是我的填充

254
00:09:40,559 --> 00:09:43,000
就是说你一共要在

255
00:09:43,399 --> 00:09:45,799
填多少行和一共要填多少列

256
00:09:46,000 --> 00:09:46,799
是不一样的

257
00:09:50,679 --> 00:09:52,720
形状通常是一个整数

258
00:09:52,720 --> 00:09:54,600
如果不是整数怎么处理

259
00:09:55,080 --> 00:09:56,360
这个形状你

260
00:09:59,919 --> 00:10:00,879
我有点

261
00:10:03,120 --> 00:10:03,799
想不出来

262
00:10:03,800 --> 00:10:05,200
你不是整数会怎么样

263
00:10:05,200 --> 00:10:06,840
你整形状是说

264
00:10:06,840 --> 00:10:09,200
你有多少个像数

265
00:10:09,200 --> 00:10:11,400
像数有多少个元素

266
00:10:11,400 --> 00:10:11,840
你的

267
00:10:12,120 --> 00:10:14,560
就是说你的张量的形状

268
00:10:14,560 --> 00:10:17,280
我理解应该是不会有小数的

269
00:10:17,280 --> 00:10:18,080
都是整数

270
00:10:18,280 --> 00:10:20,120
所以你的形状就是一个张量的形状

271
00:10:20,120 --> 00:10:22,040
是应该不会有小数这个情况

272
00:10:22,080 --> 00:10:23,320
所以一定是一个

273
00:10:23,360 --> 00:10:24,480
所以你如果不是整数

274
00:10:24,720 --> 00:10:25,080
就是说

275
00:10:25,080 --> 00:10:27,280
所以我们在计算shape的时候

276
00:10:27,640 --> 00:10:29,080
都有往上取整

277
00:10:29,080 --> 00:10:30,560
往下取整这样子的操作

278
00:10:30,600 --> 00:10:32,480
是因为你必然是一个整数

279
00:10:34,800 --> 00:10:35,560
问题10

280
00:10:35,560 --> 00:10:37,360
为什么用3×3卷集合

281
00:10:37,360 --> 00:10:39,120
3×3的像素视野很小

282
00:10:39,160 --> 00:10:39,920
没错

283
00:10:39,960 --> 00:10:41,520
3×3视野是很小

284
00:10:41,520 --> 00:10:43,720
但是你如果你说很深的情况下

285
00:10:44,440 --> 00:10:45,680
我们就很搭了

286
00:10:46,040 --> 00:10:49,680
那么我们上一次也问到这个问题

287
00:10:49,800 --> 00:10:51,080
干脆我就给大家画一下

288
00:10:51,080 --> 00:10:51,800
就是说

289
00:10:53,800 --> 00:10:54,800
就假设

290
00:10:55,960 --> 00:10:57,160
假设你是一个3×3

291
00:10:57,360 --> 00:10:59,040
我们是这一层有一个

292
00:10:59,600 --> 00:11:01,200
3×3在这个地方

293
00:11:02,200 --> 00:11:04,520
就当然3×3是一个很小的东西了

294
00:11:04,800 --> 00:11:06,920
但接下来是说我在第二层

295
00:11:06,920 --> 00:11:08,680
我还有两层的话

296
00:11:08,720 --> 00:11:10,520
那么接下来看一个3×3

297
00:11:12,120 --> 00:11:14,320
这个3×3画的不是那么好

298
00:11:14,480 --> 00:11:16,759
那么第一个元素是从3×3来的

299
00:11:16,759 --> 00:11:17,280
对吧

300
00:11:17,640 --> 00:11:19,879
那么你这个元素其实是

301
00:11:19,920 --> 00:11:22,920
假设你的stride等于1的话

302
00:11:22,960 --> 00:11:24,520
那么就是来自于这个地方

303
00:11:25,920 --> 00:11:28,280
就是说你3×3

304
00:11:28,280 --> 00:11:29,480
那么第三层

305
00:11:29,639 --> 00:11:31,320
第三层为这一个元素

306
00:11:31,320 --> 00:11:31,800
是从

307
00:11:31,800 --> 00:11:33,080
我就换一个颜色

308
00:11:34,200 --> 00:11:36,240
第三层这一个元素的输出

309
00:11:36,680 --> 00:11:38,639
那就是说你是来自于第二层

310
00:11:38,639 --> 00:11:39,920
一个3×3的东西

311
00:11:40,159 --> 00:11:42,720
然后它是在下一层

312
00:11:42,720 --> 00:11:44,360
它对应的是一个

313
00:11:45,800 --> 00:11:47,440
一个应该是5×5了

314
00:11:49,240 --> 00:11:52,840
就是说你随着你的深度的增加的话

315
00:11:52,840 --> 00:11:56,519
你到比如说你用很多层的话

316
00:11:57,519 --> 00:11:59,840
你最后一层你那个东西

317
00:11:59,840 --> 00:12:01,559
其实你是往下对应的

318
00:12:01,559 --> 00:12:03,480
你原始图片是一个比较大的一个东西

319
00:12:03,480 --> 00:12:05,399
这不再是一个3×3了

320
00:12:06,399 --> 00:12:07,039
它就是很大

321
00:12:07,039 --> 00:12:08,720
它是一个线性相关的一个过程

322
00:12:08,759 --> 00:12:10,079
如果你用了stride的话

323
00:12:10,079 --> 00:12:11,120
它会变得更大

324
00:12:12,079 --> 00:12:13,079
所以你要这么想

325
00:12:13,360 --> 00:12:15,960
假设我最后的输出

326
00:12:15,960 --> 00:12:17,720
是一个1×1的元素的话

327
00:12:17,720 --> 00:12:18,960
这是一个1×1的话

328
00:12:19,000 --> 00:12:20,799
那么其实你这个1×1的元素

329
00:12:20,799 --> 00:12:23,799
你绝对是在整个你stride

330
00:12:23,799 --> 00:12:25,399
如果你没有取得特别大的情况下

331
00:12:25,799 --> 00:12:28,120
一定是能看到整个数的图片

332
00:12:29,519 --> 00:12:32,480
所以3×3其实是没问题的

333
00:12:32,480 --> 00:12:34,879
3×3只是说我用的比较小的卷积

334
00:12:34,879 --> 00:12:37,919
和我需要用比较深的神经网络

335
00:12:37,959 --> 00:12:42,679
使得我能够最后的最后一层

336
00:12:42,679 --> 00:12:43,439
每一个元素

337
00:12:43,439 --> 00:12:46,639
能看到足够多的图片里面的信息

338
00:12:50,360 --> 00:12:51,480
问题11

339
00:12:51,879 --> 00:12:53,480
有什么办法

340
00:12:53,639 --> 00:12:57,320
能让超参数一起参加训练吗

341
00:12:57,360 --> 00:12:58,320
有的

342
00:12:58,519 --> 00:13:00,800
这个就是我们上次提到的NAS

343
00:13:00,800 --> 00:13:04,120
就是Neural Network Architecture Search

344
00:13:04,360 --> 00:13:06,039
就是NAS

345
00:13:06,360 --> 00:13:09,240
也是自动机器学习的一块了

346
00:13:10,360 --> 00:13:12,159
确实有很多这样子的操作

347
00:13:12,200 --> 00:13:14,399
能使得最简单的情况

348
00:13:14,399 --> 00:13:15,120
就是说我搜

349
00:13:15,800 --> 00:13:18,080
有时候我就去尝试不同的

350
00:13:19,600 --> 00:13:22,440
我设计很多不一样的网络结构

351
00:13:22,480 --> 00:13:25,680
然后来选一个比较好的

352
00:13:26,200 --> 00:13:30,400
第二个是说你想要稍微便宜一点

353
00:13:30,400 --> 00:13:31,280
便宜点怎么做

354
00:13:32,040 --> 00:13:33,160
我在一个很大

355
00:13:33,200 --> 00:13:36,600
我构造一个特别大的网络

356
00:13:36,600 --> 00:13:40,040
里面有各种各样不同的设计模块

357
00:13:40,080 --> 00:13:42,200
然后我让模型自动去选

358
00:13:42,600 --> 00:13:44,560
我要选这么一条路下来

359
00:13:44,680 --> 00:13:45,520
这样子后

360
00:13:45,640 --> 00:13:48,680
最后我说我激活重建的一小块

361
00:13:48,680 --> 00:13:50,280
我把最后把这一小块拎出来

362
00:13:50,280 --> 00:13:51,560
是我要的一个结果

363
00:13:52,240 --> 00:13:54,520
当然这一块我们就不那么介绍了

364
00:13:54,560 --> 00:13:59,440
因为我觉得除非你是要有特殊的情况

365
00:13:59,480 --> 00:14:00,640
就是说我们之前说的

366
00:14:01,000 --> 00:14:03,080
你的输入形状特别不一样

367
00:14:03,120 --> 00:14:05,600
你跟我们这种经典的应用的

368
00:14:05,600 --> 00:14:07,120
一些图片的尺寸

369
00:14:07,360 --> 00:14:08,080
物体的大小

370
00:14:08,080 --> 00:14:09,680
特别不一样的情况下

371
00:14:09,840 --> 00:14:12,720
你可以考虑自己设计或自己去搜

372
00:14:12,760 --> 00:14:13,920
另外一个是说

373
00:14:14,600 --> 00:14:16,240
你有自己特别的硬件

374
00:14:16,240 --> 00:14:17,440
你想在手机上跑

375
00:14:17,480 --> 00:14:19,440
或者你想在一个很奇怪的硬件上跑

376
00:14:19,520 --> 00:14:22,040
不是主流的常用的硬件的话

377
00:14:22,080 --> 00:14:23,880
你可能会针对这个硬件

378
00:14:23,880 --> 00:14:26,840
去搜一个效率比较好的一个模型

379
00:14:27,040 --> 00:14:30,480
这个是通常那主要用的地方

380
00:14:30,520 --> 00:14:32,880
如果你是比较常见的情况下

381
00:14:32,920 --> 00:14:34,800
其实没必要用经典的就行了

382
00:14:38,040 --> 00:14:38,800
问题12

383
00:14:39,160 --> 00:14:41,480
通过多层卷积

384
00:14:41,480 --> 00:14:44,760
最后输出和输入形状相同

385
00:14:44,760 --> 00:14:46,600
特征会丢失吗

386
00:14:46,920 --> 00:14:50,040
我的理解是说

387
00:14:50,040 --> 00:14:52,320
假设你的输出和输入的形状是一样

388
00:14:52,320 --> 00:14:53,600
你特征会不会丢失

389
00:14:53,600 --> 00:14:54,840
就信息有没有丢失

390
00:14:58,279 --> 00:15:03,800
我其实觉得从信息论的角度来讲

391
00:15:05,600 --> 00:15:08,879
你可以认为信息永远是要丢失的

392
00:15:10,560 --> 00:15:13,480
就是说你的信息或特征

393
00:15:13,639 --> 00:15:14,840
其实看你怎么想了

394
00:15:14,960 --> 00:15:16,960
图片里面其实信息挺多了

395
00:15:16,960 --> 00:15:19,360
就是你可认为卷积就是一个

396
00:15:20,280 --> 00:15:21,840
你这么说吧

397
00:15:22,399 --> 00:15:23,639
机器学习

398
00:15:24,360 --> 00:15:26,080
本质上是干什么事情的

399
00:15:26,680 --> 00:15:28,200
它是一个压缩算法

400
00:15:29,480 --> 00:15:29,960
对吧

401
00:15:30,000 --> 00:15:31,399
我给你张图片

402
00:15:31,840 --> 00:15:33,600
你最后压缩成一个标号

403
00:15:33,600 --> 00:15:34,759
就是一个值

404
00:15:34,879 --> 00:15:35,800
就是一个类别

405
00:15:35,800 --> 00:15:37,399
就是一个我给你一张

406
00:15:37,399 --> 00:15:40,519
我给你拍张照片

407
00:15:40,560 --> 00:15:43,200
然后你predict输出

408
00:15:43,240 --> 00:15:45,280
那么假设我有一千类的话

409
00:15:45,320 --> 00:15:47,680
那真的就几个数值了

410
00:15:48,040 --> 00:15:52,040
就是说你把我的图片压缩到一个很小的

411
00:15:52,040 --> 00:15:52,680
值上面

412
00:15:52,720 --> 00:15:54,480
这个只是有余意信息的

413
00:15:55,120 --> 00:15:59,120
所以机器学习算法永远是会丢失信息的

414
00:15:59,800 --> 00:16:01,840
它就是一个很极端的压缩算法

415
00:16:01,879 --> 00:16:04,640
它不跟我们JPEG压缩或者别的压缩是一样

416
00:16:04,680 --> 00:16:07,640
压缩是人眼看不出它有区别

417
00:16:07,640 --> 00:16:09,160
还是能大部分的化学

418
00:16:09,440 --> 00:16:12,040
机器学习就是一个非常极端的压缩算法

419
00:16:14,080 --> 00:16:18,240
但是它会把非常原始的计算机能理解的

420
00:16:18,240 --> 00:16:19,600
一些像素信息

421
00:16:19,600 --> 00:16:21,759
一些文字的一些字符串信息

422
00:16:21,800 --> 00:16:25,000
压缩到一个人能理解的语义的空间里面

423
00:16:25,400 --> 00:16:27,080
你可以认为机器学习就干这个事情

424
00:16:29,600 --> 00:16:30,400
问题13

425
00:16:30,400 --> 00:16:31,680
Auto grown有自动调参

426
00:16:31,680 --> 00:16:33,440
深度学习网络的功能吗

427
00:16:33,440 --> 00:16:35,640
能大致讲一下调参有吗

428
00:16:35,680 --> 00:16:37,240
有的大家可以去看一下文档

429
00:16:37,240 --> 00:16:42,320
但我还是跟上次一样

430
00:16:42,360 --> 00:16:44,879
不那么建议大家用的东西特别贵

431
00:16:44,920 --> 00:16:47,760
我们之前做了一个实验

432
00:16:47,879 --> 00:16:50,360
然后一个时间跑了100万美金

433
00:16:50,640 --> 00:16:53,720
所以导致那个月我们都特别穷

434
00:16:56,240 --> 00:16:57,840
14跟之前是一样

435
00:16:57,840 --> 00:16:59,200
Auto grown有做NAS

436
00:16:59,240 --> 00:17:02,040
但是我们没有特别深入的做

437
00:17:02,040 --> 00:17:05,400
因为我觉得那是有钱人的游戏

438
00:17:08,600 --> 00:17:09,799
自动训练参数的话

439
00:17:09,839 --> 00:17:12,200
是不是更容易过拟核

440
00:17:13,559 --> 00:17:14,200
过拟核

441
00:17:14,200 --> 00:17:16,519
你其实我觉得过拟核这个事情

442
00:17:16,519 --> 00:17:20,319
如果你的验证级设置的比较好的话

443
00:17:20,319 --> 00:17:22,279
过拟核其实可以很好的避免

444
00:17:25,000 --> 00:17:27,639
如果你验证级许的不那么好的话

445
00:17:27,639 --> 00:17:29,240
过拟核确实会比较容易的

446
00:17:29,879 --> 00:17:34,159
但是我觉得自动训练参数的话

447
00:17:34,159 --> 00:17:36,240
当然会带来一些过拟核

448
00:17:36,279 --> 00:17:38,799
但是大家还是可以做比较好的控制的

449
00:17:40,319 --> 00:17:42,480
问题16

450
00:17:43,240 --> 00:17:45,039
多层的三乘三卷集

451
00:17:45,039 --> 00:17:49,759
是不是用不同大小的卷集和分别对原始图片

452
00:17:49,759 --> 00:17:51,720
做处理后再联合算出这些特征

453
00:17:51,720 --> 00:17:52,240
进行分类

454
00:17:52,240 --> 00:17:53,279
这个情况等价

455
00:17:56,159 --> 00:17:59,159
就是说多层的三乘三卷集

456
00:17:59,200 --> 00:18:01,839
是不是说我可以等价成为

457
00:18:01,839 --> 00:18:04,279
比如说我用比如三个三乘三的卷集

458
00:18:04,279 --> 00:18:07,159
是不是能用两个5乘5的卷集来替代

459
00:18:07,880 --> 00:18:11,600
其实你可以认为在效果上来说

460
00:18:11,600 --> 00:18:12,240
是的

461
00:18:12,279 --> 00:18:13,720
是可以的

462
00:18:15,880 --> 00:18:16,800
就是说

463
00:18:17,640 --> 00:18:18,800
我想一想

464
00:18:18,800 --> 00:18:21,600
我其实没有仔细去算过这个事情

465
00:18:21,600 --> 00:18:22,200
就是说

466
00:18:22,519 --> 00:18:25,240
比如说10层的三乘三的卷集

467
00:18:25,279 --> 00:18:28,519
我应该是可以等效成从效果上来说

468
00:18:28,880 --> 00:18:31,519
等效成一个可能是5层或6层的

469
00:18:31,519 --> 00:18:32,519
5乘5的卷集

470
00:18:34,160 --> 00:18:34,920
但是

471
00:18:35,400 --> 00:18:37,040
三乘三简直会快一些

472
00:18:38,160 --> 00:18:39,279
理解吗

473
00:18:40,360 --> 00:18:41,920
我们的我们之后会讲

474
00:18:42,120 --> 00:18:45,320
因为你的计算复杂度是你跟你的

475
00:18:46,160 --> 00:18:47,320
是平方向

476
00:18:47,320 --> 00:18:49,800
是跟你的三乘三的一个

477
00:18:49,800 --> 00:18:51,720
就是说你的计算复杂度是你的

478
00:18:52,080 --> 00:18:53,920
卷集的高层

479
00:18:53,960 --> 00:18:56,240
你的宽跟它相关的

480
00:18:56,279 --> 00:18:58,440
但是你对一个高宽的限制

481
00:18:58,440 --> 00:19:00,120
是说你三乘三改5乘5

482
00:19:00,120 --> 00:19:02,080
是个线性一个关系

483
00:19:02,120 --> 00:19:03,000
递减的关系

484
00:19:03,000 --> 00:19:05,360
所以5乘5用5乘5其实会

485
00:19:05,920 --> 00:19:06,880
更贵一些

486
00:19:07,360 --> 00:19:09,440
这个也是Inception

487
00:19:09,440 --> 00:19:12,759
就是GoogleNet的一个最核心的设计思路

488
00:19:12,759 --> 00:19:14,000
就我应该用小的卷集

489
00:19:14,000 --> 00:19:14,800
这样子快一点

490
00:19:17,960 --> 00:19:19,240
就底层用大颗浓

491
00:19:19,240 --> 00:19:20,200
上层用小颗浓

492
00:19:20,200 --> 00:19:23,279
相对都用比较宽的颗浓

493
00:19:23,279 --> 00:19:27,320
哪一种更适合目标多尺度的情况

494
00:19:30,680 --> 00:19:32,640
就是说这是一个很高级的问题了

495
00:19:32,920 --> 00:19:35,560
你可认为说我们现在主流的做法

496
00:19:35,559 --> 00:19:37,960
是说我底层那一层

497
00:19:38,000 --> 00:19:40,319
可以用一个相对来说比较大一点的

498
00:19:40,319 --> 00:19:42,159
depends on你的图屏有多大

499
00:19:42,159 --> 00:19:43,079
你可以用5乘5

500
00:19:43,079 --> 00:19:43,599
7乘7

501
00:19:43,599 --> 00:19:45,399
或者用11乘11都有可能

502
00:19:45,960 --> 00:19:48,399
但是之后大家还是用三乘三居多

503
00:19:48,599 --> 00:19:52,480
没有说你三乘三会怎么样

504
00:19:52,480 --> 00:19:54,079
就是说还是说三乘三会

505
00:19:54,079 --> 00:19:55,559
一便宜一点点算起来

506
00:19:55,559 --> 00:19:57,039
二简单一点

507
00:19:57,039 --> 00:19:59,480
都是三乘三相对来说比较好

508
00:19:59,480 --> 00:20:01,919
好够构造比较深的模型

509
00:20:02,200 --> 00:20:05,480
如果你用大一点小一点

510
00:20:05,640 --> 00:20:08,319
就是说AlexNet其实是这么做的了

511
00:20:08,319 --> 00:20:10,720
Nernet我们之后要讲的也是这样子的

512
00:20:11,400 --> 00:20:13,960
但是这个东西就是说你

513
00:20:14,799 --> 00:20:16,319
写起来不那么方便

514
00:20:16,319 --> 00:20:18,039
就是构造起来不那么方便

515
00:20:18,079 --> 00:20:19,319
就最后的最后

516
00:20:19,360 --> 00:20:22,839
哪样子的审计网络会被人记住

517
00:20:22,839 --> 00:20:24,640
简单的会被人记住

518
00:20:26,000 --> 00:20:28,240
就算我简单一点效果差一点

519
00:20:28,279 --> 00:20:29,079
但是我简单

520
00:20:29,079 --> 00:20:29,799
那就很容易用

521
00:20:29,799 --> 00:20:31,960
拿过来很容易改

522
00:20:32,079 --> 00:20:33,759
简单的会被通用

523
00:20:33,920 --> 00:20:35,559
你做的特别fancy的东西

524
00:20:35,559 --> 00:20:38,119
也许是你可以在你的数据上

525
00:20:38,119 --> 00:20:39,279
做的比较好的效果

526
00:20:39,319 --> 00:20:41,240
但是反过来讲

527
00:20:41,240 --> 00:20:42,960
很有可能大家觉得太复杂了

528
00:20:42,960 --> 00:20:44,119
搞不清你是怎么想的

529
00:20:45,839 --> 00:20:49,119
所以我觉得我你可以去试

530
00:20:49,160 --> 00:20:51,559
但是如果你觉得效果不那么明显的话

531
00:20:51,559 --> 00:20:52,960
那就干脆用同样宽度的

532
00:20:52,960 --> 00:20:54,200
和比较简单一点

533
00:20:58,920 --> 00:21:00,759
多层卷迹算出来

534
00:21:00,759 --> 00:21:04,559
是不是可以理解成图像的

535
00:21:04,559 --> 00:21:06,759
多种不同的纹理的特征

536
00:21:07,119 --> 00:21:08,799
我们之后可以解释一下

537
00:21:08,799 --> 00:21:09,879
大概是什么样子

538
00:21:09,920 --> 00:21:12,720
我们等会会解释应该是

539
00:21:12,960 --> 00:21:15,839
其实你可认为一个卷迹

540
00:21:15,839 --> 00:21:17,279
确实是去看一些纹理的

541
00:21:17,279 --> 00:21:18,879
一个特定的卷迹层

542
00:21:18,920 --> 00:21:21,160
可以去匹配一种特定的纹理

543
00:21:21,920 --> 00:21:22,960
你不同的卷迹层

544
00:21:22,960 --> 00:21:24,240
你可以认为可以理解

545
00:21:24,240 --> 00:21:25,319
有不同的纹理特征

546
00:21:25,319 --> 00:21:27,839
我们之后会讲一下这个情况

547
00:21:28,839 --> 00:21:29,920
问题19

548
00:21:30,960 --> 00:21:33,759
现在要做深度学习方向的任务的话

549
00:21:33,759 --> 00:21:35,400
感觉是有钱人的游戏

550
00:21:35,440 --> 00:21:36,799
有什么方向不太需要

551
00:21:36,799 --> 00:21:38,440
算力的深度学习的任务吗

552
00:21:42,279 --> 00:21:45,200
首先深度学习不是有钱人的游戏

553
00:21:46,640 --> 00:21:48,720
深度学习其实还是

554
00:21:48,759 --> 00:21:49,480
就是说

555
00:21:50,200 --> 00:21:51,000
你这么想

556
00:21:51,440 --> 00:21:52,119
一个东西

557
00:21:52,119 --> 00:21:53,720
如果是有钱人的游戏的话

558
00:21:53,720 --> 00:21:55,079
它不会被流行的

559
00:21:55,559 --> 00:21:56,960
说深度学习很流行

560
00:21:57,000 --> 00:21:58,039
所以意味着深度学习

561
00:21:58,039 --> 00:21:59,200
不是有钱人的游戏

562
00:21:59,440 --> 00:22:00,120
所以这个就是说

563
00:22:00,120 --> 00:22:02,519
你可以这么大致这么想

564
00:22:02,559 --> 00:22:03,360
为什么

565
00:22:04,000 --> 00:22:06,519
是因为没有深度学习的时候

566
00:22:06,559 --> 00:22:09,720
你得用人去做特征的提取

567
00:22:09,759 --> 00:22:11,480
去调很多东西

568
00:22:11,519 --> 00:22:14,200
你虽然机器成本可能会低一点

569
00:22:14,240 --> 00:22:15,720
但是效果会差一点

570
00:22:15,920 --> 00:22:17,200
那么你就效果差

571
00:22:17,200 --> 00:22:18,000
你要怎么办

572
00:22:18,200 --> 00:22:19,880
我需要去更多的调餐

573
00:22:19,880 --> 00:22:20,440
这是第一点

574
00:22:20,480 --> 00:22:21,720
这是要人力成本的

575
00:22:21,759 --> 00:22:22,279
第二

576
00:22:22,279 --> 00:22:23,720
我需要更多的数据

577
00:22:24,279 --> 00:22:25,840
数据是很贵的一件事情

578
00:22:26,480 --> 00:22:27,880
数据是非常贵的

579
00:22:28,600 --> 00:22:31,400
所以现在深度学习

580
00:22:31,440 --> 00:22:33,400
让机器也是GPU

581
00:22:34,520 --> 00:22:36,560
算力来剔掉到

582
00:22:36,840 --> 00:22:39,040
剔掉了很多数据的成本

583
00:22:39,040 --> 00:22:39,880
人力的成本

584
00:22:39,920 --> 00:22:41,280
其实它是赚的

585
00:22:41,960 --> 00:22:43,360
所以深度学习流行

586
00:22:43,360 --> 00:22:46,200
是因为它确实是成本的下降

587
00:22:46,200 --> 00:22:47,480
就达到同样的

588
00:22:47,480 --> 00:22:49,120
最后的精度的话

589
00:22:49,120 --> 00:22:50,640
深度学习对数据的要求

590
00:22:50,640 --> 00:22:52,160
和对人的要求会变低

591
00:22:52,680 --> 00:22:53,760
但它的代价是说

592
00:22:53,760 --> 00:22:54,960
计算要变高

593
00:22:55,280 --> 00:22:57,320
所以你可认为是工业革命

594
00:22:57,319 --> 00:22:59,119
核心就是用机器来替代人

595
00:23:00,399 --> 00:23:01,119
所以工业革命

596
00:23:01,119 --> 00:23:02,879
从最早蒸汽机发展到现在

597
00:23:03,000 --> 00:23:04,039
深度学习是一样的

598
00:23:04,079 --> 00:23:05,879
用算力来替代人的事情

599
00:23:06,039 --> 00:23:07,679
是奥特曼也是干这个事情

600
00:23:07,879 --> 00:23:08,759
NAS

601
00:23:09,039 --> 00:23:10,319
我们之前说的NAS

602
00:23:10,319 --> 00:23:12,480
就是网络结构的搜索

603
00:23:12,480 --> 00:23:13,960
也是干这个事情

604
00:23:14,039 --> 00:23:15,799
但是我说NAS

605
00:23:15,839 --> 00:23:17,639
现在是有钱人的游戏

606
00:23:17,759 --> 00:23:19,519
深度学习我倒觉得还好了

607
00:23:20,079 --> 00:23:21,079
NAS是有钱人

608
00:23:21,200 --> 00:23:22,240
现在我建议

609
00:23:22,240 --> 00:23:23,159
除非你特别有钱

610
00:23:23,159 --> 00:23:24,119
你要做研究

611
00:23:24,119 --> 00:23:24,879
你可以去用

612
00:23:24,919 --> 00:23:25,919
但是你可能

613
00:23:25,920 --> 00:23:27,360
如果你不做的话

614
00:23:27,360 --> 00:23:29,000
你可以稍微等一段

615
00:23:29,000 --> 00:23:29,840
再等个一两年

616
00:23:29,840 --> 00:23:30,640
或者三四年

617
00:23:30,640 --> 00:23:31,680
可能就很便宜了

618
00:23:32,560 --> 00:23:33,080
OK

619
00:23:33,160 --> 00:23:34,519
所以任何事情是说

620
00:23:34,519 --> 00:23:35,560
都不是一个相对的

621
00:23:35,920 --> 00:23:36,840
任何一个科技

622
00:23:36,840 --> 00:23:37,960
都是一个很贵的

623
00:23:37,960 --> 00:23:39,720
科技慢慢变得很小

624
00:23:39,880 --> 00:23:41,320
当年磁盘那么大

625
00:23:41,440 --> 00:23:42,800
计算机那么大

626
00:23:42,800 --> 00:23:43,840
一个计算架那么小

627
00:23:43,840 --> 00:23:44,480
现在那么小

628
00:23:44,480 --> 00:23:44,960
对吧

629
00:23:45,039 --> 00:23:47,120
所以任何的科学的进步

630
00:23:47,120 --> 00:23:48,519
都是一个很贵的东西

631
00:23:48,519 --> 00:23:49,759
慢慢做便宜

632
00:23:49,840 --> 00:23:52,080
我只是说现在NAS很贵

633
00:23:52,080 --> 00:23:53,480
还不那么通用

634
00:23:53,800 --> 00:23:54,680
深度学习

635
00:23:54,680 --> 00:23:56,279
我觉得已经很便宜了

