1
00:00:00,000 --> 00:00:03,240
我们先来讲批量规划

2
00:00:03,240 --> 00:00:04,679
就跟之前说到的

3
00:00:04,679 --> 00:00:08,080
现在几乎所有的主流的

4
00:00:08,080 --> 00:00:10,200
卷迹神级网络

5
00:00:10,200 --> 00:00:13,839
都是或多或少的用了批量规划这个层

6
00:00:13,839 --> 00:00:17,519
虽然我们之前看到的那些层

7
00:00:17,519 --> 00:00:19,440
比如不管是pooling也好

8
00:00:19,440 --> 00:00:21,160
也是convolution也好

9
00:00:21,160 --> 00:00:24,039
其实在80年代早就出现过了

10
00:00:24,039 --> 00:00:26,440
是现在我们把它做到更深更大

11
00:00:26,440 --> 00:00:28,400
但是批量规划这个层

12
00:00:28,560 --> 00:00:30,160
虽然这个思想倒是不行

13
00:00:30,160 --> 00:00:34,160
但是特定的层确实是最近几年才出来的

14
00:00:34,160 --> 00:00:37,120
应该是可能是16年还是左右

15
00:00:37,120 --> 00:00:39,240
这是新出来的

16
00:00:39,240 --> 00:00:41,520
然后出来之后大家都发现效果很好

17
00:00:41,520 --> 00:00:44,000
特别是你要做很深的神级网络的时候

18
00:00:44,000 --> 00:00:46,200
这个是不能避免的一个层

19
00:00:46,200 --> 00:00:47,560
OK

20
00:00:47,560 --> 00:00:48,840
所以今天我们来讲一下

21
00:00:48,840 --> 00:00:50,680
这一个层是长什么样子

22
00:00:52,719 --> 00:00:54,480
首先我们来看一下一个问题

23
00:00:54,480 --> 00:00:58,359
就是说当你的神级网络特别深的时候

24
00:00:58,559 --> 00:01:00,920
你发现你的数据在这个地方

25
00:01:00,920 --> 00:01:01,879
就在下面

26
00:01:01,879 --> 00:01:03,839
你的损失函数在上面

27
00:01:05,079 --> 00:01:06,640
然后你会有什么问题呢

28
00:01:06,640 --> 00:01:09,799
就是我们在自动T2球岛就讲过

29
00:01:09,799 --> 00:01:11,959
你正向的时候

30
00:01:11,959 --> 00:01:14,640
你是从下面数据一直往上走

31
00:01:14,640 --> 00:01:15,840
叫forward函数

32
00:01:16,319 --> 00:01:18,039
但是你算backward的时候

33
00:01:18,039 --> 00:01:20,480
其实你是从上面开始往下

34
00:01:21,560 --> 00:01:23,200
上面开始往下传的时候

35
00:01:23,200 --> 00:01:24,039
问题是什么

36
00:01:24,039 --> 00:01:27,239
问题是说你的T2在上面的时候会比较大

37
00:01:28,159 --> 00:01:30,719
越到下面越容易变小

38
00:01:30,719 --> 00:01:32,919
因为你其实在很多时候

39
00:01:32,919 --> 00:01:35,239
你是n个很小的数相乘

40
00:01:35,239 --> 00:01:36,439
一直乘到最后

41
00:01:36,439 --> 00:01:37,799
你的T2就变得比较小了

42
00:01:37,799 --> 00:01:38,079
对吧

43
00:01:39,119 --> 00:01:40,560
但T2包罩是另外一回事

44
00:01:40,560 --> 00:01:41,799
但是正常情况下

45
00:01:41,799 --> 00:01:43,799
你的T2就是都是比较小的

46
00:01:43,799 --> 00:01:47,919
就会乘到最下的越靠近数据的话

47
00:01:47,919 --> 00:01:49,479
那些层的T2就会变少

48
00:01:51,319 --> 00:01:53,519
那么他的问题是说

49
00:01:53,519 --> 00:01:56,199
你上面的层的T2比较大

50
00:01:56,200 --> 00:01:57,560
那么每次更新的时候

51
00:01:57,560 --> 00:01:59,240
上面T2就会不断去更新

52
00:02:00,240 --> 00:02:02,480
然后你因为你的学习率是一个

53
00:02:02,480 --> 00:02:05,480
大家都是最简单情况就是一个值

54
00:02:06,400 --> 00:02:08,080
所以但是你下面层呢

55
00:02:08,080 --> 00:02:09,319
因为你T2比较小

56
00:02:09,319 --> 00:02:11,199
对你的权重的更新就比较少

57
00:02:12,640 --> 00:02:13,680
那么你的问题就是说

58
00:02:13,680 --> 00:02:16,400
你的上面的东西会很快的

59
00:02:16,400 --> 00:02:17,319
可能会收敛

60
00:02:17,319 --> 00:02:19,159
下面东西会比较慢

61
00:02:19,159 --> 00:02:21,080
那么导致一个问题是说

62
00:02:21,080 --> 00:02:23,640
每一次你更新下面的东西

63
00:02:23,640 --> 00:02:25,599
下面那些靠近数据的东西

64
00:02:25,639 --> 00:02:28,479
我们知道这些东西其实去

65
00:02:28,479 --> 00:02:30,359
他会去尝试去抽取那些

66
00:02:30,359 --> 00:02:33,919
比较底层的那一些特征

67
00:02:33,919 --> 00:02:36,919
比如说那些局部的一些边缘

68
00:02:36,919 --> 00:02:40,079
一些很简单的纹理信息

69
00:02:40,079 --> 00:02:43,599
上面就是一些高层语语的信息

70
00:02:43,599 --> 00:02:45,240
但是你的问题是说

71
00:02:45,240 --> 00:02:46,519
你上面收敛比较快

72
00:02:46,519 --> 00:02:47,759
一会就劝得比较好了

73
00:02:47,759 --> 00:02:49,719
但是你下面变化就比较慢

74
00:02:49,719 --> 00:02:51,479
但是每一次你下面一变

75
00:02:52,439 --> 00:02:54,199
那么上面得重新开始训练

76
00:02:55,200 --> 00:02:55,760
对吧

77
00:02:55,760 --> 00:02:58,760
因为底层的信息都变了

78
00:02:58,760 --> 00:03:02,480
那么你上面那些权重就是白学了

79
00:03:02,480 --> 00:03:05,280
所以这里会导致一个问题

80
00:03:05,280 --> 00:03:06,240
这个问题是说

81
00:03:06,240 --> 00:03:09,480
你在训练持续的进行的时候

82
00:03:09,480 --> 00:03:11,760
我顶部的变化比较快

83
00:03:11,760 --> 00:03:12,880
底部的变化比较慢

84
00:03:12,880 --> 00:03:15,160
在底部在不断的往前变的时候

85
00:03:15,160 --> 00:03:17,520
你的顶部会不断的去重新训练

86
00:03:17,520 --> 00:03:19,920
重新去理和底部变化导致的问题

87
00:03:21,800 --> 00:03:23,960
所以这个问题就会导致你的收敛

88
00:03:23,960 --> 00:03:24,680
会比较慢

89
00:03:26,040 --> 00:03:28,080
所以那么我们的一个问题就是说

90
00:03:28,080 --> 00:03:31,600
我们能不能在学习底部的时候

91
00:03:31,600 --> 00:03:33,600
就改变底部那些特征的时候

92
00:03:33,600 --> 00:03:36,719
能够避免顶部会不断的重新训练

93
00:03:37,080 --> 00:03:40,520
这是批量规划所要考虑的一个问题

94
00:03:42,439 --> 00:03:45,640
就他的一个核心的想法是挺简单的

95
00:03:46,240 --> 00:03:47,080
就是说

96
00:03:47,480 --> 00:03:48,840
他想法是说

97
00:03:48,879 --> 00:03:50,920
我们回忆一下

98
00:03:50,920 --> 00:03:52,960
我们之前在讲

99
00:03:53,480 --> 00:03:54,200
损失

100
00:03:54,360 --> 00:03:56,680
讲数值稳定性的时候讲过说

101
00:03:56,840 --> 00:03:57,920
你为什么会变

102
00:03:58,040 --> 00:04:02,920
是因为你方差和均值整个分布

103
00:04:02,920 --> 00:04:06,680
会在不同层之前会变化

104
00:04:07,880 --> 00:04:09,560
那么一个简单办法是说

105
00:04:09,600 --> 00:04:12,120
我假设把分布给你固定住了

106
00:04:12,480 --> 00:04:14,840
就不管每一层你的输出也好

107
00:04:14,840 --> 00:04:16,120
你的梯度也好

108
00:04:16,160 --> 00:04:18,400
假设我都符合某一个分布

109
00:04:18,480 --> 00:04:19,600
那么相对而说

110
00:04:19,600 --> 00:04:20,639
它就是比较稳定的

111
00:04:20,639 --> 00:04:22,039
当然你分布的具体的东西

112
00:04:22,039 --> 00:04:22,959
你可以细调

113
00:04:22,959 --> 00:04:23,839
但整体来看

114
00:04:24,039 --> 00:04:25,360
整个分布长这样子

115
00:04:25,439 --> 00:04:29,039
就不会带来一个特别大的转变的话

116
00:04:29,079 --> 00:04:32,120
那么你在学习中间细微的变动

117
00:04:32,120 --> 00:04:33,000
时候就比较容易

118
00:04:33,959 --> 00:04:35,120
所以批量规划

119
00:04:35,120 --> 00:04:37,719
其实也是一个很简单的思想

120
00:04:37,919 --> 00:04:39,159
就是说他说

121
00:04:39,159 --> 00:04:42,199
我尝试去把一个小批量

122
00:04:42,240 --> 00:04:43,759
就是一个mini batch里面

123
00:04:43,919 --> 00:04:46,399
在不同层的不同地方的

124
00:04:46,399 --> 00:04:48,479
mini batch那些数据进去

125
00:04:48,480 --> 00:04:49,439
它的输出

126
00:04:49,480 --> 00:04:51,800
它的均值和方差给你固定住

127
00:04:53,439 --> 00:04:54,800
就说具体来说

128
00:04:54,920 --> 00:04:56,720
我们先讲一个最简单的情况

129
00:04:56,720 --> 00:04:57,400
是说

130
00:05:00,560 --> 00:05:02,800
就说这个地方是

131
00:05:04,280 --> 00:05:06,520
大b就是我的小批量

132
00:05:06,520 --> 00:05:08,640
这些所谓的下边的索引

133
00:05:08,879 --> 00:05:10,600
就是说我对所有的样本

134
00:05:10,800 --> 00:05:12,800
就是对小批量里面样本

135
00:05:12,840 --> 00:05:13,920
然后求和

136
00:05:13,960 --> 00:05:15,319
处于你的批量大小

137
00:05:15,319 --> 00:05:16,680
当然就得到均值了

138
00:05:17,480 --> 00:05:19,680
然后我再做一个简单的假设

139
00:05:19,680 --> 00:05:21,280
就是说我的x就是一条向量

140
00:05:21,439 --> 00:05:23,360
我们等会拓展到我的卷迹

141
00:05:23,360 --> 00:05:24,400
甚至网络情况

142
00:05:25,280 --> 00:05:26,840
然后我的方差

143
00:05:27,000 --> 00:05:30,360
就是我的sigma平方

144
00:05:30,360 --> 00:05:32,960
那就等于它的减去均值平方

145
00:05:33,160 --> 00:05:35,480
加上一个很小的数

146
00:05:35,480 --> 00:05:36,240
就是

147
00:05:37,720 --> 00:05:39,040
就是防止你会变成0

148
00:05:40,639 --> 00:05:43,319
那么然后我再就批量规划

149
00:05:43,319 --> 00:05:43,920
干的事情

150
00:05:43,920 --> 00:05:45,560
其实就是很简单的

151
00:05:45,959 --> 00:05:47,480
就是给定一个

152
00:05:48,839 --> 00:05:50,360
x是我的输入的话

153
00:05:50,360 --> 00:05:52,600
那么我这个批量规划的输出

154
00:05:52,600 --> 00:05:53,439
就是说

155
00:05:53,480 --> 00:05:55,360
对里面的每一个样本

156
00:05:55,399 --> 00:05:56,519
减去均值

157
00:05:56,600 --> 00:05:57,720
除以方差

158
00:05:57,959 --> 00:06:01,519
然后再乘以一个

159
00:06:01,600 --> 00:06:02,560
伽玛

160
00:06:03,000 --> 00:06:05,360
再加上一个b

161
00:06:06,439 --> 00:06:08,120
就是说这个地方

162
00:06:08,759 --> 00:06:10,920
mu b和sigma b

163
00:06:10,920 --> 00:06:12,279
就是根据我的数据来的

164
00:06:12,279 --> 00:06:14,240
就根据我当前数据算出来的

165
00:06:14,759 --> 00:06:18,120
那么伽玛和b是你给

166
00:06:18,120 --> 00:06:20,360
就是说这是一个可以学习的参数

167
00:06:21,600 --> 00:06:23,560
这个是我批量规划的

168
00:06:23,560 --> 00:06:25,639
尝试去把它学出来的

169
00:06:25,800 --> 00:06:27,680
那么它的作用你可以认为是说

170
00:06:27,720 --> 00:06:28,879
假设分布

171
00:06:29,079 --> 00:06:31,160
就是说你把它变成均值

172
00:06:31,519 --> 00:06:33,280
为0方差为1分布

173
00:06:33,280 --> 00:06:35,079
可能不是那么的适合的话

174
00:06:35,120 --> 00:06:38,759
那么我可以通过去学习一个新的均值

175
00:06:38,920 --> 00:06:40,319
一个新的方差

176
00:06:40,319 --> 00:06:42,240
来使得你学习更加

177
00:06:42,759 --> 00:06:44,840
使得你的值

178
00:06:45,160 --> 00:06:46,639
对我的神经网络更好一点

179
00:06:47,199 --> 00:06:48,639
但是我会限定住你

180
00:06:48,639 --> 00:06:50,519
这个b和伽玛的变化

181
00:06:50,519 --> 00:06:52,400
不要变化的过于猛烈

182
00:06:53,560 --> 00:06:53,840
OK

183
00:06:53,840 --> 00:06:55,000
这就是其实

184
00:06:55,519 --> 00:06:57,160
这个就是最简单的

185
00:06:57,160 --> 00:07:00,280
批量规划的一个思想

186
00:07:00,800 --> 00:07:04,199
等会我们细节那些具体怎么实现

187
00:07:04,199 --> 00:07:05,879
我们一会再来看

188
00:07:06,000 --> 00:07:06,840
我们先来看一下

189
00:07:07,079 --> 00:07:09,680
批量规划给你带来的是什么样的东西

190
00:07:10,680 --> 00:07:11,720
首先说

191
00:07:11,759 --> 00:07:13,400
那么这里面可以学的东西

192
00:07:13,400 --> 00:07:15,240
是伽玛和b

193
00:07:16,480 --> 00:07:17,840
具体它是一个什么东西

194
00:07:17,840 --> 00:07:18,720
我们等会再讲

195
00:07:18,879 --> 00:07:21,040
其实是说可以认为是说

196
00:07:21,079 --> 00:07:23,160
对每一个特征或者每一列

197
00:07:23,360 --> 00:07:24,759
如果是全连接的话

198
00:07:24,800 --> 00:07:26,759
那么就有一个对应的伽玛

199
00:07:26,759 --> 00:07:28,000
一个b

200
00:07:29,079 --> 00:07:31,560
那么如果它是作用在

201
00:07:32,960 --> 00:07:35,720
全连接或者卷积层的输出上时

202
00:07:36,120 --> 00:07:38,800
它会作用在激活怀疏前面

203
00:07:40,680 --> 00:07:43,040
就是说你一个卷积层

204
00:07:43,040 --> 00:07:44,360
或者一个全连接层

205
00:07:44,600 --> 00:07:45,920
在输出之后

206
00:07:45,920 --> 00:07:48,120
直接贴一个批量过于化成

207
00:07:49,560 --> 00:07:50,480
这个层的作用

208
00:07:50,480 --> 00:07:53,120
就是把你的输出里面的均值方差均值

209
00:07:53,120 --> 00:07:54,319
一减方差一除

210
00:07:54,360 --> 00:07:56,280
然后再加上一个可以学习的

211
00:07:56,280 --> 00:07:57,480
一个伽玛和b

212
00:07:58,280 --> 00:08:00,000
然后再做激活怀疏

213
00:08:03,800 --> 00:08:06,040
就理解你为什么放在激活之前

214
00:08:06,199 --> 00:08:08,360
如果你是放在relu之后的话

215
00:08:08,360 --> 00:08:10,400
而relu把你所有东西都变成正数

216
00:08:10,439 --> 00:08:12,480
那么你又把它全重新拉回去

217
00:08:12,480 --> 00:08:13,600
这挺奇怪的

218
00:08:13,600 --> 00:08:13,960
对吧

219
00:08:14,080 --> 00:08:15,400
所以它就放在之前

220
00:08:15,439 --> 00:08:17,439
所以它可以认为它是一个线性变换

221
00:08:17,800 --> 00:08:19,879
所以激活怀疏加入非线性

222
00:08:20,040 --> 00:08:22,240
所以批量过于化是一个线性变化

223
00:08:22,439 --> 00:08:24,720
所以它就是把你的均值方差

224
00:08:24,720 --> 00:08:25,600
给拉的比较好

225
00:08:25,600 --> 00:08:26,960
让你变化不那么剧烈

226
00:08:27,879 --> 00:08:28,680
但反过来讲

227
00:08:28,680 --> 00:08:30,800
你也可以作用在全连接层

228
00:08:30,800 --> 00:08:32,879
或者卷积层的输入的上面

229
00:08:33,120 --> 00:08:35,039
就是说对输入做一个线性变化

230
00:08:35,560 --> 00:08:38,159
然后使你的输入的方差均值比较好

231
00:08:39,360 --> 00:08:45,159
那么在作用到全连接层和在卷积层的时候

232
00:08:45,159 --> 00:08:47,440
它作用的维度是不一样的

233
00:08:48,560 --> 00:08:50,159
对全连接层

234
00:08:50,360 --> 00:08:54,000
就是说它作用在是你的特征维上面

235
00:08:55,159 --> 00:08:57,639
就你的输入就是一个

236
00:08:57,680 --> 00:08:58,960
你是一个二维的输入

237
00:08:59,519 --> 00:09:01,240
那么每一行就是你的样本

238
00:09:01,480 --> 00:09:02,920
每一列是你的特征

239
00:09:03,680 --> 00:09:05,159
对全连接层的话

240
00:09:05,200 --> 00:09:06,879
它是对每一个特征

241
00:09:07,439 --> 00:09:09,759
计算一个标量的均值

242
00:09:09,759 --> 00:09:10,840
标量的方差

243
00:09:11,080 --> 00:09:15,639
然后把它特征变成均值为0方差为1

244
00:09:15,840 --> 00:09:17,279
大家在做预处理的时候

245
00:09:17,279 --> 00:09:18,879
数据处理经常做这个事情

246
00:09:19,559 --> 00:09:20,720
所以这里不一样的是说

247
00:09:20,720 --> 00:09:23,279
我对每一个全连接的输出或者输入

248
00:09:23,320 --> 00:09:24,279
都做这个事情

249
00:09:24,320 --> 00:09:26,159
而不是只作用在数据上

250
00:09:26,919 --> 00:09:27,759
另外的话

251
00:09:27,799 --> 00:09:30,879
他也会去重新用自己学到的一个

252
00:09:30,879 --> 00:09:32,600
伽玛尔贝塔重新作用一下

253
00:09:32,600 --> 00:09:34,360
把你的均值和方差再做一次

254
00:09:34,360 --> 00:09:35,960
校验

255
00:09:37,200 --> 00:09:38,799
那么对卷积层的话

256
00:09:38,799 --> 00:09:40,879
它是作用在通道位上面

257
00:09:41,519 --> 00:09:42,600
就怎么理解呢

258
00:09:42,919 --> 00:09:45,240
我们其实思想看过很多次

259
00:09:45,240 --> 00:09:47,600
就是说特别是在一乘一的卷积上面

260
00:09:47,600 --> 00:09:49,439
我们说它等价一个全连接层

261
00:09:49,639 --> 00:09:51,320
我们说它等价的意思

262
00:09:51,360 --> 00:09:52,480
其实是说

263
00:09:54,039 --> 00:09:55,360
对每一个像素

264
00:09:56,399 --> 00:09:57,879
它不是有多通道吗

265
00:09:58,039 --> 00:09:58,759
对每一个像素

266
00:09:58,759 --> 00:10:00,480
你的通道数是100的话

267
00:10:00,519 --> 00:10:02,519
那么像素它其实是有一个

268
00:10:02,519 --> 00:10:05,519
长100位的一个像量

269
00:10:06,079 --> 00:10:07,720
那么你可认为这个像量

270
00:10:07,720 --> 00:10:10,120
是像素的一个特征

271
00:10:10,919 --> 00:10:12,319
那么你可以认为是说

272
00:10:12,319 --> 00:10:15,279
对于一个输入一个有高宽来说

273
00:10:15,279 --> 00:10:16,639
里面的每一个像素

274
00:10:16,639 --> 00:10:17,799
它就是一个样本

275
00:10:19,199 --> 00:10:20,759
所以对于一个卷积层来讲

276
00:10:20,759 --> 00:10:23,039
假设你的输入是批量大小乘以高

277
00:10:23,039 --> 00:10:23,639
乘以宽

278
00:10:23,639 --> 00:10:25,199
再乘以通道数的话

279
00:10:25,439 --> 00:10:29,639
那么你的样本数就是你的批量大小

280
00:10:29,639 --> 00:10:31,519
乘以你的高和乘以宽

281
00:10:31,559 --> 00:10:32,559
就整个批量里面

282
00:10:32,559 --> 00:10:34,679
所有的像素都是一个样本

283
00:10:34,800 --> 00:10:36,840
那么它对应的通道

284
00:10:36,840 --> 00:10:39,600
它就是你的特征

285
00:10:39,880 --> 00:10:41,120
所以一乘一的卷积

286
00:10:41,120 --> 00:10:42,760
就是说这么把它拉成一个

287
00:10:42,760 --> 00:10:44,120
这样子的二维矩阵之后

288
00:10:44,120 --> 00:10:45,080
再做一个全连接

289
00:10:45,080 --> 00:10:46,040
就是一乘一卷积

290
00:10:46,520 --> 00:10:47,920
在这里的话一样的

291
00:10:48,040 --> 00:10:50,040
就是说它把你所有的像素

292
00:10:50,040 --> 00:10:51,280
当做是样本

293
00:10:51,400 --> 00:10:53,880
所以我来计算它的均值和方差

294
00:10:54,120 --> 00:10:56,480
那就是指作用在通道层

295
00:10:56,480 --> 00:10:58,520
通道层就是当做是卷积层的

296
00:10:58,520 --> 00:10:59,360
一个特征位

297
00:10:59,680 --> 00:11:01,280
所以这个是批量归化层

298
00:11:01,280 --> 00:11:02,800
作不同的作用

299
00:11:03,800 --> 00:11:04,360
好

300
00:11:04,360 --> 00:11:05,960
这个是批量归化怎么用

301
00:11:06,160 --> 00:11:09,040
我们待会会仔细讲代码式

302
00:11:09,040 --> 00:11:11,400
现在可以看到是具体的怎么用了

303
00:11:11,840 --> 00:11:13,480
那么接下来讲一下说

304
00:11:14,640 --> 00:11:16,280
具体它在干什么事情

305
00:11:17,360 --> 00:11:19,160
就是说批量归化也是一个

306
00:11:19,160 --> 00:11:21,920
我们之前吐槽过的一个经典的案例

307
00:11:21,920 --> 00:11:24,560
就是说这个论文提出来的时候

308
00:11:24,560 --> 00:11:26,640
作者说我这个东西

309
00:11:26,800 --> 00:11:30,280
是用来减少内部斜变量的转移

310
00:11:31,279 --> 00:11:33,959
就这个是一个在基因学系里面

311
00:11:33,959 --> 00:11:35,120
一个经常被讨论的

312
00:11:35,120 --> 00:11:35,720
就是说

313
00:11:36,480 --> 00:11:37,679
比如说你的

314
00:11:37,720 --> 00:11:40,879
我用今天的

315
00:11:41,159 --> 00:11:42,120
当前的

316
00:11:42,159 --> 00:11:42,959
比如说一些数据

317
00:11:42,959 --> 00:11:44,679
是你和明天的话

318
00:11:44,679 --> 00:11:46,639
明天可能整个分布都变了

319
00:11:46,639 --> 00:11:47,480
明天可能就是说

320
00:11:47,480 --> 00:11:49,600
疫情前的世界和疫情后的世界

321
00:11:49,639 --> 00:11:50,799
你是很不一样的

322
00:11:50,839 --> 00:11:52,600
你用疫情前的世界去

323
00:11:52,639 --> 00:11:54,519
你去年一个数据

324
00:11:54,519 --> 00:11:55,759
然后作用在疫情后

325
00:11:55,759 --> 00:11:57,360
现在的我们还在疫情中

326
00:11:57,720 --> 00:11:59,000
国内当然就好一些了

327
00:11:59,799 --> 00:12:01,960
所以你现在什么东西都变了

328
00:12:01,960 --> 00:12:04,080
那么就是你的整个分布都变了

329
00:12:04,120 --> 00:12:07,039
你有covariance shift

330
00:12:07,720 --> 00:12:10,120
那么你这个模型肯定会有问题

331
00:12:10,559 --> 00:12:12,639
同样是说批量归化

332
00:12:12,639 --> 00:12:14,519
觉得是说它能够减少

333
00:12:14,519 --> 00:12:16,480
你内部那些层

334
00:12:16,679 --> 00:12:17,799
不同的层去作用的时候

335
00:12:17,799 --> 00:12:19,600
导致你整个数据

336
00:12:19,600 --> 00:12:21,639
会做一些covariance shift

337
00:12:22,720 --> 00:12:23,799
所以就是说

338
00:12:24,559 --> 00:12:27,519
他觉得我是用来减少这一个东西

339
00:12:28,399 --> 00:12:29,600
但后续有论文说

340
00:12:29,600 --> 00:12:32,240
其实你会发现他并没有

341
00:12:32,279 --> 00:12:34,399
你去实际去看他

342
00:12:34,919 --> 00:12:36,879
你可以计算内部的血变量

343
00:12:38,960 --> 00:12:39,639
你会发现

344
00:12:39,639 --> 00:12:42,399
其实实际上来说并没有发生变化

345
00:12:42,919 --> 00:12:44,559
就是说论文也是说

346
00:12:44,679 --> 00:12:46,799
我觉得我首先加了层数

347
00:12:46,799 --> 00:12:47,639
效果特别好

348
00:12:48,799 --> 00:12:49,439
接下来说

349
00:12:49,439 --> 00:12:50,679
我觉得是减少了

350
00:12:50,679 --> 00:12:51,559
他并没有说

351
00:12:51,720 --> 00:12:53,079
我特别真的去看了

352
00:12:53,079 --> 00:12:54,000
他是减少什么东西

353
00:12:54,000 --> 00:12:55,199
我猜是这么干的

354
00:12:55,240 --> 00:12:56,559
后来有人真的去看

355
00:12:56,560 --> 00:12:58,560
因为他实在是太work了

356
00:12:58,560 --> 00:12:59,760
就说效果太好

357
00:12:59,760 --> 00:13:01,560
那就去研究发生什么

358
00:13:01,560 --> 00:13:02,040
会是

359
00:13:02,720 --> 00:13:05,960
他发现其实没有减少内部血变量转移

360
00:13:06,520 --> 00:13:08,160
他后来会觉得说

361
00:13:09,000 --> 00:13:09,960
大家会觉得说

362
00:13:10,200 --> 00:13:11,360
可能这个东西

363
00:13:11,440 --> 00:13:12,840
其实本质上是干嘛

364
00:13:13,040 --> 00:13:14,160
本质上是说

365
00:13:14,160 --> 00:13:15,720
在每一个批量里面

366
00:13:15,720 --> 00:13:18,520
加入了噪音来控制模型复杂度

367
00:13:20,520 --> 00:13:21,600
噪音是什么东西

368
00:13:22,000 --> 00:13:23,560
这个噪音就是说

369
00:13:23,720 --> 00:13:26,320
你这个μB和ΣB

370
00:13:26,400 --> 00:13:27,480
它是一个噪音

371
00:13:28,720 --> 00:13:31,560
为什么是因为你这个东西

372
00:13:31,560 --> 00:13:34,879
其实是在每一个随机的小批量上

373
00:13:34,879 --> 00:13:35,680
计算而来

374
00:13:36,120 --> 00:13:37,720
就是一个随机的均值

375
00:13:37,720 --> 00:13:38,680
一个随机的片

376
00:13:38,840 --> 00:13:40,600
一个随机的方差

377
00:13:40,760 --> 00:13:42,920
因为你每次随机取样一些样本

378
00:13:43,120 --> 00:13:47,040
所以均值是当前你的样本的均值和方差

379
00:13:47,040 --> 00:13:48,680
所以那个东西噪音是比较大的

380
00:13:49,280 --> 00:13:50,960
然后你把它一减一除

381
00:13:50,960 --> 00:13:54,200
就是说你做了一些随机的一些均值的

382
00:13:54,200 --> 00:13:55,960
一些转移方差的变化

383
00:13:56,400 --> 00:13:59,960
然后当然你会在上面还有一个γβ

384
00:13:59,960 --> 00:14:02,840
它是一个可以认的东西

385
00:14:02,840 --> 00:14:04,400
就是说它是可以学的

386
00:14:04,400 --> 00:14:05,600
学的东西的话

387
00:14:05,600 --> 00:14:06,879
你每次减一个

388
00:14:07,040 --> 00:14:09,600
每次你也知道是减一个learning rate

389
00:14:09,600 --> 00:14:10,960
所以它的变化不会很剧烈

390
00:14:10,960 --> 00:14:12,920
它就可能是一个慢慢的变化的一个过程

391
00:14:13,000 --> 00:14:14,160
取决于你的学习率

392
00:14:14,920 --> 00:14:15,879
所以就是说

393
00:14:15,960 --> 00:14:18,879
它首先加了一个随机的一个片音和缩放

394
00:14:18,920 --> 00:14:21,480
然后再通过一个学到的

395
00:14:21,480 --> 00:14:23,320
一个比较稳定的均值方差

396
00:14:23,320 --> 00:14:26,160
来使得你一变化不要那么剧烈

397
00:14:26,159 --> 00:14:27,959
二它确实有一定的随机性

398
00:14:28,480 --> 00:14:29,799
所以大家后来发现

399
00:14:30,399 --> 00:14:32,600
很多时候是一个

400
00:14:32,799 --> 00:14:34,679
你可以认为是一个

401
00:14:35,120 --> 00:14:37,839
在每一个小批量里面加入噪音

402
00:14:37,839 --> 00:14:39,799
来控制模型复杂度的一个办法

403
00:14:41,319 --> 00:14:42,439
但反过来讲

404
00:14:43,039 --> 00:14:45,039
反过来讲就是说这个东西

405
00:14:45,240 --> 00:14:47,000
大家不要觉得那是正确的

406
00:14:47,240 --> 00:14:49,319
就可能也是不对的

407
00:14:49,600 --> 00:14:51,720
所有这种东西你觉得是在

408
00:14:51,720 --> 00:14:52,799
我觉得是那样

409
00:14:52,839 --> 00:14:55,519
就是说随着大家对这个东西的理解

410
00:14:55,519 --> 00:14:56,480
慢慢的加深

411
00:14:56,519 --> 00:14:58,799
很有可能被证明这种东西都是不对的

412
00:14:59,159 --> 00:15:02,279
所以这就是说我们现在远远的功成

413
00:15:02,799 --> 00:15:04,360
走在了理论的前面

414
00:15:06,120 --> 00:15:06,639
OK

415
00:15:06,960 --> 00:15:09,279
但是我觉得只要它一直work

416
00:15:09,279 --> 00:15:10,399
一直有人用它

417
00:15:10,439 --> 00:15:13,559
慢慢的迟早大家会去提出新的理论框架

418
00:15:13,559 --> 00:15:14,600
把它来解释清楚

419
00:15:14,639 --> 00:15:16,639
所以大家不要急

420
00:15:17,480 --> 00:15:20,759
所以如果你按照这个思路来讲

421
00:15:20,759 --> 00:15:21,439
就是说

422
00:15:21,600 --> 00:15:24,399
你就是一个控制模型复杂度办法的话

423
00:15:24,439 --> 00:15:26,759
所以你是没必要跟你的dropout

424
00:15:26,759 --> 00:15:28,840
或者丢弃化混合使用

425
00:15:30,039 --> 00:15:31,079
就是意味着说

426
00:15:31,079 --> 00:15:32,720
假设你在全连接之后

427
00:15:32,720 --> 00:15:34,559
加入了批量规划的时候

428
00:15:34,600 --> 00:15:36,120
你再加一个dropout

429
00:15:36,120 --> 00:15:38,519
可能就没那么有用了

430
00:15:39,079 --> 00:15:41,439
这是根据这个理论来的一个推论

431
00:15:41,959 --> 00:15:43,279
大家有做过一些实验

432
00:15:43,319 --> 00:15:45,480
发现是有一定的道理性

433
00:15:45,759 --> 00:15:47,079
但是大家有兴趣的话

434
00:15:47,079 --> 00:15:49,279
可以自己再去研究研究

435
00:15:50,240 --> 00:15:50,720
OK

436
00:15:51,159 --> 00:15:52,799
所以总结一下就是说

437
00:15:53,360 --> 00:15:54,680
批量规划就是说

438
00:15:54,680 --> 00:15:58,160
我固定住一个小批量里面的均值和方差

439
00:15:59,480 --> 00:16:04,800
然后我再学习出适合的偏移和缩放

440
00:16:05,000 --> 00:16:06,160
就是说偏移和缩放

441
00:16:06,160 --> 00:16:09,240
就是去逆和均值和方差的作用

442
00:16:10,120 --> 00:16:11,920
当然是说你的全连接层的话

443
00:16:11,920 --> 00:16:14,560
你的作用是每一个特征位上面

444
00:16:14,560 --> 00:16:15,400
就每一个特征

445
00:16:15,400 --> 00:16:16,520
就是每一列作用

446
00:16:17,040 --> 00:16:19,160
然后你对于你的卷积的话

447
00:16:19,160 --> 00:16:20,080
它是对于每

448
00:16:20,080 --> 00:16:21,560
你把它拉成一个矩阵之后

449
00:16:21,560 --> 00:16:23,680
在输出的维度上作用

450
00:16:25,120 --> 00:16:25,960
一般来说

451
00:16:25,960 --> 00:16:28,400
它可以用来加速收敛

452
00:16:29,280 --> 00:16:31,440
就是说大家发现用了批量规划之后

453
00:16:31,440 --> 00:16:34,200
你的学习率可以调到比较大

454
00:16:34,400 --> 00:16:36,080
以前你只能调0.01

455
00:16:36,200 --> 00:16:37,840
现在你可以调到0.1

456
00:16:38,440 --> 00:16:39,280
所以相对来说

457
00:16:39,280 --> 00:16:41,360
你可以认为我的学习率大的话

458
00:16:41,360 --> 00:16:44,840
当然就速度会加快了

459
00:16:45,120 --> 00:16:47,160
它允许你可以用更大的水积

460
00:16:47,480 --> 00:16:49,720
用你更大的学习率来做训练

461
00:16:50,320 --> 00:16:51,360
你也可以理解对吧

462
00:16:51,360 --> 00:16:52,840
就是说我对每一个层

463
00:16:52,840 --> 00:16:54,759
它的均值方向都放在一起了

464
00:16:54,759 --> 00:16:56,680
就不会出现我们之前说

465
00:16:57,000 --> 00:16:58,279
学习率太大的话

466
00:16:58,279 --> 00:17:00,399
那么你的上面的东西梯度比较大

467
00:17:00,399 --> 00:17:01,879
所以它就炸掉了

468
00:17:02,000 --> 00:17:03,519
当你学习率太小的话

469
00:17:03,519 --> 00:17:04,960
你下面的层因为梯度很小

470
00:17:04,960 --> 00:17:05,880
你根本就算不懂

471
00:17:06,120 --> 00:17:08,039
现在你把每一层的输入

472
00:17:08,039 --> 00:17:11,200
都放在一个差不多的分布里面的话

473
00:17:11,200 --> 00:17:13,960
那么我用一个统一的

474
00:17:13,960 --> 00:17:15,440
用一个比较大一点的学习率

475
00:17:15,519 --> 00:17:17,079
就直观上你可以这么理解

476
00:17:19,200 --> 00:17:20,319
但是反过来讲

477
00:17:20,319 --> 00:17:23,200
大家会发现它很多时候

478
00:17:23,200 --> 00:17:25,559
它只是用来加速收敛

479
00:17:25,720 --> 00:17:27,679
它不会改变模型的精度

480
00:17:28,240 --> 00:17:33,119
就是说你如果不要P端规划的话

481
00:17:33,159 --> 00:17:34,559
你发现训练会慢一点

482
00:17:34,559 --> 00:17:36,960
但不会说加了P端规划

483
00:17:37,200 --> 00:17:38,879
你会让你的精度变好

484
00:17:39,240 --> 00:17:41,200
所以这个是它用起来的一个特点

485
00:17:41,480 --> 00:17:46,319
所以因为它不改变你的模型的均值太多

486
00:17:47,359 --> 00:17:49,159
模型的精度太多

487
00:17:49,200 --> 00:17:51,759
但是你加速收敛

488
00:17:51,759 --> 00:17:54,519
所以大家是经常会用这一个层

489
00:17:54,519 --> 00:17:55,960
OK

490
00:17:55,960 --> 00:17:58,440
这就是P端规划

