1
00:00:00,000 --> 00:00:04,799
问题一在讲XVA的时候也讲过类似的normalization

2
00:00:04,799 --> 00:00:06,799
和这里的BatchLong有什么区别

3
00:00:06,799 --> 00:00:08,720
麻烦讲一下

4
00:00:08,720 --> 00:00:11,320
就是说本质上没区别是一个思路

5
00:00:11,320 --> 00:00:15,480
就是说你想让你的模型比较稳定

6
00:00:15,480 --> 00:00:17,320
就是说在更新的时候不要炸

7
00:00:17,320 --> 00:00:19,519
也不要太小的话

8
00:00:19,519 --> 00:00:20,519
一旦你稳定了

9
00:00:20,519 --> 00:00:21,240
模型稳定了

10
00:00:21,240 --> 00:00:22,559
你收敛就不会变慢

11
00:00:23,679 --> 00:00:24,160
对吧

12
00:00:24,560 --> 00:00:29,519
所以我们之前是说你选取比较好的初始化

13
00:00:29,519 --> 00:00:32,240
使得在初始的那个时候

14
00:00:32,920 --> 00:00:33,840
下段比较稳定

15
00:00:33,840 --> 00:00:35,159
他不能保证之后

16
00:00:35,159 --> 00:00:38,799
BatchLong就是保证说你在整个模型训练时候

17
00:00:38,799 --> 00:00:43,759
我都是强行的去在每一个层后面或前面做那些

18
00:00:43,759 --> 00:00:44,920
normalization

19
00:00:44,920 --> 00:00:48,079
他统计上来说那个东西不叫normalization

20
00:00:48,079 --> 00:00:49,760
统计上来说那个东西叫归一化

21
00:00:49,760 --> 00:00:50,719
归一化就是

22
00:00:50,719 --> 00:00:53,320
这normalization在统计上是不是一个东西

23
00:00:53,320 --> 00:00:55,519
所以这个也是搞深度学习的

24
00:00:55,519 --> 00:00:56,359
数学没学好

25
00:00:56,359 --> 00:00:58,159
就是乱用的名词

26
00:00:58,800 --> 00:00:59,280
OK

27
00:00:59,480 --> 00:01:00,360
就是说

28
00:01:01,360 --> 00:01:02,920
它是方差变

29
00:01:02,920 --> 00:01:05,040
变一均值变零

30
00:01:05,040 --> 00:01:06,600
那个东西不叫normalization

31
00:01:07,320 --> 00:01:07,800
OK

32
00:01:07,800 --> 00:01:12,960
所以然后就是说BatchLong就是说我在整个训练过程中

33
00:01:12,960 --> 00:01:13,960
对每一个层

34
00:01:13,960 --> 00:01:17,080
他就是做一直做这个操作

35
00:01:17,080 --> 00:01:19,200
在之前讲初始化的时候是说

36
00:01:19,200 --> 00:01:21,240
我们觉得那样子初始化的时候

37
00:01:21,240 --> 00:01:23,879
在满足我们的假设情况下

38
00:01:23,879 --> 00:01:25,600
这样子也是可以服从分布

39
00:01:25,600 --> 00:01:27,120
使得模型比较稳定

40
00:01:27,240 --> 00:01:29,800
核心是说让你的数值比较稳定

41
00:01:29,800 --> 00:01:30,680
这是它的核心

42
00:01:32,480 --> 00:01:36,240
就p-rank归一化是不是和权重衰退有类似的作用

43
00:01:36,240 --> 00:01:39,640
这个我倒是没有觉得特别大作用

44
00:01:39,640 --> 00:01:41,719
权重衰退是说我在更新的时候

45
00:01:41,719 --> 00:01:46,080
每一次就是把你的权重除了一个小值

46
00:01:46,080 --> 00:01:49,120
就是把它变得比较像这时候比较小

47
00:01:49,120 --> 00:01:54,320
那p-rank归一化倒是不会对你前面一层的权重做太多影响

48
00:01:54,520 --> 00:01:55,040
好

49
00:01:56,840 --> 00:02:01,960
问题三怎么评价马亿老师的深度学习的第一性原理论文

50
00:02:01,960 --> 00:02:03,960
作为白盒理论解释深度学习

51
00:02:05,480 --> 00:02:07,320
这个大家去看一下知乎

52
00:02:07,320 --> 00:02:08,840
我叫知乎

53
00:02:09,360 --> 00:02:10,240
大家去搜一下知乎

54
00:02:10,240 --> 00:02:11,920
我看了我关注了那个问题

55
00:02:11,920 --> 00:02:14,920
我觉得有大家评论

56
00:02:14,920 --> 00:02:16,120
但我不是专家

57
00:02:16,120 --> 00:02:18,080
我就我其实没花时间去读

58
00:02:18,080 --> 00:02:22,640
所以这话我就看一看大家的评论

59
00:02:22,640 --> 00:02:24,920
然后等到大家都说好

60
00:02:24,920 --> 00:02:25,720
说不定我去读一下

61
00:02:25,720 --> 00:02:27,200
我就是最近那么难

62
00:02:27,200 --> 00:02:30,960
Batch Normalization能用在MLP里面

63
00:02:30,960 --> 00:02:33,360
可以的可以用在MLP里面

64
00:02:33,360 --> 00:02:37,480
但是他对深度的深级网络相对说有用一些

65
00:02:37,480 --> 00:02:40,080
对于non-net比如说没有太多太多用

66
00:02:40,080 --> 00:02:40,760
说真话

67
00:02:40,760 --> 00:02:43,320
你刚刚我们是参数没仔细调

68
00:02:43,320 --> 00:02:44,080
你仔细调的话

69
00:02:44,080 --> 00:02:45,400
non-net也不会是那样子

70
00:02:45,680 --> 00:02:47,800
所以他主要是用在比较深的时候

71
00:02:47,800 --> 00:02:50,920
因为你要很深的时候才会出现你下面不断的动

72
00:02:50,920 --> 00:02:52,440
下面在变得慢

73
00:02:52,440 --> 00:02:53,160
上面变得快

74
00:02:53,160 --> 00:02:54,840
然后你就这样子的问题

75
00:02:55,160 --> 00:02:57,800
不深的话其实没那么容易出问题

76
00:02:59,840 --> 00:03:01,680
就说对这个问题差不多

77
00:03:01,680 --> 00:03:02,680
问题5是说

78
00:03:02,680 --> 00:03:05,600
浅色MLP加Batch Normalization好像不见得很好

79
00:03:05,600 --> 00:03:07,640
就是你要是的是这样子

80
00:03:08,840 --> 00:03:09,360
问题6

81
00:03:09,360 --> 00:03:11,160
A third in R4

82
00:03:11,160 --> 00:03:12,680
就是这是Python里面的语句

83
00:03:12,680 --> 00:03:13,960
就是说我得看一下

84
00:03:13,960 --> 00:03:15,720
就是说它的Shape

85
00:03:15,720 --> 00:03:17,800
就是说你的为数要么在2

86
00:03:17,800 --> 00:03:18,600
要么在4

87
00:03:19,199 --> 00:03:20,199
就要么等于2

88
00:03:20,199 --> 00:03:20,879
要么等于4

89
00:03:20,879 --> 00:03:21,960
就是你可以说

90
00:03:21,960 --> 00:03:26,079
在Python里面是说你这个东西必须在这个list里面

91
00:03:26,079 --> 00:03:27,000
这个Tuple里面

92
00:03:27,000 --> 00:03:27,799
就third

93
00:03:27,799 --> 00:03:29,439
如果不在的话就报个错

94
00:03:29,639 --> 00:03:32,280
就这个你不写没关系

95
00:03:36,039 --> 00:03:36,639
问题7

96
00:03:36,639 --> 00:03:37,759
我不是特别理解

97
00:03:37,959 --> 00:03:41,039
这Batch Normalization只考虑的这两种输入情况怎么做

98
00:03:41,039 --> 00:03:43,199
就你是想说如果是别的输入情况

99
00:03:43,319 --> 00:03:44,560
就三维四维

100
00:03:45,319 --> 00:03:46,199
其实说白了

101
00:03:46,199 --> 00:03:47,240
你很简单

102
00:03:47,240 --> 00:03:49,439
就是说我们这个实现当时这么实现了

103
00:03:49,439 --> 00:03:50,320
大家理解

104
00:03:50,439 --> 00:03:52,480
实际上做Batch Normalization的时候

105
00:03:52,480 --> 00:03:54,520
它的框架的实验是说

106
00:03:54,520 --> 00:03:57,560
你告诉我你的feature的维度是几

107
00:03:58,960 --> 00:04:00,360
就你告诉我你的feature维度

108
00:04:00,360 --> 00:04:01,280
现在就是2

109
00:04:01,480 --> 00:04:03,200
就你不管是说一句话

110
00:04:03,200 --> 00:04:06,640
你不管是你是1D 3D 4D 5D的convolution的话

111
00:04:06,640 --> 00:04:08,560
你的第二维就是你的通道维

112
00:04:08,760 --> 00:04:10,320
所以第二是没错的

113
00:04:10,439 --> 00:04:12,680
对你的全连阶层的话

114
00:04:12,680 --> 00:04:15,400
你的第二维就是你的特征维也是你的feature维

115
00:04:15,599 --> 00:04:18,399
所以就是说我就默认的就是说feature dimension是

116
00:04:18,639 --> 00:04:20,079
是1

117
00:04:20,240 --> 00:04:22,680
就是说其实是就是01的1的

118
00:04:22,680 --> 00:04:24,680
就是第二个维度

119
00:04:24,680 --> 00:04:27,800
然后别的我全部帮你算均值算就行了

120
00:04:27,959 --> 00:04:29,959
所以就是说你那么实现的话

121
00:04:29,959 --> 00:04:32,360
你就不用管这种等于2还是等于4的

122
00:04:36,639 --> 00:04:40,600
Batch Normalization做了线性变化和加一个线性层有什么区别

123
00:04:41,600 --> 00:04:46,320
和加一个线性层没什么区别

124
00:04:46,320 --> 00:04:48,760
就是说你加一个线性层

125
00:04:48,760 --> 00:04:51,040
不一定线性层会学到你要的东西

126
00:04:51,640 --> 00:04:52,600
同样道理的话

127
00:04:52,600 --> 00:04:54,600
我就是一个线性网络的话

128
00:04:54,600 --> 00:04:55,800
我就是一个线性变化

129
00:04:55,800 --> 00:04:58,040
那我的数据做什么特征域处理

130
00:04:58,320 --> 00:05:00,120
我不需要把它均值变异方向变零

131
00:05:00,640 --> 00:05:01,520
不需要做

132
00:05:01,680 --> 00:05:03,439
就是说你如果不做的话

133
00:05:03,439 --> 00:05:05,040
很有可能你的数值不稳定

134
00:05:05,200 --> 00:05:07,840
就是说你根本就训练不到你要的好的

135
00:05:07,840 --> 00:05:08,400
那个

136
00:05:08,840 --> 00:05:09,840
指域里面去

137
00:05:14,080 --> 00:05:16,080
为什么加了Batch Normalization之后

138
00:05:16,080 --> 00:05:17,440
收敛时间变短

139
00:05:17,440 --> 00:05:20,560
它其实是你可以本质认为是说

140
00:05:20,560 --> 00:05:22,440
加了Batch Normalization之后

141
00:05:22,440 --> 00:05:24,360
就Batch Normalization本身

142
00:05:24,360 --> 00:05:26,840
加上你还可以使用更大的Learn Rate

143
00:05:26,840 --> 00:05:29,520
就Batch Normalization本身会使得T度

144
00:05:29,520 --> 00:05:31,520
它的值会变大一点点

145
00:05:31,680 --> 00:05:32,560
就Depends吧

146
00:05:32,560 --> 00:05:33,160
就变大一点

147
00:05:33,160 --> 00:05:35,840
而且每个层次的T度的值会差不多一点

148
00:05:36,160 --> 00:05:39,800
然后你的这样子导致你可以使用更大的学习率

149
00:05:40,000 --> 00:05:42,640
所以就导致你的对权重的更新会变快

150
00:05:42,840 --> 00:05:46,480
就是Batch Normalization很多时候你可以认为大概这个事情

151
00:05:51,080 --> 00:05:52,960
问题是说你怎么做严格对比

152
00:05:53,160 --> 00:05:55,920
就是说这个事情

153
00:05:55,920 --> 00:05:57,280
就是说

154
00:05:58,160 --> 00:06:00,680
就严格对比是一件很难的事情

155
00:06:00,960 --> 00:06:03,560
为什么是因为一参数太多

156
00:06:04,280 --> 00:06:04,920
二

157
00:06:05,199 --> 00:06:06,480
训练起来很贵

158
00:06:06,600 --> 00:06:07,759
就特别你特别有钱

159
00:06:08,000 --> 00:06:11,519
除非我们当时候做要开发框架的时候

160
00:06:11,519 --> 00:06:13,079
我会去严格的对比说

161
00:06:13,079 --> 00:06:14,280
你每一次要可重复

162
00:06:14,280 --> 00:06:16,800
比如说D2L我们要去严格的对比

163
00:06:16,800 --> 00:06:18,519
就每一次运行跟前次差不多

164
00:06:18,519 --> 00:06:19,759
不要出现什么问题

165
00:06:20,000 --> 00:06:21,040
但实际上来说

166
00:06:21,040 --> 00:06:23,720
你在实际过程中

167
00:06:23,720 --> 00:06:28,000
你一般不会去说我去看

168
00:06:28,000 --> 00:06:29,519
调一个东西会发生什么变化

169
00:06:29,519 --> 00:06:31,079
是因为成本太高

170
00:06:31,079 --> 00:06:32,240
很多时候你会

171
00:06:32,439 --> 00:06:33,480
当你调一个东西的时候

172
00:06:33,480 --> 00:06:35,120
你不会一次把所有东西都调了

173
00:06:35,560 --> 00:06:36,200
一次的话

174
00:06:36,200 --> 00:06:38,120
你通常会说我把这个调一个

175
00:06:38,120 --> 00:06:39,080
参数调一调看一下

176
00:06:39,080 --> 00:06:39,920
一个参数调一调

177
00:06:39,920 --> 00:06:41,800
但是你不会去把所有东西变了

178
00:06:41,800 --> 00:06:42,000
一次

179
00:06:42,000 --> 00:06:43,640
因为除非你很少数据

180
00:06:43,640 --> 00:06:44,520
不然太贵了

181
00:06:45,240 --> 00:06:46,240
所以很多时候

182
00:06:48,560 --> 00:06:51,520
就是说你说什么东西是可以忽略的

183
00:06:51,520 --> 00:06:51,759
吗

184
00:06:51,759 --> 00:06:53,439
Batch size学习率框架

185
00:06:53,439 --> 00:06:53,960
Epoch数

186
00:06:53,960 --> 00:06:56,640
其实你列了的4个东西都挺重要

187
00:06:56,640 --> 00:07:00,200
我觉得你可能都是不能忽略的

188
00:07:00,519 --> 00:07:03,519
其实4个东西都挺重要

189
00:07:03,519 --> 00:07:05,399
如果让我排个序的话

190
00:07:05,839 --> 00:07:09,279
排个序

191
00:07:09,279 --> 00:07:10,199
Epoch数

192
00:07:10,199 --> 00:07:11,319
Batch size和学习率

193
00:07:11,319 --> 00:07:14,079
三个是相互相互相关的

194
00:07:14,240 --> 00:07:15,560
Epoch数你可以选

195
00:07:15,560 --> 00:07:16,480
总是可以选大一点

196
00:07:16,480 --> 00:07:17,159
这个没关系

197
00:07:17,159 --> 00:07:18,399
就浪费一点资源

198
00:07:18,519 --> 00:07:19,279
Batch size

199
00:07:19,279 --> 00:07:20,879
你要选的合适

200
00:07:21,000 --> 00:07:22,399
你不能选太大

201
00:07:22,399 --> 00:07:23,399
也不能选太小

202
00:07:23,599 --> 00:07:25,519
就你一般取到一个

203
00:07:25,519 --> 00:07:29,240
你算起来比较就GPU的效率比较高的

204
00:07:29,240 --> 00:07:29,639
情况

205
00:07:29,639 --> 00:07:31,079
然后你再去调学习率

206
00:07:31,319 --> 00:07:33,079
就通常是说你先调Batch size

207
00:07:33,079 --> 00:07:36,120
看你的类层跟着你的类层调Batch size

208
00:07:36,439 --> 00:07:38,879
然后调完之后你去调学习率

209
00:07:39,120 --> 00:07:39,639
调完

210
00:07:39,639 --> 00:07:42,120
然后Epoch数通常你可选大一点

211
00:07:42,120 --> 00:07:44,719
你如果发现收敛已经是结束了

212
00:07:44,719 --> 00:07:45,719
你可以停掉

213
00:07:45,719 --> 00:07:47,240
就你可中途停掉

214
00:07:47,240 --> 00:07:48,639
下次你就知道设到哪里了

215
00:07:48,759 --> 00:07:51,319
框架的东西一般来说

216
00:07:53,279 --> 00:07:53,919
怎么说

217
00:07:53,919 --> 00:07:58,399
一般大家是不会怎么换框架的

218
00:07:58,400 --> 00:07:59,840
就用惯一个就用惯一个

219
00:07:59,840 --> 00:08:01,520
就我觉得框架之间都差不多

220
00:08:01,520 --> 00:08:02,200
在我看来

221
00:08:02,320 --> 00:08:05,080
像我们实现有PyTorch

222
00:08:05,080 --> 00:08:06,720
有TensorFlow

223
00:08:06,720 --> 00:08:07,280
有MSNet

224
00:08:07,280 --> 00:08:10,080
我觉得其实都差不多

225
00:08:10,080 --> 00:08:12,040
就看你个人习惯了

226
00:08:15,760 --> 00:08:18,680
PyTorch里面还有一个Layer Normalization

227
00:08:18,720 --> 00:08:20,880
它和Batch Normalization的异同

228
00:08:22,600 --> 00:08:24,160
对这个是不一样的

229
00:08:24,200 --> 00:08:25,560
Layer Normalization

230
00:08:25,560 --> 00:08:26,480
Batch Normalization

231
00:08:26,520 --> 00:08:28,080
就XX Normalization

232
00:08:28,080 --> 00:08:29,680
这个东西太多了

233
00:08:29,680 --> 00:08:32,000
就是我们最近还写了一个论文

234
00:08:32,000 --> 00:08:33,560
叫做XX Normalization

235
00:08:33,840 --> 00:08:35,480
就是说你去学什么什么

236
00:08:35,480 --> 00:08:36,039
Normalization

237
00:08:36,039 --> 00:08:37,800
是你可以说出大概50篇

238
00:08:38,000 --> 00:08:39,080
50篇肯定是有的

239
00:08:39,560 --> 00:08:40,920
核心思想是说

240
00:08:40,920 --> 00:08:42,360
其实没有本质区别

241
00:08:43,159 --> 00:08:44,399
核心思想是说

242
00:08:44,399 --> 00:08:46,000
我们刚刚是在

243
00:08:46,159 --> 00:08:47,360
在Feature维度

244
00:08:47,360 --> 00:08:49,120
对样本做Normalization

245
00:08:49,960 --> 00:08:51,120
这是Batch Normalization

246
00:08:51,120 --> 00:08:52,039
Layer Normalization

247
00:08:52,039 --> 00:08:53,960
就是说每一个样本里面做

248
00:08:53,960 --> 00:08:55,759
就是不是跨样本做Feature

249
00:08:55,759 --> 00:08:58,159
我是样本里面的Feature做Normalization

250
00:08:58,559 --> 00:08:59,799
但我还可以别的做法

251
00:08:59,799 --> 00:09:00,120
对吧

252
00:09:00,120 --> 00:09:00,759
我可以

253
00:09:02,480 --> 00:09:04,240
反正我可以随便找一个维度

254
00:09:04,240 --> 00:09:05,279
随便找几个维度

255
00:09:05,279 --> 00:09:07,519
然后把它做一下Normalization都可以

256
00:09:07,519 --> 00:09:08,799
反正有茫茫多

257
00:09:10,000 --> 00:09:11,679
所以Layer Normalization

258
00:09:11,679 --> 00:09:14,240
通常是用于你比较大的一些网络

259
00:09:14,240 --> 00:09:15,159
你的Batch

260
00:09:15,399 --> 00:09:16,159
就是说

261
00:09:16,439 --> 00:09:17,399
经常用什么

262
00:09:17,439 --> 00:09:19,439
比如说Semantic Segmentation

263
00:09:19,439 --> 00:09:20,159
我们之后会讲

264
00:09:20,159 --> 00:09:22,360
你的图是一个4K的图进来

265
00:09:22,399 --> 00:09:23,799
你的Batch Size就等于1

266
00:09:23,799 --> 00:09:25,319
你做不了Batch Normalization

267
00:09:25,919 --> 00:09:27,200
所以大家会去说

268
00:09:27,200 --> 00:09:28,159
就做Layer Normalization

269
00:09:28,159 --> 00:09:29,319
或做别的Normalization

270
00:09:31,799 --> 00:09:32,519
问题12

271
00:09:33,080 --> 00:09:34,879
它能不能用在几乎函数之后

272
00:09:34,879 --> 00:09:36,439
一般不用在几乎函数之后

273
00:09:37,439 --> 00:09:39,759
因为它是一个对输入输出的线性变换

274
00:09:40,360 --> 00:09:43,039
你可以试一下用在之后会怎么样

275
00:09:45,600 --> 00:09:45,840
对

276
00:09:45,840 --> 00:09:47,399
就是说Batch Normalization

277
00:09:47,399 --> 00:09:48,840
Instance Normalization

278
00:09:49,039 --> 00:09:50,600
这个就是说里面有茫茫多

279
00:09:50,600 --> 00:09:51,679
就大家去搜一下

280
00:09:51,679 --> 00:09:54,279
我觉得网上有一张很简单的图

281
00:09:54,279 --> 00:09:56,759
就是说告诉你各种XX Normalization

282
00:09:58,039 --> 00:09:59,519
我自己就经常搞混

283
00:09:59,639 --> 00:10:01,439
就是XXX Normalization

284
00:10:01,439 --> 00:10:03,919
到底是在哪一个维度做Normalization

285
00:10:04,199 --> 00:10:04,799
大家搜一下

286
00:10:05,240 --> 00:10:07,000
就是说网上有茫茫多图片了

287
00:10:10,399 --> 00:10:12,759
就Batch Size是把GPU显存占满好

288
00:10:12,759 --> 00:10:14,480
还是利用率100%就可以

289
00:10:14,839 --> 00:10:17,000
利用率你的到90%就行了

290
00:10:17,000 --> 00:10:17,720
你不要100%

291
00:10:17,879 --> 00:10:19,360
就是说你如果发现

292
00:10:19,399 --> 00:10:21,839
某一个Batch Size使得GPU的

293
00:10:21,879 --> 00:10:23,079
或者你这么讲

294
00:10:23,800 --> 00:10:25,639
就是说你去增加Batch

295
00:10:25,639 --> 00:10:27,320
你去增加你的Batch Size

296
00:10:27,360 --> 00:10:29,920
去看一下你每秒钟能处理的样本数

297
00:10:29,960 --> 00:10:31,400
如果你增到一个程度

298
00:10:31,400 --> 00:10:34,720
你发现可能不会给你带来太多的

299
00:10:34,759 --> 00:10:36,080
你就可以停了

300
00:10:36,840 --> 00:10:38,879
但是说这个是不是对Nernet这种

301
00:10:38,920 --> 00:10:43,360
Nernet我可以做到一个很大很大Batch Size

302
00:10:43,759 --> 00:10:45,160
但是这样子你会

303
00:10:45,520 --> 00:10:47,560
我会做到可能1000都可以

304
00:10:47,600 --> 00:10:49,720
但是你这样子你的收敛就有问题了

305
00:10:50,560 --> 00:10:51,920
所以你就是试一下

306
00:10:52,519 --> 00:10:54,399
增大发现你实测发现

307
00:10:54,399 --> 00:10:55,479
Number of Examples

308
00:10:55,759 --> 00:10:57,159
就每秒处理都没有必要

309
00:10:57,159 --> 00:10:58,559
划太快的就行了

