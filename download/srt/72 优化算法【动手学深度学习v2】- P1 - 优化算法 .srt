1
00:00:00,000 --> 00:00:02,879
好,我们来看一下答案,问题一

2
00:00:04,000 --> 00:00:06,080
就是说在目标检测里面

3
00:00:06,080 --> 00:00:08,519
就先把图片切下来再用classification做

4
00:00:08,519 --> 00:00:13,759
在实际中用直接做object detection有什么区别吗

5
00:00:13,759 --> 00:00:16,240
在设计

6
00:00:16,240 --> 00:00:17,879
就是说

7
00:00:17,879 --> 00:00:21,600
你是说你把你的图片那一块东西切下来做classification

8
00:00:21,600 --> 00:00:22,879
当然是可以这么做的

9
00:00:22,879 --> 00:00:26,320
就是说大家之所以用的不那么用还是有原因的

10
00:00:26,320 --> 00:00:28,920
一个是说你这么做相对来说

11
00:00:29,720 --> 00:00:30,800
比较啰嗦一点了

12
00:00:30,800 --> 00:00:36,440
其实你看到最早的RCN其实是干这个事情对吧

13
00:00:36,440 --> 00:00:38,640
就是每一次把那个batch抽下来

14
00:00:38,640 --> 00:00:39,480
然后做那个东西

15
00:00:39,480 --> 00:00:41,359
就是说计算量会比较大一点

16
00:00:41,359 --> 00:00:44,760
因为很多框之间是share图片的

17
00:00:44,760 --> 00:00:45,799
你把它切下来之后

18
00:00:45,799 --> 00:00:48,079
你就说成不一样的图片了

19
00:00:48,079 --> 00:00:50,480
导致你要算多次

20
00:00:50,480 --> 00:00:54,079
所以现在也用的不那么多

21
00:00:54,920 --> 00:00:58,159
像问题二

22
00:00:58,159 --> 00:01:00,799
像fastRCN或者ULO是不是自己实现比较好

23
00:01:00,799 --> 00:01:02,079
还是直接用开源比较好

24
00:01:02,079 --> 00:01:06,079
我觉得你这个问题问得很好

25
00:01:06,079 --> 00:01:09,039
我觉得你如果你能自己实现一遍

26
00:01:09,039 --> 00:01:10,359
当然自己实现一遍

27
00:01:10,359 --> 00:01:11,719
这绝对是没错的

28
00:01:11,719 --> 00:01:13,840
但是实现会比较难一点

29
00:01:13,840 --> 00:01:15,840
就是说不是说这个算法很难

30
00:01:15,840 --> 00:01:19,159
是说你想实现一遍容易

31
00:01:19,159 --> 00:01:22,120
但是你想实现一遍实现的效果很好

32
00:01:22,120 --> 00:01:22,519
比较难

33
00:01:22,519 --> 00:01:24,319
因为你去看一下人家library里面

34
00:01:24,319 --> 00:01:25,920
有大量的trick

35
00:01:26,519 --> 00:01:28,039
就大量的这些细节

36
00:01:28,039 --> 00:01:30,159
所以我建议你是自己实现一遍

37
00:01:30,159 --> 00:01:31,319
实现个简单版本

38
00:01:31,319 --> 00:01:33,000
作为学习目的

39
00:01:33,000 --> 00:01:36,920
然后去看开源的实现的细节

40
00:01:36,959 --> 00:01:37,759
这些细节

41
00:01:37,920 --> 00:01:38,919
这些所谓的这些

42
00:01:38,919 --> 00:01:40,439
我说我这个比那个好

43
00:01:40,439 --> 00:01:43,599
很多时候不是我的算法实现的比较好

44
00:01:43,599 --> 00:01:45,719
是我的那些细节的处理比较好一点

45
00:01:45,759 --> 00:01:48,599
就是说基本上比如说我这里做一做

46
00:01:48,599 --> 00:01:48,640
对

47
00:01:48,640 --> 00:01:50,679
做一个这样子的normalization

48
00:01:50,680 --> 00:01:52,120
你说那边做一个那一个

49
00:01:52,120 --> 00:01:53,120
那一个normalization

50
00:01:53,120 --> 00:01:55,160
什么IMS怎么调整的东西

51
00:01:55,160 --> 00:01:55,680
对吧

52
00:01:56,320 --> 00:01:58,240
那些细节可以带去

53
00:01:58,280 --> 00:02:01,480
需要去通过读别人的代码才能知道

54
00:02:01,480 --> 00:02:04,240
问题三

55
00:02:04,240 --> 00:02:06,600
感觉每张图片训练节标注的话不一样

56
00:02:06,600 --> 00:02:08,400
有些是边框里的比较近

57
00:02:08,439 --> 00:02:09,840
有些就比较大的边缘

58
00:02:09,840 --> 00:02:11,800
这些标注会影响效果吗

59
00:02:11,840 --> 00:02:14,120
实际中对标注有什么要求

60
00:02:14,159 --> 00:02:15,960
比如说标注会比较好

61
00:02:16,800 --> 00:02:17,920
这个问题也挺好玩的

62
00:02:17,920 --> 00:02:20,360
就是说你会发现你去看这个图片

63
00:02:20,360 --> 00:02:21,480
发现他标的很糟糕

64
00:02:21,520 --> 00:02:21,840
对吧

65
00:02:21,840 --> 00:02:23,560
就是说你东一块西一块

66
00:02:23,560 --> 00:02:24,520
这个图片

67
00:02:24,520 --> 00:02:26,440
反正一下在这个角落

68
00:02:26,440 --> 00:02:27,200
一下在那个角落

69
00:02:27,200 --> 00:02:27,640
对吧

70
00:02:28,360 --> 00:02:29,160
所以这个地方

71
00:02:29,520 --> 00:02:32,240
我们这个数据还不算是特别脏的

72
00:02:32,240 --> 00:02:33,480
还算是比较干净的

73
00:02:33,880 --> 00:02:37,840
但是你去确实是比学术界的数据会脏

74
00:02:37,840 --> 00:02:38,400
很多

75
00:02:38,400 --> 00:02:40,600
这个就是工业界和学术界的最大的

76
00:02:40,600 --> 00:02:41,080
区别

77
00:02:41,120 --> 00:02:43,160
学术界的数据都是非常干净的

78
00:02:43,160 --> 00:02:46,200
都是通过都是学生自己标的比较好

79
00:02:46,200 --> 00:02:47,920
因为学生我要用这个数据

80
00:02:47,920 --> 00:02:50,440
所以我得给自己标好一点

81
00:02:50,440 --> 00:02:50,680
对吧

82
00:02:50,680 --> 00:02:52,360
不然给大家造成很大麻烦

83
00:02:52,360 --> 00:02:55,320
像M-List都是Align好的

84
00:02:56,000 --> 00:02:58,240
但是在工业界数据通常会比较脏

85
00:03:01,200 --> 00:03:02,560
我们这个数据还算好的

86
00:03:02,560 --> 00:03:03,760
看真正的工业界数据

87
00:03:03,760 --> 00:03:05,400
真的会特别

88
00:03:06,160 --> 00:03:07,760
图片可能就采的不好

89
00:03:07,760 --> 00:03:08,680
第二个是标注

90
00:03:08,680 --> 00:03:09,880
你找人标的话

91
00:03:09,880 --> 00:03:11,560
那些人不一定负责

92
00:03:11,720 --> 00:03:13,480
所以大家都是就赚个钱

93
00:03:13,640 --> 00:03:14,640
大家也不容易

94
00:03:14,640 --> 00:03:15,040
对吧

95
00:03:16,480 --> 00:03:17,160
赚了点钱

96
00:03:17,159 --> 00:03:19,120
我还需要保证每个框都画的很好

97
00:03:20,759 --> 00:03:21,759
当然不一定了

98
00:03:22,039 --> 00:03:22,919
问题4

99
00:03:23,319 --> 00:03:25,280
Aops Mops有什么相关

100
00:03:25,280 --> 00:03:26,280
我们等会会讲

101
00:03:28,120 --> 00:03:28,680
问题5

102
00:03:28,680 --> 00:03:30,439
Batch size取整个训练级大小

103
00:03:30,439 --> 00:03:31,079
PDS

104
00:03:31,719 --> 00:03:32,719
对的

105
00:03:32,719 --> 00:03:34,719
就如果Batch size取整个训练级大小

106
00:03:34,719 --> 00:03:35,719
就变成了T2下降

107
00:03:35,719 --> 00:03:36,519
是没错的

108
00:03:38,960 --> 00:03:40,519
对你这个问的很好

109
00:03:40,519 --> 00:03:42,039
就为什么这充量法

110
00:03:42,039 --> 00:03:43,280
他没有移除beta

111
00:03:43,280 --> 00:03:45,519
是因为充量法做的比较早

112
00:03:45,800 --> 00:03:46,520
就这个东西

113
00:03:46,520 --> 00:03:47,880
这个一减beta这个东西

114
00:03:48,080 --> 00:03:48,880
就理论上

115
00:03:48,880 --> 00:03:49,520
你做一减beta

116
00:03:49,520 --> 00:03:50,159
会比较好一点

117
00:03:50,159 --> 00:03:50,760
我觉得

118
00:03:51,080 --> 00:03:52,360
但是就充量法

119
00:03:52,360 --> 00:03:53,439
这个东西提出了比较早

120
00:03:53,439 --> 00:03:54,520
所以后面的那些东西

121
00:03:54,520 --> 00:03:56,280
都是用了一减beta这个东西了

122
00:03:59,400 --> 00:04:00,840
就是说你做牛仔检测

123
00:04:00,840 --> 00:04:03,640
Fast.CN配合Adam相对SGD

124
00:04:03,840 --> 00:04:04,480
都差很多

125
00:04:04,480 --> 00:04:05,240
是什么原因

126
00:04:05,920 --> 00:04:06,360
我说了

127
00:04:06,360 --> 00:04:08,040
就是说Adam不见得比SGD要好

128
00:04:08,200 --> 00:04:09,080
还是这个问题

129
00:04:09,879 --> 00:04:11,160
Adam太平滑了一点

130
00:04:11,160 --> 00:04:12,560
你可能调教Adam的参数

131
00:04:12,560 --> 00:04:13,960
也许是可以调好一点

132
00:04:14,000 --> 00:04:15,719
Adam你beta1beta2可以调的

133
00:04:15,719 --> 00:04:16,160
对吧

134
00:04:19,480 --> 00:04:21,680
请问迁移学习理论方向怎么样

135
00:04:21,680 --> 00:04:22,720
我们等会会

136
00:04:25,160 --> 00:04:26,639
学习理论这个方向

137
00:04:26,639 --> 00:04:27,720
你就是说学习理论吗

138
00:04:27,720 --> 00:04:28,319
Theory吗

139
00:04:28,319 --> 00:04:29,040
确信

140
00:04:30,439 --> 00:04:31,079
这个有点意思

141
00:04:31,160 --> 00:04:31,639
我不懂

142
00:04:31,639 --> 00:04:32,400
说实话

143
00:04:32,439 --> 00:04:33,040
理论

144
00:04:33,040 --> 00:04:34,360
我不知道迁移学习理论

145
00:04:34,360 --> 00:04:34,960
是什么样子

146
00:04:35,120 --> 00:04:36,000
这个很好玩

147
00:04:36,360 --> 00:04:36,680
就是说

148
00:04:36,680 --> 00:04:37,480
你就说

149
00:04:37,800 --> 00:04:38,600
要多个样本

150
00:04:39,040 --> 00:04:40,920
其实我就不给建议了

151
00:04:40,920 --> 00:04:41,240
理论

152
00:04:41,240 --> 00:04:42,400
我这一块理论

153
00:04:42,400 --> 00:04:43,040
我真不懂

154
00:04:43,760 --> 00:04:44,760
为什么T度下降

155
00:04:44,760 --> 00:04:46,160
X收敛点总是0

156
00:04:46,360 --> 00:04:47,439
公式一定是

157
00:04:48,760 --> 00:04:50,400
T度下降的收敛点不是0

158
00:04:50,400 --> 00:04:52,560
T度下降的收敛点是T度是0

159
00:04:52,560 --> 00:04:54,080
当你的T度变0的时候

160
00:04:54,080 --> 00:04:54,760
你T度下降

161
00:04:54,760 --> 00:04:55,840
你就更新不了了

162
00:04:55,840 --> 00:04:56,080
对吧

163
00:04:56,080 --> 00:04:56,840
就收敛了

164
00:04:57,200 --> 00:04:57,760
OK

165
00:04:58,760 --> 00:05:00,240
为什么一定是减T度

166
00:05:01,240 --> 00:05:01,759
减T度

167
00:05:01,759 --> 00:05:03,480
就是T度指向的是你的

168
00:05:03,480 --> 00:05:04,560
值变大的方向

169
00:05:04,560 --> 00:05:05,680
减去它就是

170
00:05:05,840 --> 00:05:07,360
朝着值变小的方向

171
00:05:08,920 --> 00:05:10,520
工业界模型的编辑效应

172
00:05:10,520 --> 00:05:11,280
是不是在变低

173
00:05:11,280 --> 00:05:11,640
是的

174
00:05:11,639 --> 00:05:12,959
我们等会会来讲这个问题

175
00:05:13,879 --> 00:05:14,879
优化问题

176
00:05:14,879 --> 00:05:16,159
一个是稀疏性问题

177
00:05:18,279 --> 00:05:19,120
稀疏性说

178
00:05:19,120 --> 00:05:21,399
低质矩阵分解的p量

179
00:05:21,800 --> 00:05:22,800
Fact size

180
00:05:23,000 --> 00:05:24,360
大一点还是小一点

181
00:05:25,199 --> 00:05:26,120
你这个很好玩

182
00:05:27,199 --> 00:05:28,000
如果我们没讲

183
00:05:28,000 --> 00:05:29,839
我就不深入了

184
00:05:30,279 --> 00:05:32,000
对于稀疏矩阵的更新

185
00:05:32,000 --> 00:05:32,479
这个东西

186
00:05:32,479 --> 00:05:33,479
它理论上来讲

187
00:05:33,479 --> 00:05:35,879
它不是一个理论上来讲

188
00:05:35,879 --> 00:05:38,039
它不是一个真正的T度下降

189
00:05:38,039 --> 00:05:38,839
因为对稀疏点

190
00:05:38,839 --> 00:05:39,519
你没有更新的

191
00:05:39,519 --> 00:05:39,919
对吧

192
00:05:39,959 --> 00:05:41,360
这一套是要理论的

193
00:05:41,639 --> 00:05:43,800
而且这一块的话

194
00:05:45,560 --> 00:05:46,120
还是

195
00:05:46,159 --> 00:05:47,479
但是实际上来说

196
00:05:47,519 --> 00:05:48,479
跟我们之前没区别

197
00:05:48,479 --> 00:05:49,360
Fact size大的话

198
00:05:49,360 --> 00:05:51,519
就是计算性能更好一点

199
00:05:51,519 --> 00:05:53,079
但收敛其实会慢一些

200
00:05:53,120 --> 00:05:53,759
小一点的话

201
00:05:53,759 --> 00:05:54,680
收敛会快一些

202
00:05:55,800 --> 00:05:56,759
问题12

203
00:05:56,919 --> 00:05:58,800
大数据训练出来的模型

204
00:05:58,839 --> 00:05:59,279
鲁波星

205
00:05:59,279 --> 00:06:00,719
一定比小数据训练出来的

206
00:06:00,719 --> 00:06:02,599
鲁波星强吗

207
00:06:05,560 --> 00:06:06,360
泛化性

208
00:06:06,360 --> 00:06:07,599
你讲泛化性

209
00:06:07,599 --> 00:06:08,159
对吧

210
00:06:08,240 --> 00:06:10,319
大数据的模型的泛化性

211
00:06:10,319 --> 00:06:11,879
和小数据模型的泛化性

212
00:06:11,879 --> 00:06:13,120
不是那么比的

213
00:06:13,759 --> 00:06:14,599
就是说

214
00:06:15,399 --> 00:06:16,439
大数据

215
00:06:16,439 --> 00:06:17,560
你可以用相对来说

216
00:06:17,560 --> 00:06:18,480
比较大的模型

217
00:06:18,480 --> 00:06:19,279
小数据的话

218
00:06:19,279 --> 00:06:20,319
你得用相对来说

219
00:06:20,319 --> 00:06:21,040
比较小的模型

220
00:06:21,040 --> 00:06:22,560
假设你的小数据

221
00:06:22,560 --> 00:06:23,120
用的小模型

222
00:06:23,120 --> 00:06:24,199
大数据用的大模型

223
00:06:24,199 --> 00:06:25,399
相不一定说

224
00:06:25,399 --> 00:06:26,240
小数据小模型

225
00:06:26,240 --> 00:06:27,159
比大数据大模型

226
00:06:27,159 --> 00:06:28,480
一定差太多

227
00:06:28,719 --> 00:06:30,639
小数据用大模型一定会差

228
00:06:30,839 --> 00:06:32,039
大数据用小模型

229
00:06:32,039 --> 00:06:34,079
就是那也效果也不会好

230
00:06:34,079 --> 00:06:34,560
对吧

231
00:06:34,600 --> 00:06:35,519
所以就是说

232
00:06:36,519 --> 00:06:38,759
数据和模型是要匹配的

233
00:06:39,039 --> 00:06:41,120
但如果在完美的匹配情况下

234
00:06:41,120 --> 00:06:42,279
在匹配的比较好的情况下

235
00:06:42,279 --> 00:06:43,039
大数据相对来说

236
00:06:43,039 --> 00:06:43,800
反而会好一点

237
00:06:43,800 --> 00:06:44,800
因为看到的更多

238
00:06:44,800 --> 00:06:45,279
对吧

239
00:06:48,319 --> 00:06:49,279
后续的课

240
00:06:49,399 --> 00:06:50,879
我们等会可以来讲一下

241
00:06:53,120 --> 00:06:54,959
有没有完整一点的

242
00:06:55,560 --> 00:06:56,959
完整一点的

243
00:06:58,159 --> 00:06:59,039
感觉这里学一点

244
00:06:59,039 --> 00:06:59,439
那里学一点

245
00:06:59,439 --> 00:06:59,959
啥都没学到

246
00:06:59,959 --> 00:07:02,240
这个评论很好玩

247
00:07:02,279 --> 00:07:03,000
就是说

248
00:07:03,199 --> 00:07:04,920
我们讲建设视觉

249
00:07:04,920 --> 00:07:05,439
没讲太多

250
00:07:05,439 --> 00:07:07,279
我们这是一个深度学习的入门课

251
00:07:07,800 --> 00:07:08,960
不是一个

252
00:07:09,320 --> 00:07:11,200
针对计算机视觉和NLP的

253
00:07:11,200 --> 00:07:12,240
我们等会会来讲一下

254
00:07:12,240 --> 00:07:13,360
我们哪些东西没讲

255
00:07:14,360 --> 00:07:16,440
如果有上千上百上千特征

256
00:07:16,440 --> 00:07:17,480
又有离散又有连续

257
00:07:17,480 --> 00:07:18,520
需要做不同的处理

258
00:07:18,520 --> 00:07:20,320
怎么判断是离散连续

259
00:07:21,520 --> 00:07:23,520
怎么判断是离散是连续

260
00:07:24,520 --> 00:07:25,480
你就算一下

261
00:07:25,480 --> 00:07:26,080
其实最简单

262
00:07:26,080 --> 00:07:26,720
你就算一下

263
00:07:26,720 --> 00:07:28,040
它有多少个

264
00:07:28,080 --> 00:07:29,280
Unique number

265
00:07:29,360 --> 00:07:30,360
如果Unique number

266
00:07:30,360 --> 00:07:32,120
大概是等价于你的样本个数

267
00:07:32,120 --> 00:07:34,280
那就是连续的

268
00:07:34,320 --> 00:07:36,080
如果是相对说小一点的话

269
00:07:36,080 --> 00:07:36,920
就是离散的

270
00:07:38,000 --> 00:07:39,400
采样2万个样本

271
00:07:39,400 --> 00:07:40,960
摄像头向下采样

272
00:07:42,720 --> 00:07:43,880
就是说你说

273
00:07:44,080 --> 00:07:45,560
摄像头向下采样的

274
00:07:45,560 --> 00:07:46,160
2万个样本

275
00:07:46,160 --> 00:07:47,280
和平的时候

276
00:07:47,280 --> 00:07:48,760
看之前还有用吗

277
00:07:48,760 --> 00:07:49,600
是有用的

278
00:07:49,600 --> 00:07:51,720
但是它的有用的程度

279
00:07:51,760 --> 00:07:53,840
比会打折

280
00:07:54,080 --> 00:07:56,000
就想可能就是说

281
00:07:56,000 --> 00:07:57,040
用肯定是能用的

282
00:07:57,160 --> 00:07:58,120
因为它还是能学的

283
00:07:58,120 --> 00:07:59,600
因为就算角度不一样

284
00:07:59,600 --> 00:08:02,160
但是光照各种目的的纹理

285
00:08:02,240 --> 00:08:03,840
这东西它都是能共享的

286
00:08:03,840 --> 00:08:05,200
但是它确实没有

287
00:08:05,479 --> 00:08:07,800
你要平的会看的效果好

288
00:08:10,319 --> 00:08:11,479
学了很久的深度学习

289
00:08:11,479 --> 00:08:12,360
问一个基本公文题

290
00:08:12,360 --> 00:08:13,399
写代码自己写不出来

291
00:08:13,399 --> 00:08:14,439
看别人能看懂

292
00:08:14,639 --> 00:08:15,639
提升

293
00:08:17,240 --> 00:08:18,279
我觉得写代码写不出来

294
00:08:18,279 --> 00:08:18,839
没关系

295
00:08:18,839 --> 00:08:19,800
你看懂就行了

296
00:08:19,800 --> 00:08:21,519
你就是说最可怕的就是

297
00:08:21,519 --> 00:08:22,159
写也写不出来

298
00:08:22,159 --> 00:08:23,319
看也看不懂才是最难的

299
00:08:23,319 --> 00:08:24,279
如果能看懂别人的

300
00:08:24,279 --> 00:08:25,039
那么就没关系

301
00:08:25,039 --> 00:08:25,719
能看懂别人

302
00:08:25,719 --> 00:08:26,839
能用别人代码就行了

303
00:08:26,839 --> 00:08:27,439
因为是开源

304
00:08:27,759 --> 00:08:28,800
现在开源时代

305
00:08:29,680 --> 00:08:30,800
你能看懂别人东西

306
00:08:30,800 --> 00:08:32,519
能用别人代码就很不错了

307
00:08:32,519 --> 00:08:33,919
不要担心这个事情

308
00:08:34,919 --> 00:08:35,839
不同训练级

309
00:08:35,839 --> 00:08:36,839
训练出来相同结构

310
00:08:36,839 --> 00:08:37,360
两个模型

311
00:08:37,360 --> 00:08:38,959
如何比较两个模型的差异

312
00:08:41,199 --> 00:08:41,839
这个很难

313
00:08:42,279 --> 00:08:43,079
这个不容易

314
00:08:43,199 --> 00:08:44,000
不同

315
00:08:45,159 --> 00:08:48,079
我觉得你不好这么比较

316
00:08:48,279 --> 00:08:48,919
就是说

317
00:08:48,919 --> 00:08:50,559
因为你看你训练级长什么样子

318
00:08:50,559 --> 00:08:52,439
这一块我们之后会来讲

319
00:08:52,559 --> 00:08:54,079
我们等会来讲这个问题

320
00:08:54,759 --> 00:08:55,120
OK

321
00:08:55,120 --> 00:08:57,959
我们就NOP入门的比赛

322
00:08:59,159 --> 00:09:00,199
开口上有一些

323
00:09:00,319 --> 00:09:02,240
就是说NOP有挺多的

324
00:09:03,080 --> 00:09:04,720
NOP那些会有很多竞赛

325
00:09:04,720 --> 00:09:05,440
你可以去看一下

326
00:09:06,000 --> 00:09:06,799
开口上有

327
00:09:06,799 --> 00:09:09,960
然后各种NOP的会议有竞赛

328
00:09:10,440 --> 00:09:11,360
你也可以去看一下

329
00:09:11,680 --> 00:09:12,639
还是有挺多的

330
00:09:12,639 --> 00:09:13,000
OK

