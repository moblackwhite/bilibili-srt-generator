1
00:00:00,000 --> 00:00:00,920
好

2
00:00:00,920 --> 00:00:06,040
现在我们来讲一下线性回归如何从零开始实现

3
00:00:06,480 --> 00:00:08,199
所谓的从零开始实现

4
00:00:08,199 --> 00:00:14,880
就是说我们不使用任何的深度学习框架提供的一些计算

5
00:00:14,919 --> 00:00:18,800
而且只使用最简单的在Tensor上面的计算

6
00:00:18,839 --> 00:00:22,879
来实现我们所有讲过的算法和一些技术细节

7
00:00:23,280 --> 00:00:27,039
这样的好处是说可以帮助大家从很底层的地方

8
00:00:27,039 --> 00:00:29,839
了解每一个模块具体是怎么实现的

9
00:00:30,000 --> 00:00:32,960
虽然当然在实际应用中我们不一定会从

10
00:00:33,000 --> 00:00:36,039
真的从零开始实现我们的算法

11
00:00:36,079 --> 00:00:38,719
但是它却是一个很好的教学的工具

12
00:00:39,039 --> 00:00:40,159
我们先开始吧

13
00:00:41,439 --> 00:00:44,840
首先我们来导入几个包

14
00:00:44,840 --> 00:00:46,560
首先我们导入一个random的包

15
00:00:46,560 --> 00:00:50,679
因为是我们要随机梯度下降和随机化

16
00:00:50,799 --> 00:00:52,799
初始化我们的权重

17
00:00:53,280 --> 00:00:58,879
另外一个是说我们会把一些我们用过的实现过的算法

18
00:00:59,200 --> 00:01:02,840
函数放在一个d2l的包里面

19
00:01:03,280 --> 00:01:06,280
然后我们现在是因为是只有PyTorch的版本

20
00:01:06,280 --> 00:01:09,439
所以我们导入的从d2l导入的是Torch的module

21
00:01:09,879 --> 00:01:12,719
但这里面就是一个很简单的Python文件

22
00:01:13,040 --> 00:01:15,159
就是一些我们讲过的一些函数

23
00:01:15,159 --> 00:01:16,240
把它存在那个地方

24
00:01:16,240 --> 00:01:17,760
没有特别的地方在里面

25
00:01:18,640 --> 00:01:21,320
最上面我们会说matplotinline

26
00:01:21,359 --> 00:01:23,560
就是说在plot的时候

27
00:01:23,599 --> 00:01:27,320
我们默认是嵌入到mat notebook里面

28
00:01:28,320 --> 00:01:29,840
这是我们的导入

29
00:01:30,600 --> 00:01:34,040
首先我们要来构造一个数据集

30
00:01:34,080 --> 00:01:36,680
我们讲过房价数据集

31
00:01:36,680 --> 00:01:38,440
但我们这里讲一个更简单的

32
00:01:38,720 --> 00:01:42,160
就是构造一个很小的人造数据集

33
00:01:43,160 --> 00:01:45,120
构造人造数据集的好处是说

34
00:01:45,120 --> 00:01:47,920
我们知道真实的w和b

35
00:01:48,400 --> 00:01:52,560
现在我们假设我们用真实的w等于2和-3.4

36
00:01:52,840 --> 00:01:54,080
b是一个4.2

37
00:01:54,399 --> 00:01:58,280
然后我们的y的生成就是x

38
00:01:58,280 --> 00:02:00,679
就是一些随机数乘以我们的权重

39
00:02:00,799 --> 00:02:03,560
加上b加上一个随机的噪音

40
00:02:04,039 --> 00:02:04,239
好

41
00:02:04,239 --> 00:02:06,560
接下来我们来实现一下这个函数

42
00:02:07,959 --> 00:02:10,000
假设我们给定了w和b

43
00:02:10,000 --> 00:02:12,400
和我们要生成n个样本的情况下

44
00:02:12,840 --> 00:02:15,240
我们首先生成我们的x

45
00:02:15,800 --> 00:02:17,360
它是一个均值为0

46
00:02:17,360 --> 00:02:19,039
方差为1的一个随机数

47
00:02:19,360 --> 00:02:21,879
它的大小是说我要n个样板

48
00:02:22,039 --> 00:02:24,439
然后它的列数就是w的长度

49
00:02:25,359 --> 00:02:28,519
那么的y就是我们的x乘以w了

50
00:02:28,719 --> 00:02:30,479
再加上我们的偏差b

51
00:02:31,400 --> 00:02:33,479
而且为了让这个问题稍微难一点点

52
00:02:33,479 --> 00:02:35,359
我们加入了一个随机噪音

53
00:02:35,599 --> 00:02:36,519
这是均值为0

54
00:02:36,519 --> 00:02:38,280
方差为0.1的一个

55
00:02:38,280 --> 00:02:40,919
而且它的形状跟y的长度是一样

56
00:02:41,079 --> 00:02:41,680
加进去

57
00:02:42,319 --> 00:02:46,519
最后我们把x和y把它做成一个列下量返回

58
00:02:48,000 --> 00:02:50,879
然后我们定义我们的真实的w和真实的b

59
00:02:51,479 --> 00:02:53,719
然后这样子我们就可以通过这个函数

60
00:02:53,719 --> 00:02:57,960
来生成我们的特征和我们的标注了

61
00:02:59,280 --> 00:02:59,479
好

62
00:02:59,479 --> 00:03:02,560
这样子我们就有了我们的训练样本

63
00:03:04,560 --> 00:03:06,560
可以看一下这个训练样本长什么样子

64
00:03:06,560 --> 00:03:10,400
所以说它的第0个样本

65
00:03:10,560 --> 00:03:13,039
它就是一个常为2的一个向量

66
00:03:13,479 --> 00:03:15,960
它的标号那就是一个标量了

67
00:03:16,840 --> 00:03:18,319
那么当然我们可以画一下它

68
00:03:18,319 --> 00:03:19,639
画一下它

69
00:03:19,640 --> 00:03:24,560
就是说我们把特征的第一列

70
00:03:24,560 --> 00:03:26,960
但这个框还是不需要的

71
00:03:27,520 --> 00:03:29,080
detach的意思是说

72
00:03:29,080 --> 00:03:33,000
我们在Pytorch的一些版本里面

73
00:03:33,000 --> 00:03:35,280
你需要把它从计算图里面

74
00:03:35,680 --> 00:03:36,680
detach出来

75
00:03:36,680 --> 00:03:38,320
然后才能转到lumpi里面去

76
00:03:39,360 --> 00:03:45,000
然后接下来是我们把的第一列和label

77
00:03:45,000 --> 00:03:47,280
做一个画一下

78
00:03:47,280 --> 00:03:48,880
可以看到它当然是有相关性

79
00:03:49,039 --> 00:03:50,919
因为它是一个线性相关的一个过程

80
00:03:53,240 --> 00:03:53,560
好

81
00:03:53,560 --> 00:03:55,439
接下来一个重要的事情是说

82
00:03:55,439 --> 00:03:59,079
我们要每次读取一个小批量

83
00:03:59,599 --> 00:04:02,000
我们实现一个函数来读取小批量

84
00:04:03,240 --> 00:04:08,240
我们这个函数就采用接受一个batch size

85
00:04:08,240 --> 00:04:09,639
就是批量大小的数

86
00:04:09,680 --> 00:04:12,039
和你的所谓的特征和标号

87
00:04:12,840 --> 00:04:14,479
接下来我们干的事情是说

88
00:04:14,479 --> 00:04:16,279
第一我们先来看一下

89
00:04:16,480 --> 00:04:17,680
有number of examples

90
00:04:17,680 --> 00:04:18,800
那么多个样本

91
00:04:19,159 --> 00:04:22,560
接下来我们要生成我们对每个样本的

92
00:04:22,560 --> 00:04:24,199
index

93
00:04:24,199 --> 00:04:25,839
我们把它生成出来

94
00:04:25,839 --> 00:04:28,439
叫range就表示从0一直到n-1

95
00:04:28,439 --> 00:04:30,360
要把它转成一个python的list

96
00:04:31,480 --> 00:04:34,360
接下来我们用random shuffle这个函数

97
00:04:34,360 --> 00:04:37,560
把这些下标完全打乱

98
00:04:38,240 --> 00:04:39,079
这样子的话

99
00:04:39,079 --> 00:04:41,319
那么就可以因为随机打乱之后

100
00:04:41,319 --> 00:04:43,159
我就可以用一个随机的顺序

101
00:04:43,159 --> 00:04:44,360
去访问每个样本

102
00:04:45,759 --> 00:04:48,399
那么接下来这里的干的事情就是说

103
00:04:48,600 --> 00:04:52,840
我们对于每一次从0开始

104
00:04:52,840 --> 00:04:54,680
到最后number of examples

105
00:04:55,000 --> 00:04:57,520
然后每一次跳batch size个大小

106
00:04:58,200 --> 00:05:03,000
然后我们先把那些batch的index找出来

107
00:05:03,000 --> 00:05:07,480
就是说这里每次从i开始

108
00:05:08,160 --> 00:05:10,360
然后到i加上batch size

109
00:05:10,840 --> 00:05:14,360
我们要拿第一个index出来

110
00:05:14,720 --> 00:05:16,280
但因为我们可能会超出

111
00:05:16,280 --> 00:05:17,760
我们最后的样本的个数

112
00:05:17,759 --> 00:05:20,439
所以最后一个如果没有拉满的话

113
00:05:20,439 --> 00:05:21,879
我们会取一个最小值

114
00:05:21,879 --> 00:05:24,759
就是最后一个p就可能没有那么多

115
00:05:24,759 --> 00:05:25,560
但没关系

116
00:05:26,039 --> 00:05:28,399
然后我们拿到这些随机的

117
00:05:28,399 --> 00:05:29,399
这已经被随机化了

118
00:05:29,399 --> 00:05:29,800
对吧

119
00:05:29,800 --> 00:05:31,920
所以我们就可以拿出来一个构造

120
00:05:31,920 --> 00:05:34,079
一个batch的index

121
00:05:34,959 --> 00:05:39,000
最后就是每一次我们产生一个

122
00:05:39,039 --> 00:05:43,279
通过这些index产生那些随机顺序的

123
00:05:43,319 --> 00:05:47,680
特征和随对应的随机顺序的一些标号

124
00:05:48,360 --> 00:05:49,599
这一二就是一个

125
00:05:49,599 --> 00:05:50,639
如果大家不知道的话

126
00:05:50,639 --> 00:05:52,839
那就是一个python的一个iterator

127
00:05:53,399 --> 00:05:57,159
在每一次等于是返回了一个x

128
00:05:57,159 --> 00:05:57,920
返回了一个y

129
00:05:57,920 --> 00:05:59,560
然后你不断的去调用这个函数

130
00:05:59,560 --> 00:06:00,560
可以不断的返回

131
00:06:00,560 --> 00:06:03,439
一直到全部完成为止

132
00:06:04,800 --> 00:06:05,680
我们可以看一下

133
00:06:06,240 --> 00:06:08,959
假设我们给定batch size等于10的话

134
00:06:09,159 --> 00:06:12,680
那么我们就可以说调用这个函数

135
00:06:12,680 --> 00:06:14,000
返回我的iterator

136
00:06:14,199 --> 00:06:17,439
然后每一次我可以从中

137
00:06:17,439 --> 00:06:19,920
拿到一个y和拿到一个x

138
00:06:20,279 --> 00:06:21,560
我们可以print一下

139
00:06:21,600 --> 00:06:22,120
print一下

140
00:06:22,120 --> 00:06:23,360
当然是说这个会

141
00:06:25,680 --> 00:06:28,199
x就是一个10乘以2的一个tensor

142
00:06:28,240 --> 00:06:31,600
y就是一个10乘以1的一个向量

143
00:06:32,079 --> 00:06:32,519
好

144
00:06:32,839 --> 00:06:34,519
这就是这个函数

145
00:06:34,519 --> 00:06:37,360
就是说给我一些样本标号

146
00:06:37,360 --> 00:06:39,639
每一次我随机的从里面选取

147
00:06:39,879 --> 00:06:41,839
逼一个样本返回出来

148
00:06:42,000 --> 00:06:43,800
这样子来参与计算

149
00:06:43,840 --> 00:06:44,199
好

150
00:06:47,240 --> 00:06:49,240
接下来的话我们数据已经弄好了

151
00:06:49,240 --> 00:06:50,160
我们下来

152
00:06:50,439 --> 00:06:53,120
下面是要来把模型定义好

153
00:06:53,720 --> 00:06:55,319
我们知道我们的模型参数

154
00:06:55,560 --> 00:06:57,080
因为我们的输入维度是2

155
00:06:57,080 --> 00:06:59,240
所以我们的w就是一个常为

156
00:06:59,319 --> 00:07:02,639
就是一个常为2的一个向量

157
00:07:03,319 --> 00:07:06,000
然后我们把它随机初始化

158
00:07:06,040 --> 00:07:07,920
乘均值为0

159
00:07:08,199 --> 00:07:10,680
方常为0.1的一个随机的一个均

160
00:07:11,240 --> 00:07:12,120
正台分布

161
00:07:12,800 --> 00:07:15,000
同样我们说我们需要计算梯度

162
00:07:15,040 --> 00:07:17,480
所以require grad等于true

163
00:07:18,240 --> 00:07:21,319
对标偏差来说

164
00:07:21,319 --> 00:07:22,759
我们就是直接给0

165
00:07:23,199 --> 00:07:25,639
它就是一个标量

166
00:07:25,959 --> 00:07:28,280
同样的话我们又要对偏差进行更新

167
00:07:28,280 --> 00:07:31,000
所以我们require grad也是等于true

168
00:07:31,800 --> 00:07:32,920
有了w和b

169
00:07:33,079 --> 00:07:35,519
那么我们就可以定我们的线性回归模型了

170
00:07:36,199 --> 00:07:40,519
所以给定输入就是一个批量大小

171
00:07:41,120 --> 00:07:42,279
的样本

172
00:07:42,319 --> 00:07:44,599
然后给定w和b

173
00:07:44,759 --> 00:07:48,560
那么就是输入x乘以w

174
00:07:48,599 --> 00:07:49,879
矩阵乘以向量

175
00:07:50,079 --> 00:07:52,199
再加上我们的偏差

176
00:07:52,519 --> 00:07:54,240
返回的就是我们的预测了

177
00:07:54,240 --> 00:07:56,399
就是我们最简单的线性模型

178
00:07:57,519 --> 00:07:59,599
好我们模型也有了

179
00:07:59,879 --> 00:08:02,279
那么接下来就是要定义损失函数

180
00:08:02,959 --> 00:08:06,199
损失函数我们使用的是均方误差

181
00:08:06,439 --> 00:08:08,120
其实也是挺简单

182
00:08:08,519 --> 00:08:11,319
假设我们的y hat就是我们的预测值

183
00:08:11,360 --> 00:08:12,879
y是真实值

184
00:08:13,240 --> 00:08:15,480
那就是y hat减去y

185
00:08:15,680 --> 00:08:18,439
但因为我们虽然他们两个应该是元素

186
00:08:18,439 --> 00:08:19,319
个数是一样

187
00:08:19,319 --> 00:08:20,680
但是它可能会有

188
00:08:20,720 --> 00:08:23,000
一个是可能是一个向量

189
00:08:23,040 --> 00:08:26,480
一个可能是一个行向量或者列向量

190
00:08:26,519 --> 00:08:28,280
所以为了统一起见

191
00:08:28,280 --> 00:08:31,600
我们把y reshape成y hat的shape

192
00:08:32,279 --> 00:08:34,279
然后按元素做减法

193
00:08:34,360 --> 00:08:36,039
然后按元素做平方

194
00:08:36,159 --> 00:08:38,639
再除以2

195
00:08:38,959 --> 00:08:40,959
注意这里我们没有做均值

196
00:08:41,199 --> 00:08:42,759
我们就直接加起来了

197
00:08:44,360 --> 00:08:46,120
那么接下来就是说

198
00:08:46,159 --> 00:08:47,799
我们定义我们的优化算法

199
00:08:48,319 --> 00:08:50,120
优化算法就是SGD

200
00:08:50,799 --> 00:08:52,439
SGD它的输入是说

201
00:08:52,439 --> 00:08:55,399
给定我的所有的参数

202
00:08:55,399 --> 00:08:56,399
就是一个list

203
00:08:56,639 --> 00:08:58,319
里面包含了w和b

204
00:08:58,559 --> 00:09:00,039
给定我们的学习率

205
00:09:00,159 --> 00:09:01,719
给定我们的batch size

206
00:09:02,120 --> 00:09:03,120
那么怎么做呢

207
00:09:03,399 --> 00:09:04,919
首先说我们说

208
00:09:04,919 --> 00:09:07,159
这个不需要计算t度

209
00:09:07,159 --> 00:09:09,000
因为我们更新的时候

210
00:09:09,000 --> 00:09:11,000
不要参与t度计算

211
00:09:11,120 --> 00:09:12,399
所以我们说no grad

212
00:09:13,000 --> 00:09:15,600
接下来对于我们参数里面

213
00:09:15,600 --> 00:09:16,480
每一个参数

214
00:09:16,480 --> 00:09:18,279
就可能是w可能是b

215
00:09:18,679 --> 00:09:21,039
我们它减去

216
00:09:21,279 --> 00:09:24,240
learning rate乘以它的t度

217
00:09:24,600 --> 00:09:26,560
记得我们在自动求道里面讲过

218
00:09:27,120 --> 00:09:30,000
t度会存在你的.grad里面

219
00:09:30,320 --> 00:09:32,559
然后记得我们之前的

220
00:09:32,559 --> 00:09:34,759
损失就没有求均值

221
00:09:34,759 --> 00:09:36,759
所以这里我们求了均值

222
00:09:37,159 --> 00:09:41,120
因为乘法对于t度来说

223
00:09:41,120 --> 00:09:42,200
是一个线性的关系

224
00:09:42,200 --> 00:09:45,360
所以把除以除在上面和除在下面

225
00:09:45,440 --> 00:09:46,879
其实是一样的效果

226
00:09:47,799 --> 00:09:48,919
那么做完之后

227
00:09:49,200 --> 00:09:50,720
最后我说我们可以

228
00:09:50,759 --> 00:09:52,919
接下来可以把我的t度设成0

229
00:09:53,200 --> 00:09:54,240
因为PyTorch

230
00:09:54,399 --> 00:09:57,200
它不会自动把你设t度设0

231
00:09:57,200 --> 00:09:59,320
所以你需要手动的把t度设成0

232
00:09:59,320 --> 00:10:01,400
这样子下一次计算t度的时候

233
00:10:01,440 --> 00:10:03,240
我就不会跟上一次相关了

234
00:10:03,800 --> 00:10:04,000
好

235
00:10:04,000 --> 00:10:06,320
这就是SGD的实现

236
00:10:08,840 --> 00:10:11,879
那么我们基本上把所有的模块

237
00:10:11,879 --> 00:10:12,800
都实现好了

238
00:10:12,800 --> 00:10:14,240
接下来就是一个

239
00:10:15,160 --> 00:10:16,560
训练的一个过程

240
00:10:16,920 --> 00:10:17,080
好

241
00:10:17,080 --> 00:10:18,600
我们来看一下我们的训练函数

242
00:10:20,120 --> 00:10:22,560
首先我们指定一些超参数

243
00:10:22,879 --> 00:10:24,720
缺席率等于0.03

244
00:10:25,360 --> 00:10:26,640
我们number of epoch

245
00:10:26,639 --> 00:10:29,519
就是说我们把整个数据扫三遍

246
00:10:30,279 --> 00:10:31,679
然后我们的network

247
00:10:31,679 --> 00:10:32,720
就是我们的模型

248
00:10:32,720 --> 00:10:35,759
就是我们之前定义的linear regression

249
00:10:36,240 --> 00:10:37,319
我们之所以这么写

250
00:10:37,319 --> 00:10:39,199
是因为之后我们很方便的

251
00:10:39,199 --> 00:10:40,879
换成不一样别的模型

252
00:10:41,240 --> 00:10:41,919
那么的loss

253
00:10:41,919 --> 00:10:43,480
就是我们的军方损失

254
00:10:44,360 --> 00:10:47,519
那么训练的实现

255
00:10:47,519 --> 00:10:48,639
基本上大同小异

256
00:10:48,879 --> 00:10:50,399
它就是两层for loop

257
00:10:50,879 --> 00:10:53,559
第一层就是每一次对数据扫一遍

258
00:10:54,560 --> 00:10:56,160
接下来就是对于

259
00:10:57,840 --> 00:11:00,320
每次拿出一个批量大小的

260
00:11:00,320 --> 00:11:01,600
一个x和一个y

261
00:11:03,280 --> 00:11:07,600
然后把x w和b

262
00:11:07,600 --> 00:11:10,000
放进我的network里面来做预测

263
00:11:10,680 --> 00:11:14,160
把预测的y和真实的y来做损失

264
00:11:14,960 --> 00:11:15,680
这样子的话

265
00:11:15,680 --> 00:11:18,320
损失就是一个常为一个批量大小的

266
00:11:18,320 --> 00:11:20,040
一个向量

267
00:11:20,520 --> 00:11:21,640
然后对它来讲

268
00:11:21,639 --> 00:11:23,840
我来做求和

269
00:11:24,480 --> 00:11:26,000
求和之后算t度

270
00:11:27,679 --> 00:11:28,600
算完t度之后

271
00:11:28,600 --> 00:11:30,399
我就有能够访问t度了

272
00:11:30,399 --> 00:11:30,879
这样子

273
00:11:30,879 --> 00:11:34,360
我们就用std来对w和b进行更新

274
00:11:35,240 --> 00:11:35,679
好

275
00:11:36,559 --> 00:11:38,799
当然这里的话注意到是说

276
00:11:39,639 --> 00:11:40,360
这个批量大小

277
00:11:40,360 --> 00:11:42,080
这其实不是那么的正确

278
00:11:42,080 --> 00:11:43,840
就是说如果是最后一个批量

279
00:11:43,840 --> 00:11:45,199
不能被整除的话

280
00:11:45,240 --> 00:11:47,879
可能最后一个会少一些元素

281
00:11:48,960 --> 00:11:50,039
所以这里就是说

282
00:11:50,039 --> 00:11:51,200
可能会多出了一点点

283
00:11:51,200 --> 00:11:53,200
但是我们这里选的

284
00:11:53,200 --> 00:11:55,000
batch size等于10

285
00:11:55,000 --> 00:11:56,040
我们的样本是100

286
00:11:56,040 --> 00:11:57,840
所以其实我们这里没关系

287
00:11:57,879 --> 00:12:00,800
但是我们今后不会这么直接实现了

288
00:12:02,160 --> 00:12:02,400
好

289
00:12:02,400 --> 00:12:03,680
接下来就是说

290
00:12:03,879 --> 00:12:06,000
对数据扫完一遍之后

291
00:12:06,400 --> 00:12:10,560
我们说我们来评价一下我们的进度

292
00:12:11,000 --> 00:12:13,240
而且这一块是不需要计算t度的

293
00:12:13,240 --> 00:12:15,360
所以我们把它放在lowgrad里面

294
00:12:15,680 --> 00:12:18,280
就是说把整个features

295
00:12:18,600 --> 00:12:19,879
整个数据传进去

296
00:12:19,879 --> 00:12:21,240
计算它的预测

297
00:12:21,279 --> 00:12:23,279
和你的真实的label

298
00:12:23,279 --> 00:12:24,279
做一下损失

299
00:12:24,279 --> 00:12:25,399
然后print一下

300
00:12:26,279 --> 00:12:27,759
也基本看到是说

301
00:12:27,799 --> 00:12:29,720
我们的print就是从

302
00:12:30,399 --> 00:12:31,439
扫完一遍之后

303
00:12:31,439 --> 00:12:32,919
loss是0.02

304
00:12:32,960 --> 00:12:35,320
然后再一遍之后就变得很小

305
00:12:35,679 --> 00:12:38,120
在第三遍就已经是非常小了

306
00:12:38,919 --> 00:12:40,759
当然因为我们是人工数据集

307
00:12:40,799 --> 00:12:42,960
我们能看到真实的w和b

308
00:12:43,000 --> 00:12:44,440
所以我们可以看一下

309
00:12:44,519 --> 00:12:47,240
我们学习到的跟真实的区别

310
00:12:47,759 --> 00:12:49,960
可以看到是说

311
00:12:50,000 --> 00:12:52,759
真实的qw-w的话

312
00:12:52,879 --> 00:12:57,399
那就是他们的差值是0.004和0.0003

313
00:12:57,600 --> 00:12:58,600
已经很小了

314
00:12:58,879 --> 00:13:02,039
真实的b和我们学到的b也是0.0008

315
00:13:02,560 --> 00:13:03,200
好

316
00:13:04,519 --> 00:13:05,960
接下来我们就是说

317
00:13:06,000 --> 00:13:07,600
我们提过

318
00:13:07,840 --> 00:13:09,960
我们这里有很多超参数

319
00:13:10,320 --> 00:13:12,399
所以我们讲这个的好处

320
00:13:12,399 --> 00:13:13,840
是说大家可以去

321
00:13:14,120 --> 00:13:15,360
真的去实验一下

322
00:13:15,360 --> 00:13:17,440
我们不同的超参数的选择

323
00:13:17,440 --> 00:13:18,560
会有什么样的效果

324
00:13:18,800 --> 00:13:20,120
我们这里举两个例子

325
00:13:20,320 --> 00:13:21,759
当然是说我们不可能说

326
00:13:21,759 --> 00:13:24,039
把所有的想试的都试一遍

327
00:13:24,039 --> 00:13:25,759
只是给大家一些

328
00:13:26,200 --> 00:13:27,279
例子说

329
00:13:27,279 --> 00:13:29,159
可以大家可以去试哪一样的东西

330
00:13:29,399 --> 00:13:30,800
我建议大家课后

331
00:13:30,800 --> 00:13:32,360
可以去真的去跑一跑

332
00:13:32,519 --> 00:13:33,480
体验一下

333
00:13:33,919 --> 00:13:35,960
比如说我们学说过

334
00:13:36,080 --> 00:13:37,600
学习率不能太小

335
00:13:37,600 --> 00:13:38,519
不能太大

336
00:13:38,639 --> 00:13:40,399
比如说我们调一个比较小的

337
00:13:40,399 --> 00:13:40,920
会怎么样

338
00:13:41,600 --> 00:13:42,879
调一个0.01

339
00:13:43,879 --> 00:13:46,120
当然是说我们要重新训练的话

340
00:13:46,120 --> 00:13:48,439
我们要把我们之前的w

341
00:13:48,439 --> 00:13:50,799
重新随机初始画一次

342
00:13:50,960 --> 00:13:53,000
这样子就不会跟着上一次的

343
00:13:53,000 --> 00:13:54,080
剃头结果来了

344
00:13:55,439 --> 00:13:55,679
好

345
00:13:55,679 --> 00:13:56,919
我们跑一遍

346
00:13:57,480 --> 00:13:58,720
可以看到是说

347
00:13:58,759 --> 00:14:00,960
如果你学习率特别小的话

348
00:14:01,080 --> 00:14:02,799
那么跑完第1次之后

349
00:14:02,799 --> 00:14:04,279
你的损失还是挺大的

350
00:14:04,399 --> 00:14:05,600
跑完3次之后

351
00:14:05,600 --> 00:14:07,240
你的还是一个很大的值

352
00:14:07,600 --> 00:14:08,559
当然如果你要

353
00:14:08,559 --> 00:14:09,360
你还是有办法

354
00:14:09,360 --> 00:14:09,639
对吧

355
00:14:09,639 --> 00:14:10,480
我可以多跑一点

356
00:14:11,120 --> 00:14:12,039
可以多跑一点

357
00:14:12,039 --> 00:14:12,679
就是

358
00:14:13,719 --> 00:14:14,759
可以看到是说

359
00:14:14,799 --> 00:14:16,759
跑了10个apoc之后

360
00:14:17,000 --> 00:14:19,079
那就是可以降到还可以

361
00:14:19,079 --> 00:14:20,599
但是还是挺大的

362
00:14:20,799 --> 00:14:23,039
就是说意味着是说

363
00:14:23,240 --> 00:14:24,639
这个学习率太小了

364
00:14:25,199 --> 00:14:26,399
反过来我们来看一下

365
00:14:26,399 --> 00:14:27,839
很大的学习率会怎么样

366
00:14:28,919 --> 00:14:30,439
重新初始画一下

367
00:14:32,120 --> 00:14:33,279
假设我们取一个很大

368
00:14:33,519 --> 00:14:34,519
取一个10

369
00:14:36,519 --> 00:14:36,839
好

370
00:14:36,839 --> 00:14:38,039
可以看到是说

371
00:14:38,199 --> 00:14:39,120
太大了

372
00:14:39,360 --> 00:14:40,799
因为他的求导的时候

373
00:14:40,800 --> 00:14:42,880
可能会除0或者除1

374
00:14:43,840 --> 00:14:46,480
有一些可能无限的值在出现

375
00:14:46,520 --> 00:14:48,080
最后导致我的loss

376
00:14:48,080 --> 00:14:49,240
是一个loss number

377
00:14:49,480 --> 00:14:51,120
这已经超出了

378
00:14:51,120 --> 00:14:53,080
我们的负的运算的计算范围了

379
00:14:53,200 --> 00:14:55,360
这就是学习率太大的后果

380
00:14:56,680 --> 00:14:57,040
好

381
00:14:57,040 --> 00:14:59,640
大家可以去真的去实验一下

382
00:15:00,480 --> 00:15:01,800
调整不一样的

383
00:15:02,560 --> 00:15:03,320
比如说

384
00:15:03,480 --> 00:15:05,680
我们的w是怎么随机初始化的

385
00:15:05,680 --> 00:15:07,120
那里面还有点超参数

386
00:15:07,280 --> 00:15:09,280
然后以及node of epochs

387
00:15:09,480 --> 00:15:10,240
learning rate

388
00:15:10,240 --> 00:15:11,200
所有这些东西

389
00:15:11,200 --> 00:15:12,519
对我们的训练的结果

390
00:15:12,519 --> 00:15:13,440
产生的影响

391
00:15:13,680 --> 00:15:15,159
大家可以去实验一下

392
00:15:15,799 --> 00:15:16,279
好

393
00:15:16,519 --> 00:15:18,320
信息回归的从零开始

394
00:15:18,320 --> 00:15:19,720
我们就讲到这里

