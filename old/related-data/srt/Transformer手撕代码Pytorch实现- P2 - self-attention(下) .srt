1
00:00:00,000 --> 00:00:02,560
现在我们讲forward的部分

2
00:00:03,080 --> 00:00:06,400
forward首先我们看给多头注意力机制的

3
00:00:07,440 --> 00:00:08,800
这个输入是什么呀

4
00:00:09,279 --> 00:00:11,800
forward是实现我们前向通道流程

5
00:00:11,839 --> 00:00:14,560
既然我们这儿是self-attention

6
00:00:14,599 --> 00:00:16,000
也就是这个多头注意力

7
00:00:16,600 --> 00:00:20,320
所以这个前向通道就是右边这样一坨东西

8
00:00:21,600 --> 00:00:23,359
那左边这一类的东西的话

9
00:00:23,359 --> 00:00:26,800
它其实就是Scaled-up Product Attention里面的内容

10
00:00:27,080 --> 00:00:28,400
所以左边这个东西呢

11
00:00:28,400 --> 00:00:33,039
我们在前向通道这个过程当中要进行相应的实现

12
00:00:34,560 --> 00:00:35,560
我们先观察这个

13
00:00:35,560 --> 00:00:38,160
它的输入只有三个发现栏面QKV

14
00:00:38,679 --> 00:00:42,320
所以这儿送给forward的输入自然就是

15
00:00:43,520 --> 00:00:46,640
values我们按照从左到右的顺序

16
00:00:47,079 --> 00:00:48,320
values case

17
00:00:49,840 --> 00:00:51,840
various

18
00:00:54,400 --> 00:00:55,840
应该是这几个就结束了

19
00:00:56,040 --> 00:00:57,840
那我们看一下人论文

20
00:00:59,199 --> 00:01:04,000
会发现Multi-head attention

21
00:01:04,000 --> 00:01:05,039
还有一个mass

22
00:01:05,480 --> 00:01:07,200
Multi-head attention

23
00:01:07,640 --> 00:01:08,599
mass是什么意思

24
00:01:08,599 --> 00:01:09,560
是遮挡住了

25
00:01:10,200 --> 00:01:12,359
我们了解了transformer理论之后发现

26
00:01:12,359 --> 00:01:15,159
decoder其实会把我们的M版本

27
00:01:15,200 --> 00:01:19,000
再次输入给我们的多头注意力机制去预测

28
00:01:20,280 --> 00:01:23,799
但是要避免我们输入当前词项量的时候

29
00:01:24,480 --> 00:01:26,960
我们已经看到后面词项量是什么了

30
00:01:27,000 --> 00:01:29,680
会影响我们后面输出结果的准确性

31
00:01:30,880 --> 00:01:34,240
所以我们要在这里做一个遮挡

32
00:01:36,080 --> 00:01:37,840
要把后面词项量遮挡住

33
00:01:38,840 --> 00:01:41,840
让我们decoder看到你需要解码的这部分

34
00:01:42,080 --> 00:01:43,800
而看不到你后面那部分

35
00:01:44,320 --> 00:01:46,560
关于这一套详细理论

36
00:01:46,880 --> 00:01:49,520
请大家去查阅transformer论文

37
00:01:49,520 --> 00:01:52,320
或者是更深的transformer理论的讲解

38
00:01:52,600 --> 00:01:53,760
所以我们发现

39
00:01:54,280 --> 00:01:55,760
Multi-head attention之外

40
00:01:55,760 --> 00:01:57,480
还有一个mass multi-head attention

41
00:01:58,560 --> 00:02:02,160
mass只是在attention技术上加了一个遮挡

42
00:02:02,200 --> 00:02:05,359
所以我们这把mass这个参数也复制进来

43
00:02:05,400 --> 00:02:07,840
这样的话我们模块可复用

44
00:02:08,800 --> 00:02:11,080
也就是说我们在multi-head attention的时候

45
00:02:11,120 --> 00:02:12,400
mass关掉就可以

46
00:02:12,480 --> 00:02:14,360
在mass multi-head attention的时候

47
00:02:14,480 --> 00:02:16,400
只要把mass这个参数打开

48
00:02:16,400 --> 00:02:18,319
就成了mass multi-head attention

49
00:02:18,599 --> 00:02:21,680
所以这样加这个参数之后让我们模块可复用

50
00:02:22,680 --> 00:02:24,680
因为除了这个mass之外

51
00:02:24,719 --> 00:02:26,040
它们功能完全一致

52
00:02:26,080 --> 00:02:27,400
接下来让我们去写

53
00:02:28,480 --> 00:02:30,719
首先我们要去看一下

54
00:02:31,439 --> 00:02:32,920
我们第一步要做什么

55
00:02:34,080 --> 00:02:35,879
我们要把QAP输入进去

56
00:02:35,920 --> 00:02:37,960
多头注意力我们说是并联的

57
00:02:38,000 --> 00:02:39,360
它这个并联怎么实现

58
00:02:40,120 --> 00:02:42,319
是代码里面写了很多个什么

59
00:02:42,319 --> 00:02:43,879
scale without product attention吗

60
00:02:44,480 --> 00:02:45,120
并不是

61
00:02:45,159 --> 00:02:47,920
它是用了一种很巧妙的方式

62
00:02:48,000 --> 00:02:50,600
给矩阵再增加两个维度进行量化分

63
00:02:51,680 --> 00:02:52,960
听这乱没有关系

64
00:02:53,000 --> 00:02:54,879
我们下面开始写我们代码

65
00:02:54,920 --> 00:02:56,040
首先你要获取

66
00:02:57,400 --> 00:03:02,080
你这个有几行

67
00:03:04,360 --> 00:03:06,599
我们把n作为这个参数

68
00:03:06,719 --> 00:03:09,319
然后就要获取你到底有几滴

69
00:03:09,360 --> 00:03:11,520
每个磁相量的大小到底是多少

70
00:03:12,600 --> 00:03:14,120
然后klen

71
00:03:17,520 --> 00:03:19,960
然后最后querylen

72
00:03:21,680 --> 00:03:28,439
就等于value.cp

73
00:03:28,719 --> 00:03:29,719
1就是列码

74
00:03:30,240 --> 00:03:32,319
就是你这个磁相量的大小是多少

75
00:03:44,240 --> 00:03:44,920
这是k

76
00:03:46,480 --> 00:03:49,080
然后下面这个就是query

77
00:03:51,680 --> 00:03:57,439
然后打错了

78
00:03:57,480 --> 00:03:58,520
这应该是1

79
00:03:58,760 --> 00:04:00,120
这部分是获取列

80
00:04:00,159 --> 00:04:01,719
然后这是case

81
00:04:02,800 --> 00:04:03,360
OK

82
00:04:03,879 --> 00:04:05,680
现在就获取我们行和列了

83
00:04:05,840 --> 00:04:08,000
我们如何实现这样的

84
00:04:08,600 --> 00:04:10,640
把一个完整的qqv

85
00:04:11,040 --> 00:04:13,159
把它切成好几个hide的部分

86
00:04:13,240 --> 00:04:15,879
然后再送给scale without product attention

87
00:04:16,519 --> 00:04:19,319
那就是一个很巧妙的设计了

88
00:04:20,159 --> 00:04:21,800
进行一个reshape操作

89
00:04:25,920 --> 00:04:30,120
首先我们把values这个矩阵进行一个reshape

90
00:04:38,920 --> 00:04:40,199
reshape是什么样子呢

91
00:04:40,240 --> 00:04:45,800
首先第一个参数肯定还是它的行

92
00:04:45,839 --> 00:04:48,519
然后第二个是它的磁相量的大小

93
00:04:48,879 --> 00:04:50,079
valueslen

94
00:04:51,759 --> 00:04:54,359
那么三四参数是什么

95
00:04:55,120 --> 00:04:57,799
我们想要对它进行一个定性的预测

96
00:04:58,439 --> 00:05:01,240
我们要把完整的一个qqv矩阵给它拆开

97
00:05:01,279 --> 00:05:02,479
拆成很多个头

98
00:05:02,560 --> 00:05:05,799
然后每个头的维度是hide

99
00:05:06,560 --> 00:05:07,240
dimension

100
00:05:07,279 --> 00:05:08,879
然后我们这已经定义了

101
00:05:09,639 --> 00:05:12,479
所以要把切块这么几个头怎么办

102
00:05:12,519 --> 00:05:17,519
我们直接在这对它进行一个reshape

103
00:05:17,839 --> 00:05:19,680
reshape第三个参数就是hide

104
00:05:20,120 --> 00:05:22,639
把它在hide的尺度上再次划分一下

105
00:05:22,879 --> 00:05:25,719
然后每一个hide多的是hide的d

106
00:05:27,879 --> 00:05:29,079
这样一划分之后

107
00:05:29,439 --> 00:05:31,799
它相当于把原来一块完整的大矩阵

108
00:05:31,839 --> 00:05:37,560
啪啪啪切分成好几个并行的这样一个矩阵

109
00:05:38,159 --> 00:05:41,719
然后这个并行的矩阵相当于就是我们这后面这些重影

110
00:05:41,719 --> 00:05:45,959
然后它交给一个scale without product attention

111
00:05:46,000 --> 00:05:49,279
就相当于进行了一个完整的变形的操作

112
00:05:50,000 --> 00:05:51,159
这一部分大家不理解

113
00:05:51,319 --> 00:05:53,000
可以想象一下二维矩阵

114
00:05:54,120 --> 00:05:56,519
然后用二维矩阵类比一下

115
00:05:56,919 --> 00:05:57,799
去了解这个

116
00:05:58,000 --> 00:05:59,799
因为本身它这个n

117
00:06:00,639 --> 00:06:03,519
然后这个列的大小是固定的

118
00:06:03,560 --> 00:06:06,000
然后reshape之后会发生相应的一些变化

119
00:06:06,240 --> 00:06:09,799
然后多两个尺度就相当于把完整矩阵拆开了

120
00:06:09,879 --> 00:06:11,639
拆成很多个变形的部分

121
00:06:11,879 --> 00:06:14,159
变形几份就变形hide的份

122
00:06:14,439 --> 00:06:17,120
然后每个变形后的hide大小多少

123
00:06:17,120 --> 00:06:18,080
hide的第一幕份

124
00:06:18,800 --> 00:06:21,120
所以类似的后面其实也是同理

125
00:06:21,120 --> 00:06:22,439
然后case就等于

126
00:06:23,080 --> 00:06:23,840
case

127
00:06:36,000 --> 00:06:37,800
然后这改成case

128
00:06:39,320 --> 00:06:40,640
然后最后就是queries

129
00:06:40,680 --> 00:06:43,800
这个都是我们的输入的矩阵

130
00:06:44,320 --> 00:06:47,640
qries.reshape

131
00:06:48,680 --> 00:06:50,920
然后再复制一下里面内容

132
00:06:54,000 --> 00:06:58,720
然后里面内容把这个case改成queries

133
00:06:59,960 --> 00:07:00,360
ok

134
00:07:00,640 --> 00:07:01,600
现在reshape之后

135
00:07:01,640 --> 00:07:03,200
我们实现这样一个维度之后

136
00:07:04,200 --> 00:07:05,800
实现并联化的输入之后

137
00:07:05,800 --> 00:07:10,040
我们就可以做我们的scale.productattention

138
00:07:10,080 --> 00:07:12,879
它其实scale.productattention就是这个式子

139
00:07:13,600 --> 00:07:15,839
就是solmax qk t转折

140
00:07:15,879 --> 00:07:19,560
比上一个输入的k的一个维度

141
00:07:20,000 --> 00:07:21,639
然后再成一个位矩阵

142
00:07:23,079 --> 00:07:25,560
那这个我们给它起个名叫energy

143
00:07:26,240 --> 00:07:29,439
energy就等于torch.ns3

144
00:07:29,600 --> 00:07:34,719
这个是torch里面自带一个矩阵相乘的一个函数

145
00:07:34,759 --> 00:07:35,639
它很有意思

146
00:07:35,719 --> 00:07:36,719
它有三个参数

147
00:07:36,719 --> 00:07:37,639
第二个参数

148
00:07:37,759 --> 00:07:42,240
第三个参数就是你要做相乘的两个矩阵

149
00:07:42,519 --> 00:07:44,439
一个queries一个case

150
00:07:45,400 --> 00:07:47,360
我们因为要对qk t转制

151
00:07:47,560 --> 00:07:51,840
然后这里边这个引号里面要填什么呢

152
00:07:51,840 --> 00:07:55,240
首先第一个就是你第一个queries的矩阵大小

153
00:07:55,280 --> 00:07:57,600
第二个case的矩阵大小

154
00:07:57,680 --> 00:08:01,199
nqhd就是它你写的一个代号

155
00:08:01,240 --> 00:08:02,280
这个代号随便改

156
00:08:03,519 --> 00:08:05,160
然后这就是两个矩阵的维度

157
00:08:05,160 --> 00:08:07,000
你写出来然后写一个箭头

158
00:08:07,000 --> 00:08:09,480
代表着你这两个维度要去怎么相乘

159
00:08:09,960 --> 00:08:14,800
我们希望qk的转制也就是完成这样一个查询和

160
00:08:14,800 --> 00:08:17,040
箭的注意力的匹配

161
00:08:17,160 --> 00:08:19,120
qk之后其实就有了注意力了

162
00:08:20,480 --> 00:08:23,480
那它要变成什么样子呢

163
00:08:24,600 --> 00:08:26,360
我们这可以想二维矩阵

164
00:08:28,400 --> 00:08:33,039
二维矩阵什么2×3×3×2就等于2×2行的

165
00:08:34,039 --> 00:08:37,039
在我们这个式当中

166
00:08:38,559 --> 00:08:42,480
d这个维度其实是打末尾的

167
00:08:42,480 --> 00:08:44,399
2×3×3×2等于2×2

168
00:08:44,839 --> 00:08:46,759
最后这个维度它因为没有变

169
00:08:46,759 --> 00:08:50,360
所以它最后会被小掉

170
00:08:50,360 --> 00:08:52,480
然后就会变成一个nhqk

171
00:08:53,159 --> 00:08:56,039
代表着我们qk矩阵

172
00:08:57,199 --> 00:08:59,039
它俩做了一个乘积

173
00:08:59,039 --> 00:09:03,039
然后最后变成一个n×h维度的

174
00:09:03,439 --> 00:09:06,519
q×k查询后的一个值

175
00:09:08,919 --> 00:09:15,480
这4个维度乘积之后就变成了这样一个维度nhqk

176
00:09:17,120 --> 00:09:18,839
这才是我们想要的

177
00:09:24,399 --> 00:09:27,199
关于这一块到底具体怎么实现

178
00:09:27,400 --> 00:09:30,400
我觉得大家还是去看一下知乎和cstn比较好

179
00:09:31,000 --> 00:09:34,759
因为我现在能表述出来的可能很难给大家讲清楚

180
00:09:34,759 --> 00:09:37,759
为什么这个地方成为之后会变成这个样子

181
00:09:38,400 --> 00:09:40,560
而且我们今天是一个代码课

182
00:09:40,560 --> 00:09:41,759
我们先关注代码

183
00:09:42,280 --> 00:09:45,960
默认大家对理论上有了一些更深的了解

184
00:09:46,200 --> 00:09:48,040
所以我们先关注代码部分

185
00:09:49,320 --> 00:09:51,960
这个地方也可以有一个浅的印象

186
00:09:51,960 --> 00:09:54,680
然后我们后面也会不断去聊这个事

187
00:09:55,680 --> 00:09:59,240
然后这完成一个energy设计之后就完成这个qkt了

188
00:10:00,360 --> 00:10:04,040
然后接下来我们可以考虑这个mask了

189
00:10:04,560 --> 00:10:06,040
如果这个mask打开了

190
00:10:06,040 --> 00:10:07,280
这个mask是什么呀

191
00:10:07,280 --> 00:10:10,680
就是我们说的要做这个沿码

192
00:10:11,160 --> 00:10:12,480
什么时候需要沿码

193
00:10:12,680 --> 00:10:18,200
沿码谁在我们后面会设计一个函数去把它做这个操作

194
00:10:18,600 --> 00:10:21,680
我们只要知道它需要沿码的地方

195
00:10:21,880 --> 00:10:23,760
会把它只变成0

196
00:10:25,080 --> 00:10:26,920
我们为什么要做这个mask fill

197
00:10:26,920 --> 00:10:30,760
因为我们要把需要沿码的地方换成一个非常小的值

198
00:10:31,320 --> 00:10:32,920
一个非常大的复数

199
00:10:33,280 --> 00:10:34,960
我们用这个float来表示

200
00:10:35,600 --> 00:10:38,960
这个float非常大的复数

201
00:10:39,280 --> 00:10:41,120
也就是我们mask等于0

202
00:10:41,120 --> 00:10:43,320
就是我们整个这样一个磁相量

203
00:10:44,600 --> 00:10:45,960
需要遮蔽到的部分

204
00:10:46,280 --> 00:10:48,200
我们把它换成这么大一个复值

205
00:10:48,840 --> 00:10:50,360
这么大一个复值意义在哪

206
00:10:50,520 --> 00:10:53,440
在于softmax之后这一部分会被忽略掉

207
00:10:53,840 --> 00:10:57,760
softmax之后这一部分不会有值的变化

208
00:10:58,000 --> 00:11:03,040
它就softmax不会对这一部分做过度的关注

209
00:11:03,040 --> 00:11:04,960
所以说我们最后整个注意力

210
00:11:05,280 --> 00:11:07,200
对于我们需要被遮掉的这部分

211
00:11:07,320 --> 00:11:09,320
也不会有任何注意力上的变化

212
00:11:09,320 --> 00:11:11,480
因为我们给了它一个很大的复值

213
00:11:13,560 --> 00:11:15,640
你想一下这个很大的复值

214
00:11:15,880 --> 00:11:17,640
energy就是这个qk的转折

215
00:11:17,640 --> 00:11:18,840
这是很大一个复值

216
00:11:19,520 --> 00:11:22,080
然后再乘这个也是超级大一个复值

217
00:11:22,080 --> 00:11:24,320
超级大一个复值做softmax等于多少

218
00:11:25,480 --> 00:11:25,960
对吧

219
00:11:26,080 --> 00:11:28,360
所以它最后让它会变得

220
00:11:29,520 --> 00:11:30,720
被遮掩到这部分

221
00:11:30,720 --> 00:11:32,160
他们的所有的

222
00:11:32,720 --> 00:11:34,879
这个最后的注意力的效果是一样的

223
00:11:34,960 --> 00:11:37,000
都是一个复值

224
00:11:37,000 --> 00:11:38,480
就是很小的一个

225
00:11:39,520 --> 00:11:41,600
没有任何变化的一个注意力的反应

226
00:11:43,280 --> 00:11:45,560
然后设置完这块研码

227
00:11:45,560 --> 00:11:48,800
关于研码这个mask是在后面给的

228
00:11:48,800 --> 00:11:51,040
是在完成transformer结构的时候设计到的

229
00:11:51,199 --> 00:11:53,279
所以我们现在不用过多关注这两行

230
00:11:53,519 --> 00:11:54,719
有个印象即可

231
00:11:54,959 --> 00:11:56,719
最后完成我们整个公式

232
00:11:57,079 --> 00:12:00,679
attention就等于nm.softmax

233
00:12:03,240 --> 00:12:04,959
softmax我们要写这个公式

234
00:12:04,959 --> 00:12:07,559
energy除以什么

235
00:12:07,919 --> 00:12:10,000
除以这个√dk

236
00:12:10,000 --> 00:12:10,759
√dk是啥

237
00:12:11,120 --> 00:12:12,879
我们输入到这个

238
00:12:13,399 --> 00:12:14,719
scale.product

239
00:12:15,480 --> 00:12:18,639
scale.product.attention里面的这个

240
00:12:19,399 --> 00:12:20,679
k的维度

241
00:12:20,760 --> 00:12:23,520
其实就是scale.ampd.size

242
00:12:29,800 --> 00:12:32,680
scale.ampd.size就是我们输入到这个

243
00:12:33,560 --> 00:12:35,960
注意机制里面的一个次相量的大小

244
00:12:38,840 --> 00:12:39,720
然后呢

245
00:12:40,440 --> 00:12:42,160
我们要对它开根号

246
00:12:42,160 --> 00:12:44,680
我们给它称一个1次

247
00:12:45,040 --> 00:12:46,920
它的1次就是开根号

248
00:12:48,240 --> 00:12:49,960
然后我们还没有称微矩阵

249
00:12:49,960 --> 00:12:53,400
微矩阵因为又是涉及到4个维度的一个矩阵成绩

250
00:12:53,400 --> 00:12:54,480
并行化成绩

251
00:12:54,480 --> 00:12:57,320
所以我们还是用这个touch点这个函数去做

252
00:12:57,320 --> 00:12:58,720
我们在下一步工作

253
00:12:59,840 --> 00:13:02,080
这个softmax我们需要关注一点

254
00:13:02,080 --> 00:13:06,160
是要它要在第4个维度上开始做这个softmax

255
00:13:07,160 --> 00:13:10,200
第4个就是0123就是k维度

256
00:13:10,680 --> 00:13:12,440
我们这个矩阵变成nhq就可以了

257
00:13:12,440 --> 00:13:14,320
就第4个维度也就是k维度上

258
00:13:14,480 --> 00:13:15,879
做softmax的变换

259
00:13:16,200 --> 00:13:18,800
softmax会把整个的这个值

260
00:13:18,800 --> 00:13:21,760
energy的值压缩到0到1的范围内

261
00:13:22,240 --> 00:13:25,640
所以说在第4个维度k维度做变换之后

262
00:13:26,560 --> 00:13:29,040
就这个energy这个attention

263
00:13:31,320 --> 00:13:35,320
当前我们这个attention还是这个qkt比根号dk还没称对

264
00:13:35,560 --> 00:13:38,640
它会表现出对每一个键

265
00:13:38,640 --> 00:13:41,280
这个k位置上的注意力的大小

266
00:13:41,680 --> 00:13:43,440
这个注意力越大

267
00:13:43,440 --> 00:13:46,680
代表着我们网络是要关注这个位置越多

268
00:13:47,680 --> 00:13:50,800
所以说它要在我们k这个维度上做

269
00:13:50,800 --> 00:13:51,960
因为这是一个4维度的

270
00:13:51,960 --> 00:13:55,320
我们要在最后一个维度上做这个注意力的softmax

271
00:13:55,920 --> 00:13:58,320
在第4维度上把它压入到0到1范围内

272
00:13:58,320 --> 00:14:02,000
看一看我整个网络目前算下来的这个权重大小

273
00:14:02,320 --> 00:14:08,400
对我这个k上的哪一个位置需要更多的关注

274
00:14:08,400 --> 00:14:11,800
哪一个次项量需要更多的加注意力

275
00:14:12,520 --> 00:14:13,600
所以写完attention之后

276
00:14:13,600 --> 00:14:16,320
我们完成attention最后一步就是乘k

277
00:14:17,120 --> 00:14:23,240
所以我们最后的out就等于out.

278
00:14:28,160 --> 00:14:30,000
同理我们要对哪些处理

279
00:14:30,000 --> 00:14:37,040
我们要对attention和values去做这个相乘

280
00:14:38,000 --> 00:14:41,880
它是nhql

281
00:14:43,520 --> 00:14:44,520
我们叫l

282
00:14:45,000 --> 00:14:46,759
就一个k我们叫做l

283
00:14:47,559 --> 00:14:48,360
当然k也行

284
00:14:48,360 --> 00:14:50,000
然后这nlhd

285
00:14:50,439 --> 00:14:52,519
它相乘之后要变成什么样呢

286
00:14:52,519 --> 00:14:53,879
变成nqhd

287
00:14:58,600 --> 00:15:00,879
变成nqhd

288
00:15:02,960 --> 00:15:05,919
之后我们对它进行一个reshape操作

289
00:15:06,679 --> 00:15:08,600
因为接下来我们马上就要输出了

290
00:15:10,319 --> 00:15:11,519
马上就要输出

291
00:15:12,519 --> 00:15:14,439
我们要把它变回去

292
00:15:14,439 --> 00:15:15,879
把它行

293
00:15:16,279 --> 00:15:18,720
然后它的列是querylen

294
00:15:19,639 --> 00:15:21,799
这个len的话你换任何一个len都行

295
00:15:21,799 --> 00:15:25,559
因为queryq为它们输的是一样的

296
00:15:26,000 --> 00:15:28,360
然后第三个维度self.hide

297
00:15:28,360 --> 00:15:30,799
第四个维度self.hide

298
00:15:31,360 --> 00:15:34,519
就把每一个维度表示出来

299
00:15:34,519 --> 00:15:38,000
表示出来之后我们可以给它输出了

300
00:15:38,679 --> 00:15:40,559
out就等于self.

301
00:15:42,519 --> 00:15:44,519
fcout

302
00:15:50,039 --> 00:15:51,279
fcout

303
00:15:52,519 --> 00:15:56,879
我们的fcout输入维度就是我们这个

304
00:15:57,159 --> 00:15:58,679
hide成hideDim

305
00:15:59,240 --> 00:16:02,439
然后输出的话就是这个ipdcite大小了

306
00:16:03,519 --> 00:16:04,519
这个是fout

307
00:16:04,519 --> 00:16:06,319
让我们这return一下out

308
00:16:11,519 --> 00:16:17,879
这有一个没定义

309
00:16:22,000 --> 00:16:24,159
有一个多余的空格

310
00:16:25,360 --> 00:16:28,000
发现问题是我们这多打一个括号

311
00:16:28,000 --> 00:16:29,319
我们把这个括号删掉

312
00:16:29,559 --> 00:16:30,240
回车

313
00:16:30,559 --> 00:16:31,159
好

314
00:16:32,600 --> 00:16:34,960
之前我们这多打了一个组括号

315
00:16:35,319 --> 00:16:35,759
好

316
00:16:35,759 --> 00:16:40,079
这个就是我们完整的一个self-attention的部分了

317
00:16:41,519 --> 00:16:41,919
对

318
00:16:41,919 --> 00:16:42,639
还有一点

319
00:16:42,799 --> 00:16:45,960
这个地方不是hide和dim

320
00:16:45,960 --> 00:16:50,480
是hide成self-hideDim

321
00:16:51,000 --> 00:16:54,199
因为我们你看这个输出的fcout

322
00:16:54,199 --> 00:16:55,079
linert

323
00:16:55,199 --> 00:17:00,360
输入维度是hideDim成hide

324
00:17:01,279 --> 00:17:03,879
hideDim成hide

325
00:17:03,879 --> 00:17:05,680
就应该是这个维度

326
00:17:06,680 --> 00:17:11,960
所以这个out维度经过fcout之后

327
00:17:11,960 --> 00:17:15,680
会变成一个uncurryln embdcite

328
00:17:15,680 --> 00:17:17,400
这样一个尺度

329
00:17:17,400 --> 00:17:20,720
就变成uncurryln embdcite

330
00:17:21,640 --> 00:17:22,200
OK

331
00:17:22,200 --> 00:17:23,799
这个就是我们的self-attention

