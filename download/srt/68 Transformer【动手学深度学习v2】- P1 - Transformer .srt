1
00:00:00,000 --> 00:00:01,520
问题14

2
00:00:01,520 --> 00:00:05,280
正学编码和自动学编码在这种效果上有区别吗

3
00:00:05,320 --> 00:00:06,360
这个我还真不知道

4
00:00:06,520 --> 00:00:07,679
可能区别不大

5
00:00:07,679 --> 00:00:08,960
我觉得可能区别不大

6
00:00:10,160 --> 00:00:11,080
在视觉中

7
00:00:11,080 --> 00:00:13,439
Transformer适合在用在芯片上吗

8
00:00:13,480 --> 00:00:14,280
挺好用的

9
00:00:14,280 --> 00:00:15,759
Transformer非常

10
00:00:16,120 --> 00:00:17,440
它那种可变形的东西

11
00:00:17,519 --> 00:00:19,600
就在哪里都跑得都好

12
00:00:19,640 --> 00:00:22,519
但是问题是Transformer本身比较贵

13
00:00:22,519 --> 00:00:23,480
就算起来比较大

14
00:00:23,480 --> 00:00:25,839
所以车载芯片我还真不知道

15
00:00:25,839 --> 00:00:27,600
可能看你车载有多强

16
00:00:27,600 --> 00:00:32,439
现在企业里多轮对话用什么算法

17
00:00:32,439 --> 00:00:34,039
还是多轮对话

18
00:00:34,039 --> 00:00:36,600
也现在也用的是这种Transformer架构了

19
00:00:38,600 --> 00:00:39,400
多头注意

20
00:00:39,400 --> 00:00:42,520
Concat和相加取平均怎么选择

21
00:00:43,680 --> 00:00:46,640
其实我是觉得能Concat就Concat

22
00:00:47,400 --> 00:00:51,960
Concat对保留信息的上面比加取平均要好

23
00:00:52,439 --> 00:00:53,040
Ok

24
00:00:53,760 --> 00:00:54,760
在Transformer中

25
00:00:54,760 --> 00:00:55,880
为什么在获取词

26
00:00:55,880 --> 00:00:58,359
现在就需要对词进行缩放

27
00:00:58,719 --> 00:00:59,000
对

28
00:00:59,000 --> 00:01:00,520
就是说这样子缩放

29
00:01:00,520 --> 00:01:01,880
就要embedding出来

30
00:01:02,120 --> 00:01:03,679
就是说使得通常embedding

31
00:01:03,679 --> 00:01:04,320
那个向量

32
00:01:04,719 --> 00:01:05,840
它的non等于1

33
00:01:05,840 --> 00:01:07,520
就是说当你长度变很长的时候

34
00:01:07,520 --> 00:01:10,079
你每个元素值就变成

35
00:01:10,120 --> 00:01:12,480
比较根号低分之一的scale上面

36
00:01:12,480 --> 00:01:13,400
你乘回去之后

37
00:01:13,400 --> 00:01:16,079
就保持每个数在正义负义之间

38
00:01:16,200 --> 00:01:17,480
大概是正义负义那个幅度

39
00:01:17,920 --> 00:01:20,480
就是跟你的position权重

40
00:01:20,520 --> 00:01:22,200
就差不多scale上面

41
00:01:23,200 --> 00:01:23,879
所以就是说

42
00:01:23,879 --> 00:01:25,799
你不要让positioning和coding

43
00:01:25,799 --> 00:01:26,359
dominate

44
00:01:26,359 --> 00:01:28,239
就是说是个很大的数

45
00:01:28,239 --> 00:01:29,199
然后你是个很小的数

46
00:01:29,280 --> 00:01:31,599
你就存去学那个位置了

47
00:01:31,599 --> 00:01:31,959
对吧

48
00:01:33,119 --> 00:01:34,119
NLP没有竞赛

49
00:01:34,199 --> 00:01:35,920
NLP我们就不搞竞赛了

50
00:01:35,920 --> 00:01:37,439
本来我想做一个文本分类

51
00:01:37,439 --> 00:01:38,079
但是像

52
00:01:38,679 --> 00:01:40,159
就说我今天宣布文本竞赛

53
00:01:40,159 --> 00:01:41,479
给大家搞一个月

54
00:01:41,479 --> 00:01:42,759
我一个月之后我们再来

55
00:01:43,280 --> 00:01:44,560
大家可能热度就不高了

56
00:01:45,519 --> 00:01:46,959
Number of head干嘛

57
00:01:46,959 --> 00:01:48,039
Number of head

58
00:01:48,039 --> 00:01:50,399
反正就是做了n个小的tension在里面

59
00:01:50,399 --> 00:01:52,359
这样子使得每一个tension

60
00:01:52,519 --> 00:01:54,040
可以去关注不同的地方

61
00:01:54,040 --> 00:01:56,159
就有点像卷积的多通道

62
00:01:57,400 --> 00:01:59,840
后面会讲DETR吗

63
00:01:59,840 --> 00:02:00,480
我们就不讲了

64
00:02:03,000 --> 00:02:04,719
Transformer最低需求

65
00:02:04,959 --> 00:02:06,359
Transformer没有最低需求

66
00:02:06,359 --> 00:02:07,319
Transformer还好

67
00:02:07,319 --> 00:02:07,840
Transformer

68
00:02:07,840 --> 00:02:09,120
但是说

69
00:02:10,520 --> 00:02:12,520
Transformer其实本身没问题

70
00:02:12,520 --> 00:02:14,560
是BERT和GPT-3

71
00:02:14,680 --> 00:02:16,039
就后面那些东西

72
00:02:16,039 --> 00:02:17,719
让大家要几千个GPU

73
00:02:17,960 --> 00:02:19,400
Transformer本身是没问题

74
00:02:22,920 --> 00:02:24,760
Query是不是很有实际意义

75
00:02:25,040 --> 00:02:27,160
就没有什么

76
00:02:27,160 --> 00:02:29,680
Query就没有什么实际意义

77
00:02:29,760 --> 00:02:32,760
反正就是你要选多少

78
00:02:32,960 --> 00:02:34,920
就说你要去看哪个词

79
00:02:34,920 --> 00:02:36,240
就Query就是谁

80
00:02:38,720 --> 00:02:40,040
KVQ的大小

81
00:02:40,040 --> 00:02:41,880
KVQ一般其实就是Hidden Size

82
00:02:41,880 --> 00:02:44,600
就是一般取个256

83
00:02:44,600 --> 00:02:46,200
1024随便取

84
00:02:46,480 --> 00:02:47,440
取决于你的模型大小

85
00:02:47,440 --> 00:02:49,600
跟你的Hidden Size没本事

86
00:02:49,600 --> 00:02:50,120
区别

87
00:02:50,159 --> 00:02:51,599
和MLP Hidden Size

88
00:02:51,599 --> 00:02:53,000
取起来跟没本事区别

89
00:02:55,719 --> 00:02:57,480
Unembedding用的是标准的

90
00:02:57,480 --> 00:02:58,400
正态处置化

91
00:02:59,680 --> 00:03:01,319
Embedding不是标准的

92
00:03:01,319 --> 00:03:01,920
正态处置化

93
00:03:01,920 --> 00:03:03,520
它是一个应该是有点像

94
00:03:03,759 --> 00:03:05,159
XVR的一个小变种

95
00:03:05,719 --> 00:03:06,759
它会根据你的

96
00:03:06,759 --> 00:03:07,800
Dimension的长度

97
00:03:07,800 --> 00:03:09,960
会去做一些

98
00:03:10,400 --> 00:03:16,560
对大小的方差做调整

99
00:03:18,360 --> 00:03:19,080
看懂论文

100
00:03:19,080 --> 00:03:19,800
模型只有0

101
00:03:20,000 --> 00:03:20,880
没有Decoder

102
00:03:22,000 --> 00:03:22,640
区别在哪

103
00:03:22,640 --> 00:03:23,920
BERT就是Encoder

104
00:03:24,120 --> 00:03:24,800
我们明天讲

105
00:03:26,840 --> 00:03:27,960
Transformer能处理

106
00:03:27,960 --> 00:03:29,600
非序列图像的数据吗

107
00:03:29,600 --> 00:03:29,840
可以

108
00:03:30,000 --> 00:03:30,719
Transformer可以

109
00:03:31,439 --> 00:03:34,400
就是说你可以把图片

110
00:03:34,400 --> 00:03:35,320
当做一个序列

111
00:03:35,320 --> 00:03:35,840
怎么多呢

112
00:03:35,840 --> 00:03:37,320
你就每次抠一个Batch出来

113
00:03:37,320 --> 00:03:38,120
就在图片里面

114
00:03:38,120 --> 00:03:39,080
抠出很多个Batch

115
00:03:39,080 --> 00:03:40,080
它就是一个序列了

116
00:03:42,080 --> 00:03:44,320
是否有多智能体

117
00:03:44,320 --> 00:03:46,000
处理RNN的论文的

118
00:03:46,000 --> 00:03:46,800
或模型对接器

119
00:03:46,800 --> 00:03:48,200
每个智能体处理

120
00:03:49,200 --> 00:03:50,960
我其实没看懂

121
00:03:51,160 --> 00:03:54,280
我们没看懂这个问题

122
00:03:54,280 --> 00:03:55,200
我们就越过

123
00:03:55,280 --> 00:03:56,840
我们今天时间不多了

124
00:03:59,640 --> 00:04:02,920
我想用BERT特征

125
00:04:02,920 --> 00:04:05,080
做文章抄袭检测

126
00:04:05,080 --> 00:04:06,800
100万的特征

127
00:04:06,800 --> 00:04:07,560
索取瘫痪

128
00:04:12,560 --> 00:04:14,360
特征搜索瘫痪是什么意思

129
00:04:14,360 --> 00:04:16,120
就是说你用BERT来做

130
00:04:16,120 --> 00:04:17,360
文章的抄袭检测

131
00:04:17,639 --> 00:04:19,280
你可以把你所有的100万个文章

132
00:04:19,280 --> 00:04:20,639
你可以先用BERT去处理

133
00:04:20,639 --> 00:04:20,840
好

134
00:04:20,840 --> 00:04:22,040
这样子得到特征

135
00:04:22,080 --> 00:04:23,080
你不会慢

136
00:04:23,199 --> 00:04:23,759
就是说

137
00:04:24,199 --> 00:04:25,960
如果你BERT输出是1024的话

138
00:04:26,240 --> 00:04:27,000
那就是100万

139
00:04:27,000 --> 00:04:27,920
每个文章的输出

140
00:04:27,920 --> 00:04:29,120
是1024的特征

141
00:04:29,120 --> 00:04:31,080
所以你查稿的时候

142
00:04:31,080 --> 00:04:32,120
就说查稿的时候

143
00:04:32,120 --> 00:04:34,080
只是说你在一个文本进去的时候

144
00:04:34,080 --> 00:04:36,319
你需要做一次BERT的Inference

145
00:04:36,400 --> 00:04:36,759
对吧

146
00:04:36,759 --> 00:04:38,600
但是你之后你比

147
00:04:38,600 --> 00:04:39,520
你不需要

148
00:04:39,560 --> 00:04:40,639
之前的就不需要做了

149
00:04:42,120 --> 00:04:42,480
对吧

150
00:04:42,480 --> 00:04:44,319
我觉得用BERT做文

151
00:04:44,319 --> 00:04:44,960
做抄袭

152
00:04:44,960 --> 00:04:45,879
我觉得问题不大

153
00:04:47,400 --> 00:04:48,720
我觉得问题

154
00:04:50,639 --> 00:04:51,160
问题不大

155
00:04:51,160 --> 00:04:53,040
我觉得你可能大不了

156
00:04:53,040 --> 00:04:55,680
你把BERT搞一个小一点的BERT呗

157
00:04:56,400 --> 00:04:57,800
自回归和自编码

158
00:04:57,800 --> 00:04:59,240
我觉得都没什么联系

159
00:04:59,240 --> 00:05:00,199
我觉得

160
00:05:03,800 --> 00:05:06,120
首先我们没有自编码

161
00:05:06,120 --> 00:05:07,240
我们是未知编码

162
00:05:08,199 --> 00:05:09,840
然后自回归和

163
00:05:09,879 --> 00:05:11,360
自回归和Self

164
00:05:11,840 --> 00:05:14,360
自回归倒是跟Self

165
00:05:14,360 --> 00:05:14,840
Attention

166
00:05:14,840 --> 00:05:17,320
就自注意力是有一点点关系的

167
00:05:17,320 --> 00:05:17,600
就是说

168
00:05:17,600 --> 00:05:19,000
你可以认为是自己预测自己

169
00:05:21,280 --> 00:05:22,760
RN处理非序列数据

170
00:05:22,760 --> 00:05:23,800
也可以多个Batch

171
00:05:23,800 --> 00:05:24,880
组成序列去处理吗

172
00:05:24,880 --> 00:05:25,280
可以的

173
00:05:25,280 --> 00:05:30,560
我们RN处理非序列图像

174
00:05:32,440 --> 00:05:34,080
RN处理非序列图像

175
00:05:34,080 --> 00:05:35,200
也是可以的

176
00:05:35,720 --> 00:05:37,560
就是说这一块倒是用的不多

177
00:05:37,560 --> 00:05:39,880
RN在图片上用的不那么多

178
00:05:39,880 --> 00:05:42,160
因为RN对CN比

179
00:05:42,440 --> 00:05:44,240
没有太多本质上的好处

180
00:05:44,360 --> 00:05:45,360
就Transformer

181
00:05:45,360 --> 00:05:47,600
就是说Attention比CN还是有好处的

182
00:05:47,600 --> 00:05:49,519
一个是说你视野更广

183
00:05:49,519 --> 00:05:50,879
就是能够看到很长的

184
00:05:50,879 --> 00:05:51,519
就是说

185
00:05:51,800 --> 00:05:54,199
Attention一层就能看到整个图片

186
00:05:54,199 --> 00:05:54,639
对吧

187
00:05:54,639 --> 00:05:55,800
但是你CN是不行

188
00:05:55,800 --> 00:05:57,079
CN是一层一层上去的

189
00:05:57,079 --> 00:05:57,480
对吧

