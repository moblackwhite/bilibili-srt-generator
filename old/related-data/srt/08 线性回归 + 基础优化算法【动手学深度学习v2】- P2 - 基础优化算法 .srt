1
00:00:00,000 --> 00:00:04,160
在我们提供线性回归的实现之前

2
00:00:04,160 --> 00:00:07,400
我们先来很简单的介绍一下优化方法

3
00:00:07,400 --> 00:00:11,599
我们在接下来会有一大章的内容来解释

4
00:00:11,599 --> 00:00:13,120
各种不一样的优化方法

5
00:00:13,120 --> 00:00:17,039
但是我们在这里先给大家做一个直观上的理解

6
00:00:20,480 --> 00:00:24,560
优化方法里面最常见的算法叫做梯度下降

7
00:00:24,800 --> 00:00:28,719
就是说当我一个模型没有显示解的时候

8
00:00:28,719 --> 00:00:29,760
我怎么办呢

9
00:00:30,239 --> 00:00:31,679
我的一个做法是说

10
00:00:31,679 --> 00:00:36,560
我首先挑选一个参数的随机初始值

11
00:00:36,880 --> 00:00:38,719
可以随便在什么地方都没关系

12
00:00:39,320 --> 00:00:41,200
然后我们计为W0

13
00:00:41,880 --> 00:00:43,519
在接下来的时刻里面

14
00:00:43,519 --> 00:00:45,439
我们不断的去更新W0

15
00:00:45,439 --> 00:00:48,120
使得它接近我们的最优解

16
00:00:49,200 --> 00:00:52,640
具体来说我们的更新法则是这样子的

17
00:00:53,120 --> 00:00:58,079
Wt它等于上一个时刻Wt-1

18
00:00:58,200 --> 00:00:59,519
减去一个eta

19
00:01:00,000 --> 00:01:04,159
这是一个标量和损失函数

20
00:01:04,159 --> 00:01:06,959
关于Wt-1处的梯度

21
00:01:08,159 --> 00:01:09,239
好 这有两个项

22
00:01:09,599 --> 00:01:10,920
第一个是它的梯度

23
00:01:11,200 --> 00:01:12,640
第二个是它的学习率

24
00:01:12,640 --> 00:01:13,519
这就eta

25
00:01:13,799 --> 00:01:16,280
我们来直观上解释一下这是什么意思

26
00:01:17,000 --> 00:01:18,519
我们来看一下这个图

27
00:01:18,519 --> 00:01:23,039
这是一个很简单的二次函数的一个等高图

28
00:01:24,920 --> 00:01:26,159
最优点在这个地方

29
00:01:26,159 --> 00:01:27,039
这是最小值

30
00:01:27,039 --> 00:01:28,280
外面是最大值

31
00:01:28,599 --> 00:01:29,680
如果看过地图的话

32
00:01:29,680 --> 00:01:31,599
也就是一个每一个圈

33
00:01:31,599 --> 00:01:34,719
就是函数值等于一个固定值

34
00:01:34,719 --> 00:01:36,480
同样值的一条曲线

35
00:01:37,400 --> 00:01:41,200
那么假设我们的W0是取在这个地方的话

36
00:01:41,519 --> 00:01:43,439
一般来说是随机取在一个地方

37
00:01:43,840 --> 00:01:47,920
那么我们记得我们解释过它的梯度

38
00:01:48,280 --> 00:01:52,920
也就是使得这个函数的值增加最快的方向

39
00:01:53,040 --> 00:01:54,159
那么它的负梯度

40
00:01:54,159 --> 00:01:56,599
就是它的值下降最快的方向

41
00:01:57,120 --> 00:01:58,600
比如说黄线的方向

42
00:01:58,880 --> 00:02:03,400
那么负的梯度的值指向于这个方向

43
00:02:03,840 --> 00:02:06,920
接下来学习率就是eta

44
00:02:07,240 --> 00:02:11,120
它代表是说我沿着这个方向每一次走多远

45
00:02:11,840 --> 00:02:14,640
比如说这个地方我走了大概那么远

46
00:02:14,920 --> 00:02:16,120
然后把这两个

47
00:02:16,120 --> 00:02:20,200
就是说整个这一项就代表是这一条

48
00:02:20,200 --> 00:02:22,160
这个点到这个点之间的向量

49
00:02:22,960 --> 00:02:25,960
好我们把W0和这个向量一加

50
00:02:25,960 --> 00:02:28,240
就可以拿得到W1的位置

51
00:02:28,759 --> 00:02:30,560
同样的话在W1处

52
00:02:30,800 --> 00:02:32,479
我们继续计算它的梯度

53
00:02:33,000 --> 00:02:34,919
那就是在这个点上梯度

54
00:02:34,919 --> 00:02:37,319
函数值下降最快的方向

55
00:02:37,479 --> 00:02:39,159
然后我再沿着它再走一步

56
00:02:40,159 --> 00:02:43,039
就是你可以认为我去爬山的时候

57
00:02:43,039 --> 00:02:44,400
我可以不走大路对吧

58
00:02:44,400 --> 00:02:46,319
我可以每一次沿着最陡的那条路

59
00:02:46,319 --> 00:02:47,359
一直走下去

60
00:02:47,400 --> 00:02:48,719
那么可以走到山底

61
00:02:49,560 --> 00:02:52,199
这就是梯度下降的一个直观解释

62
00:02:53,479 --> 00:02:57,000
所以这里面学习率就是步长

63
00:02:57,000 --> 00:03:01,240
它是一个叫做超参数的东西

64
00:03:01,479 --> 00:03:02,520
超参数的话

65
00:03:02,520 --> 00:03:04,400
它叫做hyperparameter

66
00:03:04,520 --> 00:03:08,360
它就是一个我们需要人为来指定的一个值

67
00:03:09,199 --> 00:03:11,719
我们接下来看一下这个东西怎么选

68
00:03:12,639 --> 00:03:14,960
首先我们不能选太小

69
00:03:15,599 --> 00:03:16,919
就是说如果选太小的话

70
00:03:16,920 --> 00:03:19,400
那么每一次走的步长很有限

71
00:03:19,560 --> 00:03:20,600
我们到达一个点

72
00:03:20,600 --> 00:03:22,920
我们需要走非常多的步走

73
00:03:23,080 --> 00:03:24,720
这个不是一件很好的事情

74
00:03:24,760 --> 00:03:27,680
这是因为计算梯度是一件很贵的事情

75
00:03:27,720 --> 00:03:28,880
我们之前有讲过

76
00:03:29,360 --> 00:03:31,480
在自动球道里面说过

77
00:03:31,520 --> 00:03:34,880
计算梯度是一件基本上是我们

78
00:03:34,880 --> 00:03:37,360
整个模型训练里面最贵的那一个部分

79
00:03:37,600 --> 00:03:40,080
所以我们尽量的要少来计算梯度

80
00:03:41,040 --> 00:03:42,160
但是反过来讲

81
00:03:42,200 --> 00:03:44,360
我们也不能走太大

82
00:03:44,920 --> 00:03:45,880
为什么是因为

83
00:03:45,879 --> 00:03:48,759
比如说我们这个地方一下子步子

84
00:03:48,759 --> 00:03:49,519
迈太大了

85
00:03:49,519 --> 00:03:52,079
就迈过了我们在下降的地方

86
00:03:52,079 --> 00:03:53,519
迈到了很远的地方

87
00:03:53,560 --> 00:03:56,280
使得我们一直在震荡

88
00:03:56,280 --> 00:03:58,280
并没有真正的在下降

89
00:03:58,639 --> 00:04:01,240
所以就是说我们学习率不能太大

90
00:04:01,240 --> 00:04:02,680
也不能太小

91
00:04:02,719 --> 00:04:07,680
我们接下来会有一系列的教程

92
00:04:07,680 --> 00:04:10,120
教大家怎么选取合适的学习率

93
00:04:12,240 --> 00:04:14,240
另外一个是说在实际中

94
00:04:14,240 --> 00:04:16,840
我们很少直接使用梯度下降

95
00:04:17,079 --> 00:04:19,840
宣读学习最常见的梯度下降版本

96
00:04:19,879 --> 00:04:22,680
叫做小批量随机梯度下降

97
00:04:23,400 --> 00:04:25,920
这是因为是说梯度下降里面

98
00:04:25,920 --> 00:04:27,560
我们每次计算梯度

99
00:04:27,600 --> 00:04:30,040
要对整个损失函数求导

100
00:04:30,400 --> 00:04:32,920
损失函数是对我们所有的样本的

101
00:04:32,920 --> 00:04:34,759
一个平均损失

102
00:04:35,280 --> 00:04:37,480
所以这意味着是说求一次梯度

103
00:04:37,480 --> 00:04:40,400
我们要把整个样本给你重新算一遍

104
00:04:40,600 --> 00:04:42,439
这个是很贵的一件事情

105
00:04:42,439 --> 00:04:45,160
它可能需要数分钟或者数个小时

106
00:04:45,319 --> 00:04:48,040
一般来说我们需要可能走个几百步

107
00:04:48,040 --> 00:04:49,439
或者几千步的样子

108
00:04:49,480 --> 00:04:51,319
这样子我们的计算代价太大了

109
00:04:52,560 --> 00:04:54,839
那么一个近似的办法怎么做呢

110
00:04:55,399 --> 00:04:57,199
就是我可以回忆一下我的损失

111
00:04:57,199 --> 00:05:01,759
就是我们所有的样本的损失的平均

112
00:05:02,480 --> 00:05:04,079
那么我们近似它的话

113
00:05:04,120 --> 00:05:06,639
我们可以随机采样第一个样本

114
00:05:06,800 --> 00:05:09,759
用它的损失的平均

115
00:05:09,800 --> 00:05:13,719
来近似于整个样本级上的损失的平均

116
00:05:14,639 --> 00:05:16,039
当你的b很大的时候

117
00:05:16,039 --> 00:05:18,360
这个近似当然很精确

118
00:05:18,480 --> 00:05:20,039
当你b等于小的时候

119
00:05:20,079 --> 00:05:21,800
它的近似不那么精确

120
00:05:21,839 --> 00:05:23,959
但是比你很小的时候

121
00:05:23,959 --> 00:05:25,439
计算它的梯度

122
00:05:25,639 --> 00:05:26,839
那就是比较容易

123
00:05:26,959 --> 00:05:28,759
因为梯度的计算复杂度

124
00:05:28,759 --> 00:05:31,879
是跟你样本的个数是线性相关的

125
00:05:33,039 --> 00:05:36,879
所以这里pb又叫做p量大小

126
00:05:37,039 --> 00:05:39,599
是另外一个重要的超参数

127
00:05:40,759 --> 00:05:42,199
同样的话

128
00:05:42,199 --> 00:05:44,399
p量大小不能太大

129
00:05:44,759 --> 00:05:46,159
也不能太小

130
00:05:46,839 --> 00:05:48,279
如果你太小的话

131
00:05:48,279 --> 00:05:49,480
一个问题是说

132
00:05:49,480 --> 00:05:52,120
我每次就算那么几个样本的梯度

133
00:05:52,159 --> 00:05:53,599
很难以并行

134
00:05:53,639 --> 00:05:56,680
今之后我们会利用GPU来做计算

135
00:05:56,719 --> 00:05:59,920
GPU的话动不动就上百上千个核

136
00:06:00,039 --> 00:06:01,719
当你p量太小

137
00:06:01,759 --> 00:06:02,920
那就是计算不好

138
00:06:02,920 --> 00:06:04,199
很大很好利用

139
00:06:04,480 --> 00:06:05,719
但你也不能太大

140
00:06:05,719 --> 00:06:06,639
太大的话

141
00:06:06,680 --> 00:06:08,439
你的内存跟你的p量大小

142
00:06:08,439 --> 00:06:10,079
很多时候是成正比的

143
00:06:10,319 --> 00:06:12,560
所以你的特别使用GPU的话

144
00:06:12,560 --> 00:06:15,719
内存是一个很大的一个瓶颈

145
00:06:16,079 --> 00:06:18,000
因为你可能就那么16GB

146
00:06:18,000 --> 00:06:19,800
或者32GB的内存

147
00:06:20,480 --> 00:06:21,680
接下来是说

148
00:06:22,000 --> 00:06:23,680
你也可能会浪费计算

149
00:06:23,879 --> 00:06:25,120
举一个极端的例子

150
00:06:25,680 --> 00:06:27,720
所有的样本都是一样的话

151
00:06:27,759 --> 00:06:30,040
那么你不管p量大小多大

152
00:06:30,079 --> 00:06:32,000
你用一个样本计算梯度

153
00:06:32,480 --> 00:06:33,600
还是用10个

154
00:06:33,600 --> 00:06:34,480
还是用100个

155
00:06:34,480 --> 00:06:36,120
计算的效果都是一样的

156
00:06:36,279 --> 00:06:37,040
所以的话

157
00:06:37,040 --> 00:06:38,640
假设你一个p量里面

158
00:06:38,960 --> 00:06:40,160
存在着大量的

159
00:06:40,160 --> 00:06:41,640
差不多的样本的时候

160
00:06:41,680 --> 00:06:43,760
那么你就在很多时候

161
00:06:43,760 --> 00:06:44,680
浪费了计算了

162
00:06:44,680 --> 00:06:45,800
所以也不要太大

163
00:06:46,400 --> 00:06:47,680
这就是p量大小

164
00:06:47,760 --> 00:06:50,000
这也是我们会去

165
00:06:50,000 --> 00:06:52,800
在今后来一系列的教程中间

166
00:06:52,800 --> 00:06:54,879
会给大家一些直观上的解释

167
00:06:55,360 --> 00:06:57,439
怎么样选择p量的大小

168
00:06:59,320 --> 00:07:00,200
好总结一下

169
00:07:00,200 --> 00:07:01,000
就是说

170
00:07:01,800 --> 00:07:02,640
梯度下降

171
00:07:02,640 --> 00:07:05,280
就是不断的沿着梯度的反方向

172
00:07:05,280 --> 00:07:07,320
来更新我们的模型求解

173
00:07:07,560 --> 00:07:08,720
它的好处是说

174
00:07:08,720 --> 00:07:11,840
我不需要知道显示解什么样子

175
00:07:11,880 --> 00:07:13,120
我只要不断的知道

176
00:07:13,120 --> 00:07:14,920
怎么求导数就行了

177
00:07:16,000 --> 00:07:17,960
我们知道在自动求导那一章

178
00:07:17,960 --> 00:07:18,960
我们讲过说

179
00:07:19,080 --> 00:07:21,240
我给你一个函数

180
00:07:21,240 --> 00:07:22,440
给你一个模型

181
00:07:22,480 --> 00:07:24,000
我的深度学习框架

182
00:07:24,000 --> 00:07:25,480
是能够帮你自动求导的

183
00:07:25,480 --> 00:07:26,360
所以的话

184
00:07:26,400 --> 00:07:28,680
这一块就是做的比较简单了

185
00:07:29,840 --> 00:07:31,080
另外一个是说

186
00:07:31,120 --> 00:07:32,080
小p量

187
00:07:32,200 --> 00:07:33,840
梯度随机梯度下降

188
00:07:34,120 --> 00:07:35,040
是深度学习

189
00:07:35,040 --> 00:07:36,320
默认的求解方法

190
00:07:36,360 --> 00:07:38,240
虽然我们还有更好的

191
00:07:39,320 --> 00:07:39,880
算法

192
00:07:39,880 --> 00:07:41,200
但是一般来说

193
00:07:41,200 --> 00:07:43,440
它是最稳定的

194
00:07:43,440 --> 00:07:44,960
而且是最简单的

195
00:07:45,000 --> 00:07:46,600
所以我们通常使用它

196
00:07:48,000 --> 00:07:50,200
其中小p量随机梯度下降

197
00:07:50,200 --> 00:07:52,520
有没有两个重要的超参数

198
00:07:52,560 --> 00:07:54,360
一个是p量的大小

199
00:07:54,400 --> 00:07:55,640
一个是学习率

200
00:07:55,680 --> 00:07:57,720
我们待会在今后给大家解释

201
00:07:57,720 --> 00:07:59,960
如何选择合适的p量大小

202
00:07:59,960 --> 00:08:01,080
和学习率

203
00:08:01,400 --> 00:08:01,720
好

204
00:08:01,720 --> 00:08:04,880
这就是非常简单的优化算法的介绍

205
00:08:04,920 --> 00:08:05,960
当然优化算法

206
00:08:05,960 --> 00:08:07,920
是一个非常大的方向

207
00:08:07,960 --> 00:08:11,160
我们可能会有几节课

208
00:08:11,160 --> 00:08:14,000
专门来讨论更好的

209
00:08:14,040 --> 00:08:16,240
更稳定的一些算法

210
00:08:16,280 --> 00:08:19,160
但是知道最简单的版本

211
00:08:19,160 --> 00:08:21,840
已经在接下来的几个星期里面

212
00:08:21,840 --> 00:08:23,400
我们先是够用了

