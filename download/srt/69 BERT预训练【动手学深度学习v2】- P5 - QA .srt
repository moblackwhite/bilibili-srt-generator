1
00:00:00,000 --> 00:00:04,400
好,我们有了BERT模型定义好了之后

2
00:00:04,400 --> 00:00:06,520
数据也准备好了之后

3
00:00:06,520 --> 00:00:09,240
那么我们就可以开始pre-trainBERT了

4
00:00:09,240 --> 00:00:14,679
pre-trainBERT就是说刚刚我们是把数据集读进来

5
00:00:14,679 --> 00:00:16,519
接下来我们定义我们的BERT

6
00:00:16,519 --> 00:00:17,760
就是刚刚我们定义的东西

7
00:00:17,760 --> 00:00:20,280
我们用了两个layer

8
00:00:20,280 --> 00:00:21,440
就两个transform layer

9
00:00:21,440 --> 00:00:23,400
就看一下这个东西在干嘛

10
00:00:23,400 --> 00:00:26,440
Number of hinters我们用的是128

11
00:00:26,440 --> 00:00:27,240
就相对比较小

12
00:00:27,240 --> 00:00:29,519
我们用的是肯定是比较小的东西

13
00:00:29,879 --> 00:00:32,240
不然的话大家劝不动

14
00:00:32,240 --> 00:00:33,920
然后就说hintersize是128

15
00:00:33,920 --> 00:00:37,840
意味着说每一个词我会抽取一个长了128倍的特征

16
00:00:37,840 --> 00:00:41,600
然后这个东西是在那个

17
00:00:41,600 --> 00:00:43,960
就是BERT transformer里面那个FFN

18
00:00:43,960 --> 00:00:44,759
中间那个hintersize

19
00:00:44,759 --> 00:00:46,079
我们就把它做大一点

20
00:00:46,079 --> 00:00:48,280
做到256

21
00:00:48,280 --> 00:00:50,399
Number of heads用的是2

22
00:00:50,399 --> 00:00:54,600
就是多头attention用的是2

23
00:00:54,600 --> 00:00:57,239
然后layer就是两个transformer block

24
00:00:57,280 --> 00:00:59,480
别的那些东西都是什么dropout

25
00:00:59,480 --> 00:01:01,200
key这个东西都是128

26
00:01:01,200 --> 00:01:03,400
东西都是128

27
00:01:04,600 --> 00:01:04,960
好

28
00:01:04,960 --> 00:01:06,920
然后我当然是说这个东西训练比较贵

29
00:01:07,040 --> 00:01:09,159
所以我们需要把所有的GPU都用起来

30
00:01:09,480 --> 00:01:11,400
Loss就是一个cross entropy loss

31
00:01:11,400 --> 00:01:13,480
因为你不管是你的语言模型

32
00:01:13,480 --> 00:01:15,159
还是你的下一个句子预测

33
00:01:15,159 --> 00:01:16,480
它其实就是个分类模型

34
00:01:16,480 --> 00:01:16,920
对吧

35
00:01:19,280 --> 00:01:19,520
好

36
00:01:19,520 --> 00:01:21,120
然后接下来就是说

37
00:01:21,120 --> 00:01:22,520
我给你一个

38
00:01:22,520 --> 00:01:24,560
然后算一个batch的loss

39
00:01:24,560 --> 00:01:26,599
这东西就是说

40
00:01:26,600 --> 00:01:27,720
把我们刚刚说白了

41
00:01:27,720 --> 00:01:29,360
就是把我们刚刚那个data

42
00:01:29,360 --> 00:01:29,880
iterator

43
00:01:30,120 --> 00:01:31,640
那些东西都丢进来

44
00:01:31,840 --> 00:01:33,240
就token segment

45
00:01:33,680 --> 00:01:34,600
value length

46
00:01:35,520 --> 00:01:36,320
要去

47
00:01:36,760 --> 00:01:38,720
语言模型要预测的那些位置

48
00:01:38,720 --> 00:01:40,240
丢到net里面来

49
00:01:40,520 --> 00:01:41,560
第一个我们不需要了

50
00:01:41,560 --> 00:01:43,040
就是说第一个我们就是

51
00:01:43,280 --> 00:01:44,720
因为我们是做预训练

52
00:01:45,440 --> 00:01:46,280
Find tuning需要

53
00:01:46,280 --> 00:01:47,240
预训练我们不需要

54
00:01:47,240 --> 00:01:49,480
就是BERT的encoder的输出

55
00:01:49,480 --> 00:01:51,440
我们只需要的就是语言模型

56
00:01:51,440 --> 00:01:54,000
和NSP的预测输出

57
00:01:55,000 --> 00:01:56,120
语言模型

58
00:01:57,040 --> 00:02:00,200
反正知道它其实就是一个分类模型

59
00:02:00,200 --> 00:02:00,680
对吧

60
00:02:00,799 --> 00:02:03,000
就把它丢到cross-entropy loss里面

61
00:02:03,200 --> 00:02:04,960
当然是说我们要把它

62
00:02:06,359 --> 00:02:07,319
跟

63
00:02:07,359 --> 00:02:08,520
跟那个

64
00:02:09,639 --> 00:02:10,840
那个weight乘一下

65
00:02:11,000 --> 00:02:12,000
就跟weight乘一下

66
00:02:12,000 --> 00:02:13,000
就是说这样子的话

67
00:02:13,000 --> 00:02:14,960
就是说被pad的那些东西

68
00:02:14,960 --> 00:02:16,400
我就不去算loss

69
00:02:17,360 --> 00:02:18,680
另外一个就是说

70
00:02:19,000 --> 00:02:19,840
对于你那个

71
00:02:20,039 --> 00:02:20,960
句子对预测

72
00:02:21,199 --> 00:02:22,479
那就也是丢进去

73
00:02:22,479 --> 00:02:24,039
算一个cross-entropy loss

74
00:02:24,560 --> 00:02:26,560
最后就是这两个loss相加

75
00:02:26,599 --> 00:02:28,479
就拿回了我们的整loss

76
00:02:28,679 --> 00:02:30,560
我们也没有做weight在这个地方

77
00:02:30,560 --> 00:02:32,359
就是反正

78
00:02:32,879 --> 00:02:34,319
你当然你可以考虑说

79
00:02:34,319 --> 00:02:35,959
两个之间

80
00:02:35,959 --> 00:02:37,839
你可以用一个权重把它加起来

81
00:02:37,839 --> 00:02:39,879
但是我们这里要直接加起来了

82
00:02:40,079 --> 00:02:41,599
就直接拿到了我们一个

83
00:02:41,839 --> 00:02:42,879
Batch的loss

84
00:02:44,759 --> 00:02:45,000
好

85
00:02:45,000 --> 00:02:45,919
拿到Batch loss之后

86
00:02:45,919 --> 00:02:47,599
我们就直接可以写我们的

87
00:02:47,919 --> 00:02:49,359
training那个东西了

88
00:02:50,039 --> 00:02:51,319
这东西看上去很长

89
00:02:51,319 --> 00:02:52,000
但实际上

90
00:02:52,959 --> 00:02:54,319
跟我们之前没什么区别

91
00:02:55,359 --> 00:02:56,000
就是说

92
00:02:56,000 --> 00:02:57,159
虽然就是说这个东西

93
00:02:57,519 --> 00:02:59,959
显得东西就是

94
00:03:00,439 --> 00:03:00,959
比较多

95
00:03:01,079 --> 00:03:02,239
实际上你可以看一下

96
00:03:02,239 --> 00:03:02,959
没什么东西

97
00:03:04,120 --> 00:03:05,159
Net是一样的

98
00:03:05,280 --> 00:03:07,280
Net我们要放到data parallel里面

99
00:03:07,280 --> 00:03:08,639
因为做多个GPU

100
00:03:09,639 --> 00:03:10,959
分类器用的是added

101
00:03:11,159 --> 00:03:13,039
然后画一下图

102
00:03:13,919 --> 00:03:15,000
另外有一个不一样的

103
00:03:15,000 --> 00:03:16,039
说我们这个地方

104
00:03:16,359 --> 00:03:18,199
是用的跟Bert是一样的

105
00:03:19,039 --> 00:03:20,719
就不用data epoc了

106
00:03:21,719 --> 00:03:22,879
他用的就是说

107
00:03:23,520 --> 00:03:25,400
看是你算了多少个step

108
00:03:25,400 --> 00:03:26,759
就算了多少个batch

109
00:03:27,199 --> 00:03:27,879
多少个epoc

110
00:03:28,000 --> 00:03:28,639
大家

111
00:03:28,639 --> 00:03:29,520
虽然我也不知道

112
00:03:29,520 --> 00:03:30,800
为什么大家不用多少个epoc

113
00:03:30,840 --> 00:03:33,199
可能是整个数据比较大

114
00:03:33,360 --> 00:03:34,319
就是说你

115
00:03:34,479 --> 00:03:36,800
你大家就很难实现

116
00:03:36,800 --> 00:03:37,919
说我真的就扫一遍

117
00:03:37,919 --> 00:03:40,479
因为数据都是几百G几个T

118
00:03:40,479 --> 00:03:42,719
所以每一次他就是真的就随机

119
00:03:42,719 --> 00:03:43,840
随机里面去

120
00:03:43,879 --> 00:03:44,639
读一块出来

121
00:03:44,639 --> 00:03:45,759
然后做一点读一块出来

122
00:03:45,759 --> 00:03:46,199
做一点

123
00:03:46,240 --> 00:03:48,919
所以就换算成data epoc

124
00:03:48,919 --> 00:03:50,039
可能比较难一点

125
00:03:50,199 --> 00:03:51,439
所以他就是直接说

126
00:03:51,439 --> 00:03:52,799
我就是用了多少个

127
00:03:53,079 --> 00:03:54,439
算多少个batch

128
00:03:56,239 --> 00:03:57,039
然后就是说

129
00:03:57,039 --> 00:03:59,120
每一次在iterator里面读出来

130
00:03:59,959 --> 00:04:00,560
至于对代码

131
00:04:00,560 --> 00:04:01,799
我们还没写的特别好

132
00:04:01,799 --> 00:04:02,359
就是说白了

133
00:04:02,359 --> 00:04:03,719
就copy到GPU上

134
00:04:03,719 --> 00:04:04,000
对吧

135
00:04:04,000 --> 00:04:05,039
把所有东西要copy到

136
00:04:05,039 --> 00:04:06,120
我们的GPU上

137
00:04:06,519 --> 00:04:10,239
然后丢到我们的net

138
00:04:10,280 --> 00:04:12,000
就算一个batch的loss

139
00:04:12,599 --> 00:04:14,959
然后再对L做backward

140
00:04:15,239 --> 00:04:16,159
再做更新

141
00:04:16,439 --> 00:04:17,039
就没了

142
00:04:17,199 --> 00:04:17,600
就没了

143
00:04:17,600 --> 00:04:18,079
对吧

144
00:04:18,360 --> 00:04:19,159
核心

145
00:04:19,480 --> 00:04:22,680
就虽然模型不一样

146
00:04:22,840 --> 00:04:23,680
实际上

147
00:04:24,960 --> 00:04:26,240
本质上没什么区别

148
00:04:26,240 --> 00:04:27,400
我觉得deep learning的好处

149
00:04:27,400 --> 00:04:28,160
真的就是

150
00:04:28,880 --> 00:04:32,000
圈你函数和网络的data

151
00:04:32,000 --> 00:04:33,080
都是可以分离开的

152
00:04:33,080 --> 00:04:33,680
就是说

153
00:04:33,920 --> 00:04:34,280
基本上

154
00:04:34,280 --> 00:04:35,600
虽然我们在圈的函数

155
00:04:35,680 --> 00:04:36,520
写的

156
00:04:37,840 --> 00:04:38,640
不同的章节

157
00:04:38,640 --> 00:04:39,640
写的有点不一样

158
00:04:39,879 --> 00:04:42,080
但实际上它的整体逻辑没区别

159
00:04:42,080 --> 00:04:42,920
就是说每一换的

160
00:04:42,920 --> 00:04:43,960
就是data iterator

161
00:04:44,000 --> 00:04:45,200
和你net

162
00:04:45,320 --> 00:04:45,760
对吧

163
00:04:45,760 --> 00:04:47,080
其实主要换的是两个东西

164
00:04:47,080 --> 00:04:48,360
别的东西都没怎么换过

165
00:04:48,360 --> 00:04:48,840
对吧

166
00:04:49,640 --> 00:04:50,280
Ok

167
00:04:50,439 --> 00:04:51,560
所以最后我们训练一下

168
00:04:52,240 --> 00:04:52,800
训练一下

169
00:04:52,800 --> 00:04:54,120
我们就训练的比较短

170
00:04:54,360 --> 00:04:56,280
我们训练的就训练了50个

171
00:04:56,280 --> 00:04:58,120
我们是训练50个batch

172
00:04:59,480 --> 00:05:01,480
当然明显的没有收敛

173
00:05:01,480 --> 00:05:01,800
对吧

174
00:05:01,800 --> 00:05:03,040
你可以看到我们

175
00:05:03,080 --> 00:05:05,400
首先我们训练的

176
00:05:05,640 --> 00:05:06,760
其实速度还行

177
00:05:06,760 --> 00:05:07,320
你看见没有

178
00:05:07,320 --> 00:05:09,560
就是说我们训练是每秒

179
00:05:09,600 --> 00:05:11,879
就能处理1200个句子

180
00:05:11,879 --> 00:05:12,480
对

181
00:05:13,040 --> 00:05:14,040
我们之前是说

182
00:05:14,040 --> 00:05:15,760
我们能处理多少个token

183
00:05:16,080 --> 00:05:17,200
就多少个词

184
00:05:17,240 --> 00:05:17,800
现在是说

185
00:05:17,800 --> 00:05:18,960
我能处理多少个句子

186
00:05:18,959 --> 00:05:20,039
就多少个example

187
00:05:20,079 --> 00:05:21,560
这个速度其实不慢的

188
00:05:22,560 --> 00:05:22,879
为什么

189
00:05:22,879 --> 00:05:24,959
是因为BERT就是一半个

190
00:05:24,959 --> 00:05:25,599
transformer

191
00:05:25,599 --> 00:05:25,959
对吧

192
00:05:25,959 --> 00:05:27,879
所以它其实比transformer

193
00:05:27,919 --> 00:05:30,799
在同样的那些超参数的情况下

194
00:05:30,799 --> 00:05:32,120
它其实要便宜一半的

195
00:05:32,120 --> 00:05:32,519
对吧

196
00:05:32,680 --> 00:05:34,199
但是当然BERT相对来说

197
00:05:34,199 --> 00:05:35,199
我们会做比较大

198
00:05:35,279 --> 00:05:36,879
所以BERT的训练确实是

199
00:05:37,319 --> 00:05:39,120
真正的BERT训练是比较慢的

200
00:05:40,279 --> 00:05:42,879
然后这里没有收敛

201
00:05:43,079 --> 00:05:44,319
就基本上没收敛

202
00:05:44,319 --> 00:05:45,719
你可能要训练个

203
00:05:46,520 --> 00:05:48,880
我们有训练

204
00:05:49,000 --> 00:05:50,800
我们在数据上有训练出来模型

205
00:05:50,960 --> 00:05:52,440
就是我们在

206
00:05:52,960 --> 00:05:55,160
接下来做预训练的时候

207
00:05:55,160 --> 00:05:56,280
就是做fine tune的时候

208
00:05:56,280 --> 00:05:58,360
用的是把50改成

209
00:05:59,160 --> 00:06:00,720
你可以改到一个10万

210
00:06:00,720 --> 00:06:01,600
或者10万

211
00:06:02,200 --> 00:06:03,480
改成10万的时候

212
00:06:03,760 --> 00:06:05,920
基本上是可以训练出一个

213
00:06:05,920 --> 00:06:07,920
真的还能用的一个模型

214
00:06:08,640 --> 00:06:10,280
现在我们就给大家演示一下

215
00:06:11,280 --> 00:06:12,000
BERT训练

216
00:06:12,000 --> 00:06:13,440
通常来说

217
00:06:13,600 --> 00:06:14,560
训练个

218
00:06:15,120 --> 00:06:15,959
100万左右

219
00:06:16,120 --> 00:06:18,079
在真正的数据上100万左右

220
00:06:19,160 --> 00:06:20,600
所以现在这个地方

221
00:06:20,600 --> 00:06:21,639
我们用了4块卡

222
00:06:22,040 --> 00:06:25,079
我们的batch size是多少

223
00:06:25,079 --> 00:06:25,279
来着

224
00:06:25,279 --> 00:06:26,199
512对吧

225
00:06:26,240 --> 00:06:26,879
512的话

226
00:06:27,000 --> 00:06:29,839
就是1秒钟能够处理个两个batch

227
00:06:30,079 --> 00:06:32,199
我们要训练100万个batch

228
00:06:32,680 --> 00:06:33,759
已经4个GPU了

229
00:06:33,759 --> 00:06:34,480
这个上面

230
00:06:34,920 --> 00:06:36,319
又100万的话

231
00:06:36,319 --> 00:06:39,279
那就是50万秒

232
00:06:39,439 --> 00:06:40,920
50万秒多少

233
00:06:41,120 --> 00:06:42,079
几天马要

234
00:06:42,079 --> 00:06:42,600
对吧

235
00:06:42,839 --> 00:06:43,319
OK

236
00:06:43,319 --> 00:06:45,879
所以这个就是BERT的训练

237
00:06:48,719 --> 00:06:49,439
训练完之后

238
00:06:49,759 --> 00:06:50,319
训练完之后

239
00:06:50,319 --> 00:06:51,519
我们就可以来表示

240
00:06:51,519 --> 00:06:53,079
我们的embedding了

241
00:06:53,159 --> 00:06:54,040
就给的net

242
00:06:54,040 --> 00:06:55,279
就BERT主要是用来做

243
00:06:55,279 --> 00:06:56,240
抽特征用的

244
00:06:56,719 --> 00:06:57,839
就给一个net

245
00:06:58,240 --> 00:06:59,759
给一个第一个句子

246
00:07:00,480 --> 00:07:01,680
第二个句子可以是空

247
00:07:02,240 --> 00:07:03,279
可以没有第二个句子

248
00:07:03,279 --> 00:07:04,240
因为很多应用的时候

249
00:07:04,240 --> 00:07:05,120
是没有第二个句子

250
00:07:05,120 --> 00:07:05,480
对吧

251
00:07:05,879 --> 00:07:07,600
然后就是说我们怎么做

252
00:07:07,600 --> 00:07:08,399
就是一样的

253
00:07:08,480 --> 00:07:11,079
我们拿到token和segment的表示

254
00:07:11,240 --> 00:07:13,120
把它表示成你的

255
00:07:14,000 --> 00:07:15,279
表示成你的tensor

256
00:07:15,479 --> 00:07:18,039
然后再做一个value length

257
00:07:18,039 --> 00:07:19,560
就是看它真正要多长

258
00:07:20,319 --> 00:07:21,639
这value length就是

259
00:07:22,519 --> 00:07:24,399
就是你的token是真正的有多长

260
00:07:25,680 --> 00:07:26,719
然后就给丢进去了

261
00:07:27,319 --> 00:07:28,360
丢进之后

262
00:07:28,680 --> 00:07:30,120
可以看到是我们不需要

263
00:07:30,120 --> 00:07:32,560
用两个loss的输出

264
00:07:32,560 --> 00:07:35,759
我们只需要对于token

265
00:07:36,079 --> 00:07:37,159
就每一个token

266
00:07:37,159 --> 00:07:39,719
它的特征的输出

267
00:07:40,040 --> 00:07:40,360
OK

268
00:07:40,360 --> 00:07:42,120
就是encoded x

269
00:07:42,240 --> 00:07:43,400
这就是我们BERT

270
00:07:43,400 --> 00:07:45,480
抽encoder抽出来特征

271
00:07:45,800 --> 00:07:47,360
这就是抽特征的用了

272
00:07:49,120 --> 00:07:49,560
举个例子

273
00:07:49,560 --> 00:07:49,920
就是说

274
00:07:49,920 --> 00:07:52,040
当然你可以说我的token是一个

275
00:07:52,160 --> 00:07:54,200
a crane is flying

276
00:07:54,560 --> 00:07:55,280
就一个句子

277
00:07:55,480 --> 00:07:56,640
我们丢进之后

278
00:07:56,680 --> 00:07:58,080
那么就会出来一个

279
00:07:58,440 --> 00:07:59,520
基本上你的

280
00:08:00,880 --> 00:08:02,720
这个东西是用来做分类的

281
00:08:02,720 --> 00:08:04,600
这个句子就第一个special token

282
00:08:04,600 --> 00:08:07,120
它的形状是一个128

283
00:08:07,120 --> 00:08:08,840
因为我们刚刚那个hidden size

284
00:08:08,840 --> 00:08:09,800
是128

285
00:08:11,960 --> 00:08:13,560
然后当然整个句子长度

286
00:08:13,560 --> 00:08:14,560
应该就是一个

287
00:08:14,560 --> 00:08:15,800
1×6×128

288
00:08:15,800 --> 00:08:16,600
因为这里要

289
00:08:17,880 --> 00:08:19,240
就是第一个special的

290
00:08:19,480 --> 00:08:21,200
有一个special的token在后面

291
00:08:21,200 --> 00:08:22,680
然后后面有一个分割符

292
00:08:22,680 --> 00:08:23,840
中间加4个token

293
00:08:23,840 --> 00:08:24,840
就是一共是6个

294
00:08:24,840 --> 00:08:26,240
所以是116128

295
00:08:26,600 --> 00:08:28,520
然后当然是说做分类的话

296
00:08:28,520 --> 00:08:29,480
就是直接用128

297
00:08:29,480 --> 00:08:30,920
那个embedding就做分类了

298
00:08:34,040 --> 00:08:34,400
好

299
00:08:34,400 --> 00:08:36,120
如果就是说

300
00:08:36,400 --> 00:08:37,200
我们做一个句子

301
00:08:37,200 --> 00:08:37,440
对

302
00:08:37,640 --> 00:08:38,240
就是

303
00:08:39,640 --> 00:08:40,200
然后

304
00:08:41,080 --> 00:08:41,800
driver来了

305
00:08:41,800 --> 00:08:43,800
然后他走了

306
00:08:44,280 --> 00:08:45,560
然后把它拼起来之后

307
00:08:45,840 --> 00:08:47,400
就是说第一个是有4个词

308
00:08:47,440 --> 00:08:48,600
后面是3个词

309
00:08:48,600 --> 00:08:49,520
7个词

310
00:08:49,560 --> 00:08:50,920
再加两个分割符

311
00:08:50,920 --> 00:08:52,400
再加一个分类符

312
00:08:52,400 --> 00:08:53,400
那就10个

313
00:08:53,440 --> 00:08:55,120
所以它的encoder的输出

314
00:08:55,240 --> 00:08:56,760
是1×10×128

315
00:08:57,160 --> 00:08:57,760
OK

316
00:08:58,080 --> 00:08:58,600
所以一样的

317
00:08:58,600 --> 00:08:59,440
你可以把

318
00:08:59,480 --> 00:09:00,280
然后就每一个词

319
00:09:00,280 --> 00:09:01,080
我们都有特征了

320
00:09:01,560 --> 00:09:03,520
所以这样的话

321
00:09:03,520 --> 00:09:04,760
我们就

322
00:09:05,320 --> 00:09:06,640
用wikitext

323
00:09:06,639 --> 00:09:08,720
构造了一个小一点的

324
00:09:08,720 --> 00:09:09,799
预训练数据集

325
00:09:09,799 --> 00:09:11,199
然后构造了一个小一点的

326
00:09:11,199 --> 00:09:11,639
BERT

327
00:09:11,840 --> 00:09:13,199
我们训练了一个模型

328
00:09:13,240 --> 00:09:15,000
但我们没训练完这个地方

329
00:09:15,039 --> 00:09:17,120
我们之后有一个训练好的模型

330
00:09:17,120 --> 00:09:18,879
我们之后会加载它

331
00:09:18,919 --> 00:09:20,199
然后用来做fine tuning

332
00:09:20,399 --> 00:09:21,919
然后现在我们看到说

333
00:09:22,279 --> 00:09:23,199
训练完之后

334
00:09:23,519 --> 00:09:24,720
不管是给一个句子

335
00:09:24,720 --> 00:09:25,840
还是一个句子

336
00:09:25,840 --> 00:09:26,879
对我的BERT

337
00:09:26,879 --> 00:09:28,039
都能够给你抽取

338
00:09:28,080 --> 00:09:28,720
每个词

339
00:09:28,720 --> 00:09:32,080
抽取一个常为128的一个特征

340
00:09:32,120 --> 00:09:33,679
然后我们主要是用来

341
00:09:33,679 --> 00:09:34,759
给之后的应用

342
00:09:34,759 --> 00:09:36,319
做fine tuning用的

