1
00:00:00,000 --> 00:00:04,480
这个图是2014年的时候我们农历新年那一天

2
00:00:04,480 --> 00:00:07,799
那时候我跟我老板在CMU装机器

3
00:00:07,799 --> 00:00:10,839
你看到这个芯片这个东西有4块卡

4
00:00:10,839 --> 00:00:12,800
那是我记得是

5
00:00:13,320 --> 00:00:16,440
我记得这是980就4块980

6
00:00:17,600 --> 00:00:20,120
这4块卡

7
00:00:20,600 --> 00:00:23,640
然后我们装了第一台花了5000块钱

8
00:00:23,640 --> 00:00:25,600
装了第一台的多GPU的机器

9
00:00:25,640 --> 00:00:28,960
然后这台机器没装好

10
00:00:28,960 --> 00:00:29,920
散热有问题

11
00:00:29,920 --> 00:00:31,679
因为GPU之间靠太近了

12
00:00:31,720 --> 00:00:32,679
然后我怎么办

13
00:00:32,799 --> 00:00:33,359
我就是的

14
00:00:33,359 --> 00:00:36,439
匹兹堡的纬度是在黑龙江差不多

15
00:00:36,439 --> 00:00:37,480
比北京要北一点

16
00:00:38,600 --> 00:00:41,280
所以我就是把机箱拆了

17
00:00:41,280 --> 00:00:42,600
放在实验室里面

18
00:00:42,600 --> 00:00:44,320
把实验室的窗开开冷却

19
00:00:44,359 --> 00:00:45,799
但实际上用了一个月之后

20
00:00:45,799 --> 00:00:46,840
烧掉了一块GPU

21
00:00:46,880 --> 00:00:48,280
这块GPU被烧掉了

22
00:00:48,760 --> 00:00:52,240
这是我们第一次装多GPU的犯了一个错误

23
00:00:52,240 --> 00:00:53,280
那是2014年

24
00:00:53,359 --> 00:00:55,079
已经是7年前了

25
00:00:55,760 --> 00:00:56,200
好

26
00:00:56,200 --> 00:00:57,120
给大家讲一下

27
00:00:57,280 --> 00:00:58,680
我们今天首先讲

28
00:00:59,080 --> 00:01:00,880
单机多卡并行

29
00:01:02,000 --> 00:01:03,480
就是说单机现在我们知道

30
00:01:03,480 --> 00:01:07,400
一台机器能装一般一个GPU

31
00:01:07,400 --> 00:01:09,040
或者可以装了16个GPU

32
00:01:10,080 --> 00:01:12,000
那么如果你有16个GPU的话

33
00:01:12,040 --> 00:01:14,080
在训练和预测的时候

34
00:01:14,120 --> 00:01:16,560
我们都可以将一个小批量切割

35
00:01:16,560 --> 00:01:18,600
多次到多个GPU

36
00:01:18,600 --> 00:01:21,400
每个GPU做一些运算

37
00:01:21,400 --> 00:01:24,000
或者把整个模型切割到多个GPU

38
00:01:24,000 --> 00:01:25,120
来完成加速

39
00:01:26,120 --> 00:01:29,040
就是说我们同样一个小批量

40
00:01:29,040 --> 00:01:31,240
让用多个GPU同时运行

41
00:01:31,240 --> 00:01:33,520
来一起完成计算梯度的过程

42
00:01:34,320 --> 00:01:38,200
就是说你常用的方案有数据并行

43
00:01:38,400 --> 00:01:40,680
模型并行和通道并行

44
00:01:40,680 --> 00:01:42,120
通道并行就是这两个模型

45
00:01:42,120 --> 00:01:43,200
这两个一起加起来

46
00:01:43,200 --> 00:01:47,360
所以我们主要关心的是数据并行

47
00:01:47,400 --> 00:01:48,320
而模型并行

48
00:01:48,320 --> 00:01:49,520
我们暂时也

49
00:01:49,560 --> 00:01:51,640
我们会大概讲一下这个事情

50
00:01:51,680 --> 00:01:52,719
不会深入

51
00:01:52,760 --> 00:01:57,480
就是说数据并行的原理是说

52
00:01:57,480 --> 00:01:58,879
我将一个小批量

53
00:01:59,200 --> 00:02:01,000
假设我有一个批量里面

54
00:02:01,000 --> 00:02:02,480
有128个样本

55
00:02:02,840 --> 00:02:04,519
然后我有两个GPU的话

56
00:02:04,840 --> 00:02:07,200
那么每个GPU会拿到64个样本

57
00:02:07,680 --> 00:02:09,360
就是说有n个GPU的话

58
00:02:09,360 --> 00:02:11,120
我会把小批量切成n块

59
00:02:11,240 --> 00:02:14,280
然后每个GPU拿到完整的参数

60
00:02:14,319 --> 00:02:16,639
来计算你这一块数据的梯度

61
00:02:17,680 --> 00:02:18,919
我们知道反正梯度

62
00:02:18,919 --> 00:02:22,639
就是你每个样本上的梯度加核

63
00:02:22,719 --> 00:02:24,280
所以的话每个GPU算完

64
00:02:24,280 --> 00:02:25,680
这一块数据的梯度的时候

65
00:02:25,680 --> 00:02:26,639
把它加起来

66
00:02:26,639 --> 00:02:29,599
就会完成整个小批量梯度的计算

67
00:02:30,719 --> 00:02:33,479
通常来说它的性能会比较好一点

68
00:02:33,919 --> 00:02:36,039
因为比较均匀分的

69
00:02:36,879 --> 00:02:38,439
模型并行是说

70
00:02:38,479 --> 00:02:40,479
我把模型分成n块

71
00:02:40,599 --> 00:02:43,919
比如说我有100层的ResNet

72
00:02:44,240 --> 00:02:45,560
我有两个GPU的话

73
00:02:45,599 --> 00:02:47,000
一个GPU拿50层

74
00:02:47,039 --> 00:02:48,680
另外一个GPU拿另外50层

75
00:02:49,199 --> 00:02:50,759
那么一个GPU是说

76
00:02:51,359 --> 00:02:53,639
第0号GPU拿到完整的数据

77
00:02:53,840 --> 00:02:56,039
把自己50层算完之后

78
00:02:56,039 --> 00:02:58,639
把结果给到GPU1

79
00:02:58,679 --> 00:03:00,280
GPU接着再往下算

80
00:03:01,039 --> 00:03:03,479
然后往上梯度的时候就倒过来

81
00:03:04,479 --> 00:03:05,639
就它的bug是什么

82
00:03:05,639 --> 00:03:08,399
它bug是说你GPU0算的时候

83
00:03:08,399 --> 00:03:10,039
GPU1可能在空的

84
00:03:10,680 --> 00:03:12,599
GPU1在算的时候

85
00:03:12,599 --> 00:03:14,159
GPU0可能在空的

86
00:03:14,239 --> 00:03:16,519
所以这个是它的一些

87
00:03:16,560 --> 00:03:17,639
它的性能

88
00:03:17,879 --> 00:03:19,639
相对来说比较难优化一点

89
00:03:19,920 --> 00:03:22,760
它主要用在的场景是说

90
00:03:22,800 --> 00:03:24,360
当你的模型大到

91
00:03:24,400 --> 00:03:26,400
你的单GPU的内存放不下

92
00:03:26,440 --> 00:03:28,280
特别是到transformer那一块的话

93
00:03:28,320 --> 00:03:29,840
你整个模型

94
00:03:30,520 --> 00:03:32,320
100个GB的话

95
00:03:32,360 --> 00:03:34,320
那么你单GPU算不下

96
00:03:34,360 --> 00:03:36,280
那么就Batch Test为1的时候

97
00:03:36,280 --> 00:03:37,960
我的单GPU还算不下的时候

98
00:03:37,960 --> 00:03:39,920
我就会把它分割到多个GPU来做

99
00:03:39,960 --> 00:03:42,520
就模型并行常用来这个事情

100
00:03:44,520 --> 00:03:46,000
因为我们给大家演示一下

101
00:03:46,000 --> 00:03:47,640
就数据并行是怎么做的

102
00:03:48,320 --> 00:03:50,160
假设我们有一些样本的地方

103
00:03:50,160 --> 00:03:51,200
有4块卡

104
00:03:52,720 --> 00:03:55,760
首先就是说我把这个样本切成4块

105
00:03:56,400 --> 00:03:58,680
就每个GPU读一块样本

106
00:03:59,960 --> 00:04:00,760
接下来就是说

107
00:04:00,760 --> 00:04:02,840
我假设我的参数放在一个地方

108
00:04:03,240 --> 00:04:05,640
我们假设就是一个p-value store了

109
00:04:06,040 --> 00:04:08,080
就是说你可能就放在内存里

110
00:04:09,080 --> 00:04:12,280
然后我就每个GPU把你的参数

111
00:04:12,280 --> 00:04:13,120
完整的参数

112
00:04:13,120 --> 00:04:14,480
整个模型参数拿回来

113
00:04:14,920 --> 00:04:16,040
就当前的梯度

114
00:04:16,360 --> 00:04:17,480
当前的参数

115
00:04:18,639 --> 00:04:20,399
接下来就是说

116
00:04:20,399 --> 00:04:22,919
每个GPU拿着自己那一块样本

117
00:04:22,919 --> 00:04:24,759
使用完整的参数

118
00:04:24,759 --> 00:04:26,159
就是你的parameter

119
00:04:26,159 --> 00:04:27,839
来计算你的梯度

120
00:04:27,839 --> 00:04:29,360
就是梯度是绿色的东西

121
00:04:30,919 --> 00:04:31,599
最后就是说

122
00:04:31,599 --> 00:04:33,959
你把梯度发回给你这一个

123
00:04:33,959 --> 00:04:35,199
存参数的东西

124
00:04:35,199 --> 00:04:37,560
我们是做一个抽象的存参数的

125
00:04:37,560 --> 00:04:39,399
有这个k-value store的话

126
00:04:39,399 --> 00:04:41,599
那就把梯度全部传给它

127
00:04:41,959 --> 00:04:42,879
就等于是这个东西

128
00:04:42,879 --> 00:04:44,479
可能会在CPU里面

129
00:04:45,039 --> 00:04:46,240
就传回到CPU

130
00:04:47,199 --> 00:04:49,000
然后在这一段

131
00:04:49,000 --> 00:04:51,280
它就把4个梯度给你加起来

132
00:04:52,040 --> 00:04:52,600
加起来之后

133
00:04:52,600 --> 00:04:53,680
就可以更新梯度了

134
00:04:55,319 --> 00:04:56,199
就更新梯度的意思

135
00:04:56,199 --> 00:04:56,920
就是说

136
00:04:56,920 --> 00:04:59,600
你就可以去

137
00:05:00,800 --> 00:05:01,759
下一次的话

138
00:05:02,079 --> 00:05:03,280
这就完成了一个batch

139
00:05:03,280 --> 00:05:04,480
一个小批量的运算

140
00:05:05,079 --> 00:05:06,120
下一个batch的话

141
00:05:06,120 --> 00:05:07,040
那就是样本

142
00:05:07,040 --> 00:05:08,079
新的样本一来

143
00:05:08,400 --> 00:05:09,600
GPU拿到这个东西

144
00:05:10,079 --> 00:05:11,840
然后又回过去

145
00:05:11,840 --> 00:05:13,600
然后又拿到

146
00:05:13,600 --> 00:05:16,120
就是说又去拿到新的更新后的参数

147
00:05:16,240 --> 00:05:17,199
用来计算梯度

148
00:05:18,759 --> 00:05:19,280
OK

149
00:05:19,280 --> 00:05:20,879
所以这就是数据变形了

150
00:05:21,560 --> 00:05:23,800
所以它相对来说

151
00:05:23,800 --> 00:05:24,560
它的

152
00:05:25,280 --> 00:05:26,960
它比较好变形

153
00:05:26,960 --> 00:05:29,079
是因为一般来说

154
00:05:29,079 --> 00:05:30,480
你每个人的梯度大小

155
00:05:30,480 --> 00:05:31,560
是一样的情况下

156
00:05:31,560 --> 00:05:32,319
这个梯

157
00:05:32,319 --> 00:05:34,560
就每一个人的数据量

158
00:05:34,560 --> 00:05:35,680
是一样的情况下

159
00:05:36,199 --> 00:05:38,160
一般你1848 256

160
00:05:38,160 --> 00:05:39,400
反正你GPU要么是2

161
00:05:39,400 --> 00:05:39,920
要么是4

162
00:05:39,920 --> 00:05:41,319
一般能整出的情况下

163
00:05:41,319 --> 00:05:43,400
所以每个人拿的计算量差不多

164
00:05:44,080 --> 00:05:46,760
就是说你拿它梯度

165
00:05:47,200 --> 00:05:48,280
就算梯度的时间

166
00:05:48,280 --> 00:05:49,360
大家都差不多

167
00:05:49,600 --> 00:05:50,400
就是说这一块

168
00:05:50,400 --> 00:05:51,680
就算梯度这一块时间

169
00:05:51,680 --> 00:05:52,960
就基本上是可以

170
00:05:52,960 --> 00:05:54,200
大家都是各搞各的

171
00:05:54,200 --> 00:05:55,000
不用管别人

172
00:05:55,840 --> 00:05:57,520
所以在这一块的话

173
00:05:58,280 --> 00:06:00,840
基本上这个变形程度是比较好的

174
00:06:00,960 --> 00:06:03,520
所以说数据变形相对来说

175
00:06:03,560 --> 00:06:04,920
它的性能比较高

176
00:06:06,960 --> 00:06:08,760
就是说总结一下

177
00:06:08,760 --> 00:06:10,000
就是很简单了

178
00:06:11,160 --> 00:06:12,240
就当一个模型

179
00:06:12,240 --> 00:06:14,040
用单卡做计算的时候

180
00:06:14,240 --> 00:06:16,360
就是说我一个模型能放进去

181
00:06:16,680 --> 00:06:17,519
如果我的单卡

182
00:06:17,519 --> 00:06:19,079
就是说能够跑个batch size为1

183
00:06:19,079 --> 00:06:21,079
2甚至大一点的话

184
00:06:21,160 --> 00:06:24,079
通常我们会用来数据逼行来做

185
00:06:25,439 --> 00:06:27,560
计算来拓展到多卡

186
00:06:29,079 --> 00:06:32,160
如果你的模型大到放不下的时候

187
00:06:32,160 --> 00:06:36,040
我们会把它用模型进行

188
00:06:36,040 --> 00:06:37,439
就把模型切开

189
00:06:37,439 --> 00:06:38,600
切到各个GPU上

190
00:06:39,600 --> 00:06:40,160
OK

191
00:06:40,160 --> 00:06:41,760
所以这个就是一个很简单的

192
00:06:41,760 --> 00:06:46,360
一个单机多卡的变形的一个介绍

193
00:06:46,400 --> 00:06:48,600
我们会在下个星期给大家讲

194
00:06:48,600 --> 00:06:49,960
怎么样来实现

195
00:06:49,960 --> 00:06:52,160
就从零开始给大家实现一下

196
00:06:52,160 --> 00:06:53,320
怎么样做这个东西

197
00:06:53,360 --> 00:06:55,640
但已经经常大家用的时候

198
00:06:55,640 --> 00:06:58,280
会用的高层的API长什么样子

