1
00:00:00,000 --> 00:00:04,320
好 我们来看一下NIN是怎么实现的

2
00:00:05,200 --> 00:00:08,080
首先我们当然跟前面实现VGG是一样

3
00:00:08,080 --> 00:00:10,560
我们先来实现这个NIN的快

4
00:00:11,120 --> 00:00:13,280
NIN的快你可以认为是说

5
00:00:13,280 --> 00:00:15,679
我们定一个函数 NIN BLOCK

6
00:00:16,160 --> 00:00:17,199
那么它的

7
00:00:17,199 --> 00:00:20,199
首先你告诉我你这个输入的通道数

8
00:00:20,199 --> 00:00:22,280
就是你的数据的通道数是什么样子

9
00:00:22,760 --> 00:00:25,320
和你要求的输出的通道数

10
00:00:26,120 --> 00:00:28,920
和你的第一个卷积层它的

11
00:00:28,920 --> 00:00:30,519
kernel的大小

12
00:00:30,519 --> 00:00:32,320
你的不符你的padding

13
00:00:33,719 --> 00:00:35,320
那么它实际做的就是说

14
00:00:35,320 --> 00:00:38,439
我首先定义第一个空2D

15
00:00:38,880 --> 00:00:40,039
那么你是

16
00:00:40,359 --> 00:00:42,920
你输入当时你的输入数据的通道数

17
00:00:43,439 --> 00:00:46,280
输出就直接变成了我的输出的通道数

18
00:00:47,799 --> 00:00:49,600
那么它的所谓的kernel size

19
00:00:49,960 --> 00:00:51,280
stripes padding

20
00:00:51,400 --> 00:00:53,120
都是用在第一个卷积层上面的

21
00:00:54,120 --> 00:00:54,800
好的

22
00:00:56,560 --> 00:00:58,920
接下来我用一个relu做计划函数

23
00:00:59,200 --> 00:01:01,040
接着我跟第二个

24
00:01:01,320 --> 00:01:04,240
第二个你会发现是说kernel size等于1

25
00:01:05,160 --> 00:01:06,000
那么你的padding

26
00:01:06,000 --> 00:01:06,640
默认等于0

27
00:01:06,640 --> 00:01:07,760
stripe默认等于1

28
00:01:08,439 --> 00:01:11,000
而且它是不会改变通道数的

29
00:01:11,000 --> 00:01:14,080
就是说它的输入通道和输出通道是同一个数

30
00:01:15,560 --> 00:01:18,240
那么我加一个relu的

31
00:01:18,240 --> 00:01:19,520
还是得到废卸性行

32
00:01:20,280 --> 00:01:22,760
然后再加一个1乘1的卷积

33
00:01:22,880 --> 00:01:23,680
还是一样的

34
00:01:24,560 --> 00:01:25,400
不改变通道数

35
00:01:26,400 --> 00:01:27,800
kernel size等于1

36
00:01:28,359 --> 00:01:29,120
padding等于0

37
00:01:29,359 --> 00:01:30,240
stripe等于1

38
00:01:31,000 --> 00:01:32,800
最后再加一个relu的计划函数

39
00:01:34,000 --> 00:01:35,600
这就是NNN的block

40
00:01:36,120 --> 00:01:37,960
基本上就是对着前面那个图

41
00:01:37,960 --> 00:01:39,200
我们实现一遍

42
00:01:41,680 --> 00:01:43,520
那么接下来看就是NNN的模型

43
00:01:44,280 --> 00:01:45,880
NNN的模型就是说

44
00:01:46,200 --> 00:01:50,120
它其实是一个比较早的一个网络结构

45
00:01:50,359 --> 00:01:53,359
还是从AlexNet那一块是过来的

46
00:01:53,920 --> 00:01:56,079
就是说你可以认为NNN这个模型

47
00:01:56,079 --> 00:01:58,120
是基于AlexNet那个架构的

48
00:01:58,520 --> 00:02:01,160
所以你看到它的设计上是很像AlexNet

49
00:02:03,080 --> 00:02:06,680
首先看到是说我第一个就是一个NNN block

50
00:02:07,280 --> 00:02:08,280
我们输入是1

51
00:02:08,280 --> 00:02:10,640
这是因为我们这个地方用的是灰度图

52
00:02:10,640 --> 00:02:11,159
所以是1

53
00:02:11,159 --> 00:02:12,000
跟之前是一样

54
00:02:12,800 --> 00:02:13,920
接下来到96

55
00:02:14,400 --> 00:02:16,080
96是AlexNet的96

56
00:02:16,080 --> 00:02:18,120
就是说这个参数是从AlexNet来的

57
00:02:18,719 --> 00:02:21,439
同样其实你会发现这个都是从AlexNet来的

58
00:02:21,800 --> 00:02:22,840
就是说说白了

59
00:02:22,840 --> 00:02:25,240
他就是把AlexNet的第一个卷积层搬过来

60
00:02:25,240 --> 00:02:28,879
在后面加了两个一层一的卷积层

61
00:02:30,120 --> 00:02:31,560
那么接下来就是Max pooling

62
00:02:31,560 --> 00:02:33,240
就是说你的kernel size等于3

63
00:02:33,240 --> 00:02:34,200
你的strategy等于2

64
00:02:34,200 --> 00:02:37,360
就把输入的高宽减半

65
00:02:38,920 --> 00:02:40,680
第二个你发现它的卷积层

66
00:02:40,680 --> 00:02:42,920
一样的是AlexNet的第二个卷积层

67
00:02:42,920 --> 00:02:45,000
就是从通道数96

68
00:02:46,040 --> 00:02:47,200
增加到252

69
00:02:47,200 --> 00:02:48,000
256

70
00:02:48,240 --> 00:02:49,680
然后你的kernel size等于5

71
00:02:49,680 --> 00:02:51,200
strategy等于2

72
00:02:52,680 --> 00:02:55,599
就是说你把AlexNet的第二个卷积层搬过来

73
00:02:55,599 --> 00:02:58,000
又给他加两个一层一的卷积层

74
00:02:58,680 --> 00:02:59,759
然后再加个Max pooling

75
00:02:59,759 --> 00:03:01,319
第三个其实也是一样

76
00:03:01,319 --> 00:03:04,200
最后的你是也是从AlexNet的就是说

77
00:03:04,200 --> 00:03:05,240
那个3x3

78
00:03:05,240 --> 00:03:06,280
但是它不一样的是说

79
00:03:06,280 --> 00:03:08,080
他没有加两个

80
00:03:08,080 --> 00:03:10,759
再在后面再加两个卷积层

81
00:03:11,040 --> 00:03:12,240
就是说这个就没干了

82
00:03:12,640 --> 00:03:13,920
然后你做完之后

83
00:03:13,920 --> 00:03:15,840
最后一个NN的block

84
00:03:16,240 --> 00:03:19,000
那他会把你的通道数降到10

85
00:03:19,000 --> 00:03:19,520
为什么

86
00:03:19,520 --> 00:03:21,599
是因为你的10是你的类别的个数

87
00:03:22,080 --> 00:03:23,840
这里我们用的是Fetch and List

88
00:03:23,840 --> 00:03:25,280
它的类别数是10

89
00:03:25,280 --> 00:03:28,199
所以我们这要加成10

90
00:03:28,199 --> 00:03:29,800
当你用一个别的数据集的话

91
00:03:29,800 --> 00:03:32,240
你需要把它改成你要的number of class

92
00:03:33,360 --> 00:03:34,560
kernel size等于3

93
00:03:34,560 --> 00:03:35,439
strategy

94
00:03:35,439 --> 00:03:36,120
padding等于1

95
00:03:36,120 --> 00:03:37,240
就跟之前是一样的

96
00:03:37,960 --> 00:03:39,759
最后我不用做Max pooling

97
00:03:39,759 --> 00:03:42,319
我就直接做一个Adaptive Average Pool

98
00:03:42,319 --> 00:03:45,680
就是一个全局的平均赤化层

99
00:03:46,840 --> 00:03:50,439
然后得到1乘以就是说告诉你说

100
00:03:50,439 --> 00:03:53,360
我就是要高宽都会变成1

101
00:03:53,360 --> 00:03:55,439
这是PyTorch的这一个写法

102
00:03:56,199 --> 00:03:58,960
最后我去因为你出来是一个4D的东西

103
00:03:59,240 --> 00:04:00,480
我最后把它flatten掉

104
00:04:00,879 --> 00:04:02,159
就是说把最后两个维度

105
00:04:02,159 --> 00:04:03,120
你已经是等于1了

106
00:04:03,120 --> 00:04:04,560
就是直接消掉

107
00:04:04,560 --> 00:04:09,080
就变成了一个batch size乘以10的一个矩阵

108
00:04:09,439 --> 00:04:10,000
就这个东西

109
00:04:10,560 --> 00:04:12,400
就直接可以拿去softmax

110
00:04:13,000 --> 00:04:15,960
所以这个就是NNN这个模型

111
00:04:16,600 --> 00:04:18,160
可以看到基本上就是说

112
00:04:18,160 --> 00:04:22,160
就是把AlexNet前面几层

113
00:04:22,160 --> 00:04:23,199
卷进层拿过来

114
00:04:23,360 --> 00:04:26,120
后面塞入1乘以的卷进层

115
00:04:26,399 --> 00:04:31,360
最后把通道数改成你要的类别数

116
00:04:31,360 --> 00:04:34,240
加一个全局的平均赤化层

117
00:04:34,319 --> 00:04:35,600
就完成了这个模型

118
00:04:37,079 --> 00:04:37,560
OK

119
00:04:38,560 --> 00:04:40,680
而我们可以看一下

120
00:04:40,680 --> 00:04:42,920
它的每一个快的输出形状

121
00:04:43,560 --> 00:04:47,280
我们还是给它一个24x24的一个输入

122
00:04:47,879 --> 00:04:50,920
你可以看到其实第一个就是

123
00:04:50,920 --> 00:04:52,280
第一个NNN block

124
00:04:52,280 --> 00:04:53,240
那就是

125
00:04:54,279 --> 00:04:57,160
因为你用的是AlexNet的第一个卷进层

126
00:04:57,160 --> 00:04:58,079
strat比较大

127
00:04:58,079 --> 00:05:00,120
就直接变成了54x54

128
00:05:00,680 --> 00:05:01,879
然后你的通道数95

129
00:05:02,319 --> 00:05:05,399
所以这个就是让你计算量就比较小了

130
00:05:06,000 --> 00:05:06,920
那么第二个

131
00:05:06,920 --> 00:05:09,160
你看到第二个NNN block的输出

132
00:05:09,320 --> 00:05:10,920
就是一个256

133
00:05:11,360 --> 00:05:13,600
通道数从96涨到了256

134
00:05:13,600 --> 00:05:15,840
但是因为你的第二个

135
00:05:15,840 --> 00:05:18,600
因为你是用了strat的2在这个地方

136
00:05:18,600 --> 00:05:20,720
所以你从54变成了26

137
00:05:20,920 --> 00:05:22,000
所以这里是没变的

138
00:05:22,600 --> 00:05:24,800
那么接下来当然你用一个max pool

139
00:05:24,800 --> 00:05:26,400
那就是不改变通道数

140
00:05:26,400 --> 00:05:27,560
但是高宽解满

141
00:05:28,360 --> 00:05:29,120
同样的道理

142
00:05:29,440 --> 00:05:32,000
这个NNN block就是把256

143
00:05:32,200 --> 00:05:35,080
增加到了384通道数

144
00:05:35,440 --> 00:05:36,680
高宽不变

145
00:05:37,320 --> 00:05:38,760
然后最后一个max pool

146
00:05:39,320 --> 00:05:42,880
5x5

147
00:05:43,960 --> 00:05:46,960
这个地方为什么我们有个drop out

148
00:05:48,720 --> 00:05:50,000
我觉得

149
00:05:53,760 --> 00:05:56,440
其实我也不知道drop out是哪里来的

150
00:05:56,440 --> 00:05:57,040
anyway

151
00:05:59,000 --> 00:06:02,960
然后再加了一个NNN block

152
00:06:02,959 --> 00:06:05,079
然后你发现这个地方

153
00:06:05,079 --> 00:06:09,239
就是从384变成了我要的类别数

154
00:06:09,879 --> 00:06:13,279
最后我的全局的平均持话层

155
00:06:13,279 --> 00:06:14,319
就把5x5

156
00:06:14,879 --> 00:06:16,599
就对每个通道数

157
00:06:16,599 --> 00:06:17,759
你都不管是多大

158
00:06:17,759 --> 00:06:19,879
我都把你平均值拿出来

159
00:06:19,879 --> 00:06:20,959
就变成了1x1

160
00:06:21,399 --> 00:06:24,719
最后的flatten就把最后两个维度消掉

161
00:06:24,759 --> 00:06:26,199
就拿到了1x10

162
00:06:26,799 --> 00:06:28,279
所以这个也就是我们要的

163
00:06:28,319 --> 00:06:31,560
我们要将一个输入就是批量大小

164
00:06:32,240 --> 00:06:34,560
维度是1x2

165
00:06:34,560 --> 00:06:36,040
2x4

166
00:06:36,040 --> 00:06:38,040
变成了一个常为10的一个向量

167
00:06:38,040 --> 00:06:39,720
因为10是我们的类别的个数

168
00:06:40,280 --> 00:06:40,480
好

169
00:06:40,480 --> 00:06:42,319
这就是我们整个模型要干的事情

170
00:06:42,319 --> 00:06:46,079
所以看到是NNN是怎么样

171
00:06:46,079 --> 00:06:48,240
把我们输入一步步变下去的

172
00:06:49,480 --> 00:06:49,840
OK

173
00:06:49,840 --> 00:06:50,639
所以核心是这样

174
00:06:50,639 --> 00:06:54,040
是说第一次的卷积层大量的

175
00:06:55,040 --> 00:06:57,600
通过常为4的不服

176
00:06:57,600 --> 00:06:58,399
把它减小

177
00:06:58,440 --> 00:07:01,200
然后接下来是通过最大

178
00:07:01,840 --> 00:07:03,280
吃话层来

179
00:07:04,520 --> 00:07:05,360
每次减半

180
00:07:05,360 --> 00:07:06,280
最后是的

181
00:07:06,280 --> 00:07:09,680
最后就是一个使用一个全剧的吃话层

182
00:07:09,680 --> 00:07:12,680
来得到一个把5x5变成1x1

183
00:07:12,680 --> 00:07:14,040
这就是NNN的做法

184
00:07:16,600 --> 00:07:17,360
当我们来看一下

185
00:07:17,360 --> 00:07:18,840
就是说你的训练效果

186
00:07:19,680 --> 00:07:20,840
这个训练效果就是说

187
00:07:20,840 --> 00:07:23,560
跟我们之前用的是一样的

188
00:07:24,040 --> 00:07:25,960
我们对learning rate取了0.1

189
00:07:26,600 --> 00:07:29,120
同样的迭代10次数据

190
00:07:29,120 --> 00:07:30,960
Batch size是188

191
00:07:30,960 --> 00:07:33,520
我们的Data set仍然是我们之前

192
00:07:33,520 --> 00:07:34,560
我们的FreshM list

193
00:07:35,840 --> 00:07:37,720
然后我们就是调用我们之前实现

194
00:07:37,720 --> 00:07:39,320
这个函数来做训练

195
00:07:39,480 --> 00:07:42,120
我们大家如果不记得了

196
00:07:42,120 --> 00:07:43,560
或者是没有听过的话

197
00:07:43,560 --> 00:07:45,880
可以去看一下前面的视频

198
00:07:45,880 --> 00:07:48,360
我们介绍这个函数是怎么样实现的

199
00:07:49,440 --> 00:07:50,840
所以可以看到是说

200
00:07:51,680 --> 00:07:52,560
可以看到这张图

201
00:07:52,759 --> 00:07:53,800
我同样的是说

202
00:07:53,800 --> 00:07:56,319
我是在直播开始之前

203
00:07:56,839 --> 00:07:58,040
运行的下这个算法

204
00:07:58,600 --> 00:07:59,759
这样因为他跑起来

205
00:07:59,759 --> 00:08:00,879
大概也跑了几分钟

206
00:08:01,120 --> 00:08:03,600
我就之前就预先把它重新运行了一次

207
00:08:05,000 --> 00:08:05,800
可以看到是说

208
00:08:05,800 --> 00:08:07,759
它其实精度也不算很高

209
00:08:09,000 --> 00:08:09,840
你也看到是说

210
00:08:09,840 --> 00:08:12,040
你的圈你的accuracy是0.86

211
00:08:12,040 --> 00:08:13,959
那test是0.83

212
00:08:14,160 --> 00:08:15,319
所以这个大家知道

213
00:08:15,319 --> 00:08:17,120
跟我们之前比不算高

214
00:08:18,160 --> 00:08:22,360
然后你的训练速度是3232

215
00:08:22,400 --> 00:08:23,720
example per second

216
00:08:24,400 --> 00:08:25,400
就我们可以对比一下

217
00:08:25,400 --> 00:08:27,800
我们之前跟我们的

218
00:08:29,240 --> 00:08:30,280
我们来对比一下

219
00:08:30,280 --> 00:08:33,320
之前跟我们的AlexNet会怎么样

220
00:08:33,960 --> 00:08:35,680
大家记得AlexNet是

221
00:08:35,680 --> 00:08:37,560
因为他很多是基于AlexNet来的

222
00:08:38,320 --> 00:08:40,480
我们来看一下AlexNet是

223
00:08:40,960 --> 00:08:44,360
到底是长什么样子

224
00:08:44,680 --> 00:08:45,519
结果是什么样子

225
00:08:45,519 --> 00:08:50,319
OK

226
00:08:50,319 --> 00:08:55,319
这个是AlexNet的结果

227
00:08:56,919 --> 00:08:57,799
可以看到是说

228
00:08:57,799 --> 00:08:58,919
其实我们这个地方

229
00:08:59,399 --> 00:09:01,360
我们的精度还没有AlexNet高

230
00:09:01,360 --> 00:09:03,079
AlexNet是0.88

231
00:09:03,079 --> 00:09:07,720
我们是0.83

232
00:09:08,240 --> 00:09:10,039
我们的速度是3232

233
00:09:12,240 --> 00:09:15,360
就是说速度其实也没有比AlexNet高

234
00:09:15,360 --> 00:09:16,000
太多

235
00:09:16,519 --> 00:09:20,600
这是因为我们额外加入了大量的

236
00:09:20,600 --> 00:09:21,960
1乘1的卷积层

237
00:09:22,000 --> 00:09:25,480
确实是使得你的计算会变慢

238
00:09:26,000 --> 00:09:27,159
这是因为

239
00:09:27,279 --> 00:09:29,399
我觉得主要是因为还是因为

240
00:09:29,399 --> 00:09:31,440
我们这个数据集相对来说比较少

241
00:09:31,960 --> 00:09:33,680
实际上在ImageNet上面

242
00:09:33,680 --> 00:09:36,279
NIN你是可以做到比AlexNet稍微好

243
00:09:36,279 --> 00:09:37,120
那么一点点的

244
00:09:37,560 --> 00:09:39,840
就是说NIN曾经也是在ImageNet上

245
00:09:39,840 --> 00:09:41,279
拿过非常好的成绩

246
00:09:41,519 --> 00:09:43,279
所以但是在我们这个数据上

247
00:09:43,279 --> 00:09:45,079
你其实还目前看不出

248
00:09:45,120 --> 00:09:47,799
太看不出我们到底是1乘1的卷

249
00:09:47,799 --> 00:09:49,639
积给你带来多大的好处

250
00:09:50,199 --> 00:09:52,639
但是我们在之后讲GoogleNet的时候

251
00:09:52,639 --> 00:09:54,000
会看到GoogleNet

252
00:09:54,000 --> 00:09:56,079
怎么更创造性的使用1乘1的卷

253
00:09:56,079 --> 00:09:56,439
积

254
00:09:57,199 --> 00:09:57,480
好

255
00:09:57,480 --> 00:10:00,319
我们NIN就讲到这个地方

256
00:10:00,480 --> 00:10:01,039
我们

