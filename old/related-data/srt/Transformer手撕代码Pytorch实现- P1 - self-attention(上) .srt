1
00:00:00,000 --> 00:00:02,000
欢迎回来

2
00:00:02,000 --> 00:00:04,000
今天呢我们讲这篇

3
00:00:04,000 --> 00:00:08,000
Attention is all you need

4
00:00:08,000 --> 00:00:11,000
这篇论文是非常著名的Transform

5
00:00:11,000 --> 00:00:14,000
它的结果来源

6
00:00:14,000 --> 00:00:19,000
那经典的这个Transformer的图呢

7
00:00:19,000 --> 00:00:23,000
就是在论文里面这张图

8
00:00:23,000 --> 00:00:26,000
这个Transformer自NLP火了之后呢

9
00:00:26,000 --> 00:00:29,000
在视觉领域也诞生了非常多的成果

10
00:00:29,000 --> 00:00:33,000
比如说像VIT啦DETR啦

11
00:00:33,000 --> 00:00:36,000
包括基于DETR的各种变体

12
00:00:36,000 --> 00:00:38,000
有非常多非常多的应用

13
00:00:38,000 --> 00:00:40,000
Self-Attention机制

14
00:00:40,000 --> 00:00:45,000
它带给世界这种更多的可能性

15
00:00:45,000 --> 00:00:47,000
现在也成为越来越多的

16
00:00:47,000 --> 00:00:49,000
研究者关注的问题

17
00:00:49,000 --> 00:00:51,000
所以今天我们就来去

18
00:00:51,000 --> 00:00:55,000
手撕Transformer这样一个代码

19
00:00:55,000 --> 00:00:58,000
我们先观察一下这样一个结构

20
00:00:58,000 --> 00:01:01,000
它分为Encoder和Decoder两部分

21
00:01:01,000 --> 00:01:03,000
Encoder负责编码

22
00:01:03,000 --> 00:01:07,000
对句子或者图片进行编码

23
00:01:07,000 --> 00:01:10,000
然后把编码后的结果送给Decoder

24
00:01:10,000 --> 00:01:13,000
可以看到这有两个输入

25
00:01:13,000 --> 00:01:17,000
送给Decoder第二个这个Multi-Head Attention

26
00:01:17,000 --> 00:01:19,000
多头注意力

27
00:01:19,000 --> 00:01:21,000
然后做Decoder解码

28
00:01:21,000 --> 00:01:24,000
所以我们实现它的第一步就是

29
00:01:24,000 --> 00:01:26,000
从左至右

30
00:01:26,000 --> 00:01:28,000
从内到外

31
00:01:28,000 --> 00:01:31,000
可以看到其实对于整个Transformer结构来说

32
00:01:31,000 --> 00:01:33,000
有一些我们熟悉的点

33
00:01:33,000 --> 00:01:35,000
比如说这个Add和Norm

34
00:01:35,000 --> 00:01:39,000
Normal的话就是这个Numeration

35
00:01:39,000 --> 00:01:40,000
就是政策化的

36
00:01:40,000 --> 00:01:43,000
Add的话就是ResNet的那个加

37
00:01:43,000 --> 00:01:44,000
把两个加起来

38
00:01:44,000 --> 00:01:48,000
然后这个Feedforward不就是反馈吗

39
00:01:48,000 --> 00:01:52,000
它里面可能就是扩充一下我们的层

40
00:01:52,000 --> 00:01:55,000
所以说整个这样一个结构

41
00:01:55,000 --> 00:01:57,000
如果说这样一个结构我们不熟悉的话

42
00:01:57,000 --> 00:01:59,000
这个Solver Max我们也很熟悉

43
00:01:59,000 --> 00:02:02,000
不熟悉的地方就是这个多头注意力

44
00:02:02,000 --> 00:02:05,000
Multi-Head Attention

45
00:02:05,000 --> 00:02:09,000
这个多头注意力和右边这个Mask Multi-Head Attention

46
00:02:09,000 --> 00:02:11,000
为什么这儿加了个Mask

47
00:02:11,000 --> 00:02:13,000
大家既然来听这个代码课

48
00:02:13,000 --> 00:02:16,000
说明大家对Transformer有理解了

49
00:02:16,000 --> 00:02:20,000
就是因为右边的话

50
00:02:20,000 --> 00:02:23,000
这个Decoder的时候还会把这个

51
00:02:23,000 --> 00:02:26,000
我们的Patch输入给这个Decoder

52
00:02:26,000 --> 00:02:32,000
因为你的输入就是你的输出

53
00:02:32,000 --> 00:02:35,000
所以避免你在输出前一个字的时候

54
00:02:35,000 --> 00:02:37,000
你已经看了后面的结果

55
00:02:37,000 --> 00:02:39,000
所以它要加一个Mask延码

56
00:02:39,000 --> 00:02:43,000
把后面的东西给码掉

57
00:02:43,000 --> 00:02:44,000
那这个具体理论

58
00:02:44,000 --> 00:02:48,000
大家可以去看更多的关于Transformer的理解

59
00:02:48,000 --> 00:02:50,000
所以我们今天就来实现

60
00:02:50,000 --> 00:02:53,000
第一个部分多头注意力机制

61
00:02:53,000 --> 00:02:56,000
也就是Self-Attention的关键

62
00:02:56,000 --> 00:02:58,000
也就是这个式子

63
00:02:58,000 --> 00:02:59,000
我们现在开始

64
00:02:59,000 --> 00:03:04,000
首先和我们之前两次课讲的都一样

65
00:03:04,000 --> 00:03:10,000
我们现在这儿改一下Transformer

66
00:03:10,000 --> 00:03:11,000
然后呢

67
00:03:11,000 --> 00:03:12,000
首先我们把这个图

68
00:03:12,000 --> 00:03:16,000
还是像我们之前一样给贴过来

69
00:03:16,000 --> 00:03:19,000
需要把它转成Markdown的

70
00:03:19,000 --> 00:03:20,000
可是

71
00:03:26,000 --> 00:03:29,000
我们把这个多头注意力弄过来

72
00:03:29,000 --> 00:03:32,000
然后我们把这个公式也拉过来

73
00:03:32,000 --> 00:03:35,000
因为我们这一课就是要去实现这样一个多头注意力

74
00:03:35,000 --> 00:03:38,000
也就是Self-Attention的关键

75
00:03:39,000 --> 00:03:41,000
这儿也换成Markdown

76
00:03:46,000 --> 00:03:49,000
好

77
00:03:49,000 --> 00:03:51,000
我们可以写了

78
00:03:51,000 --> 00:03:52,000
老规矩

79
00:03:52,000 --> 00:03:54,000
还是先导入我们需要的包

80
00:03:54,000 --> 00:03:56,000
Import Torch

81
00:03:56,000 --> 00:03:58,000
然后Import

82
00:03:58,000 --> 00:04:01,000
And Torch.n

83
00:04:05,000 --> 00:04:07,000
导入我们神经的模块

84
00:04:08,000 --> 00:04:09,000
OK

85
00:04:09,000 --> 00:04:10,000
完成这步之后

86
00:04:10,000 --> 00:04:12,000
我们就可以写我们的代码

87
00:04:12,000 --> 00:04:14,000
首先我们这一块部分呢

88
00:04:14,000 --> 00:04:17,000
我们也是给它用类来打包

89
00:04:17,000 --> 00:04:19,000
跟我们之前写的所有网络一样

90
00:04:19,000 --> 00:04:20,000
我们先给它起个名字

91
00:04:20,000 --> 00:04:22,000
它是Self-Attention

92
00:04:24,000 --> 00:04:28,000
然后它同样要继承UnderModule这个类

93
00:04:30,000 --> 00:04:32,000
继承这个类之后呢

94
00:04:33,000 --> 00:04:36,000
它还是我们之前的框架

95
00:04:36,000 --> 00:04:38,000
首先是先初始化

96
00:04:38,000 --> 00:04:40,000
就是初始化我们需要的一些模块

97
00:04:40,000 --> 00:04:42,000
一个参数Self

98
00:04:42,000 --> 00:04:45,000
然后第二个就是Forward

99
00:04:47,000 --> 00:04:49,000
Forward

100
00:04:50,000 --> 00:04:54,000
里面参数我们边写边补

101
00:04:55,000 --> 00:04:58,000
首先想一下我们在这个里面

102
00:04:58,000 --> 00:04:59,000
需要适应哪些东西

103
00:04:59,000 --> 00:05:00,000
这个初始化

104
00:05:01,000 --> 00:05:02,000
多头注意力

105
00:05:02,000 --> 00:05:04,000
可以看右边这个图

106
00:05:05,000 --> 00:05:07,000
这个QKV就是QKV三个矩阵

107
00:05:07,000 --> 00:05:09,000
可以看到后面有重影

108
00:05:09,000 --> 00:05:11,000
这个重影其实就是多头

109
00:05:11,000 --> 00:05:13,000
我们把同样一个QKV

110
00:05:13,000 --> 00:05:15,000
分成了这么多个头

111
00:05:15,000 --> 00:05:16,000
也就是这么多个部分

112
00:05:17,000 --> 00:05:19,000
并行地给到这个

113
00:05:20,000 --> 00:05:23,000
典成的Attention的注意力机制这个位置

114
00:05:23,000 --> 00:05:26,000
Scale.ProductAttention这个位置

115
00:05:27,000 --> 00:05:28,000
然后做完之后

116
00:05:28,000 --> 00:05:29,000
Concate

117
00:05:29,000 --> 00:05:33,000
把QKV的多头处理的结果再拼到一块

118
00:05:34,000 --> 00:05:35,000
Concate

119
00:05:35,000 --> 00:05:37,000
然后走一个线性层输出

120
00:05:38,000 --> 00:05:40,000
这是我们的Transformer结构

121
00:05:41,000 --> 00:05:46,000
所以说可以看到

122
00:05:46,000 --> 00:05:48,000
首先要把QKV

123
00:05:49,000 --> 00:05:50,000
它要分块

124
00:05:51,000 --> 00:05:52,000
分头

125
00:05:53,000 --> 00:05:54,000
我们这就可以写了

126
00:05:56,000 --> 00:05:57,000
所以它需要什么

127
00:06:00,000 --> 00:06:02,000
需要我们的块的大小

128
00:06:03,000 --> 00:06:06,000
大家理解Transformer之后

129
00:06:06,000 --> 00:06:07,000
大家会知道

130
00:06:07,000 --> 00:06:10,000
在图像里面会把每个图像切块

131
00:06:10,000 --> 00:06:11,000
然后拉伸

132
00:06:12,000 --> 00:06:13,000
就是VIT的一个做法

133
00:06:14,000 --> 00:06:15,000
包括后面有Spin Transformer

134
00:06:15,000 --> 00:06:17,000
可能会加一点变形

135
00:06:18,000 --> 00:06:19,000
其实它都是

136
00:06:19,000 --> 00:06:21,000
不管是LRP还是CV

137
00:06:21,000 --> 00:06:23,000
都是把它变成一个Patch

138
00:06:23,000 --> 00:06:25,000
一个项链

139
00:06:26,000 --> 00:06:28,000
然后把它拉伸之后

140
00:06:28,000 --> 00:06:30,000
再送给这个多头注意力机制

141
00:06:30,000 --> 00:06:32,000
所以第一个要输的就是

142
00:06:32,000 --> 00:06:34,000
我们输的这个Patch的大小

143
00:06:34,000 --> 00:06:35,000
也就是Empty Size

144
00:06:36,000 --> 00:06:38,000
然后第二个就是我们的几个头

145
00:06:40,000 --> 00:06:41,000
多头多头

146
00:06:41,000 --> 00:06:44,000
我们得有头才能去进行控制

147
00:06:44,000 --> 00:06:47,000
因为我们初始化

148
00:06:47,000 --> 00:06:48,000
这个Self-Attention的时候

149
00:06:49,000 --> 00:06:52,000
我们可以这样设计之后

150
00:06:52,000 --> 00:06:55,000
就可以设计我们多头注意力机制

151
00:06:55,000 --> 00:06:56,000
输入的Patch大小

152
00:06:57,000 --> 00:06:58,000
以及我们输入几个头

153
00:06:59,000 --> 00:07:00,000
这样都可以自由的控制

154
00:07:01,000 --> 00:07:02,000
所以要这两个参数

155
00:07:03,000 --> 00:07:06,000
然后我们复出Empty Size

156
00:07:06,000 --> 00:07:09,000
就等于我们输入的Empty Size

157
00:07:10,000 --> 00:07:13,000
然后我们的程序里面掉了头

158
00:07:13,000 --> 00:07:15,000
就等于我们输入的头数

159
00:07:17,000 --> 00:07:18,000
这时候呢

160
00:07:20,000 --> 00:07:24,000
我们知道我们现在输入的Patch大小了

161
00:07:24,000 --> 00:07:25,000
也知道我们输入了几个头

162
00:07:26,000 --> 00:07:28,000
我们就可以看一下

163
00:07:28,000 --> 00:07:31,000
我们这个多头注意力机制的头的围堵大小

164
00:07:32,000 --> 00:07:34,000
我们把它叫做HighD

165
00:07:34,000 --> 00:07:39,000
这就等于这个磁相量的大小

166
00:07:40,000 --> 00:07:42,000
我们是向下取整

167
00:07:42,000 --> 00:07:44,000
除以这个High的数

168
00:07:46,000 --> 00:07:53,000
就是我们每一个头里面包含的磁相量的围堵大小

169
00:07:54,000 --> 00:07:57,000
因为你想我们输入的这个磁相量是这么大的

170
00:07:57,000 --> 00:07:59,000
QKV分别是这么大的

171
00:07:59,000 --> 00:08:02,000
然后我们封了High这个头

172
00:08:02,000 --> 00:08:06,000
所以我们整个这个每部分磁相量

173
00:08:06,000 --> 00:08:08,000
除以High这个头

174
00:08:08,000 --> 00:08:11,000
就是每个头的一个大小

175
00:08:11,000 --> 00:08:12,000
围堵大小

176
00:08:12,000 --> 00:08:13,000
好

177
00:08:13,000 --> 00:08:16,000
那还有什么东西呢

178
00:08:18,000 --> 00:08:25,000
既然我们这个输入的磁相量大小和这个头的围堵已经确定

179
00:08:25,000 --> 00:08:28,000
那我们下面就可以去设置一下

180
00:08:28,000 --> 00:08:30,000
我们输入的三个这个QKV的矩阵

181
00:08:30,000 --> 00:08:33,000
QKV其实他们输入的是一样的

182
00:08:34,000 --> 00:08:36,000
我们先定义第一个Values

183
00:08:36,000 --> 00:08:39,000
我们先给他进行一个初始化

184
00:08:41,000 --> 00:08:43,000
他第一个参数

185
00:08:43,000 --> 00:08:44,000
n.liner

186
00:08:44,000 --> 00:08:45,000
大家还记得吗

187
00:08:45,000 --> 00:08:46,000
之前我们讲过

188
00:08:46,000 --> 00:08:48,000
第一个参数是输入的围堵大小

189
00:08:49,000 --> 00:08:50,000
输入的围堵大小

190
00:08:50,000 --> 00:08:51,000
amplifySize

191
00:08:52,000 --> 00:08:55,000
QKV都要先经过一个线性层

192
00:08:55,000 --> 00:08:58,000
再给这个ScalableProtocol测试

193
00:08:59,000 --> 00:09:02,000
所以我们这就定义这个线性层

194
00:09:02,000 --> 00:09:03,000
QKV

195
00:09:03,000 --> 00:09:04,000
首先输入的大小

196
00:09:04,000 --> 00:09:06,000
就是我们的amplifySize

197
00:09:06,000 --> 00:09:08,000
是我们给定的amplifySize

198
00:09:08,000 --> 00:09:10,000
然后第二个是输出的围堵大小

199
00:09:11,000 --> 00:09:12,000
经过n.liner参数

200
00:09:12,000 --> 00:09:14,000
输出围堵要变成多少

201
00:09:14,000 --> 00:09:18,000
要变成self.hideDim

202
00:09:19,000 --> 00:09:22,000
这就是我们这求hideDim的原因

203
00:09:25,000 --> 00:09:27,000
因为我们这算出来了

204
00:09:27,000 --> 00:09:29,000
每一个头它的围堵大小是多少

205
00:09:30,000 --> 00:09:32,000
那我们的QKV输入

206
00:09:32,000 --> 00:09:34,000
怎么变成我们输入大小

207
00:09:36,000 --> 00:09:38,000
想要输入大小的话

208
00:09:38,000 --> 00:09:40,000
就要通过n.liner

209
00:09:40,000 --> 00:09:41,000
这是输入围堵

210
00:09:41,000 --> 00:09:42,000
这是输出围堵

211
00:09:42,000 --> 00:09:44,000
然后最后还是老规矩

212
00:09:44,000 --> 00:09:48,000
把我们的偏置打成False

213
00:09:48,000 --> 00:09:50,000
又记错了

214
00:09:51,000 --> 00:09:53,000
把我们偏置打成False

215
00:09:53,000 --> 00:09:54,000
因为False

216
00:09:54,000 --> 00:09:57,000
这个偏置对我们整个网络的帮助很小

217
00:09:57,000 --> 00:09:59,000
并且它会引入一些计算的复杂度

218
00:09:59,000 --> 00:10:01,000
所以就把它关掉

219
00:10:01,000 --> 00:10:03,000
然后第二个同理

220
00:10:03,000 --> 00:10:05,000
QKV

221
00:10:05,000 --> 00:10:06,000
大家还记得它名字吗

222
00:10:06,000 --> 00:10:08,000
QueriesKValues

223
00:10:08,000 --> 00:10:09,000
Values打完了

224
00:10:09,000 --> 00:10:10,000
我们K呗

225
00:10:10,000 --> 00:10:12,000
按照我们这个论文的顺序

226
00:10:13,000 --> 00:10:15,000
Self.ksu等于n.liner

227
00:10:16,000 --> 00:10:17,000
其实是一样的

228
00:10:17,000 --> 00:10:18,000
我们直接复制下来

229
00:10:20,000 --> 00:10:23,000
它们三个输出的辞向大小都是相同的

230
00:10:24,000 --> 00:10:25,000
然后呢

231
00:10:25,000 --> 00:10:28,000
都是要转化成这个每个头的大小

232
00:10:28,000 --> 00:10:31,000
然后输入给我们的多头注意力

233
00:10:32,000 --> 00:10:34,000
然后这个是Queries

234
00:10:36,000 --> 00:10:38,000
这三个部分呢

235
00:10:38,000 --> 00:10:41,000
就定义好了我们的这个输出的尺度

236
00:10:42,000 --> 00:10:45,000
当我们给定输入的辞向大小的时候

237
00:10:45,000 --> 00:10:49,000
它还会转到我们对应的头的大小

238
00:10:50,000 --> 00:10:52,000
然后输入给我们ScaledUp

239
00:10:52,000 --> 00:10:56,000
然后每一个这个做完之后

240
00:10:56,000 --> 00:10:58,000
再concate起来

241
00:10:58,000 --> 00:10:59,000
平起来

242
00:10:59,000 --> 00:11:02,000
相当于把一个辞向量分给了多个头去做

243
00:11:02,000 --> 00:11:03,000
那为什么这样做呢

244
00:11:03,000 --> 00:11:04,000
Why

245
00:11:04,000 --> 00:11:06,000
我们要讨论一下Why

246
00:11:06,000 --> 00:11:08,000
就是因为分开这样多头之后

247
00:11:08,000 --> 00:11:12,000
它相当于和我们之前学的那种

248
00:11:12,000 --> 00:11:13,000
引入更多的参数

249
00:11:14,000 --> 00:11:15,000
学校更好一样

250
00:11:15,000 --> 00:11:17,000
它其实就引入了更多的注意力

251
00:11:17,000 --> 00:11:21,000
就是每一个小部分都会关注自身一个注意力

252
00:11:21,000 --> 00:11:22,000
然后多个加起来之后

253
00:11:22,000 --> 00:11:25,000
它是不是对自身的一些特点提取的更明显

254
00:11:25,000 --> 00:11:27,000
所以分多头其实就是对它

255
00:11:27,000 --> 00:11:31,000
一个更好的一个注意力机制的理解

256
00:11:33,000 --> 00:11:34,000
是这个意思

257
00:11:34,000 --> 00:11:36,000
然后我们还有什么部分呢

258
00:11:39,000 --> 00:11:41,000
我们整个的这个多头注意力

259
00:11:41,000 --> 00:11:42,000
我们看右边

260
00:11:44,000 --> 00:11:45,000
QKV

261
00:11:47,000 --> 00:11:48,000
还需要有什么东西呢

262
00:11:48,000 --> 00:11:50,000
还需要有一个线性层

263
00:11:50,000 --> 00:11:52,000
也就是我们输出的线性层

264
00:11:53,000 --> 00:11:54,000
输出线性层

265
00:11:54,000 --> 00:11:56,000
我们左边这个图大家别搞乱

266
00:11:56,000 --> 00:11:58,000
左边这个图你看它左上角写了

267
00:11:59,000 --> 00:12:00,000
Scalable Product Attention

268
00:12:00,000 --> 00:12:02,000
也就是我们这个小蓝块里面内容

269
00:12:02,000 --> 00:12:04,000
所以我们左边这块先不要看

270
00:12:04,000 --> 00:12:06,000
右边是整个多头注意力机制

271
00:12:06,000 --> 00:12:08,000
别被左边搞迷糊了

272
00:12:08,000 --> 00:12:09,000
先看右边

273
00:12:10,000 --> 00:12:11,000
右边是这样的

274
00:12:13,000 --> 00:12:16,000
所以它最后还有一个输出FCout

275
00:12:16,000 --> 00:12:18,000
FCout就等于

276
00:12:20,000 --> 00:12:21,000
Liner

277
00:12:22,000 --> 00:12:23,000
大家想啊

278
00:12:23,000 --> 00:12:26,000
这个concat之后它的维度大小是多少

279
00:12:26,000 --> 00:12:28,000
首先它concat了之后

280
00:12:29,000 --> 00:12:30,000
是

281
00:12:31,000 --> 00:12:37,000
每一个头的这个维度乘以总共的头的数目

282
00:12:37,000 --> 00:12:41,000
然后再输出的大小是Ambly Size

283
00:12:41,000 --> 00:12:42,000
磁相量的大小

284
00:12:46,000 --> 00:12:57,000
刚才点错了

285
00:12:57,000 --> 00:12:59,000
刚才是想点到这个论文的页面

286
00:12:59,000 --> 00:13:01,000
其实我们可以看啊

287
00:13:01,000 --> 00:13:03,000
这个多头注意力机制输出之后

288
00:13:03,000 --> 00:13:05,000
要和原始的一个输入

289
00:13:05,000 --> 00:13:07,000
做一个相加add操作

290
00:13:07,000 --> 00:13:09,000
所以这个输出大小的话

291
00:13:09,000 --> 00:13:12,000
这应该是Ambly Size

292
00:13:13,000 --> 00:13:15,000
恢复到我们原来的Ambly Size相加呀

293
00:13:16,000 --> 00:13:18,000
你想这个过程

294
00:13:18,000 --> 00:13:19,000
降维又升维

295
00:13:19,000 --> 00:13:21,000
中间可能引入了很多的一个参数

296
00:13:21,000 --> 00:13:24,000
所以网络引入更多的参数

297
00:13:24,000 --> 00:13:26,000
它学习效果可能会更好

298
00:13:26,000 --> 00:13:29,000
但是它相应的计算复杂量会变大

299
00:13:29,000 --> 00:13:31,000
所以Transformer它这个训练起来

300
00:13:31,000 --> 00:13:34,000
还是很吃这个性能的

301
00:13:35,000 --> 00:13:37,000
大家记住这个多头注意力机制下来

302
00:13:37,000 --> 00:13:40,000
就是一个add和normal的工作

303
00:13:41,000 --> 00:13:43,000
所以要恢复到原始输入大小

304
00:13:43,000 --> 00:13:44,000
和我们输入相加

305
00:13:44,000 --> 00:13:47,000
那这时候我想我们这个初始化部分

306
00:13:47,000 --> 00:13:48,000
已经写完了

307
00:13:49,000 --> 00:13:51,000
时间是13分钟

308
00:13:52,000 --> 00:13:53,000
13分钟的话

309
00:13:53,000 --> 00:13:55,000
我们就先暂停

310
00:13:55,000 --> 00:13:58,000
然后我们下一个10分钟的课里面

311
00:13:58,000 --> 00:14:00,000
我们去讲这个forward怎么写

