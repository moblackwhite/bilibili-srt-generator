1
00:00:00,000 --> 00:00:04,799
我们接下来讲另一个非常重要的一个概念

2
00:00:04,799 --> 00:00:05,879
是叫丢弃法

3
00:00:05,879 --> 00:00:06,919
也叫dropout

4
00:00:07,200 --> 00:00:14,200
这个也是我们在深度学习崛起中间最早提出的一个算法

5
00:00:14,960 --> 00:00:19,679
它可能会比我们之前的群众衰退效果更好

6
00:00:21,480 --> 00:00:26,240
然后这个图的就是这个图是你的high school

7
00:00:26,240 --> 00:00:28,719
graduation的dropout rate

8
00:00:29,559 --> 00:00:30,719
就是退学率

9
00:00:36,079 --> 00:00:37,560
就它的动机是这样子的

10
00:00:37,560 --> 00:00:40,439
就是说你一个好的模型

11
00:00:41,519 --> 00:00:45,239
需要对你的输入数据的扰动鲁邦

12
00:00:45,920 --> 00:00:47,159
假设你看我这个图

13
00:00:48,000 --> 00:00:50,079
这星球大战里面那个盔甲

14
00:00:50,679 --> 00:00:51,439
就是说

15
00:00:52,679 --> 00:00:55,280
我不管我的图里面加入多少噪音

16
00:00:55,439 --> 00:00:57,599
就加到最大的时候我也能看清

17
00:00:57,599 --> 00:00:57,920
对吧

18
00:00:57,920 --> 00:00:59,079
人是没关系的

19
00:00:59,679 --> 00:01:00,880
你花一点糊一点

20
00:01:00,880 --> 00:01:02,200
我也是能看清楚的

21
00:01:04,560 --> 00:01:08,840
那么使用有噪音的数据

22
00:01:09,359 --> 00:01:12,359
它等加于一个正则项叫做T

23
00:01:12,760 --> 00:01:14,439
这是一个俄语

24
00:01:14,439 --> 00:01:15,439
我其实也不知道怎么念

25
00:01:15,439 --> 00:01:16,439
就T正则

26
00:01:17,320 --> 00:01:19,879
就是说正则就跟我们讲过

27
00:01:19,879 --> 00:01:22,280
我们讲了L2正则

28
00:01:22,280 --> 00:01:27,640
正则就是一个使得你的权重不要特别大

29
00:01:27,799 --> 00:01:29,320
是个避免过拟合的一个方法

30
00:01:29,320 --> 00:01:31,439
就正则你都可以认为是一个叫

31
00:01:31,439 --> 00:01:33,159
penalty或者叫regularization

32
00:01:33,159 --> 00:01:36,719
就是说使得你不要权重的值的范围不要太大

33
00:01:36,719 --> 00:01:39,280
使得它可以避免一定的过拟合

34
00:01:40,079 --> 00:01:43,760
所以在数据里面加入噪音

35
00:01:43,920 --> 00:01:46,120
等加于一个正则

36
00:01:48,120 --> 00:01:49,920
那跟之前我们加的噪音不一样

37
00:01:50,200 --> 00:01:51,560
之前是固定噪音

38
00:01:51,560 --> 00:01:52,519
这个是随机噪音

39
00:01:52,519 --> 00:01:53,960
就不断的随机加噪音

40
00:01:55,879 --> 00:01:57,240
那丢弃法呢

41
00:01:57,280 --> 00:01:59,920
就是说我不再输入加噪音

42
00:01:59,960 --> 00:02:02,200
我在层之间加入噪音

43
00:02:02,200 --> 00:02:03,240
这就是丢弃法

44
00:02:04,080 --> 00:02:06,640
所以这里面有的还有一个选项是说

45
00:02:06,640 --> 00:02:08,480
丢弃法其实是一个正则的

46
00:02:10,960 --> 00:02:12,040
OK我们来看一下

47
00:02:12,879 --> 00:02:13,600
就是说

48
00:02:15,439 --> 00:02:17,800
假设X是我们的一个

49
00:02:19,200 --> 00:02:20,640
一个到下一层

50
00:02:20,640 --> 00:02:23,400
就是一层到下一层之间的一个输出的话

51
00:02:24,080 --> 00:02:26,520
我们希望对我们X

52
00:02:26,840 --> 00:02:29,120
加入噪音得到X一撇

53
00:02:29,960 --> 00:02:31,920
然后我希望是说

54
00:02:31,920 --> 00:02:33,840
虽然我加了噪音

55
00:02:33,879 --> 00:02:35,480
但不要改变我的期望

56
00:02:36,280 --> 00:02:37,400
就是说平均上来说

57
00:02:37,400 --> 00:02:38,719
我这个值还是对的

58
00:02:40,000 --> 00:02:41,879
所以这个就是我的唯一的要求

59
00:02:42,800 --> 00:02:46,800
那么丢弃法就是做一个非常简单的事情

60
00:02:46,960 --> 00:02:48,800
就是说X一撇等于什么呢

61
00:02:49,920 --> 00:02:51,680
我给定一个概率P

62
00:02:52,680 --> 00:02:55,000
在P的概率里面

63
00:02:55,000 --> 00:02:56,960
我把输入

64
00:02:57,640 --> 00:03:00,879
就XI就是你真实的原始数变成0

65
00:03:02,360 --> 00:03:03,680
在剩下的地方

66
00:03:03,719 --> 00:03:06,159
我把你除以1-P

67
00:03:06,439 --> 00:03:07,920
就输白了就把你变大一点

68
00:03:07,920 --> 00:03:09,560
因为P是一个0到1的东西

69
00:03:09,879 --> 00:03:12,200
所以这个地方就一定概率

70
00:03:12,200 --> 00:03:12,960
我把你变成0

71
00:03:12,960 --> 00:03:14,240
一定概率我把你变大

72
00:03:14,800 --> 00:03:16,280
那它的期望是不变的

73
00:03:16,280 --> 00:03:17,840
为什么我可以稍微写一下

74
00:03:17,840 --> 00:03:20,800
就是说X一撇的期望

75
00:03:21,560 --> 00:03:24,400
I就是你的第一个元素了

76
00:03:24,439 --> 00:03:25,840
因为你的X是一个向量

77
00:03:25,840 --> 00:03:28,120
所以它等于是

78
00:03:28,760 --> 00:03:30,480
首先它第一个是0

79
00:03:30,719 --> 00:03:34,600
这个0等于是P乘以0

80
00:03:35,439 --> 00:03:37,640
再加上1-P

81
00:03:37,640 --> 00:03:39,240
就是它的概率乘以它的值

82
00:03:39,240 --> 00:03:41,800
就是XI除以1-P

83
00:03:42,400 --> 00:03:43,680
那么很简单

84
00:03:43,800 --> 00:03:45,680
就是这里消掉

85
00:03:45,680 --> 00:03:46,520
这里消掉

86
00:03:46,600 --> 00:03:49,240
那么它就会等于XI

87
00:03:49,920 --> 00:03:52,120
所以就是说XI撇

88
00:03:52,120 --> 00:03:54,360
它的期望没有发生变化

89
00:03:54,360 --> 00:03:55,719
就是跟之前是一样的

90
00:03:55,960 --> 00:03:58,280
所以这就是说核心是为什么

91
00:03:58,280 --> 00:03:59,960
我们要除个1-P在这个地方

92
00:04:01,000 --> 00:04:05,000
这就是Dropout的定义

93
00:04:05,360 --> 00:04:06,040
简单吧

94
00:04:09,560 --> 00:04:09,719
好

95
00:04:09,719 --> 00:04:11,879
我们来看一下Dropout用在什么地方

96
00:04:12,960 --> 00:04:13,800
就是说

97
00:04:13,960 --> 00:04:15,200
它其实就是说

98
00:04:15,240 --> 00:04:17,120
回忆一下我们之前是怎么讲的

99
00:04:17,120 --> 00:04:18,879
我们假设我们要第一层

100
00:04:18,879 --> 00:04:19,879
第一个隐含层

101
00:04:20,680 --> 00:04:22,600
我们的输入乘以我们的权重

102
00:04:22,600 --> 00:04:23,360
W1

103
00:04:23,560 --> 00:04:25,759
加上我们的偏移B1

104
00:04:26,000 --> 00:04:28,040
让拿到我们的

105
00:04:28,160 --> 00:04:29,360
加入我们的计划函数

106
00:04:29,560 --> 00:04:30,639
拿到我们的H

107
00:04:30,759 --> 00:04:35,000
H就是我们第一个隐藏层的输出

108
00:04:36,680 --> 00:04:39,240
然后对第一个隐藏层

109
00:04:39,280 --> 00:04:40,759
我们作用Dropout

110
00:04:41,319 --> 00:04:41,879
就说白了

111
00:04:41,879 --> 00:04:44,519
就是把H中间就每一个元素

112
00:04:44,519 --> 00:04:45,720
每一个元素作用Dropout

113
00:04:46,320 --> 00:04:47,760
作用之前我们那个函数

114
00:04:47,960 --> 00:04:50,720
使得在P的概率变成0

115
00:04:51,000 --> 00:04:52,160
P的概率变成

116
00:04:52,880 --> 00:04:55,280
它除以1-P就变大

117
00:04:56,160 --> 00:04:56,480
然后

118
00:04:57,600 --> 00:04:58,400
第二层

119
00:04:58,560 --> 00:05:00,600
假设我们是单隐藏层

120
00:05:01,200 --> 00:05:03,560
第二层就是之前我们是H是吧

121
00:05:03,600 --> 00:05:05,160
之前我们是直接来这个H

122
00:05:05,200 --> 00:05:06,840
现在我们就H一撇

123
00:05:07,080 --> 00:05:09,480
H一撇乘以W2

124
00:05:09,640 --> 00:05:10,680
这是我的权重

125
00:05:10,680 --> 00:05:11,760
加上我的B2

126
00:05:11,840 --> 00:05:12,680
那就是说

127
00:05:12,960 --> 00:05:14,800
然后当Softmax作为输出

128
00:05:15,720 --> 00:05:16,960
就可以看到是说

129
00:05:17,160 --> 00:05:19,840
我的后面一层

130
00:05:20,080 --> 00:05:21,440
他拿到的输入

131
00:05:21,440 --> 00:05:23,280
是我们前面一层的

132
00:05:23,280 --> 00:05:24,560
把一些元素变成0

133
00:05:24,560 --> 00:05:27,080
把另外一些人做scale掉的一个结果

134
00:05:27,480 --> 00:05:28,520
但你可以从

135
00:05:29,600 --> 00:05:30,280
图上来看

136
00:05:30,440 --> 00:05:32,000
这个是原始的情况下

137
00:05:32,120 --> 00:05:33,760
我们讲这5个隐藏层

138
00:05:34,680 --> 00:05:36,040
那么你用了Dropout

139
00:05:36,040 --> 00:05:37,240
很有可能会变成这样子

140
00:05:37,240 --> 00:05:38,320
很有可能就是说

141
00:05:38,360 --> 00:05:41,160
这两个被换上0了

142
00:05:41,960 --> 00:05:42,720
就没了

143
00:05:43,720 --> 00:05:46,000
那就是说他下一层的看到

144
00:05:46,000 --> 00:05:48,840
就是说我们把中心一些权重都去掉了

145
00:05:48,840 --> 00:05:49,680
下一层

146
00:05:49,800 --> 00:05:50,920
那么就是

147
00:05:52,200 --> 00:05:55,520
当这三个元素会做一些变大了

148
00:05:55,840 --> 00:05:56,680
就是说这个就是说

149
00:05:56,680 --> 00:05:57,800
你当你下一次跑的时候

150
00:05:57,800 --> 00:05:58,760
可能会变化

151
00:05:58,800 --> 00:06:00,320
就是你再下一次的话

152
00:06:00,320 --> 00:06:01,200
因为是一个随机

153
00:06:01,320 --> 00:06:02,880
下一次我有可能是把

154
00:06:03,560 --> 00:06:05,320
把这两个去掉

155
00:06:05,360 --> 00:06:06,320
或者三个去掉

156
00:06:06,320 --> 00:06:07,400
把这两个保留下来

157
00:06:07,400 --> 00:06:08,200
有可能对吧

158
00:06:08,360 --> 00:06:10,320
下一次我可能是全部保留

159
00:06:10,320 --> 00:06:11,240
或者全部去掉

160
00:06:11,560 --> 00:06:12,600
这就是Dropout

161
00:06:13,720 --> 00:06:17,000
这是训练

162
00:06:17,000 --> 00:06:18,600
训练是这样子的

163
00:06:20,840 --> 00:06:23,120
推理就是在inference过程中

164
00:06:23,120 --> 00:06:24,240
在预测的过程中

165
00:06:24,240 --> 00:06:25,440
我不训练的话

166
00:06:25,480 --> 00:06:27,160
我是不使用Dropout的

167
00:06:27,680 --> 00:06:29,160
就是说我的

168
00:06:29,200 --> 00:06:32,480
这是因为Dropout是一个正则项

169
00:06:33,360 --> 00:06:35,760
正则项只在训练中使用

170
00:06:36,000 --> 00:06:38,040
就不管我们之前的L2的正则

171
00:06:38,040 --> 00:06:39,320
还是现在正则

172
00:06:39,360 --> 00:06:40,840
只在训练的时候使用

173
00:06:40,880 --> 00:06:43,840
因为它只会对你的权重产生影响

174
00:06:43,960 --> 00:06:45,400
当你在预测的时候

175
00:06:45,400 --> 00:06:47,640
我们权重不需要发生变化的情况下

176
00:06:47,640 --> 00:06:49,040
我们是不需要正则的

177
00:06:49,200 --> 00:06:50,920
所以就是说我在推理中

178
00:06:50,920 --> 00:06:52,200
我是不使用Dropout

179
00:06:52,240 --> 00:06:54,760
那就意味着说在推理的时候

180
00:06:54,760 --> 00:06:57,320
我的Dropout输出的是我的本身

181
00:06:57,360 --> 00:06:59,000
就不会在它做任何

182
00:06:59,880 --> 00:07:02,760
验领或者scale的操作了

183
00:07:03,120 --> 00:07:05,240
这就是能保证我们有一个

184
00:07:05,280 --> 00:07:06,840
确定性的输出

185
00:07:08,200 --> 00:07:10,040
这是推理中的Dropout

186
00:07:10,840 --> 00:07:12,440
当然反过来讲

187
00:07:13,120 --> 00:07:16,000
我们这里就是教Dropout的时候

188
00:07:16,000 --> 00:07:18,320
就是教它是一个正则项

189
00:07:18,680 --> 00:07:20,120
最早Dropout出来的时候

190
00:07:20,120 --> 00:07:21,280
是Hinton他们做出来的

191
00:07:21,280 --> 00:07:26,480
也就是大家认为的工人的深度学习之父

192
00:07:27,360 --> 00:07:30,440
就他们当时候做出来的时候

193
00:07:30,440 --> 00:07:31,880
其实Dropout不是这么做的

194
00:07:31,880 --> 00:07:33,720
Dropout它其实从

195
00:07:33,800 --> 00:07:36,720
在Hinton的想法里面

196
00:07:36,760 --> 00:07:38,200
Dropout是一种

197
00:07:38,400 --> 00:07:40,000
一个in-jumbo

198
00:07:40,279 --> 00:07:42,680
每一次跟就是说回到之前样例

199
00:07:44,399 --> 00:07:45,680
回到样例

200
00:07:46,879 --> 00:07:47,480
样例

201
00:07:48,240 --> 00:07:50,759
就是说在Hinton当年的paper里面

202
00:07:50,759 --> 00:07:51,959
他觉得是说

203
00:07:52,000 --> 00:07:54,160
因为每一次Dropout

204
00:07:54,839 --> 00:07:55,600
被

205
00:07:56,120 --> 00:07:58,040
每一次就是说等于是我这一次

206
00:07:58,040 --> 00:07:59,079
我把这一个

207
00:07:59,480 --> 00:08:01,120
权重全部去掉了

208
00:08:01,399 --> 00:08:05,079
就变成了一个三隐隐藏层数为三的

209
00:08:05,079 --> 00:08:07,360
就是隐藏大小为三的一个神经网络

210
00:08:07,360 --> 00:08:09,199
是一个子神经网络

211
00:08:09,360 --> 00:08:10,560
下一次的时候

212
00:08:10,560 --> 00:08:13,279
我又是可能激活了另外一块的地方

213
00:08:13,560 --> 00:08:15,399
我两个把头像去一下

214
00:08:15,759 --> 00:08:16,560
就是这个地方

215
00:08:17,240 --> 00:08:18,360
就是说这一次的话

216
00:08:18,360 --> 00:08:20,319
我激活的是一个

217
00:08:20,360 --> 00:08:21,800
它一个子神经网络

218
00:08:21,839 --> 00:08:23,560
它的隐藏层大小为三

219
00:08:23,599 --> 00:08:26,480
下一次可能会激活另外一个一些地方

220
00:08:26,560 --> 00:08:28,360
那么你可以从

221
00:08:28,759 --> 00:08:30,439
你可以理解成是说

222
00:08:30,439 --> 00:08:34,319
我每一次随机的采样一些子

223
00:08:34,399 --> 00:08:36,399
的神经网络来做训练

224
00:08:36,439 --> 00:08:38,840
那么就是说我可以认为我是

225
00:08:38,960 --> 00:08:40,920
我整个在训练过程中

226
00:08:40,920 --> 00:08:42,480
就是拿到了一堆

227
00:08:42,519 --> 00:08:44,480
很小的神经网络做了隐藏

228
00:08:44,480 --> 00:08:45,440
所谓的隐藏

229
00:08:45,480 --> 00:08:47,280
就是为分个小神经网络

230
00:08:47,280 --> 00:08:48,080
最后去平均

231
00:08:48,600 --> 00:08:50,040
这是当年Hinton的解释

232
00:08:50,040 --> 00:08:51,240
他说这样子的话

233
00:08:51,280 --> 00:08:53,000
我这样子训练一个模型

234
00:08:53,000 --> 00:08:55,399
能拿到很多小的神经网络

235
00:08:55,399 --> 00:08:57,040
做一个平均

236
00:08:57,120 --> 00:08:59,360
那么我的结果当然会好了

237
00:09:00,120 --> 00:09:04,240
但是在他出来之后

238
00:09:04,400 --> 00:09:06,320
就是在应该是12年

239
00:09:06,320 --> 00:09:07,120
还是13年

240
00:09:07,960 --> 00:09:09,080
结果出来之后

241
00:09:09,080 --> 00:09:11,399
在过去在之后的三四年里面

242
00:09:11,399 --> 00:09:13,519
大家一直对这个东西进行研究

243
00:09:13,560 --> 00:09:15,480
最后大家看来看去

244
00:09:15,480 --> 00:09:16,879
其实也没有说

245
00:09:16,919 --> 00:09:18,159
也没有理论依据

246
00:09:18,279 --> 00:09:19,000
最后看来看去

247
00:09:19,000 --> 00:09:21,000
觉得从实验上来说

248
00:09:21,039 --> 00:09:24,320
它跟正则效果是一样的

249
00:09:24,759 --> 00:09:27,000
它很符合一个正则向的

250
00:09:27,039 --> 00:09:28,639
一个实验结果

251
00:09:28,679 --> 00:09:30,799
所以现在我们主流是把

252
00:09:31,519 --> 00:09:33,440
dropout当做一个正则向

253
00:09:33,440 --> 00:09:35,000
就跟我们刚刚说的一样

254
00:09:35,039 --> 00:09:36,360
我在我的输入里面

255
00:09:36,360 --> 00:09:38,080
我把输入一些输入的

256
00:09:38,080 --> 00:09:39,480
一些指数变成0

257
00:09:39,480 --> 00:09:40,639
它也是一个正则向

258
00:09:40,639 --> 00:09:43,560
这个是数学有证明的

259
00:09:43,600 --> 00:09:44,560
所以现在是说

260
00:09:44,560 --> 00:09:46,440
你把我的中介结果变成0

261
00:09:46,440 --> 00:09:48,159
你也可以认为是一个正则向

262
00:09:48,879 --> 00:09:50,399
所以这个也是解释了

263
00:09:50,399 --> 00:09:52,919
我说我们的理论和我们的实际

264
00:09:52,919 --> 00:09:54,240
是有一定的

265
00:09:55,600 --> 00:09:56,440
gap的

266
00:09:56,919 --> 00:09:58,360
就是我们发现一个东西

267
00:09:58,399 --> 00:09:59,840
挺好的效果

268
00:09:59,840 --> 00:10:00,960
我们会去解释它

269
00:10:00,960 --> 00:10:02,360
人总是会尝试解释

270
00:10:02,360 --> 00:10:05,279
但解释很有可能是不对的

271
00:10:05,559 --> 00:10:07,120
但是它因为它结果好

272
00:10:07,120 --> 00:10:08,039
它不影响

273
00:10:08,879 --> 00:10:10,000
大家用它

274
00:10:10,039 --> 00:10:10,919
只要大家在用

275
00:10:10,919 --> 00:10:13,159
就会有新的人过来去探索这个领域

276
00:10:13,159 --> 00:10:14,839
然后慢慢的把它发掘出来

277
00:10:15,000 --> 00:10:16,199
当然现在我们教的东西

278
00:10:16,199 --> 00:10:17,240
很多东西也是错的

279
00:10:17,279 --> 00:10:18,360
我觉得是错的

280
00:10:18,360 --> 00:10:21,039
所以可能很多年后

281
00:10:21,039 --> 00:10:23,519
又会被有人说这个东西不对

282
00:10:23,759 --> 00:10:24,720
但是反过来讲

283
00:10:24,720 --> 00:10:25,759
我们就是说

284
00:10:25,799 --> 00:10:28,279
更多时候我们强调它的有效性

285
00:10:28,279 --> 00:10:29,600
就是实际上有用

286
00:10:29,759 --> 00:10:32,439
然后我们再去加一定的理论

287
00:10:32,439 --> 00:10:34,720
所以大家是要谨慎看待

288
00:10:34,720 --> 00:10:37,320
我们的数学和我们的实际

289
00:10:37,320 --> 00:10:40,080
中间是有一定的不和关系

290
00:10:40,120 --> 00:10:41,399
也是因为我们这个领域

291
00:10:41,399 --> 00:10:42,399
还在不断的发展

292
00:10:45,519 --> 00:10:46,960
就最后总结一下

293
00:10:47,720 --> 00:10:48,480
dropout

294
00:10:48,759 --> 00:10:49,879
就丢弃法

295
00:10:50,320 --> 00:10:52,399
丢弃法将一些输出项

296
00:10:52,440 --> 00:10:56,080
特别是隐藏层的输出项

297
00:10:56,240 --> 00:10:57,639
随机变成0

298
00:10:57,759 --> 00:10:59,800
来控制模型的复杂度

299
00:11:00,400 --> 00:11:05,120
它常作用在多层感知机的

300
00:11:05,160 --> 00:11:06,880
隐藏层的输出上

301
00:11:07,440 --> 00:11:10,400
就是对于全链接层的隐藏层输出上

302
00:11:10,600 --> 00:11:11,760
它很少用在

303
00:11:11,760 --> 00:11:13,520
比如说之后我们的CNN

304
00:11:13,800 --> 00:11:14,880
CNN的模型的上面

305
00:11:14,880 --> 00:11:15,720
我们会解释

306
00:11:17,600 --> 00:11:19,640
所以丢弃的概率

307
00:11:19,640 --> 00:11:22,280
是控制模型复杂度的超参数

308
00:11:23,240 --> 00:11:24,080
就丢弃概率

309
00:11:24,080 --> 00:11:25,200
假设是1的话

310
00:11:25,200 --> 00:11:26,400
那么全部丢掉

311
00:11:26,880 --> 00:11:28,440
那么w就是等于0了

312
00:11:29,080 --> 00:11:31,200
丢弃值等于0的话就不丢

313
00:11:31,240 --> 00:11:33,280
他就是不会做任何控制

314
00:11:33,800 --> 00:11:35,280
所以丢弃一般来说

315
00:11:35,280 --> 00:11:36,280
P一般是丢弃率

316
00:11:36,280 --> 00:11:37,560
一般取0.5

317
00:11:37,560 --> 00:11:38,920
或者0.9

318
00:11:38,920 --> 00:11:39,640
或者0.1

319
00:11:39,640 --> 00:11:41,960
这三个是最常见的一个丢弃概率

320
00:11:43,040 --> 00:11:44,840
所以这个也是一个控制模型

321
00:11:44,840 --> 00:11:46,640
复杂度一个超参数

322
00:11:47,240 --> 00:11:49,760
而且它的很有可能效果

323
00:11:49,760 --> 00:11:51,440
会比我们之前的L2的

324
00:11:51,440 --> 00:11:52,800
弄要好一点点

325
00:11:53,880 --> 00:11:54,400
OK

326
00:11:54,399 --> 00:11:59,000
那这就是我们的丢弃法

