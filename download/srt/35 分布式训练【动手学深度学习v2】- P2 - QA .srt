1
00:00:00,000 --> 00:00:02,120
分布式计算

2
00:00:02,919 --> 00:00:05,839
这个是15年的时候

3
00:00:05,839 --> 00:00:08,560
我们在CMU装的一个小机群

4
00:00:08,919 --> 00:00:13,160
这个机群是在当时候在eBay

5
00:00:13,160 --> 00:00:14,839
不是eBay是Newegg

6
00:00:14,839 --> 00:00:15,599
就是新旦

7
00:00:15,599 --> 00:00:17,039
就是在美国等于叫京东

8
00:00:17,199 --> 00:00:17,879
在国内的

9
00:00:18,160 --> 00:00:18,719
是买的

10
00:00:18,719 --> 00:00:21,359
就很便宜的机箱

11
00:00:21,359 --> 00:00:23,080
我记得30块钱一个的机箱

12
00:00:23,120 --> 00:00:25,600
然后里面GPU是我的

13
00:00:25,800 --> 00:00:27,120
我有两个老板

14
00:00:27,120 --> 00:00:30,440
然后当时CMU一个是做深度学习的

15
00:00:30,440 --> 00:00:31,520
一个做Machine Learning

16
00:00:31,520 --> 00:00:33,240
一个是做系统方向的

17
00:00:33,760 --> 00:00:36,840
是我系统方向老板挖矿

18
00:00:37,600 --> 00:00:39,520
淘汰的GPU装在里面

19
00:00:39,520 --> 00:00:40,240
那是15年

20
00:00:40,799 --> 00:00:44,079
我挖矿的老板早就财务自由了

21
00:00:45,320 --> 00:00:48,120
所以我们把挖矿的那些淘汰的GPU

22
00:00:48,120 --> 00:00:49,120
装了一个机群

23
00:00:49,480 --> 00:00:50,679
都是一些

24
00:00:50,679 --> 00:00:52,960
然后里面有30台机器

25
00:00:53,400 --> 00:00:54,439
30台机器

26
00:00:54,439 --> 00:00:55,000
一个机群

27
00:00:55,000 --> 00:00:57,640
我们有大概60块GPU

28
00:00:57,799 --> 00:00:58,359
60块GPU

29
00:00:58,359 --> 00:01:00,719
可能一共就花了三四万美金的样子

30
00:01:00,719 --> 00:01:03,479
就是十几万人民币

31
00:01:04,319 --> 00:01:05,599
还没有花三四万美金

32
00:01:05,799 --> 00:01:06,679
三三万美金

33
00:01:06,759 --> 00:01:07,640
20万人民币

34
00:01:08,840 --> 00:01:09,879
还是挺花得来的

35
00:01:09,879 --> 00:01:10,560
高性价比

36
00:01:11,799 --> 00:01:12,920
最亏的是说

37
00:01:12,920 --> 00:01:14,280
你当年我们就

38
00:01:14,920 --> 00:01:17,239
跑了太多深度学习的时间

39
00:01:17,239 --> 00:01:17,959
没有去挖矿

40
00:01:18,239 --> 00:01:19,200
错过了一个亿

41
00:01:19,400 --> 00:01:20,879
就少写点paper

42
00:01:20,879 --> 00:01:22,120
现在也就财务自由了

43
00:01:23,280 --> 00:01:23,959
OK

44
00:01:25,159 --> 00:01:25,759
我讲一下

45
00:01:27,000 --> 00:01:28,480
分布式计算

46
00:01:28,519 --> 00:01:30,359
其实我们讲的是Data Parallelism

47
00:01:30,560 --> 00:01:31,280
Data Parallelism

48
00:01:31,319 --> 00:01:32,319
就数据并行

49
00:01:32,319 --> 00:01:35,079
跟之前的单机多卡没有本质区别

50
00:01:35,879 --> 00:01:38,039
我们给大家看一看

51
00:01:38,039 --> 00:01:39,000
就是说

52
00:01:39,000 --> 00:01:40,680
在分布式的情况下

53
00:01:40,680 --> 00:01:44,439
从之前的单机多卡拓展了分布式

54
00:01:44,439 --> 00:01:44,959
是什么样子

55
00:01:44,959 --> 00:01:46,599
其实本质上是没任何区别的

56
00:01:47,039 --> 00:01:47,959
首先我们

57
00:01:48,200 --> 00:01:50,159
假设有个样本在这个地方

58
00:01:50,400 --> 00:01:51,519
我们有4块GPU

59
00:01:51,799 --> 00:01:53,719
我们这还是抽象出了一个

60
00:01:54,039 --> 00:01:55,439
key value store的东西在里面

61
00:01:57,039 --> 00:01:58,480
用来存我的参数的

62
00:01:59,400 --> 00:02:00,920
首先分布式

63
00:02:00,920 --> 00:02:02,400
第一个不同是说

64
00:02:02,400 --> 00:02:04,039
我的数据可能是放在一个

65
00:02:04,039 --> 00:02:05,840
分布式的文件系统上

66
00:02:05,959 --> 00:02:07,239
而不是放在

67
00:02:07,599 --> 00:02:09,680
机器本地的硬盘上面

68
00:02:10,319 --> 00:02:11,479
所以就是说

69
00:02:11,479 --> 00:02:12,360
所有的机器

70
00:02:12,360 --> 00:02:14,280
都能够去读取样本

71
00:02:14,280 --> 00:02:16,280
这个样本一般是被分开

72
00:02:16,280 --> 00:02:18,439
存在不同的磁盘上面

73
00:02:20,280 --> 00:02:21,560
第二个是说

74
00:02:21,560 --> 00:02:22,360
通常来说

75
00:02:22,360 --> 00:02:23,960
你是有多台机器

76
00:02:24,120 --> 00:02:25,800
每台机器里面有多个GPU

77
00:02:25,800 --> 00:02:26,960
我们这个机器叫做

78
00:02:26,960 --> 00:02:27,680
worker

79
00:02:27,719 --> 00:02:29,479
就是叫做工作站

80
00:02:30,479 --> 00:02:31,719
然后这里就是我们假设

81
00:02:31,719 --> 00:02:32,560
两台机器

82
00:02:32,560 --> 00:02:33,840
然后有两个GPU

83
00:02:33,960 --> 00:02:35,360
每台机器有两个GPU

84
00:02:36,080 --> 00:02:37,240
那么接下来就是说

85
00:02:37,240 --> 00:02:38,159
你的服务器

86
00:02:38,280 --> 00:02:39,759
你存参数的地方

87
00:02:40,920 --> 00:02:41,800
通常来说

88
00:02:41,800 --> 00:02:42,960
你会放在多个

89
00:02:42,960 --> 00:02:44,680
也是放在多个server上面

90
00:02:46,039 --> 00:02:46,360
那么

91
00:02:47,439 --> 00:02:49,120
其实在计算上来说

92
00:02:49,120 --> 00:02:50,000
根本就没区别

93
00:02:50,000 --> 00:02:51,879
首先你要去读你的数据

94
00:02:51,879 --> 00:02:52,400
读数据

95
00:02:52,400 --> 00:02:54,319
就之前我们是磁盘读内存

96
00:02:54,439 --> 00:02:54,960
现在是说

97
00:02:54,960 --> 00:02:56,599
我们通过网络读进来

98
00:02:57,159 --> 00:02:58,000
那么就是说

99
00:02:58,000 --> 00:02:58,800
而且

100
00:02:59,120 --> 00:03:01,039
收发T度的时候

101
00:03:01,439 --> 00:03:02,199
之前是说

102
00:03:02,199 --> 00:03:03,479
GPU copy的

103
00:03:03,479 --> 00:03:04,400
另外一个GPU

104
00:03:04,439 --> 00:03:04,919
现在是说

105
00:03:04,919 --> 00:03:06,439
我们通过网络

106
00:03:06,599 --> 00:03:07,280
从一台机器

107
00:03:07,280 --> 00:03:08,520
跑到另外一台机器

108
00:03:09,599 --> 00:03:10,360
就是说

109
00:03:10,879 --> 00:03:11,719
就本质上没区别

110
00:03:11,879 --> 00:03:12,960
就是基本上可以看到

111
00:03:12,960 --> 00:03:15,560
是就这么一点区别

112
00:03:16,360 --> 00:03:17,120
所以

113
00:03:17,639 --> 00:03:19,879
除了跨了网络之外

114
00:03:19,879 --> 00:03:20,759
从逻辑上来讲

115
00:03:20,759 --> 00:03:21,639
大概是一样的

116
00:03:22,079 --> 00:03:22,960
但性能上来说

117
00:03:22,960 --> 00:03:23,639
我们可以看一下

118
00:03:23,639 --> 00:03:24,280
性能上来说

119
00:03:24,280 --> 00:03:25,359
通常会有一点点

120
00:03:25,359 --> 00:03:25,960
不一样的地方

121
00:03:25,960 --> 00:03:26,799
在这里面

122
00:03:27,439 --> 00:03:28,560
第一个是说

123
00:03:28,599 --> 00:03:29,319
我们来看一下

124
00:03:29,319 --> 00:03:30,079
一般来说

125
00:03:30,079 --> 00:03:30,840
你的

126
00:03:31,960 --> 00:03:32,879
GPU的架构

127
00:03:32,879 --> 00:03:34,560
是长这个样子的

128
00:03:34,680 --> 00:03:35,680
在这个架构图

129
00:03:35,680 --> 00:03:36,840
这个数字是比较老了

130
00:03:36,840 --> 00:03:37,719
相对来说

131
00:03:38,479 --> 00:03:40,919
首先你假设你这是一个

132
00:03:41,199 --> 00:03:42,719
一台机器有4个GPU

133
00:03:43,000 --> 00:03:44,519
那么你的GPU之间

134
00:03:44,519 --> 00:03:47,120
比如说用PCIe的

135
00:03:47,120 --> 00:03:49,039
3.0 16x连的话

136
00:03:49,159 --> 00:03:50,599
那么GPU到GPU的通讯

137
00:03:50,599 --> 00:03:51,239
其实不错的

138
00:03:52,120 --> 00:03:53,439
就是说一般GPU里面

139
00:03:53,439 --> 00:03:54,319
有一个比较

140
00:03:54,319 --> 00:03:55,120
有一个交换机

141
00:03:55,120 --> 00:03:56,719
叫做PCIe的交换机

142
00:03:57,000 --> 00:03:58,039
但你用的高端的

143
00:03:58,039 --> 00:03:59,000
Media的话

144
00:03:59,000 --> 00:04:01,079
它有个M-Link switch

145
00:04:01,079 --> 00:04:02,239
叫更高端一点

146
00:04:02,799 --> 00:04:04,479
和本质上就是说

147
00:04:04,479 --> 00:04:05,599
它就是一个小机群

148
00:04:05,599 --> 00:04:06,199
在里面

149
00:04:06,439 --> 00:04:08,599
所以GPU到GPU的带宽

150
00:04:08,599 --> 00:04:09,399
是不错的

151
00:04:09,399 --> 00:04:10,039
这个地方

152
00:04:10,039 --> 00:04:12,079
我们是63GB per second

153
00:04:13,840 --> 00:04:16,480
那么你要GPU去CPU的话

154
00:04:16,480 --> 00:04:17,159
你要通过

155
00:04:17,159 --> 00:04:18,319
也是通过一个PCIe

156
00:04:18,319 --> 00:04:19,599
到CPU到主内存

157
00:04:20,199 --> 00:04:21,599
这里通常是一根线

158
00:04:21,599 --> 00:04:22,279
在这个地方

159
00:04:22,879 --> 00:04:24,399
所以看到是说

160
00:04:24,399 --> 00:04:25,879
GPU到CPU的话

161
00:04:25,879 --> 00:04:27,040
带宽其实不大

162
00:04:27,040 --> 00:04:28,079
就16GB

163
00:04:29,279 --> 00:04:31,600
然后如果你是要去

164
00:04:31,600 --> 00:04:33,319
走到一个别的机器的话

165
00:04:33,319 --> 00:04:34,920
那就通过一个交换机

166
00:04:34,959 --> 00:04:36,000
那么你比如说

167
00:04:36,000 --> 00:04:38,159
我用10个Gigabit的话

168
00:04:38,159 --> 00:04:39,879
那就是1.25GB

169
00:04:41,719 --> 00:04:42,839
你能看到的是说

170
00:04:42,839 --> 00:04:44,000
这个问题是说

171
00:04:45,480 --> 00:04:47,439
GPU到GPU的通讯是很快的

172
00:04:48,879 --> 00:04:50,319
然后GPU到CPU

173
00:04:50,439 --> 00:04:52,639
通讯就可以降个几倍

174
00:04:52,639 --> 00:04:53,839
5倍或者10倍

175
00:04:54,120 --> 00:04:55,839
然后又跨机器的话

176
00:04:55,839 --> 00:04:57,680
你的通讯就会降的更多

177
00:04:58,319 --> 00:04:59,240
所以就是说

178
00:04:59,240 --> 00:05:00,719
你需要去意识到

179
00:05:00,719 --> 00:05:03,120
这里面是有个层次化的结构的

180
00:05:03,240 --> 00:05:04,560
所以尽量的是说

181
00:05:04,560 --> 00:05:06,240
尽量本地多通讯

182
00:05:06,959 --> 00:05:07,519
实在不行

183
00:05:07,519 --> 00:05:08,959
可以去内存走一走

184
00:05:09,000 --> 00:05:11,600
尽量的少在机器之间做通讯

185
00:05:11,600 --> 00:05:13,399
这就是整个性能的关键

186
00:05:14,959 --> 00:05:15,719
比如说

187
00:05:16,720 --> 00:05:18,320
通常来说

188
00:05:18,600 --> 00:05:19,520
你可以认为

189
00:05:19,800 --> 00:05:22,000
我们一般来说的做法是说

190
00:05:22,000 --> 00:05:23,320
你可以用一个

191
00:05:23,560 --> 00:05:24,840
你说如果是primary server

192
00:05:24,840 --> 00:05:25,640
这个架构的话

193
00:05:26,440 --> 00:05:27,720
就是说你的做法是

194
00:05:27,720 --> 00:05:29,960
我在GPU里面

195
00:05:29,960 --> 00:05:31,720
还做一层server在这个地方

196
00:05:33,080 --> 00:05:34,920
就能够尽量在这一层server

197
00:05:34,920 --> 00:05:37,760
处把你的一些东西给

198
00:05:38,720 --> 00:05:40,600
通讯尽量在本地多通讯

199
00:05:40,760 --> 00:05:42,760
在发送到别的remote的时候

200
00:05:43,080 --> 00:05:44,080
尽量会少一点

201
00:05:44,560 --> 00:05:47,479
我们接下来直接看一个样例

202
00:05:47,599 --> 00:05:48,719
具体是怎么做的

203
00:05:48,719 --> 00:05:50,240
这也是通常来说

204
00:05:50,240 --> 00:05:51,519
在做分布式的时候

205
00:05:51,519 --> 00:05:53,959
我们怎么样去减少化机器的通讯

206
00:05:55,399 --> 00:05:56,719
我们讲一个具体样例

207
00:05:56,759 --> 00:05:59,240
还是之前data parallelism的样例

208
00:06:00,199 --> 00:06:02,560
我们首先我们样本读进来

209
00:06:03,120 --> 00:06:05,000
就假设两台机器的话

210
00:06:05,560 --> 00:06:06,360
如果你的样本

211
00:06:06,360 --> 00:06:07,639
批量大小是100的话

212
00:06:07,639 --> 00:06:09,360
那每台机器拿到是50

213
00:06:11,159 --> 00:06:11,560
然后

214
00:06:11,560 --> 00:06:14,480
接下来就是说

215
00:06:15,000 --> 00:06:16,680
我每台机器拿到50

216
00:06:17,040 --> 00:06:18,720
每个卡拿到25

217
00:06:18,720 --> 00:06:20,199
我就在就是说

218
00:06:20,199 --> 00:06:22,240
先复制到每台机器的内存

219
00:06:22,280 --> 00:06:24,560
内存里面再把它分两块

220
00:06:24,560 --> 00:06:26,439
复制到你的GPU内存上面

221
00:06:29,199 --> 00:06:29,920
接下来就是说

222
00:06:29,920 --> 00:06:31,120
你的参数服务器里面

223
00:06:31,120 --> 00:06:33,399
就是说每个机器要去

224
00:06:33,439 --> 00:06:35,560
拿到你的新的模型

225
00:06:35,840 --> 00:06:37,279
就从网络那边拿过来

226
00:06:38,920 --> 00:06:39,399
然后

227
00:06:40,400 --> 00:06:41,880
就是说你这个模型

228
00:06:42,080 --> 00:06:44,400
是被复制到每个GPU上

229
00:06:44,960 --> 00:06:45,480
所以就是说

230
00:06:45,480 --> 00:06:46,880
你不能把GPU

231
00:06:46,880 --> 00:06:47,960
当做每一个是样例

232
00:06:47,960 --> 00:06:49,440
你不能每个GPU去问

233
00:06:49,440 --> 00:06:50,320
要一个模型过来

234
00:06:50,320 --> 00:06:52,240
因为你就要了两次

235
00:06:52,280 --> 00:06:53,240
一样的模型

236
00:06:53,280 --> 00:06:55,440
所以你应该做法是说

237
00:06:55,440 --> 00:06:57,840
你假设我还有一个server

238
00:06:57,840 --> 00:06:59,040
跑在我的内存里面

239
00:06:59,600 --> 00:07:01,600
那么我都要每次拿过来一个东西

240
00:07:01,600 --> 00:07:03,080
之后拿到主内存

241
00:07:03,080 --> 00:07:05,280
再复制到我的GPU上

242
00:07:06,080 --> 00:07:09,680
接下来就是说跟之前一样

243
00:07:09,800 --> 00:07:11,640
每个GPU算自己的T2

244
00:07:12,960 --> 00:07:14,120
T2之后

245
00:07:14,160 --> 00:07:15,840
那么跟数据

246
00:07:15,840 --> 00:07:17,120
跟我们之前讲的不一样

247
00:07:17,120 --> 00:07:18,040
是说你不要把T2

248
00:07:18,040 --> 00:07:18,920
先给发出去了

249
00:07:18,920 --> 00:07:20,760
不要先做all reduce

250
00:07:21,080 --> 00:07:21,600
你要干点事

251
00:07:21,600 --> 00:07:22,080
就是说

252
00:07:22,080 --> 00:07:24,240
我先在我的主内存里面

253
00:07:24,680 --> 00:07:26,920
把我的GPU的T2加起来

254
00:07:26,920 --> 00:07:28,280
就跟我们之前是

255
00:07:28,280 --> 00:07:31,080
跟我们之前实现单机多卡

256
00:07:31,080 --> 00:07:31,520
是一样的

257
00:07:31,800 --> 00:07:33,120
因为一个all reduce

258
00:07:33,120 --> 00:07:34,720
在本地是all reduce

259
00:07:35,160 --> 00:07:37,400
把每个GPU的T2给你加起来

260
00:07:40,160 --> 00:07:40,920
OK

261
00:07:41,240 --> 00:07:44,840
然后我们再把它加起来之后

262
00:07:44,840 --> 00:07:45,960
再发送出去

263
00:07:46,720 --> 00:07:48,080
就不是发两遍

264
00:07:48,080 --> 00:07:49,080
不是发两份T2

265
00:07:49,080 --> 00:07:50,600
是发一份加起来的T2

266
00:07:52,600 --> 00:07:53,240
OK

267
00:07:53,640 --> 00:07:54,560
然后之后的话

268
00:07:54,560 --> 00:07:55,320
你参数服务器

269
00:07:55,320 --> 00:07:56,840
就是可以做自己的更新

270
00:07:57,120 --> 00:07:57,880
更新完之后

271
00:07:58,320 --> 00:07:59,200
那就是

272
00:08:03,080 --> 00:08:04,280
更新完之后

273
00:08:04,280 --> 00:08:08,160
那么可以进行下一轮的计算了

274
00:08:08,880 --> 00:08:10,160
所以看到是说

275
00:08:10,160 --> 00:08:12,840
我们因为本地

276
00:08:12,880 --> 00:08:14,920
就是GPU之间的通讯还可以

277
00:08:14,920 --> 00:08:16,520
然后GPU的内存也还不错

278
00:08:16,520 --> 00:08:18,280
所以我们尽量在本地

279
00:08:18,600 --> 00:08:19,640
做一下聚合

280
00:08:19,960 --> 00:08:20,760
把T2加起来

281
00:08:20,760 --> 00:08:21,240
好

282
00:08:21,600 --> 00:08:22,560
如果是同样东西

283
00:08:22,560 --> 00:08:25,440
我们也不要在网络上通讯多次

284
00:08:26,240 --> 00:08:28,360
这就是一个最简单的

285
00:08:28,400 --> 00:08:30,120
利用你的层次架构

286
00:08:30,120 --> 00:08:32,279
来检定你的机器之间的开销

287
00:08:32,879 --> 00:08:33,160
OK

288
00:08:33,159 --> 00:08:34,839
这也是现在我觉得

289
00:08:34,839 --> 00:08:36,279
在数据变形里面

290
00:08:36,279 --> 00:08:38,839
大家几乎是都是这么做的

291
00:08:40,519 --> 00:08:41,199
好

292
00:08:42,679 --> 00:08:42,879
好

293
00:08:42,879 --> 00:08:44,079
我们来讲一下性能

294
00:08:44,879 --> 00:08:48,360
就是说我们刚刚有直观体验一下性能

295
00:08:48,519 --> 00:08:50,399
我们在这里再来讲一下

296
00:08:50,759 --> 00:08:52,319
具体的性能是怎么算的

297
00:08:52,799 --> 00:08:54,360
首先同步SGD

298
00:08:54,480 --> 00:08:55,799
就Synchronized SGD

299
00:08:55,799 --> 00:08:57,399
我们用的都是同步SGD

300
00:08:57,879 --> 00:08:59,879
就是说每一个worker

301
00:09:00,000 --> 00:09:01,240
就是你不管是一个worker

302
00:09:01,240 --> 00:09:02,199
是一个GPU也好

303
00:09:02,199 --> 00:09:02,959
一个机器也好

304
00:09:02,960 --> 00:09:04,240
都是来计算

305
00:09:05,240 --> 00:09:06,879
同步来计算一个批量

306
00:09:07,720 --> 00:09:09,960
那么假设我们这有N个GPU的话

307
00:09:10,800 --> 00:09:13,960
每个GPU处理B个样本的话

308
00:09:14,120 --> 00:09:17,600
那么同步SGD就等价于在单GPU

309
00:09:17,600 --> 00:09:19,840
甚至是在CPU上运行一个

310
00:09:19,879 --> 00:09:24,040
批量大小是N乘以B的一个SGD

311
00:09:25,759 --> 00:09:27,200
所以就是说我也说过

312
00:09:27,200 --> 00:09:29,840
就是说当你的超参数不变的情况下

313
00:09:29,840 --> 00:09:31,320
就是你的批量大小固定住

314
00:09:31,320 --> 00:09:33,600
你的能力rate怎么都固定住的情况下

315
00:09:33,600 --> 00:09:37,520
你把GPU从一增到增大

316
00:09:37,600 --> 00:09:38,280
增大很大

317
00:09:38,280 --> 00:09:41,840
都是不会影响你的收敛的

318
00:09:43,400 --> 00:09:44,000
OK

319
00:09:45,400 --> 00:09:46,520
另外一个是说

320
00:09:46,879 --> 00:09:49,000
假设我的每个GPU的

321
00:09:49,000 --> 00:09:50,840
我固定住我的处理的

322
00:09:50,879 --> 00:09:52,280
Batch Size是B的话

323
00:09:52,320 --> 00:09:54,000
就每个GPU总是处理B的话

324
00:09:54,000 --> 00:09:56,040
那么增大我的GPU个数

325
00:09:56,040 --> 00:09:56,920
那么等价的时候

326
00:09:56,920 --> 00:09:57,600
我们不断

327
00:09:57,760 --> 00:09:59,240
我们就会改变我们的优化算法

328
00:09:59,240 --> 00:10:01,280
因为我们在不断增大我的批量大小

329
00:10:01,879 --> 00:10:03,160
但从性能上来讲

330
00:10:03,200 --> 00:10:05,040
假设我固定住每个GPU

331
00:10:05,040 --> 00:10:06,400
处理B个样本的话

332
00:10:06,440 --> 00:10:08,760
那么我使用N个GPU的话

333
00:10:08,760 --> 00:10:10,760
会得到相对于

334
00:10:10,960 --> 00:10:12,800
单GPU的N倍加速

335
00:10:12,800 --> 00:10:14,720
就是每秒钟能处理的样本数

336
00:10:14,720 --> 00:10:15,560
会变成N

337
00:10:15,560 --> 00:10:16,640
在理想情况下

338
00:10:17,440 --> 00:10:17,960
OK

339
00:10:19,640 --> 00:10:21,040
这就是我们的

340
00:10:21,800 --> 00:10:25,520
性能和收敛的一个换算

341
00:10:25,800 --> 00:10:27,200
我们接下来仔细看一下性能

342
00:10:28,440 --> 00:10:29,080
就性能

343
00:10:29,160 --> 00:10:30,120
我们可以看到

344
00:10:30,160 --> 00:10:31,120
就性能是一个横剪

345
00:10:31,120 --> 00:10:32,560
这里是一个很简单模型

346
00:10:32,799 --> 00:10:35,440
就假设我们T1是等于是说

347
00:10:35,440 --> 00:10:37,120
在单GPU上

348
00:10:37,120 --> 00:10:40,279
计算B个样本的T度的时间

349
00:10:41,720 --> 00:10:43,919
假设我这个模型M个参数的话

350
00:10:44,440 --> 00:10:45,679
每一个worker

351
00:10:45,679 --> 00:10:46,799
就每一个GPU也好

352
00:10:46,799 --> 00:10:47,799
每台机器也好

353
00:10:47,799 --> 00:10:50,159
每一次发出去N个参数

354
00:10:50,200 --> 00:10:51,240
收回来

355
00:10:52,480 --> 00:10:53,320
N个参数

356
00:10:53,320 --> 00:10:54,560
M个参数

357
00:10:54,560 --> 00:10:55,200
不是N

358
00:10:56,120 --> 00:10:58,080
就你发T度收T度

359
00:10:58,080 --> 00:10:59,440
或发参数收成都一样

360
00:10:59,440 --> 00:11:01,080
反正T度参数是一样大的

361
00:11:01,960 --> 00:11:04,320
假设说每一次你发M个东西出去

362
00:11:04,320 --> 00:11:05,720
收M个东西回来

363
00:11:05,720 --> 00:11:08,080
那么你的时间是T2的话

364
00:11:09,440 --> 00:11:11,280
那么每个批量的计算时间

365
00:11:11,480 --> 00:11:12,480
就是每个批量

366
00:11:12,480 --> 00:11:14,680
就是说做一次通讯的时间

367
00:11:14,680 --> 00:11:16,880
就做完完整的批量的时间

368
00:11:16,880 --> 00:11:19,600
的时间是maxT1和T2

369
00:11:20,800 --> 00:11:22,200
因为你的计算和通讯

370
00:11:22,200 --> 00:11:23,560
是可以并行进行的

371
00:11:25,000 --> 00:11:26,040
但不是完美并行

372
00:11:26,120 --> 00:11:27,720
是有一定程度的并行

373
00:11:28,360 --> 00:11:30,240
所以你可以简单的模型认为是说

374
00:11:30,399 --> 00:11:34,600
如果你的计算T度的时间

375
00:11:34,600 --> 00:11:37,160
远远大于发送T度的时间的话

376
00:11:37,160 --> 00:11:38,279
发送的话

377
00:11:38,279 --> 00:11:39,639
那么你的主要是

378
00:11:39,960 --> 00:11:40,920
那么就是不错

379
00:11:40,920 --> 00:11:42,920
那就是计算时间是为主

380
00:11:43,399 --> 00:11:44,480
反过来讲

381
00:11:44,519 --> 00:11:47,000
如果你的通讯T2时间

382
00:11:47,040 --> 00:11:48,360
大于你的T的时候

383
00:11:48,360 --> 00:11:50,000
你会导致GPU在等待

384
00:11:51,440 --> 00:11:53,560
所以所有的分布式

385
00:11:53,560 --> 00:11:54,960
或者单机多卡

386
00:11:55,120 --> 00:11:56,840
多机的问题

387
00:11:57,280 --> 00:11:58,080
出现的问题

388
00:11:58,080 --> 00:12:00,759
就是说你的计算

389
00:12:00,759 --> 00:12:04,120
被你的收发给挡住了

390
00:12:04,560 --> 00:12:06,480
你的GPU在等待数据

391
00:12:06,720 --> 00:12:08,560
这就是所有的性能的问题

392
00:12:11,280 --> 00:12:12,399
那么一般的你怎么做

393
00:12:13,040 --> 00:12:14,960
我们知道T1这个东西

394
00:12:14,960 --> 00:12:16,280
几乎是跟你的batch size

395
00:12:16,280 --> 00:12:17,240
是一个线性关系

396
00:12:17,360 --> 00:12:18,200
就不是那么线性

397
00:12:18,200 --> 00:12:19,720
就是说增大batch size

398
00:12:19,720 --> 00:12:22,560
增大b每个GPU处理的样本数

399
00:12:22,600 --> 00:12:25,440
会增加我的计算时间

400
00:12:25,480 --> 00:12:27,200
所以但是增加样本数

401
00:12:27,200 --> 00:12:28,680
不会影响我的通讯

402
00:12:28,720 --> 00:12:30,120
因为通讯反正是批量

403
00:12:30,120 --> 00:12:31,800
反正是参数发出去

404
00:12:32,160 --> 00:12:33,200
你批量大小

405
00:12:33,200 --> 00:12:35,080
和你的模型参数是无关的

406
00:12:35,280 --> 00:12:37,440
所以是说增大b

407
00:12:37,440 --> 00:12:39,200
使得T1会增大

408
00:12:39,200 --> 00:12:40,560
但T2会不变

409
00:12:40,680 --> 00:12:43,200
所以我们就是选一个足够大的b

410
00:12:43,200 --> 00:12:44,960
使得你T1会大于T2

411
00:12:44,960 --> 00:12:46,720
或者远远的大于T2是最好的

412
00:12:47,280 --> 00:12:48,600
通常来说直观上来说

413
00:12:48,600 --> 00:12:49,520
我觉得大一个

414
00:12:50,080 --> 00:12:52,240
T比T2大个20%

415
00:12:52,240 --> 00:12:53,440
30%是比较好的

416
00:12:54,440 --> 00:12:56,000
然后这样子情况下

417
00:12:56,000 --> 00:12:56,400
就是说

418
00:12:56,400 --> 00:12:58,720
那你就几乎是

419
00:12:58,720 --> 00:13:00,360
如果你的框架做得不错的话

420
00:13:00,360 --> 00:13:02,000
系统优化做的还可以的话

421
00:13:02,000 --> 00:13:05,800
那么你就是不会被你的通讯所阻挡

422
00:13:05,800 --> 00:13:07,880
那么几乎就可以达到完美B型

423
00:13:09,200 --> 00:13:10,200
但反过来讲

424
00:13:10,400 --> 00:13:11,520
我们也看到了

425
00:13:11,640 --> 00:13:13,720
增加b或者增加n

426
00:13:13,720 --> 00:13:15,280
都会导致你

427
00:13:15,480 --> 00:13:18,480
你等在做一个更大批量大小的

428
00:13:18,520 --> 00:13:19,680
SGD

429
00:13:19,960 --> 00:13:22,440
就会导致你的收敛会变慢

430
00:13:22,560 --> 00:13:23,240
就是说

431
00:13:23,240 --> 00:13:26,600
你想得到同样的模型星座的时候

432
00:13:26,600 --> 00:13:28,560
你需要更多的data epoc

433
00:13:30,680 --> 00:13:32,640
所以我们刚刚是固定住的data epoc

434
00:13:32,640 --> 00:13:33,800
所以你看到最后是说

435
00:13:33,800 --> 00:13:34,680
我们会低的

436
00:13:34,880 --> 00:13:35,440
实际上是说

437
00:13:35,480 --> 00:13:37,040
你再多劝个

438
00:13:37,080 --> 00:13:39,120
可能5个epoc你也回得来

439
00:13:39,520 --> 00:13:40,360
但反过来讲说

440
00:13:40,360 --> 00:13:41,080
你劝的太多

441
00:13:41,200 --> 00:13:42,360
你就没意义了

442
00:13:42,360 --> 00:13:42,800
对吧

443
00:13:43,120 --> 00:13:43,840
所以我

444
00:13:44,080 --> 00:13:46,960
单GPU变到两个GPU

445
00:13:47,360 --> 00:13:49,560
每秒钟处理的样本数翻倍

446
00:13:49,640 --> 00:13:53,680
但是我需要做两倍多的data epoc的话

447
00:13:53,880 --> 00:13:55,160
我就何必呢

448
00:13:55,840 --> 00:13:57,200
我的时间并没有变低

449
00:13:57,240 --> 00:13:59,040
但是我的机器还多耗了一点

450
00:13:59,760 --> 00:14:00,440
OK

451
00:14:01,080 --> 00:14:04,040
所以这就是性能的问题

452
00:14:04,440 --> 00:14:05,440
所以直观上来说

453
00:14:05,440 --> 00:14:06,680
你可以看到是这个东西

454
00:14:07,240 --> 00:14:08,240
就这个是

455
00:14:08,600 --> 00:14:09,720
x是你的b

456
00:14:10,280 --> 00:14:12,440
就每个GPU的p处理的

457
00:14:12,760 --> 00:14:13,720
批量的大小

458
00:14:14,440 --> 00:14:15,680
你的y上面是好

459
00:14:15,680 --> 00:14:16,560
下面是坏

460
00:14:17,240 --> 00:14:18,520
就是说你会发现说

461
00:14:18,519 --> 00:14:19,559
系统的性能

462
00:14:19,559 --> 00:14:20,120
就是说

463
00:14:20,120 --> 00:14:21,559
每个epoc的耗时

464
00:14:21,559 --> 00:14:23,559
或者是每秒钟能处理的样本数

465
00:14:23,559 --> 00:14:26,879
是随着你的GPU的大小的

466
00:14:26,919 --> 00:14:29,519
就GPU处理的样本个数增加

467
00:14:29,519 --> 00:14:31,799
或随着p量大小增加是增加的

468
00:14:32,159 --> 00:14:33,120
但在某个点的时候

469
00:14:33,120 --> 00:14:34,439
你就saturated了

470
00:14:34,439 --> 00:14:35,079
就没

471
00:14:35,279 --> 00:14:36,000
就是说

472
00:14:36,000 --> 00:14:37,720
就基本上到这个点的时候

473
00:14:37,720 --> 00:14:38,679
p量大小够了

474
00:14:38,679 --> 00:14:40,720
基本上能够很好的利用你

475
00:14:41,039 --> 00:14:42,120
GPU的所谓线程

476
00:14:42,199 --> 00:14:43,480
所以也不会太增加了

477
00:14:43,759 --> 00:14:44,919
但是这一块是增加的

478
00:14:44,919 --> 00:14:45,799
会有比较好

479
00:14:46,919 --> 00:14:47,759
反过来讲

480
00:14:47,960 --> 00:14:49,519
增大你的p量大小

481
00:14:49,519 --> 00:14:51,799
会导致你的训练的有效性遍地

482
00:14:52,159 --> 00:14:53,679
就你的收敛会变慢

483
00:14:53,679 --> 00:14:54,080
这样子

484
00:14:54,080 --> 00:14:55,960
你会需要更多的epoc

485
00:14:55,960 --> 00:14:57,919
来达到你要的精度

486
00:14:58,720 --> 00:14:59,319
所以就是说

487
00:14:59,319 --> 00:15:00,200
你就是个权衡

488
00:15:00,720 --> 00:15:01,639
就理想的大小

489
00:15:02,279 --> 00:15:03,120
可能是

490
00:15:04,439 --> 00:15:05,720
就是一个在这个地方

491
00:15:05,720 --> 00:15:06,960
你理想大小

492
00:15:06,960 --> 00:15:07,240
就是说

493
00:15:07,240 --> 00:15:08,319
你不可能

494
00:15:08,319 --> 00:15:10,159
可能你也达不到完美的

495
00:15:10,319 --> 00:15:11,319
并行

496
00:15:11,319 --> 00:15:13,720
但是你也可能需要

497
00:15:13,720 --> 00:15:16,159
增稍微那么增加那么一点点的

498
00:15:16,719 --> 00:15:17,360
epoc

499
00:15:17,360 --> 00:15:19,279
来达到你的最终的精度

500
00:15:19,679 --> 00:15:20,199
OK

501
00:15:21,399 --> 00:15:23,839
所以这就是大家理解这里面权衡

502
00:15:25,000 --> 00:15:26,199
但是说实践里面

503
00:15:26,439 --> 00:15:27,600
实践里面就是说

504
00:15:28,319 --> 00:15:29,679
我们在深度学习

505
00:15:29,679 --> 00:15:30,240
相对来说

506
00:15:30,240 --> 00:15:31,159
是比较简单任务

507
00:15:32,039 --> 00:15:34,600
达到80%或90%的并行度

508
00:15:34,719 --> 00:15:37,639
就是几乎完美并行的情况是存在的

509
00:15:38,600 --> 00:15:40,039
这我们给的样本比较小

510
00:15:40,159 --> 00:15:41,159
实际的情况下

511
00:15:41,159 --> 00:15:43,519
其实大家效果发现挺好的

512
00:15:43,720 --> 00:15:44,600
这也没什么

513
00:15:44,600 --> 00:15:46,440
大家都去做很大的做分布式

514
00:15:47,639 --> 00:15:48,600
首先实际上

515
00:15:49,120 --> 00:15:51,320
首先你需要一个大一点的数据集

516
00:15:51,360 --> 00:15:53,519
我们这种FMList太小了

517
00:15:53,560 --> 00:15:54,399
那么小数据集

518
00:15:54,399 --> 00:15:55,560
你就跑个几秒钟

519
00:15:55,560 --> 00:15:57,360
你做并行很难

520
00:15:57,680 --> 00:15:59,200
你一个epoc能够跑个

521
00:15:59,720 --> 00:16:01,040
几分钟或者一小时

522
00:16:01,040 --> 00:16:03,720
那就有并行的可能性

523
00:16:04,279 --> 00:16:05,560
如果你就几秒钟跑完了

524
00:16:05,560 --> 00:16:07,159
就不用想了

525
00:16:07,200 --> 00:16:08,680
所以你数据要够大

526
00:16:08,759 --> 00:16:10,519
而且我们也讲过数据集

527
00:16:10,519 --> 00:16:12,120
你的类比要多一点点

528
00:16:12,159 --> 00:16:15,039
这样子能允许你用比较大的pm大小

529
00:16:17,159 --> 00:16:18,480
第二个是说

530
00:16:19,360 --> 00:16:21,159
特别是你自己装机的时候

531
00:16:21,279 --> 00:16:22,080
你得去注意

532
00:16:22,080 --> 00:16:24,480
你GPU连GPU之间的带宽

533
00:16:25,480 --> 00:16:26,879
你买主板的时候

534
00:16:26,879 --> 00:16:29,480
你发现主板上我支持4个GPU

535
00:16:29,519 --> 00:16:32,440
然后发现其实你好一点的主板

536
00:16:32,440 --> 00:16:34,200
GPU之间带宽是比较好的

537
00:16:34,240 --> 00:16:34,840
他做了一个

538
00:16:34,840 --> 00:16:37,440
他做可能是放的比较好的switch

539
00:16:37,759 --> 00:16:39,000
如果你差一点主板

540
00:16:39,000 --> 00:16:41,039
GPU到GPU之间通讯很差

541
00:16:41,360 --> 00:16:43,680
就是所以导致你GPU到GPU之间

542
00:16:43,680 --> 00:16:45,000
带宽可能是个瓶颈

543
00:16:45,560 --> 00:16:47,320
第二个如果你做分模式的话

544
00:16:47,360 --> 00:16:49,760
你要用一个好一点的交换机

545
00:16:50,480 --> 00:16:51,600
就是说好一点的网线

546
00:16:51,600 --> 00:16:52,520
好一点的交换机

547
00:16:52,560 --> 00:16:56,080
所以就是说GPU到GPU的带宽

548
00:16:56,080 --> 00:16:57,640
GPU到CPU的带宽

549
00:16:57,680 --> 00:17:01,560
和机器到机器直接带宽

550
00:17:01,560 --> 00:17:03,400
就硬件上来说越高越好

551
00:17:03,400 --> 00:17:05,120
反正就是说

552
00:17:05,120 --> 00:17:06,320
但是越高钱越多

553
00:17:06,320 --> 00:17:06,600
对吧

554
00:17:06,600 --> 00:17:08,600
就是你也是有个权衡的

555
00:17:09,599 --> 00:17:13,000
第三个经常会遇到的评论

556
00:17:13,000 --> 00:17:14,959
就是说你会发现读数据

557
00:17:15,199 --> 00:17:16,799
特别之后我们就说数据

558
00:17:16,799 --> 00:17:18,719
一处理Data Augmentation的时候

559
00:17:18,919 --> 00:17:20,599
其实挺好资源的

560
00:17:21,240 --> 00:17:22,919
特别是你要读比较复杂的

561
00:17:22,919 --> 00:17:24,879
比如说Video读视频

562
00:17:24,879 --> 00:17:26,000
里面随机采样

563
00:17:26,039 --> 00:17:28,119
基本上在CPU就处理不来

564
00:17:28,119 --> 00:17:29,119
就是你发现

565
00:17:29,199 --> 00:17:31,199
基本上你读一个Data Epo

566
00:17:31,199 --> 00:17:33,119
就是说你读数据的

567
00:17:33,119 --> 00:17:33,959
读一遍数据

568
00:17:33,959 --> 00:17:35,919
可能比你算一遍数据还慢

569
00:17:36,399 --> 00:17:38,000
所以就说你一定要去

570
00:17:38,200 --> 00:17:39,079
在你

571
00:17:39,799 --> 00:17:40,559
一定要去看一下

572
00:17:40,559 --> 00:17:42,039
你的数据读的有多

573
00:17:42,039 --> 00:17:43,880
效率有多高

574
00:17:44,319 --> 00:17:45,680
通常做法是说

575
00:17:46,200 --> 00:17:48,160
你用多进程

576
00:17:48,279 --> 00:17:49,759
在Python里面多进程

577
00:17:49,920 --> 00:17:51,039
或者你把这些

578
00:17:51,039 --> 00:17:53,319
有些东西挪到GPU去做运算

579
00:17:53,759 --> 00:17:56,559
有些可能一些图片

580
00:17:56,559 --> 00:17:57,519
就视频解码

581
00:17:57,519 --> 00:17:59,200
你都可以放在GPU上做运算

582
00:18:01,720 --> 00:18:03,119
第另外一个就是说

583
00:18:03,119 --> 00:18:03,880
我们有讲过

584
00:18:03,880 --> 00:18:05,000
刚刚QA有讲过

585
00:18:05,000 --> 00:18:07,160
就是说你需要有好的

586
00:18:07,200 --> 00:18:10,000
计算和通讯比

587
00:18:11,039 --> 00:18:12,759
这通讯就是你的模型大小

588
00:18:13,559 --> 00:18:15,680
所以说你的Flop是

589
00:18:15,680 --> 00:18:16,880
除以你的Model Size

590
00:18:16,880 --> 00:18:18,079
最好是越高越好

591
00:18:20,319 --> 00:18:21,480
对于单机来讲

592
00:18:21,480 --> 00:18:23,559
就是说那么你的

593
00:18:24,160 --> 00:18:25,440
你尽量是

594
00:18:25,759 --> 00:18:26,519
你的

595
00:18:27,279 --> 00:18:28,599
你的模型大小

596
00:18:28,920 --> 00:18:30,160
有一点点等价

597
00:18:30,160 --> 00:18:31,440
就是说每做一个Flop

598
00:18:31,440 --> 00:18:33,200
需要读多少数据

599
00:18:34,200 --> 00:18:34,720
OK

600
00:18:34,720 --> 00:18:35,799
但是是一个

601
00:18:36,880 --> 00:18:38,039
就是说更精确来讲

602
00:18:38,039 --> 00:18:39,319
就是说你最好是

603
00:18:39,360 --> 00:18:40,480
每计算一个Flop

604
00:18:40,480 --> 00:18:41,680
需要读多少个bit

605
00:18:41,680 --> 00:18:43,039
这是最精确的讲法

606
00:18:43,039 --> 00:18:44,559
但你不好衡量的话

607
00:18:44,559 --> 00:18:45,480
那就简单说

608
00:18:45,480 --> 00:18:46,440
你可以说

609
00:18:47,360 --> 00:18:49,120
就把它读的数据

610
00:18:49,120 --> 00:18:50,720
等价于你的模型大小了

611
00:18:51,000 --> 00:18:52,200
所以你会发现Inception

612
00:18:52,200 --> 00:18:52,840
我们有讲过

613
00:18:52,840 --> 00:18:54,160
Inception设计的

614
00:18:54,160 --> 00:18:56,640
使得它比同样的

615
00:18:57,080 --> 00:18:58,200
用3300卷机

616
00:18:58,200 --> 00:18:59,240
或者5300卷机

617
00:18:59,240 --> 00:19:00,600
它的模型要小

618
00:19:00,600 --> 00:19:01,120
我们有讲

619
00:19:01,120 --> 00:19:02,279
特意讲过这个事情

620
00:19:02,279 --> 00:19:04,399
所以Inception其实是

621
00:19:04,920 --> 00:19:06,519
比ResNet更好做并行

622
00:19:07,279 --> 00:19:08,920
但其他两个都差不多

623
00:19:08,920 --> 00:19:10,039
最差的是AlexNet

624
00:19:10,039 --> 00:19:10,759
AlexNet发一个

625
00:19:10,759 --> 00:19:11,559
那么大的东西出去

626
00:19:11,559 --> 00:19:13,279
它算起来要特别快

627
00:19:13,279 --> 00:19:14,799
所以AlexNet做并行

628
00:19:14,799 --> 00:19:15,720
是非常难的

629
00:19:16,599 --> 00:19:17,240
OK

630
00:19:17,279 --> 00:19:18,240
所以你用ResNet

631
00:19:18,240 --> 00:19:19,559
本质上是一般还行

632
00:19:21,799 --> 00:19:22,879
另外一个最后一个

633
00:19:23,000 --> 00:19:23,680
我们有讲过

634
00:19:23,680 --> 00:19:26,079
说你需要把P端大小大一点

635
00:19:26,119 --> 00:19:27,279
这样子你的系统性能

636
00:19:27,279 --> 00:19:28,039
会好一点

637
00:19:28,879 --> 00:19:29,559
但反过来讲

638
00:19:30,000 --> 00:19:31,559
你的P端大小大的话

639
00:19:31,559 --> 00:19:33,119
你得改你的优化算法

640
00:19:33,159 --> 00:19:34,799
我们现在调的优化算法

641
00:19:34,799 --> 00:19:36,399
就做的是最简单的

642
00:19:36,399 --> 00:19:37,720
把learning rate增加

643
00:19:37,759 --> 00:19:39,319
其实优化算法这一块

644
00:19:39,319 --> 00:19:41,000
有非常有挺多工作的

645
00:19:41,559 --> 00:19:43,000
作为超大的

646
00:19:44,119 --> 00:19:45,720
学习P端大小的

647
00:19:45,720 --> 00:19:46,960
说你怎么样做

648
00:19:46,960 --> 00:19:48,759
怎么样调整你的优化算法

649
00:19:48,759 --> 00:19:50,759
使得更适用你的P端大小

650
00:19:50,799 --> 00:19:53,039
而我们这堂课可能不会去讲

651
00:19:53,039 --> 00:19:54,879
但是大家可以去查一查这一块

652
00:19:55,119 --> 00:19:57,159
有比较成熟的方案

653
00:19:57,200 --> 00:19:58,559
就用超大的学习

654
00:19:58,559 --> 00:20:00,679
超大的P端大小

655
00:20:00,680 --> 00:20:03,519
怎么使知识超大的学习率

656
00:20:04,560 --> 00:20:05,240
OK

657
00:20:05,840 --> 00:20:08,759
就这个是一些实践的一些经验

658
00:20:08,920 --> 00:20:12,880
怎么样调多卡和多G的性能

659
00:20:13,880 --> 00:20:14,120
好

660
00:20:14,120 --> 00:20:15,160
这总结一下

661
00:20:15,840 --> 00:20:18,120
就分布式的数据同步

662
00:20:18,160 --> 00:20:18,680
并行

663
00:20:19,240 --> 00:20:22,240
就和多GPU的数据并行

664
00:20:22,440 --> 00:20:23,519
其实差不多

665
00:20:23,519 --> 00:20:24,400
就是说

666
00:20:24,400 --> 00:20:26,200
现在你要从平台网络上了

667
00:20:26,200 --> 00:20:27,840
以前是你在机器内部通讯

668
00:20:27,840 --> 00:20:29,120
现在在网络上通讯

669
00:20:29,640 --> 00:20:30,800
另外一个就是说

670
00:20:30,840 --> 00:20:32,600
你的网络通讯

671
00:20:32,600 --> 00:20:34,920
网络通讯是你的瓶颈

672
00:20:35,280 --> 00:20:36,160
就你的性能

673
00:20:36,160 --> 00:20:39,080
通常是被卡在你的通讯上面

674
00:20:41,080 --> 00:20:42,000
所以通常来说

675
00:20:42,000 --> 00:20:43,640
我们会使用大的P端大小

676
00:20:43,640 --> 00:20:45,320
但是使用大的P端大小的话

677
00:20:45,320 --> 00:20:46,480
你要特别要注意

678
00:20:46,480 --> 00:20:48,560
是你的收敛的效率

679
00:20:49,440 --> 00:20:50,000
OK

680
00:20:50,040 --> 00:20:52,200
就是说你不能为了追求

681
00:20:52,200 --> 00:20:54,080
极度追求系统的性能

682
00:20:54,080 --> 00:20:56,400
从而不考虑你的收敛

683
00:20:56,440 --> 00:20:57,680
所以很多去论文

684
00:20:57,680 --> 00:20:59,600
说我能够并行度数多高

685
00:20:59,600 --> 00:21:00,200
其实没意义

686
00:21:00,200 --> 00:21:01,240
就最后是说

687
00:21:01,240 --> 00:21:02,160
你要看的说

688
00:21:02,160 --> 00:21:04,840
达到某一个要的精度的时候

689
00:21:04,840 --> 00:21:06,120
你花的时间是多少

690
00:21:07,240 --> 00:21:08,560
另外一块就是说

691
00:21:08,560 --> 00:21:10,920
我们没有讲的是

692
00:21:10,960 --> 00:21:13,320
异步怎么做

693
00:21:14,080 --> 00:21:15,120
就是说每个机器

694
00:21:15,120 --> 00:21:17,640
有做自己的批量更新

695
00:21:17,640 --> 00:21:18,920
然后不用去同步

696
00:21:18,920 --> 00:21:19,400
这样子

697
00:21:19,400 --> 00:21:21,039
它的通讯开销会变低

698
00:21:21,240 --> 00:21:23,000
另外一块是模型并行

699
00:21:23,039 --> 00:21:24,120
分布式的模型并行

700
00:21:24,120 --> 00:21:25,480
我们是没有讲

701
00:21:25,960 --> 00:21:27,640
分布式模型并行做的不多

702
00:21:28,000 --> 00:21:29,039
通常来说

703
00:21:29,039 --> 00:21:30,839
特别就算是那种比较大的

704
00:21:30,839 --> 00:21:32,000
GPT-3那种模型

705
00:21:32,160 --> 00:21:32,960
特别大模型

706
00:21:32,960 --> 00:21:35,880
也是在单机内部做模型并行

707
00:21:35,880 --> 00:21:37,920
但是在跨机器的时候

708
00:21:37,920 --> 00:21:39,799
通常是用的是数据并行

709
00:21:40,559 --> 00:21:41,079
OK

710
00:21:41,079 --> 00:21:43,640
这就是我们的分布式了

