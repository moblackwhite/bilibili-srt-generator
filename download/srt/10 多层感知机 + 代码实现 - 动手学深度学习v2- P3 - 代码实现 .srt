1
00:00:00,000 --> 00:00:05,000
接下来我们来看多层感知器

2
00:00:05,000 --> 00:00:09,200
这个是我们现在深度学习也是经常使用的一个模型

3
00:00:09,200 --> 00:00:11,599
OK

4
00:00:11,599 --> 00:00:14,720
我们先把多层感知器过了之后

5
00:00:14,720 --> 00:00:15,919
我们统一来回答问题

6
00:00:15,919 --> 00:00:22,920
所以我们还是回到我们的XOR这个问题

7
00:00:22,920 --> 00:00:27,199
我们来看一下我们怎么样来解决这个问题

8
00:00:27,199 --> 00:00:29,560
还是这样

9
00:00:29,560 --> 00:00:32,560
我们有4个点

10
00:00:32,560 --> 00:00:33,640
这4个点

11
00:00:33,640 --> 00:00:39,840
然后我们想要把它完全分类的话

12
00:00:39,840 --> 00:00:44,840
那么你肯定是走单线性模型就不行了

13
00:00:44,840 --> 00:00:49,840
那么我们现在可以通过几步来完成它

14
00:00:49,840 --> 00:00:51,200
我们先看第一步

15
00:00:51,200 --> 00:00:53,520
第一步我们学习一条蓝色的线

16
00:00:53,520 --> 00:00:54,840
就是蓝色的线

17
00:00:54,840 --> 00:00:57,079
蓝色的线

18
00:00:57,399 --> 00:00:59,199
就是说

19
00:00:59,199 --> 00:01:01,799
等下我再写两个东西在这里

20
00:01:01,799 --> 00:01:06,400
写一个还是X和Y

21
00:01:06,400 --> 00:01:07,920
蓝色的线是说

22
00:01:07,920 --> 00:01:10,400
当你X是1的时候

23
00:01:10,400 --> 00:01:12,000
那么全部放在这一边

24
00:01:12,000 --> 00:01:14,239
你X小一点的时候全部走这一边

25
00:01:14,239 --> 00:01:16,239
所以是蓝色的分隔面干的事情

26
00:01:16,239 --> 00:01:18,439
所以它对应的是说这一行

27
00:01:18,439 --> 00:01:20,719
你可以看到是蓝色这个线

28
00:01:20,719 --> 00:01:24,719
对1和3

29
00:01:24,719 --> 00:01:26,799
我们就给的是一个正好

30
00:01:26,840 --> 00:01:28,719
就2和4

31
00:01:28,719 --> 00:01:29,679
给的是一个负好

32
00:01:29,679 --> 00:01:31,280
在这个地方看见没有

33
00:01:32,120 --> 00:01:35,599
然后我们再学习一根黄色的线

34
00:01:35,879 --> 00:01:37,039
黄色的线就是说

35
00:01:37,039 --> 00:01:39,000
你根据Y的值来看

36
00:01:39,359 --> 00:01:41,120
Y的值你可以看到是说

37
00:01:41,120 --> 00:01:42,920
当你1和2

38
00:01:42,920 --> 00:01:43,599
就1和2

39
00:01:43,599 --> 00:01:44,879
我们给一个正的

40
00:01:45,200 --> 00:01:46,319
然后3和4

41
00:01:46,319 --> 00:01:47,200
3和4

42
00:01:47,200 --> 00:01:49,079
我们是给一个负的值

43
00:01:51,120 --> 00:01:55,319
那么有了蓝色的分类器和黄色的分类器

44
00:01:55,319 --> 00:01:56,079
结果的话

45
00:01:56,079 --> 00:01:58,879
我们再对两个结果做乘法

46
00:01:59,359 --> 00:02:01,719
你看到是说相同的值

47
00:02:01,719 --> 00:02:04,239
我们就是做等于

48
00:02:05,159 --> 00:02:06,679
他不是做乘法

49
00:02:06,679 --> 00:02:08,800
就是说看你是不是一样

50
00:02:08,800 --> 00:02:09,960
就是说你是一样的话

51
00:02:09,960 --> 00:02:10,680
那么就是说

52
00:02:10,680 --> 00:02:12,000
正正我们是得正

53
00:02:12,079 --> 00:02:13,800
如果是负负的话也得正

54
00:02:13,840 --> 00:02:15,199
如果你不一样的话

55
00:02:15,240 --> 00:02:18,479
那我们就是得成做成一个负义

56
00:02:21,280 --> 00:02:21,840
OK

57
00:02:22,439 --> 00:02:23,919
所以这样子的话

58
00:02:23,920 --> 00:02:27,560
那么我们就可以对它做一个分类了

59
00:02:27,560 --> 00:02:28,560
可以看到这个图

60
00:02:29,240 --> 00:02:32,080
我们假设我们要把这个图画出来的话

61
00:02:32,080 --> 00:02:33,120
那就是

62
00:02:34,560 --> 00:02:36,560
我们这里是X

63
00:02:36,560 --> 00:02:37,560
这里是Y

64
00:02:37,960 --> 00:02:40,120
我们先进入我们的蓝色的

65
00:02:40,280 --> 00:02:41,920
一个蓝色分类器

66
00:02:41,920 --> 00:02:43,400
再进入我们的黄色分类器

67
00:02:43,440 --> 00:02:46,200
最后它的结果进入我们的灰色分类器

68
00:02:47,120 --> 00:02:48,080
灰色分类器

69
00:02:48,080 --> 00:02:50,200
我们就可以得到我们正确的结果

70
00:02:50,760 --> 00:02:51,760
所以说

71
00:02:52,759 --> 00:02:54,039
这什么意思呢

72
00:02:54,239 --> 00:02:55,239
就是说

73
00:02:55,280 --> 00:02:56,879
假设你一次做不了

74
00:02:56,879 --> 00:03:00,000
那我先学一个简单一点的函数

75
00:03:00,000 --> 00:03:01,039
再学一个简单函数

76
00:03:01,039 --> 00:03:02,439
然后再用另外一个简单函数

77
00:03:02,439 --> 00:03:03,679
组合两个函数

78
00:03:03,799 --> 00:03:07,439
那么就是说我们就从一层变成了多层

79
00:03:07,479 --> 00:03:10,439
这就是多层感知机所要干的事情

80
00:03:12,599 --> 00:03:13,319
简单来讲

81
00:03:13,319 --> 00:03:15,039
它就是一个这样子一个图

82
00:03:15,680 --> 00:03:19,439
首先我们的输入是有X1

83
00:03:19,439 --> 00:03:19,959
X2

84
00:03:19,959 --> 00:03:20,399
X3

85
00:03:20,399 --> 00:03:21,120
X4

86
00:03:21,439 --> 00:03:22,159
4个

87
00:03:22,599 --> 00:03:24,960
然后我们加入了一个隐藏层

88
00:03:25,400 --> 00:03:26,640
就是说它是一个

89
00:03:26,640 --> 00:03:28,360
假设我们有5个隐藏层的话

90
00:03:28,400 --> 00:03:29,400
那么就是说

91
00:03:30,319 --> 00:03:33,240
所谓的第1个X1到X4的话

92
00:03:33,240 --> 00:03:36,039
先进入算第1个

93
00:03:36,080 --> 00:03:36,960
H1

94
00:03:36,960 --> 00:03:39,000
然后算到H1到H5

95
00:03:39,240 --> 00:03:41,439
然后它的再作为输入

96
00:03:41,439 --> 00:03:42,599
放到一个下一个层

97
00:03:43,800 --> 00:03:47,120
所以隐藏层的大小是一个超参数

98
00:03:47,879 --> 00:03:48,400
为什么呢

99
00:03:48,400 --> 00:03:50,400
是因为输入的大小

100
00:03:50,400 --> 00:03:51,280
你是不能改的

101
00:03:51,280 --> 00:03:53,200
因为你数据维度有多大

102
00:03:53,200 --> 00:03:54,000
那就有多大

103
00:03:54,319 --> 00:03:56,719
然后但是你的输出的话

104
00:03:57,439 --> 00:03:59,920
你就是看你有多少类了

105
00:04:00,159 --> 00:04:01,840
你的输出的大小等于多少类

106
00:04:01,879 --> 00:04:04,040
所以都是你的数据决定了

107
00:04:04,080 --> 00:04:05,400
所以你唯一能干的事情

108
00:04:05,400 --> 00:04:07,120
就是说我能够设置

109
00:04:07,159 --> 00:04:09,000
我的隐藏层是有多大

110
00:04:10,120 --> 00:04:10,680
OK

111
00:04:14,560 --> 00:04:15,560
我们来

112
00:04:15,719 --> 00:04:19,600
看一下它是具体来讲是怎么做的

113
00:04:19,600 --> 00:04:22,040
首先我们还是看到感知机的模型

114
00:04:22,040 --> 00:04:24,000
我们考虑一个单分类的问题

115
00:04:24,839 --> 00:04:26,120
输入我们跟之前一样

116
00:04:26,120 --> 00:04:29,240
是一个N维的一个向量

117
00:04:30,399 --> 00:04:31,040
隐藏层

118
00:04:31,879 --> 00:04:33,800
就是说假设我们的隐藏层的大小

119
00:04:33,800 --> 00:04:35,079
是M的话

120
00:04:35,199 --> 00:04:36,360
那么的隐藏层

121
00:04:36,360 --> 00:04:38,519
它有一个W1

122
00:04:38,519 --> 00:04:41,680
那就是一个M乘以一个N的一个矩阵

123
00:04:43,399 --> 00:04:44,759
我的偏移

124
00:04:45,039 --> 00:04:46,279
我有多少个隐藏层

125
00:04:46,279 --> 00:04:48,279
那么就有多少个标量的偏移

126
00:04:48,279 --> 00:04:52,000
所以它也是一个成为M的一个向量

127
00:04:53,639 --> 00:04:54,639
输出层

128
00:04:54,959 --> 00:04:56,439
这个是我们新的一个东西

129
00:04:56,639 --> 00:04:57,240
输出层

130
00:04:57,240 --> 00:04:58,800
因为我们是做单分类

131
00:04:58,800 --> 00:05:00,279
我们只输出一个的话

132
00:05:00,360 --> 00:05:02,199
那么输出层就是一个很简单的

133
00:05:02,199 --> 00:05:04,240
一个成为M的一个向量

134
00:05:04,480 --> 00:05:05,680
因为我们的输入

135
00:05:05,719 --> 00:05:08,120
隐藏层的M隐藏层是有M的话

136
00:05:08,120 --> 00:05:09,159
那么隐藏层的输出

137
00:05:09,159 --> 00:05:11,480
就是M成为M的一个向量

138
00:05:11,480 --> 00:05:13,399
那么到对于输入层来讲

139
00:05:13,400 --> 00:05:15,080
它的输入的维度就是M

140
00:05:15,200 --> 00:05:16,280
但它是单分类

141
00:05:16,400 --> 00:05:17,840
所以它的输出就是E

142
00:05:17,840 --> 00:05:19,080
这就是一个向量

143
00:05:19,280 --> 00:05:20,440
那么它的偏移

144
00:05:20,440 --> 00:05:21,440
那也是一个标

145
00:05:21,440 --> 00:05:22,680
也是一个标量

146
00:05:23,840 --> 00:05:25,360
那么具体怎么计算的话

147
00:05:25,360 --> 00:05:26,120
就是说

148
00:05:26,560 --> 00:05:27,880
这个跟之前是一样的

149
00:05:27,880 --> 00:05:31,040
跟我们的softmax回归是一样的

150
00:05:31,200 --> 00:05:32,120
就是说

151
00:05:32,880 --> 00:05:35,400
输入乘以我们的权重

152
00:05:35,560 --> 00:05:39,080
加上我们的偏移

153
00:05:39,480 --> 00:05:41,280
那么但是这里有一个东西

154
00:05:41,280 --> 00:05:42,800
就是一个sigma东西

155
00:05:43,240 --> 00:05:45,199
sigma我们可以之后再看是什么

156
00:05:45,199 --> 00:05:46,600
这个是一个激活函数

157
00:05:46,639 --> 00:05:48,840
它是一个按元素

158
00:05:48,960 --> 00:05:50,319
做运算的一个函数

159
00:05:51,520 --> 00:05:54,759
然后H是一个成为M的一个向量

160
00:05:54,960 --> 00:05:58,960
它作为输入进入到输出层

161
00:05:59,080 --> 00:06:00,759
那么就是说输出层的

162
00:06:01,319 --> 00:06:02,720
权重是W2

163
00:06:02,759 --> 00:06:06,280
那么它的转制乘以它向量做累积

164
00:06:06,280 --> 00:06:07,519
加上我们的偏移

165
00:06:07,560 --> 00:06:09,280
那么输出就是一个标量

166
00:06:09,280 --> 00:06:10,439
就是为了O了

167
00:06:11,600 --> 00:06:12,199
OK

168
00:06:12,800 --> 00:06:14,120
所以这就是说

169
00:06:15,879 --> 00:06:19,240
单影长程做单分类是什么样的

170
00:06:22,400 --> 00:06:24,400
我们接下来往下看是说

171
00:06:25,240 --> 00:06:28,560
为什么我们要一个激活函数

172
00:06:28,960 --> 00:06:30,280
而且激活函数

173
00:06:30,280 --> 00:06:31,840
一定是要一个非线性的

174
00:06:31,879 --> 00:06:35,400
你不能c,x就等于x是不行的

175
00:06:35,400 --> 00:06:37,439
或者等于n倍x也是不行的

176
00:06:37,800 --> 00:06:39,040
我们来看一下为什么

177
00:06:39,240 --> 00:06:46,680
就假设我们的激活函数是本身的话

178
00:06:46,680 --> 00:06:49,200
那就是c,x等于x的话

179
00:06:49,240 --> 00:06:50,840
那么你会变成什么样子呢

180
00:06:51,560 --> 00:06:53,360
就是说我的H

181
00:06:53,560 --> 00:06:55,280
它就是等于你的输入了

182
00:06:55,320 --> 00:06:57,640
叫c,i就是等于本身了

183
00:06:57,960 --> 00:06:59,920
那么我如果把H

184
00:07:00,320 --> 00:07:01,560
带入到这个地方

185
00:07:02,759 --> 00:07:04,920
你可以看到是说我把这一项带进来

186
00:07:04,920 --> 00:07:05,720
那就是

187
00:07:05,960 --> 00:07:08,800
这一项会变成了一个W2的转制

188
00:07:08,800 --> 00:07:09,840
乘以W1

189
00:07:10,080 --> 00:07:10,960
乘以x

190
00:07:11,600 --> 00:07:12,760
然后后面一项我就不管了

191
00:07:12,760 --> 00:07:14,480
后面一项就是反正没有

192
00:07:15,040 --> 00:07:17,040
反正是一个项量

193
00:07:17,040 --> 00:07:18,720
跟一个另外一个项量做累积

194
00:07:18,760 --> 00:07:19,760
再加上一个标量

195
00:07:19,760 --> 00:07:20,760
还是一个标量

196
00:07:21,600 --> 00:07:22,960
就看到是说

197
00:07:23,800 --> 00:07:25,680
假设我c,x是我写一下

198
00:07:25,840 --> 00:07:29,400
假设我c,x等于x的话

199
00:07:29,800 --> 00:07:30,880
那么你这个地方

200
00:07:31,600 --> 00:07:34,600
它的输出还是一个线性函数

201
00:07:35,000 --> 00:07:36,400
因为你这个项

202
00:07:37,040 --> 00:07:39,320
这个项就等价于一个

203
00:07:39,560 --> 00:07:42,360
比如说等价一个WPA的转制

204
00:07:42,360 --> 00:07:43,600
它还是一个项量

205
00:07:43,720 --> 00:07:45,080
所以你还是一个线性模型

206
00:07:45,720 --> 00:07:47,000
你可以换成

207
00:07:47,040 --> 00:07:48,640
比如说我再加一个a进来

208
00:07:48,800 --> 00:07:49,560
加一个a进来

209
00:07:49,560 --> 00:07:50,760
也不会有什么变化

210
00:07:50,800 --> 00:07:52,040
那就再多一个a

211
00:07:52,400 --> 00:07:53,560
就是说你的sigma

212
00:07:53,560 --> 00:07:55,439
就一定不能是一个线性函数

213
00:07:55,720 --> 00:07:57,200
如果你是线性函数的话

214
00:07:57,240 --> 00:08:00,280
你会发现它等价于一个单层的

215
00:08:00,840 --> 00:08:01,560
感知剂

216
00:08:02,079 --> 00:08:05,480
所以这个也是我们实现的时候

217
00:08:05,519 --> 00:08:06,560
常犯了一个错误

218
00:08:06,560 --> 00:08:07,399
就是说

219
00:08:07,480 --> 00:08:10,000
你不小心没有加激活函数

220
00:08:11,160 --> 00:08:13,600
那么就是说你把n个权利阶层

221
00:08:13,600 --> 00:08:14,360
堕在一起

222
00:08:14,439 --> 00:08:15,399
最后发现结果

223
00:08:15,399 --> 00:08:18,720
其实还是一个最简单的线性模型

224
00:08:20,199 --> 00:08:20,519
OK

225
00:08:20,519 --> 00:08:22,920
这个就是激活函数

226
00:08:23,959 --> 00:08:26,079
激活函数我们有一些选择

227
00:08:26,399 --> 00:08:27,680
所以最简单

228
00:08:27,680 --> 00:08:28,519
也是最经典的

229
00:08:28,519 --> 00:08:29,759
叫做sigma的函数

230
00:08:30,279 --> 00:08:32,399
它就是说对于x来讲

231
00:08:32,399 --> 00:08:33,439
我不管是什么值

232
00:08:33,439 --> 00:08:36,799
我把你投影到一个0和1的一个区间里面

233
00:08:36,799 --> 00:08:38,039
而且是一个开区间

234
00:08:38,799 --> 00:08:40,960
你可认为它就是一个soft version

235
00:08:40,960 --> 00:08:42,000
就是说如果

236
00:08:42,039 --> 00:08:44,080
记得我们之前的感知剂讲的是什么

237
00:08:44,279 --> 00:08:45,039
它就是

238
00:08:45,279 --> 00:08:46,919
如果你的x大于0的话

239
00:08:46,960 --> 00:08:47,960
那我就变成1

240
00:08:48,159 --> 00:08:50,000
如果你不然的话就变成0

241
00:08:50,639 --> 00:08:52,600
但是如果你是这个函数的话

242
00:08:52,600 --> 00:08:53,240
它很硬

243
00:08:53,240 --> 00:08:54,080
这个函数的话

244
00:08:54,080 --> 00:08:56,120
它其实你要画出来就是一个

245
00:08:57,120 --> 00:08:59,960
画出来就是一个这样子的东西

246
00:09:02,960 --> 00:09:04,039
它这么一个

247
00:09:04,080 --> 00:09:05,679
大家我们之前有讲过

248
00:09:05,879 --> 00:09:08,240
这个地方不好求到

249
00:09:08,240 --> 00:09:10,399
这一块地方不好求到

250
00:09:10,519 --> 00:09:14,879
所以就是sigma其实它就是一个soft版本

251
00:09:15,120 --> 00:09:17,080
就是说你可以看到是

252
00:09:17,120 --> 00:09:20,639
我这也就是把你这么给你接过来了

253
00:09:20,639 --> 00:09:22,080
就是比较平和的一个版本

254
00:09:22,560 --> 00:09:23,840
这就是sigma

255
00:09:24,360 --> 00:09:26,160
它的具体的定义就是

256
00:09:26,200 --> 00:09:28,960
你的x我先做

257
00:09:28,960 --> 00:09:30,440
-x进去

258
00:09:30,440 --> 00:09:32,519
然后这个像把它变成一个

259
00:09:32,680 --> 00:09:33,480
正数

260
00:09:33,480 --> 00:09:35,240
然后1加上一个正数分之一

261
00:09:35,560 --> 00:09:37,960
一定是在一个0和1之间的区间里面

262
00:09:39,160 --> 00:09:39,680
OK

263
00:09:41,519 --> 00:09:45,000
另外一个常用的激活函数叫做time edge

264
00:09:45,879 --> 00:09:47,440
它其实跟sigma的很像

265
00:09:47,800 --> 00:09:49,600
但是它的区别是说

266
00:09:49,600 --> 00:09:52,080
它是将输入投影到一个

267
00:09:52,080 --> 00:09:54,160
-1和1的一个区间里面

268
00:09:55,480 --> 00:09:56,680
它的定义当然是

269
00:09:56,720 --> 00:09:58,480
1减去你的exp

270
00:09:59,720 --> 00:10:01,280
除以1加上exp

271
00:10:01,320 --> 00:10:03,280
都是有个far在里面

272
00:10:03,720 --> 00:10:05,720
我们可能之后可以给大家解释一下

273
00:10:05,720 --> 00:10:06,920
为什么要有个far

274
00:10:07,200 --> 00:10:08,400
far就是说

275
00:10:10,000 --> 00:10:13,759
当然是说你可以看到它仍然它是一个

276
00:10:13,800 --> 00:10:15,680
它是一个这个函数的

277
00:10:15,920 --> 00:10:18,400
一个soft的版本

278
00:10:19,080 --> 00:10:20,759
如果我们就是把这个函数

279
00:10:20,759 --> 00:10:22,919
很硬的拉到一个正义到负义的话

280
00:10:22,960 --> 00:10:25,279
那其实是不是那么的好

281
00:10:25,639 --> 00:10:26,879
训练起来比较麻烦

282
00:10:28,799 --> 00:10:31,600
另外一个我们其实最常用的是一个

283
00:10:31,600 --> 00:10:33,439
叫relu的一个激活函数

284
00:10:33,720 --> 00:10:36,200
它的真实的名字叫做

285
00:10:36,200 --> 00:10:38,519
rectified linear units

286
00:10:38,759 --> 00:10:41,039
其实它就是一个max x和0

287
00:10:41,360 --> 00:10:42,319
就是说你

288
00:10:42,879 --> 00:10:44,000
我们整个深度学习

289
00:10:45,960 --> 00:10:48,600
把一些东西给你从命名一下

290
00:10:49,080 --> 00:10:49,960
就是说

291
00:10:51,000 --> 00:10:52,279
backpropagation

292
00:10:52,279 --> 00:10:52,800
对吧

293
00:10:54,480 --> 00:10:56,560
那其实就是一个自动求到领域的

294
00:10:56,560 --> 00:10:57,440
一个很经典的模型

295
00:10:57,440 --> 00:10:58,720
就是说基本上

296
00:10:59,240 --> 00:11:01,600
深度学习就是把很多旧的东西

297
00:11:01,600 --> 00:11:02,480
我们重新

298
00:11:02,639 --> 00:11:03,720
至少深度学习在

299
00:11:03,720 --> 00:11:08,480
我觉得在2014年前

300
00:11:08,639 --> 00:11:10,080
基本上都是一些

301
00:11:10,200 --> 00:11:12,000
经典的东西把它从命名

302
00:11:12,040 --> 00:11:14,000
但之后我们确实有新的东西出来

303
00:11:14,240 --> 00:11:15,600
我大概就是

304
00:11:15,600 --> 00:11:18,320
当然你现在吐槽也就能吐到2014年的样子

305
00:11:19,000 --> 00:11:19,560
Ok

306
00:11:19,560 --> 00:11:21,960
所以relu函数本身就是一个很简单的东西

307
00:11:21,960 --> 00:11:23,040
它就是一个

308
00:11:23,240 --> 00:11:25,200
x和0求max

309
00:11:25,440 --> 00:11:29,320
回忆一下我们之前感知机是什么回事

310
00:11:29,320 --> 00:11:31,680
所以它的函数画起来就是个这样子的东西

311
00:11:32,240 --> 00:11:33,080
所以它的函数

312
00:11:33,080 --> 00:11:35,000
大家会记到求到是说

313
00:11:35,560 --> 00:11:38,200
它这个项是这个项的导数是正义

314
00:11:38,200 --> 00:11:38,680
对吧

315
00:11:38,759 --> 00:11:39,840
后面导数是0

316
00:11:40,240 --> 00:11:43,920
就是说如果它就是一个很简单的

317
00:11:43,920 --> 00:11:44,800
本来是你

318
00:11:45,120 --> 00:11:47,279
你cx等于x肯定是不行的

319
00:11:47,279 --> 00:11:47,560
对吧

320
00:11:47,560 --> 00:11:49,600
这个就是cx等于x的一个函数

321
00:11:49,600 --> 00:11:51,200
那线性函数是不行

322
00:11:51,320 --> 00:11:53,520
你怎么把它的线性去掉

323
00:11:53,720 --> 00:11:55,560
那就是把它砍一下

324
00:11:55,960 --> 00:11:56,400
对吧

325
00:11:56,600 --> 00:11:59,640
就把小于0的话就放掉了

326
00:12:00,560 --> 00:12:03,400
叫relu其实大家当然是说

327
00:12:03,400 --> 00:12:04,960
一开始大家出来的时候说

328
00:12:05,080 --> 00:12:05,880
这个函数特别好

329
00:12:05,880 --> 00:12:06,520
特别好

330
00:12:06,520 --> 00:12:08,280
就是写paper

331
00:12:08,440 --> 00:12:10,240
就是我们要说这个很noble

332
00:12:10,400 --> 00:12:11,080
我们要

333
00:12:11,160 --> 00:12:12,880
你就说一个max肯定是不行的

334
00:12:13,160 --> 00:12:13,840
我们说

335
00:12:13,920 --> 00:12:15,240
首先换一个名字

336
00:12:15,440 --> 00:12:16,960
说这个名字特别fancy

337
00:12:17,120 --> 00:12:19,280
第二个是说我的效果特别好

338
00:12:19,560 --> 00:12:22,240
实际上其实后来发现大家差不多

339
00:12:22,240 --> 00:12:25,600
其实没什么太多区别

340
00:12:25,600 --> 00:12:26,840
我们可能会之后

341
00:12:26,840 --> 00:12:28,920
我们应该会之后给大家稍微讲一下

342
00:12:28,920 --> 00:12:31,040
但是它的主要的好处是什么

343
00:12:31,040 --> 00:12:32,680
它算起来很快

344
00:12:32,960 --> 00:12:34,759
不要做指数运算

345
00:12:34,759 --> 00:12:36,120
看一下回忆一下之前

346
00:12:36,120 --> 00:12:38,800
之前我们要做两个指数运算

347
00:12:39,240 --> 00:12:41,080
这里我们要做一个指数运算

348
00:12:41,680 --> 00:12:43,639
指数运算是一件很贵的事情

349
00:12:44,040 --> 00:12:45,680
在cpu上

350
00:12:45,800 --> 00:12:47,400
一次指数运算

351
00:12:47,400 --> 00:12:50,520
你可能要等价于算100次乘法运算的

352
00:12:50,520 --> 00:12:51,280
成本

353
00:12:51,280 --> 00:12:52,840
GPU稍微好一点点

354
00:12:52,840 --> 00:12:55,120
它有自己的单元来做指数

355
00:12:55,120 --> 00:12:56,040
但还是很贵

356
00:12:56,040 --> 00:12:56,960
但现在好了

357
00:12:57,280 --> 00:12:58,720
我就一个max搞定了

358
00:12:59,160 --> 00:13:02,160
所以这个就是为什么大家都用relu

359
00:13:02,160 --> 00:13:03,480
主要是它简单

360
00:13:04,200 --> 00:13:04,600
OK

361
00:13:07,280 --> 00:13:07,520
好

362
00:13:07,520 --> 00:13:10,480
我们再做到多类的问题

363
00:13:10,640 --> 00:13:12,560
就是刚刚我们讲的是softmax

364
00:13:12,560 --> 00:13:13,800
是一个单类问题

365
00:13:14,360 --> 00:13:15,920
我们可以做到多类

366
00:13:16,280 --> 00:13:17,880
多类的话会怎么样

367
00:13:17,880 --> 00:13:19,800
就是说

368
00:13:19,920 --> 00:13:21,160
那么跟我们之前一样

369
00:13:21,160 --> 00:13:23,880
跟多类softmax回归没本质区别

370
00:13:24,120 --> 00:13:26,760
就是假设我要做k类分类的话

371
00:13:26,800 --> 00:13:29,800
那么我就是要输出k个元素

372
00:13:30,600 --> 00:13:33,760
然后因为我们想要得到知性度的话

373
00:13:33,960 --> 00:13:36,360
我们就放到一个softmax的一个操作

374
00:13:36,400 --> 00:13:37,400
子里面

375
00:13:37,600 --> 00:13:39,600
拿到的是y1-yk

376
00:13:40,360 --> 00:13:42,080
softmax大家还记得

377
00:13:42,680 --> 00:13:43,560
它也不干什么事情

378
00:13:43,840 --> 00:13:45,240
就是把所有的输入

379
00:13:45,240 --> 00:13:48,200
拉到一个0和1之间的区域

380
00:13:48,240 --> 00:13:49,800
然后使得它加起来

381
00:13:50,440 --> 00:13:52,680
y1-yk加起来等于1

382
00:13:52,680 --> 00:13:53,880
就变成一个

383
00:13:54,440 --> 00:13:55,400
概率

384
00:13:55,520 --> 00:13:56,920
这就是softmax干的事情

385
00:13:57,240 --> 00:13:58,680
所以你可以看到多类分类

386
00:13:58,680 --> 00:14:00,240
跟softmax没本质区别

387
00:14:00,240 --> 00:14:02,280
是说我唯一加的就是这一块了

388
00:14:02,560 --> 00:14:03,400
我来画一下

389
00:14:04,120 --> 00:14:05,760
我唯一加的就是中间那一层

390
00:14:05,920 --> 00:14:06,840
假设没有的话

391
00:14:06,840 --> 00:14:09,280
那就是我们最简单softmax回归

392
00:14:09,840 --> 00:14:11,480
假设我加了一层隐藏层

393
00:14:11,800 --> 00:14:14,480
那么它就会变成多层感知基

394
00:14:15,840 --> 00:14:16,120
OK

395
00:14:16,120 --> 00:14:17,720
这就是一个名字上的变化

396
00:14:17,720 --> 00:14:19,920
就是说你看到它就是在

397
00:14:19,960 --> 00:14:22,240
softmax回归里面加入了一层

398
00:14:26,320 --> 00:14:29,200
那么我们来看一下它的定义

399
00:14:29,399 --> 00:14:30,879
定义跟之前没本质区别

400
00:14:31,200 --> 00:14:32,519
唯一的区别就是

401
00:14:32,800 --> 00:14:34,080
这里我们变成了k

402
00:14:34,800 --> 00:14:38,120
就是说因为我们的输出要有k个单元

403
00:14:38,120 --> 00:14:40,480
所以我们的输出层的W2

404
00:14:40,480 --> 00:14:43,240
就是一个m乘以一个k的矩阵

405
00:14:43,279 --> 00:14:44,639
那么的偏移

406
00:14:45,399 --> 00:14:47,320
那就是bias也是一个

407
00:14:47,320 --> 00:14:48,600
成为k的一个向量

408
00:14:49,399 --> 00:14:52,600
所以另外一个跟之前的区别是说

409
00:14:52,759 --> 00:14:54,240
所以这里面其实没区别

410
00:14:54,440 --> 00:14:56,200
这就是说你从

411
00:14:57,759 --> 00:15:00,039
我们从一个

412
00:15:00,039 --> 00:15:01,680
因为我们是这个写法

413
00:15:01,680 --> 00:15:03,000
我们为了跟之前统一

414
00:15:03,000 --> 00:15:04,320
所以我们把k放在后面

415
00:15:04,320 --> 00:15:05,960
所以这里有个转制在这个地方

416
00:15:06,279 --> 00:15:08,920
所以你这里就是从向量变成了矩阵

417
00:15:09,040 --> 00:15:10,760
这里从标量变成了向量

418
00:15:11,280 --> 00:15:12,560
另外一个区别是说

419
00:15:12,600 --> 00:15:13,840
对于你的output来讲

420
00:15:13,840 --> 00:15:15,920
我们要做一次softmax

421
00:15:16,320 --> 00:15:17,240
这就是

422
00:15:17,760 --> 00:15:20,520
做多类分类的多层感知器

423
00:15:20,520 --> 00:15:21,440
是长什么样子

424
00:15:25,920 --> 00:15:27,440
当然我可以做多隐藏层

425
00:15:28,160 --> 00:15:28,960
就是说

426
00:15:29,160 --> 00:15:30,760
你有一层还不够

427
00:15:30,800 --> 00:15:32,080
我可以做很多层

428
00:15:32,680 --> 00:15:34,440
这里就是一个很简单的一个图

429
00:15:34,440 --> 00:15:36,440
说我可以做很多层

430
00:15:37,120 --> 00:15:38,440
输入层在这个地方

431
00:15:39,400 --> 00:15:40,360
一个隐藏层

432
00:15:40,400 --> 00:15:41,200
第二个隐藏层

433
00:15:41,200 --> 00:15:42,160
第三个隐藏层

434
00:15:42,440 --> 00:15:44,120
这是我们的输出

435
00:15:45,120 --> 00:15:45,400
那么

436
00:15:47,000 --> 00:15:48,120
数学上来说

437
00:15:48,120 --> 00:15:49,600
就是每一个隐藏层

438
00:15:49,600 --> 00:15:53,200
它都有一个自己的we和一个

439
00:15:54,400 --> 00:15:55,400
偏移b

440
00:15:56,120 --> 00:15:56,840
就是说第一层

441
00:15:56,840 --> 00:15:59,600
我们假设就记说we和be的话

442
00:15:59,640 --> 00:16:01,040
那么它的输出

443
00:16:01,520 --> 00:16:02,040
记得一下

444
00:16:02,200 --> 00:16:03,600
这里有一个激活函数

445
00:16:04,000 --> 00:16:05,480
进入到一个

446
00:16:06,480 --> 00:16:07,120
h1

447
00:16:07,120 --> 00:16:10,800
就是第一个隐藏层的输出

448
00:16:11,159 --> 00:16:12,960
那么它就会

449
00:16:14,120 --> 00:16:16,120
可能它就会作为

450
00:16:16,279 --> 00:16:18,480
第二个隐藏层的输入

451
00:16:19,080 --> 00:16:20,200
然后同样的话

452
00:16:20,200 --> 00:16:22,279
会跟权重做惩罚

453
00:16:22,279 --> 00:16:23,840
加上我们的偏移

454
00:16:24,000 --> 00:16:25,560
再做一次激活函数

455
00:16:25,759 --> 00:16:27,720
记得这个激活函数不能少

456
00:16:27,759 --> 00:16:28,680
如果你少了一个

457
00:16:28,680 --> 00:16:30,680
那么就层数就减一了

458
00:16:31,360 --> 00:16:31,920
同样的话

459
00:16:31,920 --> 00:16:33,560
它的输出是h2的话

460
00:16:33,560 --> 00:16:34,960
它会进入下一个层

461
00:16:35,840 --> 00:16:36,759
那么最后的话

462
00:16:36,759 --> 00:16:37,680
它会进入

463
00:16:38,320 --> 00:16:39,480
我们的输出

464
00:16:39,680 --> 00:16:41,279
输出是不要激活函数的

465
00:16:41,560 --> 00:16:42,639
因为激活函数

466
00:16:42,759 --> 00:16:46,039
主要是用来避免我们的层数的塌陷

467
00:16:46,039 --> 00:16:47,720
最后一层我们不需要激活函数

468
00:16:48,879 --> 00:16:51,639
那么跟之前的区别是说

469
00:16:51,639 --> 00:16:54,320
我们的超参数有多了

470
00:16:55,200 --> 00:16:57,000
第一个是说我们要选择

471
00:16:57,120 --> 00:16:58,720
要多少个隐藏层

472
00:17:00,120 --> 00:17:02,480
第二个是说每个隐藏层

473
00:17:02,680 --> 00:17:03,680
它有多大

474
00:17:05,759 --> 00:17:06,120
对吧

475
00:17:06,120 --> 00:17:07,360
这里主要是说

476
00:17:07,360 --> 00:17:08,839
我们有三个隐藏层的话

477
00:17:08,839 --> 00:17:09,759
那我们就是说

478
00:17:09,759 --> 00:17:11,680
一我们选个三这个数字

479
00:17:11,920 --> 00:17:12,599
三个隐藏层

480
00:17:12,599 --> 00:17:13,640
然后每个隐藏层

481
00:17:13,640 --> 00:17:16,039
比如说是m1

482
00:17:17,200 --> 00:17:18,200
这是m3

483
00:17:18,440 --> 00:17:19,200
m2

484
00:17:19,360 --> 00:17:20,360
m1的话

485
00:17:20,480 --> 00:17:22,039
就是说我们要配置好

486
00:17:22,039 --> 00:17:24,759
它到底每一个层长什么样子

487
00:17:25,920 --> 00:17:28,759
一般来说是有一些技术的

488
00:17:28,799 --> 00:17:30,120
一些经验上的东西

489
00:17:30,319 --> 00:17:31,079
就是说

490
00:17:31,640 --> 00:17:32,839
一般来说是说

491
00:17:32,839 --> 00:17:34,879
你把m1设的

492
00:17:35,279 --> 00:17:36,639
稍微大一点点

493
00:17:36,639 --> 00:17:39,159
取决于我们可能会讲一下

494
00:17:39,159 --> 00:17:41,519
就是说我这里可以给大家

495
00:17:41,519 --> 00:17:42,639
简单的讲一下

496
00:17:42,679 --> 00:17:44,399
当你说我越大

497
00:17:44,399 --> 00:17:45,759
我的模型就越复杂

498
00:17:45,759 --> 00:17:46,119
对吧

499
00:17:46,119 --> 00:17:47,439
我的模型就越大

500
00:17:47,839 --> 00:17:49,559
所以根据你会感觉一下

501
00:17:49,559 --> 00:17:52,039
你的输入的复杂度是有多少

502
00:17:52,959 --> 00:17:55,319
假设你觉得这个数据比较难的话

503
00:17:56,240 --> 00:17:58,119
那么你就有两个选择

504
00:17:58,279 --> 00:17:59,279
一个选择是说

505
00:17:59,279 --> 00:18:00,439
我要把

506
00:18:00,720 --> 00:18:02,720
我还是我用单隐藏层

507
00:18:02,839 --> 00:18:03,399
一个选择

508
00:18:03,519 --> 00:18:05,359
我限系模型不考虑的情况下

509
00:18:05,399 --> 00:18:09,119
假设我要用多层感知机的话

510
00:18:09,159 --> 00:18:11,439
那么我一个选择是说

511
00:18:11,439 --> 00:18:13,279
我选单隐藏层

512
00:18:13,959 --> 00:18:16,240
把m设的稍微大一点点

513
00:18:16,279 --> 00:18:18,279
假设我输入维度是128

514
00:18:18,599 --> 00:18:19,439
我说隐藏层

515
00:18:19,480 --> 00:18:21,559
我可以做64也行

516
00:18:21,599 --> 00:18:22,720
128也行

517
00:18:22,759 --> 00:18:24,119
甚至256也行

518
00:18:27,119 --> 00:18:28,399
第二个选择是说

519
00:18:28,399 --> 00:18:31,159
我可以把模型做的深一点点

520
00:18:31,400 --> 00:18:33,840
就是说比如说我就不用单隐藏层

521
00:18:33,960 --> 00:18:35,200
我用三个隐藏层

522
00:18:35,560 --> 00:18:38,240
那就是说这有三个M1 M2 M3

523
00:18:38,600 --> 00:18:39,560
那么就是说

524
00:18:40,480 --> 00:18:43,040
我如果觉得之前的模型

525
00:18:43,080 --> 00:18:44,320
单隐藏层模型

526
00:18:44,440 --> 00:18:47,920
我的隐藏层的大小是128的话

527
00:18:47,960 --> 00:18:49,279
那么如果你用三层的话

528
00:18:49,279 --> 00:18:50,160
那当然你

529
00:18:50,480 --> 00:18:52,720
你最好不要用都是128

530
00:18:52,720 --> 00:18:53,840
这个就太大了

531
00:18:53,920 --> 00:18:55,720
所以你一般来说是说

532
00:18:55,759 --> 00:18:58,440
相对于单隐藏层模型来讲

533
00:18:58,440 --> 00:18:59,920
我的多隐藏层的

534
00:18:59,920 --> 00:19:01,920
M1可能会要小一点点

535
00:19:02,360 --> 00:19:04,360
然后M2又比下面小一点

536
00:19:04,400 --> 00:19:05,960
M3又比人家小一点

537
00:19:06,320 --> 00:19:07,200
你可以认为

538
00:19:07,200 --> 00:19:08,880
为什么会有这样子的一个操作

539
00:19:08,880 --> 00:19:09,560
是说

540
00:19:10,240 --> 00:19:11,519
假设你的模型

541
00:19:11,800 --> 00:19:13,320
假设你的数据比较复杂

542
00:19:13,360 --> 00:19:14,680
那么通常来说

543
00:19:14,680 --> 00:19:16,160
你的维度是比较高的

544
00:19:16,200 --> 00:19:18,360
比如说188或者256

545
00:19:18,640 --> 00:19:19,640
那么你的输出

546
00:19:19,640 --> 00:19:21,039
相对来说是比较少的

547
00:19:21,039 --> 00:19:23,440
10类或者1类或者5类

548
00:19:23,720 --> 00:19:25,200
就是说你要把一个

549
00:19:25,240 --> 00:19:27,480
128维的输入

550
00:19:27,480 --> 00:19:29,720
然后压缩压缩到一个

551
00:19:29,839 --> 00:19:32,360
5维或者10维的一个空间的话

552
00:19:32,640 --> 00:19:33,759
就本质上你继续学习

553
00:19:33,759 --> 00:19:34,600
就是做压缩

554
00:19:34,600 --> 00:19:34,960
对吧

555
00:19:35,000 --> 00:19:36,640
你把一个很复杂的图片

556
00:19:36,680 --> 00:19:36,839
也好

557
00:19:36,839 --> 00:19:37,519
什么东西也好

558
00:19:37,519 --> 00:19:38,600
压缩到一个

559
00:19:39,000 --> 00:19:40,600
一个很简单的一个输出上面

560
00:19:40,880 --> 00:19:41,839
你要做压缩的话

561
00:19:41,960 --> 00:19:42,440
你

562
00:19:45,039 --> 00:19:46,680
你最好是

563
00:19:46,920 --> 00:19:49,039
慢慢的把它压缩回去

564
00:19:51,240 --> 00:19:52,960
就是说这个是最简单的一个做法

565
00:19:52,960 --> 00:19:54,799
就是说128的输入

566
00:19:54,839 --> 00:19:56,079
先压到64

567
00:19:56,120 --> 00:19:57,319
再压到32

568
00:19:57,319 --> 00:19:58,720
最后再压到16

569
00:19:58,720 --> 00:19:59,240
然后8

570
00:19:59,240 --> 00:20:01,000
然后最后到你的输出5

571
00:20:01,000 --> 00:20:01,880
这是第一个做法

572
00:20:01,880 --> 00:20:02,279
对吧

573
00:20:02,319 --> 00:20:03,160
这也是一个

574
00:20:03,200 --> 00:20:05,559
就是说你不断的把你信息

575
00:20:05,599 --> 00:20:06,799
进行提炼

576
00:20:07,519 --> 00:20:08,599
那你还可以说

577
00:20:08,599 --> 00:20:10,559
我可以在最下一层

578
00:20:10,559 --> 00:20:13,519
稍微把数据给你expand一下

579
00:20:13,880 --> 00:20:14,640
就你把18

580
00:20:14,640 --> 00:20:16,480
我可以先把你做到256

581
00:20:16,759 --> 00:20:18,480
然后再慢慢的把你缩回去

582
00:20:19,799 --> 00:20:20,559
但反过来讲

583
00:20:20,759 --> 00:20:22,079
一般来说你最下一层

584
00:20:22,079 --> 00:20:23,160
你可以稍微胖一点点

585
00:20:23,160 --> 00:20:23,680
没关系

586
00:20:23,680 --> 00:20:25,799
就是说这个例子

587
00:20:25,799 --> 00:20:27,039
我们其实是胖了一点点

588
00:20:27,039 --> 00:20:27,480
对吧

589
00:20:28,400 --> 00:20:30,079
这里本来是4个输入

590
00:20:30,079 --> 00:20:31,759
我们是拿了5个隐藏层

591
00:20:32,319 --> 00:20:34,039
但是大家不会反过来

592
00:20:34,079 --> 00:20:35,480
就你不会把它倒过来

593
00:20:35,519 --> 00:20:37,200
你不会把它先压到2

594
00:20:37,200 --> 00:20:38,720
然后再扩充

595
00:20:38,720 --> 00:20:39,920
因为你压到2的时候

596
00:20:39,920 --> 00:20:41,799
你很有可能损失很多信息

597
00:20:42,160 --> 00:20:43,279
就你自己压太狠

598
00:20:43,319 --> 00:20:44,480
损失的东西的话

599
00:20:44,480 --> 00:20:46,400
你后面再还原是比较难的

600
00:20:47,360 --> 00:20:49,000
但之后我们的CN的话

601
00:20:49,000 --> 00:20:50,519
大家回看到这种

602
00:20:50,640 --> 00:20:52,759
先压缩再扩张的模型

603
00:20:52,960 --> 00:20:53,799
就是说

604
00:20:54,759 --> 00:20:56,880
你如果做的比较好的话

605
00:20:56,880 --> 00:20:58,280
你这种压缩这种

606
00:20:58,280 --> 00:20:59,320
先把它压小一点

607
00:20:59,320 --> 00:21:00,280
再把它扩张

608
00:21:00,280 --> 00:21:02,120
就会避免我们模型

609
00:21:02,120 --> 00:21:03,880
太overfilling

610
00:21:03,960 --> 00:21:05,800
我们会讲overfilling是什么意思

611
00:21:06,520 --> 00:21:06,840
OK

612
00:21:06,840 --> 00:21:09,400
这就是多隐藏层的

613
00:21:09,400 --> 00:21:11,880
一些简单的设计思路

614
00:21:11,920 --> 00:21:14,680
我们之后会通过一些实际的

615
00:21:14,680 --> 00:21:15,320
一些

616
00:21:15,480 --> 00:21:16,640
我们做一次比赛

617
00:21:16,640 --> 00:21:17,880
然后来真的

618
00:21:18,160 --> 00:21:19,040
实际数据上

619
00:21:19,040 --> 00:21:20,200
大家体验一下

620
00:21:21,200 --> 00:21:22,480
怎么设这个东西

621
00:21:22,560 --> 00:21:24,200
没有太多科学可以讲

622
00:21:24,560 --> 00:21:26,680
大家纯粹是靠手感

623
00:21:27,320 --> 00:21:28,640
OK

624
00:21:30,440 --> 00:21:30,800
好

625
00:21:30,800 --> 00:21:31,560
总结一下

626
00:21:32,560 --> 00:21:33,760
多层感知机

627
00:21:33,800 --> 00:21:36,800
它其实就是使用一个隐藏层

628
00:21:37,160 --> 00:21:38,320
和激活函数

629
00:21:38,320 --> 00:21:39,960
来得到一个非线性模型

630
00:21:40,560 --> 00:21:41,920
它解决了感知机

631
00:21:41,920 --> 00:21:43,120
它是一个线性模型

632
00:21:43,120 --> 00:21:45,920
不能和XOR的一个局限性

633
00:21:46,080 --> 00:21:47,960
它通过一个加入隐藏层

634
00:21:48,040 --> 00:21:49,960
再加入一个非线性的激活函数

635
00:21:49,960 --> 00:21:51,320
得到一个非线性性

636
00:21:52,200 --> 00:21:54,520
然后它常用的激活函数

637
00:21:54,520 --> 00:21:55,320
是Sigmoid

638
00:21:55,319 --> 00:21:57,000
TiEdge和RELU

639
00:21:57,279 --> 00:21:58,079
一般来说

640
00:21:58,079 --> 00:21:59,399
因为RELU很简单

641
00:21:59,759 --> 00:22:02,799
所以大家用RELU是用的比较多一点

642
00:22:02,839 --> 00:22:06,319
所以大家假设没有别的想法的话

643
00:22:06,319 --> 00:22:07,439
就用RELU就行了

644
00:22:08,639 --> 00:22:10,639
如果你要做多分类的问题的话

645
00:22:10,679 --> 00:22:12,240
那你就使用Softmax

646
00:22:12,599 --> 00:22:14,559
那就是跟我们之前Softmax回归

647
00:22:14,559 --> 00:22:16,200
其实是没本质区别

648
00:22:16,200 --> 00:22:17,919
就是在中间加入了隐藏层

649
00:22:18,679 --> 00:22:20,519
我们的超参数

650
00:22:20,519 --> 00:22:23,319
就是有隐藏层的层数

651
00:22:23,359 --> 00:22:24,879
就是你要放几个

652
00:22:25,720 --> 00:22:28,000
然后和各个隐藏层

653
00:22:28,039 --> 00:22:28,960
到底要多大

654
00:22:28,960 --> 00:22:32,000
就是你要多宽要多窄

655
00:22:32,599 --> 00:22:32,919
OK

656
00:22:32,919 --> 00:22:35,639
这就是多层感知器

657
00:22:35,639 --> 00:22:36,839
我们讲了一下

658
00:22:37,200 --> 00:22:39,480
从感知器到多层感知器的概念

