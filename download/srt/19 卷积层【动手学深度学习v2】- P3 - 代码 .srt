1
00:00:00,000 --> 00:00:02,800
接下来我们直接讲一下卷积层

2
00:00:02,800 --> 00:00:05,560
刚刚我们讲的是卷积操作子

3
00:00:05,560 --> 00:00:10,160
我们来讲一下在神级网络里面卷积层是一个什么东西

4
00:00:12,160 --> 00:00:18,679
首先我们刚刚介绍的是一个叫做二维相关操作的一个东西

5
00:00:18,879 --> 00:00:20,879
通常我们也叫做二维卷积了

6
00:00:21,240 --> 00:00:24,160
所以可以带看一下操作具体是怎么运行的

7
00:00:24,160 --> 00:00:26,879
我这里有一个

8
00:00:27,879 --> 00:00:29,919
我这有个动画

9
00:00:29,919 --> 00:00:31,480
比如说我把我的

10
00:00:32,359 --> 00:00:36,840
可以看到是说每一次我是怎么扫过去的

11
00:00:37,320 --> 00:00:42,119
我给大家来用笔来画一下

12
00:00:43,399 --> 00:00:46,640
假设我们的输入是一个三乘三的矩阵

13
00:00:47,200 --> 00:00:49,439
然后的元素是0 1 3 4 5 6 7 8

14
00:00:50,039 --> 00:00:54,000
我们的卷积的和就是卷积W了

15
00:00:54,000 --> 00:00:56,480
我们一般叫做和叫kernel

16
00:00:57,039 --> 00:00:58,679
是一个二乘二的kernel的话

17
00:01:00,560 --> 00:01:03,240
其实就是说delta等于1

18
00:01:03,400 --> 00:01:04,280
刚刚那个东西

19
00:01:05,799 --> 00:01:07,480
我们算第一个我们的输出

20
00:01:07,480 --> 00:01:08,400
第一个输出的时候

21
00:01:08,400 --> 00:01:09,439
1是9是怎么来的

22
00:01:10,200 --> 00:01:16,599
就是说输出就是index等于0和0的时候

23
00:01:16,599 --> 00:01:19,520
他就是说从这个地方

24
00:01:21,599 --> 00:01:23,200
然后开始

25
00:01:23,799 --> 00:01:29,520
然后把它和0和0

26
00:01:29,520 --> 00:01:30,320
这个0和0

27
00:01:30,320 --> 00:01:31,359
我来画一下

28
00:01:32,359 --> 00:01:35,359
这个0和这个0相乘

29
00:01:36,280 --> 00:01:38,960
接下来往后移一点

30
00:01:39,480 --> 00:01:41,040
这个1和这个1相乘

31
00:01:42,640 --> 00:01:45,240
接下来这个3和这个2相乘

32
00:01:46,159 --> 00:01:48,719
最后这个4和这个3相乘

33
00:01:49,879 --> 00:01:51,480
4个乘法加起来等于19

34
00:01:52,120 --> 00:01:54,079
就是说这一列跟你讲的事情

35
00:01:57,000 --> 00:01:59,200
接下来我要算第二个元素的话

36
00:01:59,560 --> 00:02:00,840
就这个元素的话

37
00:02:01,840 --> 00:02:03,240
同样是和矩阵

38
00:02:03,400 --> 00:02:05,480
同样是和矩阵

39
00:02:05,800 --> 00:02:07,800
但是我的输入就是会

40
00:02:07,800 --> 00:02:12,800
因为我的输出往后往右移一位的话

41
00:02:12,800 --> 00:02:15,080
我们的输入也会重新往右移一位

42
00:02:15,080 --> 00:02:17,640
那就是说对应的窗口是这样一个窗口

43
00:02:18,120 --> 00:02:20,040
输入在这个地方和是不变的

44
00:02:20,079 --> 00:02:23,560
就是说和是跟你的输入的位置是无关的

45
00:02:23,560 --> 00:02:25,919
也就是说我们之前的平移不变性

46
00:02:27,039 --> 00:02:33,959
而且是说我们输出只使用了一个2×2的窗口

47
00:02:35,159 --> 00:02:36,560
就只使用了这个窗口

48
00:02:36,560 --> 00:02:38,560
那也就是局部性

49
00:02:38,560 --> 00:02:40,039
我没有看整个的输入

50
00:02:41,239 --> 00:02:43,039
所以这就是第二个的输入

51
00:02:43,799 --> 00:02:44,840
第三个同样的道理

52
00:02:44,959 --> 00:02:45,719
第三个的话

53
00:02:45,719 --> 00:02:48,879
你如果是上面一个往下移的话

54
00:02:48,879 --> 00:02:51,400
就是说他往下移一个的话

55
00:02:51,840 --> 00:02:54,240
那么你的和是肯定还是用整个和

56
00:02:54,240 --> 00:02:56,359
那么它的输入就会变成了这一个输入

57
00:02:57,120 --> 00:02:57,799
最后的话

58
00:02:57,799 --> 00:03:00,599
你最后一个元素一样的是往下移

59
00:03:01,599 --> 00:03:04,519
你可以看到基本上动画也是在讲这个事情

60
00:03:04,960 --> 00:03:08,079
就是说你就通过你有一个和窗口

61
00:03:08,079 --> 00:03:10,519
在不断的输入上往左移

62
00:03:10,519 --> 00:03:11,240
往下移

63
00:03:11,240 --> 00:03:11,719
往左移

64
00:03:11,719 --> 00:03:13,680
往下移来扫几遍

65
00:03:13,680 --> 00:03:14,960
然后得到我们的输出

66
00:03:15,960 --> 00:03:20,560
这就是我们的二维相交叉相关

67
00:03:20,560 --> 00:03:22,560
就是刚刚我们讲的那个计算子

68
00:03:22,560 --> 00:03:26,840
在图示上具体计算上是什么样做的

69
00:03:30,560 --> 00:03:30,760
好

70
00:03:30,760 --> 00:03:33,560
我们来看一下二维卷积层是什么东西

71
00:03:36,040 --> 00:03:39,000
二维卷积层从神经网络的角度来讲

72
00:03:39,000 --> 00:03:43,000
它就就是说我的输入是一个nh

73
00:03:43,120 --> 00:03:45,360
就高乘以Nw

74
00:03:45,360 --> 00:03:47,280
就是宽的一个矩阵

75
00:03:48,800 --> 00:03:50,319
就是这个这里是三乘三了

76
00:03:51,199 --> 00:03:52,439
我的和呢

77
00:03:52,439 --> 00:03:53,919
和是一个K

78
00:03:53,919 --> 00:03:56,199
就是K做上标

79
00:03:56,199 --> 00:03:57,680
有个高有个宽

80
00:03:58,639 --> 00:03:59,800
那我还有个偏差了

81
00:03:59,800 --> 00:04:01,879
偏差就是一个很简单的一个实数了

82
00:04:04,920 --> 00:04:08,879
那么我的输出就是一个nh

83
00:04:09,879 --> 00:04:11,759
减去Kh加e

84
00:04:12,799 --> 00:04:16,759
乘以Nw减去Kw加e的一个输出

85
00:04:16,759 --> 00:04:18,000
输出会变小

86
00:04:18,000 --> 00:04:21,079
是因为为什么是因为你往这边移的话

87
00:04:21,079 --> 00:04:23,879
我们就是为了计算写起来方便啊

88
00:04:23,879 --> 00:04:25,719
因为你往这边移的话

89
00:04:25,719 --> 00:04:27,800
移到这个地方的话没了

90
00:04:27,800 --> 00:04:28,079
对吧

91
00:04:28,079 --> 00:04:30,199
你只能从这一开始在这个地方

92
00:04:30,199 --> 00:04:30,719
对吧

93
00:04:30,719 --> 00:04:32,120
往这边一下

94
00:04:32,120 --> 00:04:32,439
一下

95
00:04:32,439 --> 00:04:34,279
你还能够完整拿一个输出

96
00:04:34,279 --> 00:04:35,560
你到这个地方的时候

97
00:04:35,560 --> 00:04:36,600
后面没了

98
00:04:36,600 --> 00:04:37,320
后面没了

99
00:04:37,320 --> 00:04:39,120
我就不输出了

100
00:04:39,120 --> 00:04:43,720
所以就是说你会为什么会在输出时候丢掉一点点东西

101
00:04:43,720 --> 00:04:48,879
就你丢掉的就是Kh减e和Kw减e

102
00:04:51,200 --> 00:04:53,240
那么你在写的话

103
00:04:53,240 --> 00:04:56,439
就是说我的y等于我的x

104
00:04:56,439 --> 00:04:58,640
新w加上b

105
00:04:58,640 --> 00:05:02,160
就新就是我那个刚刚定位的二维的交叉

106
00:05:02,160 --> 00:05:03,400
那个操作值

107
00:05:03,400 --> 00:05:05,280
但这个b就是一个实数的一个偏移

108
00:05:05,320 --> 00:05:06,560
我把写进去了

109
00:05:07,760 --> 00:05:09,880
这个地方跟之前一样啊

110
00:05:09,880 --> 00:05:12,240
w和b都是可以学习的一个参数

111
00:05:14,280 --> 00:05:15,000
这就是

112
00:05:16,320 --> 00:05:18,960
其实这就是二维卷积层

113
00:05:18,960 --> 00:05:19,960
我们来给几个例子

114
00:05:22,520 --> 00:05:24,280
就说这张图片啊

115
00:05:24,280 --> 00:05:26,200
这张图片是一个骆驼的头

116
00:05:27,640 --> 00:05:29,360
这是我们的输入

117
00:05:29,360 --> 00:05:33,760
我们可以看到不同的卷积和这就是卷积的一个例子

118
00:05:33,759 --> 00:05:36,920
不同的卷积的一些值可以带来不同的效果

119
00:05:38,879 --> 00:05:41,039
比如说这个地方

120
00:05:41,039 --> 00:05:43,039
假设我是中间一个比较大的值

121
00:05:43,039 --> 00:05:45,319
边上是一圈比较负值的话

122
00:05:45,319 --> 00:05:49,039
我通过这个和和这个输入做卷积操作

123
00:05:49,039 --> 00:05:50,839
也就是二维相关操作的话

124
00:05:50,860 --> 00:05:53,480
会得到一个边缘检测效果

125
00:05:53,500 --> 00:05:56,560
就会把那些边缘的值把你高粱出来

126
00:05:56,560 --> 00:06:01,480
然后那些比较平化的地方就是会变成基本上差不多

127
00:06:02,080 --> 00:06:05,680
另外一个是说我假设我的中心是五

128
00:06:05,680 --> 00:06:07,600
一个对角是这样子的话

129
00:06:07,600 --> 00:06:08,319
那么呢

130
00:06:08,319 --> 00:06:09,800
我会得到一个锐化的效果

131
00:06:10,920 --> 00:06:13,879
最后是一个高斯模糊的效果

132
00:06:13,900 --> 00:06:14,879
就是一个模糊效果

133
00:06:16,280 --> 00:06:20,439
就你可以认为是说我的神经网络可以去

134
00:06:21,560 --> 00:06:26,240
去学一些这样子的和来得到我们想要的一些输出

135
00:06:26,259 --> 00:06:28,200
比如说我想知道这个图片里面

136
00:06:28,220 --> 00:06:30,600
我想比如说我图片里面检测东西的时候

137
00:06:30,640 --> 00:06:32,960
我是通过去看他的边缘的一些效果

138
00:06:32,960 --> 00:06:35,240
那很有可能我就会学出这个东西出来

139
00:06:37,560 --> 00:06:39,000
这就是啊

140
00:06:39,000 --> 00:06:40,920
但如果我想要锐化效果的话

141
00:06:40,920 --> 00:06:44,640
如果锐化效果对我的网络去识别有帮助的话

142
00:06:44,640 --> 00:06:47,320
我可能就会学一个这样子的参数出来

143
00:06:47,340 --> 00:06:47,640
OK

144
00:06:48,960 --> 00:06:49,879
这是一个例子

145
00:06:51,200 --> 00:06:54,439
另外一个是说我们有提过几次啊

146
00:06:54,439 --> 00:06:58,040
就是交叉相关和卷积

147
00:06:58,060 --> 00:06:59,400
但我们这里给大家定一下

148
00:06:59,400 --> 00:07:03,280
就是说交叉相关和卷积其实没有太多区别了

149
00:07:03,280 --> 00:07:06,280
他唯一的区别是卷积在这个地方有个负号

150
00:07:08,920 --> 00:07:14,840
就是说卷积在去去索引W的时候是反过来走的

151
00:07:16,120 --> 00:07:17,320
就是一个负的东西

152
00:07:18,480 --> 00:07:19,980
但是因为它是对称的东西

153
00:07:19,980 --> 00:07:22,000
所以在实际使用中没有区别

154
00:07:22,000 --> 00:07:24,840
是因为是说我写成啊

155
00:07:24,840 --> 00:07:25,880
我写成这样

156
00:07:25,900 --> 00:07:27,560
我写成这个形式

157
00:07:27,560 --> 00:07:30,680
在我我的结果就是去学W吧

158
00:07:30,680 --> 00:07:31,399
学的东西

159
00:07:31,399 --> 00:07:33,920
我正着来反着来都没关系

160
00:07:33,920 --> 00:07:35,439
因为如果我是用二维交叉

161
00:07:35,459 --> 00:07:37,199
我学说W是这样子的话

162
00:07:37,199 --> 00:07:39,840
如果用二维卷积学学东西就是一个反过来的

163
00:07:39,860 --> 00:07:40,399
对吧

164
00:07:40,399 --> 00:07:44,600
就把它反左右反向上下反一下就会得到同样的效果

165
00:07:44,600 --> 00:07:46,160
所以因为反正W是我学的

166
00:07:46,160 --> 00:07:46,920
所以啊

167
00:07:46,920 --> 00:07:49,720
在实际使用是没有任何区别的啊

168
00:07:49,720 --> 00:07:50,800
这也是啊

169
00:07:50,800 --> 00:07:52,319
这帮高啊

170
00:07:52,340 --> 00:07:53,600
就是说搞神经网络的

171
00:07:53,600 --> 00:07:55,519
反正为了写起来方便

172
00:07:55,519 --> 00:07:59,919
就是没有用数学上卷积的严格定应该是要倒过来

173
00:08:01,159 --> 00:08:01,519
OK

174
00:08:01,519 --> 00:08:04,000
这就是所以我们啊

175
00:08:04,019 --> 00:08:05,839
说是说我们是卷积层

176
00:08:05,839 --> 00:08:10,240
但实际实际上我计算实现的是我的交叉相关啊

177
00:08:10,259 --> 00:08:12,639
大家所以碰到这个情况啊

178
00:08:12,659 --> 00:08:13,439
大家不要意外

179
00:08:17,759 --> 00:08:20,240
另外一个是说我们讲的是二维啊

180
00:08:20,240 --> 00:08:22,479
就是我们输入的是一个二维图片

181
00:08:22,500 --> 00:08:24,319
我们可以输入一维

182
00:08:24,319 --> 00:08:26,800
我们可以输入是三维一维的话

183
00:08:26,800 --> 00:08:28,800
给大家讲一下一维的话是文本

184
00:08:28,800 --> 00:08:31,680
我们之后会讲用CN来做文本

185
00:08:31,699 --> 00:08:34,279
也效果挺好的文本也好

186
00:08:34,279 --> 00:08:35,159
语言也好

187
00:08:35,159 --> 00:08:35,960
实际序列也好

188
00:08:35,960 --> 00:08:37,600
都是一个意味的项量

189
00:08:37,620 --> 00:08:38,480
所以你意味的话

190
00:08:38,480 --> 00:08:41,480
那其实就是W就是一个项量了

191
00:08:41,480 --> 00:08:42,279
但同样的话

192
00:08:42,279 --> 00:08:43,439
你就是啊

193
00:08:43,460 --> 00:08:44,360
没什么本质区别

194
00:08:44,360 --> 00:08:44,639
对吧

195
00:08:44,639 --> 00:08:47,000
就是这也是对意味求和

196
00:08:47,000 --> 00:08:48,720
如果你是三维图片的话

197
00:08:48,720 --> 00:08:50,200
你可能是视频啊

198
00:08:50,200 --> 00:08:51,519
你和医学图像医学图像

199
00:08:51,519 --> 00:08:52,919
你做什么核磁共振

200
00:08:53,079 --> 00:08:55,079
你会一空往下扫

201
00:08:55,079 --> 00:08:56,399
如果是气象图片的话

202
00:08:56,399 --> 00:08:57,199
你有个时间轴

203
00:08:57,219 --> 00:08:59,279
就是说气象会有个地球啊

204
00:08:59,279 --> 00:09:00,879
在时间上不断的走

205
00:09:01,759 --> 00:09:03,839
所以那就是多了一个维度了

206
00:09:03,839 --> 00:09:05,839
那就是你的15变3D

207
00:09:05,839 --> 00:09:07,360
你的W变3D

208
00:09:07,379 --> 00:09:08,959
然后你这样子啊

209
00:09:08,959 --> 00:09:10,479
再就多了一个维度

210
00:09:10,479 --> 00:09:14,639
就是一维和三维二维都是没本质区别的

211
00:09:14,639 --> 00:09:15,639
但图片来讲

212
00:09:15,639 --> 00:09:17,199
二维是我们的主流

213
00:09:17,219 --> 00:09:20,000
所以我们通常用的是二维的啊

214
00:09:20,019 --> 00:09:20,639
叫啊

215
00:09:21,279 --> 00:09:24,519
所以也经常会叫做2D

216
00:09:24,539 --> 00:09:26,799
所以一维的话就一地三维的话就3D

217
00:09:31,399 --> 00:09:32,399
好我们总结一下

218
00:09:33,879 --> 00:09:36,240
所以总结一下卷积层是干嘛呢

219
00:09:36,240 --> 00:09:42,319
卷积层就是将你的输入和核矩阵进行交叉相关运算

220
00:09:42,340 --> 00:09:43,879
再加上偏移得到输出

221
00:09:45,519 --> 00:09:48,039
核矩阵和偏移都是可以学习的参数

222
00:09:49,039 --> 00:09:51,759
然后你的超参数是什么

223
00:09:51,759 --> 00:09:56,439
你的超参数就是卷积那个核矩阵的大小

224
00:09:56,439 --> 00:09:58,879
就那个大小控制了你的局部性

225
00:10:01,079 --> 00:10:02,240
就是说你大一点

226
00:10:02,240 --> 00:10:05,159
就是看的每一个点看到范围多一点

227
00:10:05,159 --> 00:10:07,599
你小一点就是我的局部性小一点

228
00:10:07,599 --> 00:10:09,319
所以这就是卷积层

229
00:10:09,319 --> 00:10:13,079
我们是之前怎么样从全连接到卷积层

230
00:10:13,099 --> 00:10:17,240
所以你可以认为卷积层是一个特殊的全连接层

231
00:10:17,259 --> 00:10:17,919
但是呢

232
00:10:17,919 --> 00:10:20,439
通过限制说

233
00:10:20,439 --> 00:10:21,159
我的

234
00:10:23,199 --> 00:10:25,719
在前面做成4D之后

235
00:10:25,719 --> 00:10:27,399
前面两个维度必须长一样

236
00:10:27,399 --> 00:10:29,319
然后只有一个局部的有

237
00:10:29,319 --> 00:10:32,039
那么就是说整个核参数是一个很小的东西

238
00:10:33,199 --> 00:10:34,659
那可以看到是说

239
00:10:34,659 --> 00:10:36,479
不管你的输入有多大

240
00:10:36,479 --> 00:10:40,599
假设我说我就是要看一个三乘三的一个局部的信息的话

241
00:10:40,599 --> 00:10:44,079
我的卷积核永远是一个三乘三的矩阵

242
00:10:44,079 --> 00:10:46,399
就是你不会因为说我的输入变得特别大

243
00:10:46,399 --> 00:10:48,559
我的我的权重变得特别大

244
00:10:48,559 --> 00:10:55,279
所以这就是为什么说卷积解决了之前的问题说我的权重随着我的输入的变大

245
00:10:55,279 --> 00:10:56,039
变得特别大

246
00:10:56,039 --> 00:10:57,120
卷积不会有这个问题

247
00:10:58,120 --> 00:10:58,439
OK

248
00:10:58,439 --> 00:11:00,439
这就是卷积层

