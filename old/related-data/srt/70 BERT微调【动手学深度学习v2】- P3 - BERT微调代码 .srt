1
00:00:00,000 --> 00:00:02,240
好有了数据集了之后

2
00:00:02,359 --> 00:00:04,120
我们就可以开始我们的Finetuning了

3
00:00:04,759 --> 00:00:05,280
Ok

4
00:00:05,280 --> 00:00:10,200
Finetuning也其实基本上跟之前我们讲的

5
00:00:10,599 --> 00:00:14,080
也我觉得跟Finetuning ResNet这些东西

6
00:00:14,080 --> 00:00:15,080
其实区别不大

7
00:00:15,080 --> 00:00:17,160
给大家看一下是怎么回事

8
00:00:19,080 --> 00:00:21,719
反正就是把我们刚刚有说过

9
00:00:21,719 --> 00:00:22,440
我们

10
00:00:22,440 --> 00:00:29,160
我们是我们有train

11
00:00:29,160 --> 00:00:32,000
我们有我们刚刚train的东西

12
00:00:32,000 --> 00:00:34,240
我们叫做Birdsmore

13
00:00:34,679 --> 00:00:35,799
这是我们刚刚那个东西

14
00:00:35,880 --> 00:00:37,240
当然我们有train一个BirdsBase

15
00:00:37,359 --> 00:00:38,840
大家可以去看一下

16
00:00:38,840 --> 00:00:40,320
就是说我们刚刚的代码

17
00:00:40,760 --> 00:00:43,280
就是两层128位

18
00:00:43,280 --> 00:00:44,560
我们用的是Birdsmore

19
00:00:44,560 --> 00:00:46,320
然后我们真的是把它train完了

20
00:00:46,320 --> 00:00:46,719
train完了

21
00:00:46,719 --> 00:00:47,600
然后存起来了

22
00:00:47,600 --> 00:00:49,400
存在的那个地方

23
00:00:49,560 --> 00:00:51,320
大家可以去下下来

24
00:00:51,479 --> 00:00:52,920
就是刚刚有大家问题说

25
00:00:52,920 --> 00:00:54,480
你真的训练个几十万步

26
00:00:54,480 --> 00:00:55,280
会长什么样子

27
00:00:55,560 --> 00:00:56,880
长的样子这个样子

28
00:00:56,880 --> 00:00:57,920
反正你可以下载下来

29
00:00:57,920 --> 00:00:59,519
我们也训练一个跟BirdsBase

30
00:00:59,519 --> 00:01:00,240
差不多的模型

31
00:01:01,880 --> 00:01:03,880
当然你可以选择loadsmore

32
00:01:03,880 --> 00:01:06,640
loadsbase都ok

33
00:01:06,640 --> 00:01:09,560
当然base通常应该比smore要好一点

34
00:01:10,960 --> 00:01:12,400
然后你load之后

35
00:01:12,599 --> 00:01:13,680
反正就把它

36
00:01:14,640 --> 00:01:16,640
你就把这个模型load下来

37
00:01:16,640 --> 00:01:19,439
然后就是把它加载进去

38
00:01:20,359 --> 00:01:21,920
你把它模型构建出来之后

39
00:01:21,920 --> 00:01:24,000
你就是通过PyTorch的

40
00:01:24,000 --> 00:01:25,679
load的state

41
00:01:25,679 --> 00:01:26,239
dict

42
00:01:26,239 --> 00:01:29,640
就是把那些prime的load进来

43
00:01:29,920 --> 00:01:31,599
另外一个你要把它vocabload进来

44
00:01:32,000 --> 00:01:34,640
所以vocab也是把整个vocab是存在

45
00:01:34,640 --> 00:01:36,239
一个文本文件存在里面的

46
00:01:36,680 --> 00:01:38,439
前面那些东西都是把它

47
00:01:39,120 --> 00:01:41,000
怎么样把vocabload进来

48
00:01:42,280 --> 00:01:42,799
Ok

49
00:01:43,479 --> 00:01:44,519
这个地方就是说

50
00:01:44,519 --> 00:01:46,479
我们就把整个

51
00:01:46,920 --> 00:01:48,640
我们之前训练好的

52
00:01:48,640 --> 00:01:52,040
在Wikitext2上训练好的小Birds

53
00:01:52,480 --> 00:01:53,359
load回来

54
00:01:55,439 --> 00:01:55,840
好

55
00:01:55,840 --> 00:01:57,040
然后我们的data set

56
00:02:00,040 --> 00:02:00,840
就这个data set

57
00:02:00,840 --> 00:02:01,920
我就不给大家过了

58
00:02:01,960 --> 00:02:02,439
就说白了

59
00:02:02,680 --> 00:02:05,439
反正你返回的就是要你的

60
00:02:06,120 --> 00:02:07,799
你返回的就是Birds那个东西

61
00:02:07,920 --> 00:02:08,599
这是

62
00:02:09,039 --> 00:02:10,479
Token A加上Token B

63
00:02:10,479 --> 00:02:11,920
然后加一个segment对吧

64
00:02:12,920 --> 00:02:13,639
就是说

65
00:02:14,400 --> 00:02:16,520
然后这个地方你看到是说

66
00:02:16,960 --> 00:02:17,400
我们

67
00:02:18,400 --> 00:02:20,360
用了一点点的

68
00:02:22,439 --> 00:02:24,200
用了多线程

69
00:02:24,560 --> 00:02:25,640
Python的多线程

70
00:02:25,800 --> 00:02:27,319
这个东西就是说

71
00:02:27,439 --> 00:02:28,520
我想跟大家提一下

72
00:02:28,520 --> 00:02:29,840
就是说NOP

73
00:02:30,120 --> 00:02:30,640
NOP的

74
00:02:30,640 --> 00:02:32,280
你会发现你那些数据域处理

75
00:02:32,360 --> 00:02:33,599
都是用Python写的

76
00:02:34,040 --> 00:02:36,200
Python就是反正一堆for loop在里面

77
00:02:36,240 --> 00:02:38,840
一堆的查Dictionary

78
00:02:39,040 --> 00:02:40,439
查查这个查查那个

79
00:02:40,759 --> 00:02:41,200
导致

80
00:02:42,520 --> 00:02:45,520
Python的数据域处理是很慢的

81
00:02:46,320 --> 00:02:48,000
Bird最早出来的时候

82
00:02:48,000 --> 00:02:49,080
你去看Bird的代码

83
00:02:49,080 --> 00:02:50,400
它是先把数据

84
00:02:50,480 --> 00:02:53,280
整个数据域先训练好

85
00:02:53,360 --> 00:02:55,520
它先用map reduce域训练好

86
00:02:55,640 --> 00:02:58,120
当然Google不叫map reduce了

87
00:02:59,080 --> 00:02:59,879
它用

88
00:03:00,000 --> 00:03:01,840
就是说你当你用可能用

89
00:03:01,840 --> 00:03:03,000
Spark什么东西

90
00:03:03,040 --> 00:03:04,560
先域去训练好

91
00:03:04,560 --> 00:03:05,200
域处理好

92
00:03:05,200 --> 00:03:06,320
就处理成那些

93
00:03:06,320 --> 00:03:08,000
Mask都给你弄好了

94
00:03:08,040 --> 00:03:09,120
存起来

95
00:03:09,560 --> 00:03:11,840
存在一个机器里面

96
00:03:12,040 --> 00:03:14,160
然后你在训练区去读

97
00:03:14,240 --> 00:03:15,240
为什么要这么做

98
00:03:16,000 --> 00:03:17,039
图片相对来说

99
00:03:17,039 --> 00:03:18,120
大家都不怎么做了

100
00:03:18,240 --> 00:03:19,960
图片都是on the fly的做

101
00:03:20,000 --> 00:03:20,879
是因为

102
00:03:21,079 --> 00:03:22,359
你用Python写的话

103
00:03:22,599 --> 00:03:24,519
数据域处理非常慢

104
00:03:24,719 --> 00:03:25,359
另外一块

105
00:03:25,359 --> 00:03:27,159
如果细心的同学会发现

106
00:03:27,159 --> 00:03:29,400
我们在tokenize的时候

107
00:03:29,400 --> 00:03:31,919
没有实现Bird真正的tokenization

108
00:03:32,879 --> 00:03:34,280
Bird的tokenization实现了

109
00:03:34,280 --> 00:03:37,519
是一个用的是词根

110
00:03:37,519 --> 00:03:39,400
像是每一次去看里面一个词根

111
00:03:39,400 --> 00:03:40,840
就英语那些什么

112
00:03:40,879 --> 00:03:43,159
过去史现在史的东西

113
00:03:43,159 --> 00:03:43,919
导致说

114
00:03:43,919 --> 00:03:45,560
如果你用词表示的话

115
00:03:45,560 --> 00:03:46,840
就表示不一样

116
00:03:46,840 --> 00:03:48,159
我用词根表示的话

117
00:03:48,319 --> 00:03:49,520
你就可以表示进去

118
00:03:49,520 --> 00:03:50,520
就是说你就说

119
00:03:50,680 --> 00:03:51,519
不管是过去史

120
00:03:51,519 --> 00:03:52,280
还是未来史

121
00:03:52,280 --> 00:03:52,840
还是什么

122
00:03:52,840 --> 00:03:54,159
现在进行史什么东西

123
00:03:54,159 --> 00:03:56,000
我都能用词根把你

124
00:03:56,879 --> 00:03:59,400
尽量的map到一个词上面

125
00:03:59,879 --> 00:04:01,439
我们有一节

126
00:04:01,519 --> 00:04:03,280
我们书上面有讲这个东西

127
00:04:03,919 --> 00:04:04,960
但我们没用它

128
00:04:04,960 --> 00:04:05,800
是因为

129
00:04:06,120 --> 00:04:08,240
你如果用Python实现的东西

130
00:04:08,240 --> 00:04:09,240
就是一个很长的

131
00:04:09,240 --> 00:04:10,680
一个for loop

132
00:04:11,479 --> 00:04:13,079
两个for loop可能是

133
00:04:13,159 --> 00:04:15,039
两个for loop特别慢

134
00:04:15,479 --> 00:04:17,920
也根本上你跑不动

135
00:04:17,959 --> 00:04:19,759
所以这个地方

136
00:04:19,759 --> 00:04:22,680
我们也用了一点点多进程

137
00:04:22,800 --> 00:04:23,920
这Python里面是多进程

138
00:04:23,920 --> 00:04:25,560
就是用了4个进程来做

139
00:04:26,199 --> 00:04:28,360
Python本身是一个单线程的东西

140
00:04:29,040 --> 00:04:31,719
所以就是因为它这一块

141
00:04:31,920 --> 00:04:33,879
就很有可能

142
00:04:33,879 --> 00:04:35,240
你在做文本的时候

143
00:04:35,240 --> 00:04:37,279
你发现你模型训练的

144
00:04:37,279 --> 00:04:40,279
比你的数据预处要快很多

145
00:04:41,280 --> 00:04:43,200
CV里面之所以不是问题

146
00:04:43,320 --> 00:04:44,720
CV里面是因为

147
00:04:45,800 --> 00:04:48,080
那些operator都是C++写的

148
00:04:48,080 --> 00:04:49,640
就图片那些转来转去

149
00:04:49,760 --> 00:04:50,760
就当年做Vision

150
00:04:50,760 --> 00:04:53,720
那一波人都是用C++写代码

151
00:04:53,720 --> 00:04:55,600
所以他们都习惯比较好

152
00:04:55,640 --> 00:04:57,000
然后我们就直接调进就行了

153
00:04:57,000 --> 00:04:58,600
所以每个operator效率比较高

154
00:04:58,800 --> 00:05:01,200
另外一块在C++上面写多进程

155
00:05:01,200 --> 00:05:03,360
会比Python写多进程方便很多

156
00:05:03,800 --> 00:05:04,360
所以导致

157
00:05:04,760 --> 00:05:07,280
所以你在Python写的NLP的code的时候

158
00:05:07,600 --> 00:05:09,000
你要么就是用

159
00:05:09,920 --> 00:05:11,680
要么就是写多进程代码

160
00:05:12,279 --> 00:05:14,480
通常框架会帮你做一些事情

161
00:05:14,959 --> 00:05:17,160
框架会帮你做好很多事情

162
00:05:17,480 --> 00:05:18,399
第二块是说

163
00:05:18,399 --> 00:05:20,399
你得尽量把这些operator

164
00:05:20,399 --> 00:05:21,639
写到C++里面去

165
00:05:22,639 --> 00:05:25,079
当然Python有办法让你编译到C++

166
00:05:25,360 --> 00:05:26,519
或者说你就纯的

167
00:05:26,519 --> 00:05:27,959
真正的写C++代码

168
00:05:28,000 --> 00:05:29,720
使得你整个这一块

169
00:05:29,720 --> 00:05:31,600
就是不会成为瓶颈

170
00:05:31,720 --> 00:05:34,120
但是很多时候

171
00:05:34,120 --> 00:05:34,519
你会

172
00:05:34,519 --> 00:05:36,360
如果你发现你的代码慢的话

173
00:05:36,399 --> 00:05:37,519
你在NLP的时候

174
00:05:37,839 --> 00:05:39,079
你就去想一想

175
00:05:39,120 --> 00:05:41,159
是不是主要是你的数据

176
00:05:41,159 --> 00:05:42,519
处理那一块比较慢

177
00:05:43,159 --> 00:05:43,639
OK

178
00:05:45,039 --> 00:05:45,439
OK

179
00:05:45,439 --> 00:05:47,599
所以因为用了多进程

180
00:05:47,680 --> 00:05:48,759
所以代码长一点

181
00:05:48,799 --> 00:05:51,159
但实际上也跟之前差不多

182
00:05:51,680 --> 00:05:53,199
就是干的事情是差不多了

183
00:05:53,199 --> 00:05:53,560
对吧

184
00:05:55,319 --> 00:05:55,879
好

185
00:05:55,919 --> 00:05:57,959
然后你把数据

186
00:05:59,839 --> 00:06:00,759
数据load进来

187
00:06:00,759 --> 00:06:02,479
data iterated load进来

188
00:06:04,000 --> 00:06:06,399
然后可以看一下代码

189
00:06:08,519 --> 00:06:09,560
我们干嘛

190
00:06:09,680 --> 00:06:10,199
就是说

191
00:06:10,199 --> 00:06:11,599
首先把BERT的encoder

192
00:06:11,639 --> 00:06:12,079
弄进来

193
00:06:12,079 --> 00:06:12,599
对吧

194
00:06:14,680 --> 00:06:16,439
把BERT的hidden弄进来

195
00:06:16,719 --> 00:06:19,000
就hidden就是给

196
00:06:19,000 --> 00:06:19,759
这hidden弄进来

197
00:06:19,759 --> 00:06:22,799
是因为我们之前

198
00:06:23,039 --> 00:06:24,879
把nsp就是说

199
00:06:25,240 --> 00:06:26,560
下一个句子对的

200
00:06:27,839 --> 00:06:28,479
输出的时候

201
00:06:28,479 --> 00:06:29,479
它不是一个MLP

202
00:06:29,560 --> 00:06:30,560
它是有一个隐藏层

203
00:06:30,560 --> 00:06:32,039
有一个输出层

204
00:06:32,079 --> 00:06:34,199
我把隐藏层写在了BERT里面

205
00:06:34,560 --> 00:06:35,199
所以这样子

206
00:06:35,199 --> 00:06:36,039
你就把它拎出来

207
00:06:36,039 --> 00:06:37,279
就是说你也可以重用

208
00:06:37,280 --> 00:06:38,680
反正是隐藏层

209
00:06:38,680 --> 00:06:39,760
就是在你class

210
00:06:39,760 --> 00:06:40,960
那个token上的隐藏层

211
00:06:41,000 --> 00:06:42,200
所以你可以直接搬过来

212
00:06:42,320 --> 00:06:43,000
最后的最后

213
00:06:43,000 --> 00:06:44,200
你就要加一个

214
00:06:44,360 --> 00:06:45,160
线性层

215
00:06:45,360 --> 00:06:47,880
因为隐藏层是加个线性层

216
00:06:47,880 --> 00:06:50,440
是说你的输出是3

217
00:06:51,120 --> 00:06:53,560
因为你要的是3 012

218
00:06:53,560 --> 00:06:54,440
这三个类

219
00:06:55,880 --> 00:06:57,400
然后就是说

220
00:06:57,560 --> 00:06:58,360
你做forward

221
00:06:58,360 --> 00:06:59,040
就是

222
00:06:59,920 --> 00:07:01,000
你把input拿过来

223
00:07:01,000 --> 00:07:01,800
input就是token

224
00:07:01,960 --> 00:07:02,520
segment

225
00:07:02,520 --> 00:07:03,040
还有width

226
00:07:03,040 --> 00:07:03,520
and length

227
00:07:03,880 --> 00:07:05,000
丢进之后

228
00:07:05,360 --> 00:07:06,720
丢到encoder之后

229
00:07:06,720 --> 00:07:07,800
encoder拿到了

230
00:07:07,800 --> 00:07:08,720
encoder

231
00:07:09,880 --> 00:07:11,880
我就我只需要我的encoder

232
00:07:11,880 --> 00:07:12,640
那个

233
00:07:14,520 --> 00:07:15,360
看一下

234
00:07:16,160 --> 00:07:18,200
我只需要我的encoder的x

235
00:07:19,840 --> 00:07:22,520
然后我就去把我的

236
00:07:24,040 --> 00:07:24,600
hidden

237
00:07:25,280 --> 00:07:25,960
就是我把

238
00:07:25,960 --> 00:07:27,120
我再换个行

239
00:07:28,600 --> 00:07:29,680
我跟刚刚一样的

240
00:07:29,680 --> 00:07:32,200
我就把batch size保留

241
00:07:32,440 --> 00:07:33,200
sequence里面

242
00:07:33,200 --> 00:07:34,720
我再拿到第一个token

243
00:07:34,720 --> 00:07:35,400
它的

244
00:07:37,680 --> 00:07:39,200
embedding就是特征

245
00:07:39,920 --> 00:07:41,400
后面这个是拿下来

246
00:07:41,600 --> 00:07:43,040
丢到我的hidden里面

247
00:07:43,400 --> 00:07:46,520
再丢到我的output里面

248
00:07:47,800 --> 00:07:49,800
这个hidden是带activation的

249
00:07:49,880 --> 00:07:51,120
所以我们就不需要写了

250
00:07:51,120 --> 00:07:52,360
它丢到我的里面

251
00:07:52,360 --> 00:07:53,440
就可以完成了

252
00:07:53,600 --> 00:07:54,720
所以我做fine tuning的时候

253
00:07:54,720 --> 00:07:55,800
基本上这两

254
00:07:56,400 --> 00:07:58,640
这两个都是copy来的

255
00:07:59,160 --> 00:08:00,680
这个东西最后输出层

256
00:08:00,840 --> 00:08:03,360
是我们自己给你自己弄过来的

257
00:08:03,440 --> 00:08:04,040
OK

258
00:08:04,200 --> 00:08:05,280
所以这就是

259
00:08:05,280 --> 00:08:06,360
我们做fine tuning的时候

260
00:08:06,360 --> 00:08:07,080
用的事情

261
00:08:08,920 --> 00:08:09,480
最后的时候

262
00:08:11,320 --> 00:08:12,760
我来干脆运行一下

263
00:08:12,760 --> 00:08:14,520
我都没运行

264
00:08:14,520 --> 00:08:15,000
这个

265
00:08:15,120 --> 00:08:16,000
就是说

266
00:08:16,520 --> 00:08:17,120
最后的时候

267
00:08:17,320 --> 00:08:18,600
反正跟我们之前没区别

268
00:08:19,000 --> 00:08:20,960
我们用的是ch13

269
00:08:21,440 --> 00:08:22,080
反正

270
00:08:23,360 --> 00:08:24,320
基本上

271
00:08:25,720 --> 00:08:26,800
一个正常的训练

272
00:08:27,680 --> 00:08:29,120
然后我们用的学习力量

273
00:08:29,120 --> 00:08:29,520
可以看到

274
00:08:29,520 --> 00:08:30,280
用了fine tuning

275
00:08:30,480 --> 00:08:31,640
通常比较小

276
00:08:31,960 --> 00:08:34,040
number of epoch也是用的不多

277
00:08:34,159 --> 00:08:35,959
就是因为我们的数据级也不大

278
00:08:37,559 --> 00:08:38,360
然后另外一个

279
00:08:38,360 --> 00:08:40,039
我们就不等这个东西完成了

280
00:08:40,039 --> 00:08:40,879
因为我这个东西

281
00:08:40,879 --> 00:08:41,839
我发现我没有运行

282
00:08:41,839 --> 00:08:42,799
我得去下载

283
00:08:43,480 --> 00:08:44,439
我得去下载数据

284
00:08:44,439 --> 00:08:46,759
数据下载起来就是比较慢

285
00:08:47,240 --> 00:08:48,480
我就不给大家

286
00:08:50,000 --> 00:08:50,959
给大家看一下结果

287
00:08:50,959 --> 00:08:51,799
就是说

288
00:08:53,279 --> 00:08:53,959
等会我

289
00:08:57,360 --> 00:08:58,319
我给大家看一下结果

290
00:08:58,439 --> 00:08:58,919
就结果

291
00:08:58,919 --> 00:09:00,679
反正图上面肯定是有的

292
00:09:01,320 --> 00:09:01,960
嗯

293
00:09:02,800 --> 00:09:03,800
我们这个肯定是有的

294
00:09:08,600 --> 00:09:09,480
结果就是说

295
00:09:09,720 --> 00:09:12,000
可以看到是我们的

296
00:09:13,160 --> 00:09:15,080
accuracy大概是我们test accuracy

297
00:09:15,080 --> 00:09:16,120
就是选了5个epoch

298
00:09:16,120 --> 00:09:17,320
还是也还不错

299
00:09:17,480 --> 00:09:19,000
已经是78%了

300
00:09:19,240 --> 00:09:20,800
然后当然是我们没完成

301
00:09:20,800 --> 00:09:21,560
我们5个epoch

302
00:09:21,560 --> 00:09:23,120
基本上就没有完全收敛

303
00:09:23,120 --> 00:09:24,240
就是说你可以

304
00:09:24,480 --> 00:09:25,040
实际关系

305
00:09:25,040 --> 00:09:26,680
我们就没有往下训练

306
00:09:26,680 --> 00:09:28,480
但是你自己要玩一下的话

307
00:09:28,480 --> 00:09:30,200
你可以训练个几十个

308
00:09:30,200 --> 00:09:32,200
二三十应该是可以

309
00:09:32,440 --> 00:09:34,040
可以下降到更多一点

310
00:09:35,120 --> 00:09:37,759
然后基本上可以看到是说

311
00:09:38,360 --> 00:09:39,560
我们这个地方

312
00:09:40,640 --> 00:09:41,400
我们在这个地方

313
00:09:43,280 --> 00:09:44,560
所以我们现在已经开始了

314
00:09:44,560 --> 00:09:45,320
我们就不等他了

315
00:09:45,800 --> 00:09:46,720
基本上可以看到是说

316
00:09:46,720 --> 00:09:47,440
我们学习率

317
00:09:47,440 --> 00:09:49,080
跟我们之前的fine tuning的job

318
00:09:49,120 --> 00:09:49,759
差不多

319
00:09:49,800 --> 00:09:50,960
但这个地方

320
00:09:50,960 --> 00:09:52,120
你可以做一点的优化

321
00:09:52,120 --> 00:09:53,160
让你收敛变快一点

322
00:09:53,160 --> 00:09:53,759
就是说

323
00:09:53,800 --> 00:09:55,560
你可以把最后一层的学习率

324
00:09:55,759 --> 00:09:56,400
弄高一点

325
00:09:56,400 --> 00:09:57,000
弄个1

326
00:09:57,000 --> 00:09:57,840
什么-2

327
00:09:57,840 --> 00:09:58,680
-1都行

328
00:09:58,760 --> 00:10:00,200
然后下面那些层

329
00:10:00,320 --> 00:10:03,280
你可以是把学习率弄低一点

330
00:10:03,280 --> 00:10:04,280
这样子的话

331
00:10:04,280 --> 00:10:06,360
就是说最上面那一层

332
00:10:06,480 --> 00:10:07,760
就是随机数字化的

333
00:10:07,960 --> 00:10:09,480
你让他收敛快一点

334
00:10:09,880 --> 00:10:10,320
另外一块

335
00:10:10,320 --> 00:10:11,440
如果你想加速的话

336
00:10:11,720 --> 00:10:13,680
你可以把一些transformer

337
00:10:13,680 --> 00:10:15,080
下面一些层给固定住

338
00:10:15,080 --> 00:10:17,160
就是说那些position embedding

339
00:10:17,680 --> 00:10:19,560
那些底层的那些transformer block

340
00:10:19,560 --> 00:10:20,600
你就不让他训练了

341
00:10:20,640 --> 00:10:21,720
让他固定住他

342
00:10:22,080 --> 00:10:23,000
也这样子

343
00:10:23,000 --> 00:10:24,040
你从backward的时候

344
00:10:24,040 --> 00:10:24,880
你会快一点

345
00:10:25,200 --> 00:10:27,440
而且你收敛可能也会更快一点

346
00:10:28,320 --> 00:10:28,560
Ok

347
00:10:28,560 --> 00:10:31,200
这也是做fine tuning的常用技巧

348
00:10:31,200 --> 00:10:32,640
在所有的计算机视觉

349
00:10:32,760 --> 00:10:33,800
那些fine tuning的技巧

350
00:10:33,800 --> 00:10:34,920
你都可以搬过来

351
00:10:36,000 --> 00:10:38,520
我觉得这个就是深度学习的好处

352
00:10:39,000 --> 00:10:40,520
他之前计算机视觉

353
00:10:40,800 --> 00:10:41,320
NLP

354
00:10:41,480 --> 00:10:43,480
他们之间其实打不通了

355
00:10:44,280 --> 00:10:45,840
因为就是NLP有一套东西

356
00:10:45,840 --> 00:10:47,320
计算机视觉有一套东西

357
00:10:47,600 --> 00:10:48,640
现在的话

358
00:10:48,640 --> 00:10:51,040
你神经网络作为一种语言

359
00:10:51,440 --> 00:10:54,880
它确实能够跨越整个不同的地方

360
00:10:54,920 --> 00:10:56,720
使得同样的idea

361
00:10:56,720 --> 00:10:57,879
再来来回回用

362
00:10:58,160 --> 00:10:59,680
就是说之前是计算机视觉

363
00:10:59,680 --> 00:11:00,600
影响了NLP

364
00:11:00,600 --> 00:11:01,720
说我要做fine tuning

365
00:11:02,120 --> 00:11:06,040
现在计算机是NLP又往前走

366
00:11:06,040 --> 00:11:08,560
说我可以用大量的数据来训练

367
00:11:08,560 --> 00:11:09,600
很就是说

368
00:11:09,720 --> 00:11:10,759
没有label的数据

369
00:11:10,879 --> 00:11:11,360
这个东西

370
00:11:11,360 --> 00:11:13,200
我们现在都是没有label的训练出来

371
00:11:13,200 --> 00:11:14,120
不过它是没有label

372
00:11:14,120 --> 00:11:15,399
你认为训练的

373
00:11:15,399 --> 00:11:16,240
我可以没有label

374
00:11:16,240 --> 00:11:17,759
所以训练一个非常大的模型

375
00:11:17,840 --> 00:11:19,000
还能帮助回去

376
00:11:19,040 --> 00:11:20,080
这个思想

377
00:11:20,639 --> 00:11:23,080
又反过来在影响计算机视觉

378
00:11:23,120 --> 00:11:24,840
计算机视觉说我也行

379
00:11:24,960 --> 00:11:26,519
我就我就往上下图片

380
00:11:26,720 --> 00:11:27,480
不标了

381
00:11:27,480 --> 00:11:27,720
对吧

382
00:11:27,720 --> 00:11:29,000
我下一百一个

383
00:11:29,000 --> 00:11:30,360
十一张图片过来

384
00:11:30,399 --> 00:11:32,600
然后我在上面创一个比较大的

385
00:11:32,600 --> 00:11:33,639
transformer模型

386
00:11:34,480 --> 00:11:36,519
现在大家都在做这个事情

387
00:11:37,120 --> 00:11:38,159
但你有兴趣的话

388
00:11:38,159 --> 00:11:38,639
你也可以做

389
00:11:38,639 --> 00:11:39,919
但是你得要有钱才行

390
00:11:39,919 --> 00:11:42,240
基本上都是我看都是一些大机构在做

391
00:11:42,279 --> 00:11:44,720
因为这种东西特别烧钱

392
00:11:45,360 --> 00:11:46,240
所以就是说

393
00:11:46,439 --> 00:11:48,000
我觉得可以看到是说

394
00:11:48,000 --> 00:11:50,319
整个神经网络作为一种语言

395
00:11:50,799 --> 00:11:53,879
它确实是能够比较跨领域的

396
00:11:53,919 --> 00:11:55,000
以至于是说

397
00:11:55,000 --> 00:11:56,440
你只要学会了这个语言

398
00:11:56,440 --> 00:11:58,480
你可以在很多地方都能过去

399
00:11:58,519 --> 00:12:00,840
而且技术上也是比较通用的

400
00:12:00,840 --> 00:12:03,120
而且这边说计算机视觉技术

401
00:12:03,120 --> 00:12:04,080
也可以搬过来

402
00:12:04,200 --> 00:12:05,240
就是说fine tuning怎么做

403
00:12:05,240 --> 00:12:05,720
对吧

404
00:12:06,120 --> 00:12:08,000
之间是相互可以做的

405
00:12:08,279 --> 00:12:09,320
这样子导致说

406
00:12:10,360 --> 00:12:13,039
各个应用之间的门槛就变得很低了

