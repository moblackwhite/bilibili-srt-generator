1
00:00:00,000 --> 00:00:03,160
第一个问题是说

2
00:00:03,640 --> 00:00:07,759
可以讲一下lot number和infinity是怎么产生的

3
00:00:07,759 --> 00:00:09,080
以及怎么解决的吗

4
00:00:10,560 --> 00:00:11,519
一般来说

5
00:00:11,519 --> 00:00:13,160
infinity大家好理解

6
00:00:13,160 --> 00:00:15,759
就是说你就是太大了

7
00:00:16,719 --> 00:00:20,760
通常来说infinity通常是你的near rate调的太大了

8
00:00:20,760 --> 00:00:21,240
造成的

9
00:00:21,240 --> 00:00:23,519
或者你的权重初始的时候

10
00:00:23,519 --> 00:00:24,519
你还没更新了

11
00:00:24,519 --> 00:00:25,679
你权重初始的时候

12
00:00:25,679 --> 00:00:26,640
那些值太大了

13
00:00:26,679 --> 00:00:29,800
就导致你基本上就炸掉了

14
00:00:30,800 --> 00:00:32,439
lot number是怎么出现的

15
00:00:32,439 --> 00:00:34,159
lot number一般就是除0

16
00:00:34,159 --> 00:00:37,079
就把一个数除一个0

17
00:00:37,719 --> 00:00:39,320
那就是通常来就是说

18
00:00:39,320 --> 00:00:41,759
你比如说你的t度已经是很小了

19
00:00:41,759 --> 00:00:43,399
然后你把t度除了一个0

20
00:00:43,439 --> 00:00:44,640
会产生lot number

21
00:00:45,079 --> 00:00:46,079
就这两个

22
00:00:46,200 --> 00:00:48,079
以及怎么解决

23
00:00:48,200 --> 00:00:51,920
解决的问题其实怎么说

24
00:00:51,920 --> 00:00:54,120
它要解决的话

25
00:00:54,400 --> 00:00:59,600
一般来说就通过我们今天介绍的一些技术

26
00:00:59,800 --> 00:01:02,679
有合理的初始化你的权重

27
00:01:03,679 --> 00:01:04,799
然后你的激活函数

28
00:01:04,799 --> 00:01:05,879
你也不要选错

29
00:01:06,079 --> 00:01:07,040
然后你的学习率

30
00:01:07,040 --> 00:01:08,240
你也不要选太大

31
00:01:08,240 --> 00:01:09,359
或者一般来说

32
00:01:09,359 --> 00:01:10,799
你的学习率不要选太大

33
00:01:12,159 --> 00:01:13,920
所以你如果碰到这个问题

34
00:01:13,920 --> 00:01:17,359
你其实我建议最简单的做法是说

35
00:01:17,520 --> 00:01:19,439
你把学习率调到比较小

36
00:01:19,439 --> 00:01:20,480
一直往小调

37
00:01:20,560 --> 00:01:23,599
直到你的infinity或者lot number不出现了

38
00:01:24,400 --> 00:01:25,640
第二个是说

39
00:01:25,640 --> 00:01:27,840
你可以看一下你的权重的初始

40
00:01:28,000 --> 00:01:30,640
不要选择均值

41
00:01:30,640 --> 00:01:31,359
当然是等于0

42
00:01:31,520 --> 00:01:34,079
不要方叉区间取得比较小一点

43
00:01:34,359 --> 00:01:35,520
你一直往小走

44
00:01:35,520 --> 00:01:39,200
走到你能够正确的出一些值

45
00:01:39,200 --> 00:01:40,520
然后再慢慢把它调大

46
00:01:40,520 --> 00:01:42,240
使得他有训练有进展

47
00:01:46,879 --> 00:01:48,640
就第二个问题是说

48
00:01:48,640 --> 00:01:53,040
relu是如何你和x平方或者三次方

49
00:01:53,040 --> 00:01:54,960
这种曲线的

50
00:01:58,840 --> 00:02:00,680
我理解

51
00:02:00,680 --> 00:02:03,960
其实我们不是用relu来拟合一个东西

52
00:02:03,960 --> 00:02:07,000
是relu加上我们那些可以学的权重

53
00:02:07,000 --> 00:02:09,400
来拟合平方或三次曲线

54
00:02:09,520 --> 00:02:11,159
relu唯一的干的事情

55
00:02:11,159 --> 00:02:14,000
就是把线性给破坏掉了

56
00:02:14,000 --> 00:02:15,840
大家可以去

57
00:02:15,840 --> 00:02:17,080
比如说B站上

58
00:02:17,080 --> 00:02:17,759
重新看一下

59
00:02:17,759 --> 00:02:20,039
我们在讲MLP的时候

60
00:02:20,039 --> 00:02:21,680
多层感知机的时候讲过的

61
00:02:24,599 --> 00:02:25,360
问题三

62
00:02:25,920 --> 00:02:27,440
如果训练一开始

63
00:02:27,440 --> 00:02:30,200
在验证级上的准确率在提升

64
00:02:30,240 --> 00:02:31,280
但是

65
00:02:31,480 --> 00:02:33,160
券了两个apoc之后

66
00:02:33,200 --> 00:02:35,720
突然验证级就变成50%了

67
00:02:35,720 --> 00:02:37,000
之后稳定在50%

68
00:02:37,000 --> 00:02:37,880
这是为什么

69
00:02:38,960 --> 00:02:44,440
一般是你的权重就坏掉了

70
00:02:46,160 --> 00:02:47,880
就是说基本上50%

71
00:02:47,880 --> 00:02:49,080
就是权重可能里面

72
00:02:49,080 --> 00:02:50,640
都是一些乱七八糟的值了

73
00:02:50,800 --> 00:02:51,920
一般来说

74
00:02:51,920 --> 00:02:53,560
我觉得一般来说

75
00:02:53,599 --> 00:02:55,439
你的数值稳定性出了问题

76
00:02:56,719 --> 00:02:58,080
你怎么办

77
00:02:58,280 --> 00:03:02,080
你可以尝试把学习率变小一点

78
00:03:02,080 --> 00:03:03,680
但我觉得可能更本质的问题

79
00:03:03,680 --> 00:03:05,439
如果你不能通过调学习率

80
00:03:05,439 --> 00:03:06,240
来解决

81
00:03:06,240 --> 00:03:07,240
把学习率调小一点

82
00:03:07,240 --> 00:03:09,120
不能通过解决问题的话

83
00:03:09,159 --> 00:03:11,560
通常是你模型数值稳定性不行

84
00:03:12,039 --> 00:03:15,000
很容易就跑歪了

85
00:03:16,120 --> 00:03:18,319
就是说我们之后会讲大量的模型

86
00:03:18,480 --> 00:03:21,159
怎么样来使得更稳定

87
00:03:21,159 --> 00:03:23,599
大家说大家为什么用resnet

88
00:03:23,599 --> 00:03:25,120
为什么用stm

89
00:03:25,120 --> 00:03:25,759
为什么要这个

90
00:03:25,759 --> 00:03:26,400
为什么要那个

91
00:03:26,400 --> 00:03:28,120
其实绝大部分的

92
00:03:28,120 --> 00:03:29,560
就是让你的训练更加稳定

93
00:03:29,560 --> 00:03:31,039
就不会出现这个情况

94
00:03:36,759 --> 00:03:37,879
就是说一样的

95
00:03:37,879 --> 00:03:38,960
就是说你是不是

96
00:03:38,960 --> 00:03:40,719
如果你发现你有lot of number了

97
00:03:40,719 --> 00:03:42,479
是不是发生了T2爆炸

98
00:03:43,680 --> 00:03:44,639
对一般来说

99
00:03:44,639 --> 00:03:45,319
lot of number

100
00:03:45,319 --> 00:03:47,079
就是因为T2太大造成的

101
00:03:47,159 --> 00:03:48,280
如果你是太小的话

102
00:03:48,280 --> 00:03:50,240
就保证你根本就劝不动

103
00:03:50,240 --> 00:03:51,520
就是发现就个平的

104
00:03:51,520 --> 00:03:52,560
就没什么进展

105
00:03:52,640 --> 00:03:56,320
所以一般是一般是T2的问题

106
00:04:00,080 --> 00:04:05,320
如果我碰到一些复杂的数学公式

107
00:04:05,320 --> 00:04:07,520
看文字描述没什么感觉

108
00:04:07,520 --> 00:04:08,920
应该怎么突破一下呢

109
00:04:09,600 --> 00:04:10,560
这是个好问题

110
00:04:14,000 --> 00:04:14,800
就是说

111
00:04:17,000 --> 00:04:18,840
就深度学习的好处

112
00:04:20,360 --> 00:04:21,280
就是说

113
00:04:21,879 --> 00:04:23,439
让你不要懂数学

114
00:04:23,439 --> 00:04:25,240
也能够用很多东西

115
00:04:25,840 --> 00:04:27,000
传统的机器学习

116
00:04:27,000 --> 00:04:28,199
像SVM也好

117
00:04:28,199 --> 00:04:29,199
像别的也好

118
00:04:29,240 --> 00:04:30,960
就是你需要有很多数学

119
00:04:30,960 --> 00:04:31,519
优化也好

120
00:04:31,519 --> 00:04:32,400
有很多数学

121
00:04:32,639 --> 00:04:34,240
深度学习说你不用数学

122
00:04:34,240 --> 00:04:34,840
你神经网络

123
00:04:35,000 --> 00:04:36,480
反正科导就行了

124
00:04:36,680 --> 00:04:38,439
科导法我都给你求解

125
00:04:39,280 --> 00:04:40,720
但是反过来讲

126
00:04:40,920 --> 00:04:41,879
我觉得

127
00:04:42,360 --> 00:04:45,000
虽然深度学习对数学要求低了

128
00:04:45,000 --> 00:04:46,680
但是我觉得这个东西

129
00:04:46,759 --> 00:04:48,040
你还是得学

130
00:04:50,639 --> 00:04:52,720
就是说你可以打个比方

131
00:04:54,000 --> 00:04:56,879
你把人的数学能力和代码能力

132
00:04:56,879 --> 00:04:58,600
代码能力就是说你深度学习

133
00:04:58,600 --> 00:05:01,480
你能调参会调

134
00:05:01,480 --> 00:05:02,439
写代码很快

135
00:05:02,439 --> 00:05:03,680
实现搞数据很快

136
00:05:03,680 --> 00:05:04,600
做事情很快

137
00:05:07,040 --> 00:05:09,600
就我们把这个称为代码能力的话

138
00:05:09,840 --> 00:05:11,000
那么你的数学能力

139
00:05:11,000 --> 00:05:12,280
就是你的理解能力

140
00:05:14,960 --> 00:05:15,920
我关掉一下摄像头

141
00:05:16,079 --> 00:05:17,920
假设我有点卡的话

142
00:05:18,240 --> 00:05:21,000
那么就是说你可以认为

143
00:05:21,040 --> 00:05:22,040
相当于一个电脑

144
00:05:22,040 --> 00:05:24,040
一个电脑有两个东西比较重要

145
00:05:24,040 --> 00:05:25,280
一个是你的内存

146
00:05:25,280 --> 00:05:26,960
一个是你的CPU的频率

147
00:05:28,000 --> 00:05:29,480
就CPU的频率

148
00:05:29,480 --> 00:05:32,200
取决于你能够在单位时间里面

149
00:05:32,439 --> 00:05:34,639
以类完成多少任务

150
00:05:35,879 --> 00:05:37,639
就你CPU频率很高的话

151
00:05:37,639 --> 00:05:38,960
那么你就完成的比较好

152
00:05:39,160 --> 00:05:41,319
那么你可以认为CPU的频率

153
00:05:41,319 --> 00:05:42,759
和人的代码能力

154
00:05:42,759 --> 00:05:44,800
是一个类比关系

155
00:05:45,200 --> 00:05:45,920
我手很快

156
00:05:45,920 --> 00:05:46,960
我调东西很快

157
00:05:47,359 --> 00:05:48,799
这是大家一直追求的东西

158
00:05:49,000 --> 00:05:50,639
那就表示你的CPU频率很高

159
00:05:50,639 --> 00:05:51,759
就你能出活

160
00:05:52,359 --> 00:05:53,560
但是内存呢

161
00:05:53,759 --> 00:05:54,759
内存能干嘛

162
00:05:55,039 --> 00:05:56,079
内存

163
00:05:56,399 --> 00:05:57,519
很多时候是

164
00:05:58,159 --> 00:05:58,919
它不出问题

165
00:05:58,959 --> 00:06:00,519
你是不会觉得它有问题的

166
00:06:00,599 --> 00:06:01,879
内存是决定了

167
00:06:01,879 --> 00:06:03,879
你能跑多复杂的

168
00:06:04,199 --> 00:06:04,839
任务

169
00:06:05,839 --> 00:06:06,399
对吧

170
00:06:06,759 --> 00:06:09,000
内存就是说要么能跑

171
00:06:09,000 --> 00:06:09,959
要么不能跑

172
00:06:10,199 --> 00:06:11,479
所以你的数学能力

173
00:06:11,479 --> 00:06:14,679
取决于类比成你的内存大小

174
00:06:15,680 --> 00:06:17,600
当你数学不好的时候

175
00:06:17,800 --> 00:06:20,079
你能做的任务就会变少

176
00:06:21,280 --> 00:06:22,439
你很多东西就

177
00:06:22,560 --> 00:06:23,639
很多东西你可能看不懂

178
00:06:23,639 --> 00:06:23,959
看不懂

179
00:06:23,959 --> 00:06:24,639
你无法理解

180
00:06:24,639 --> 00:06:25,439
你无法用

181
00:06:25,879 --> 00:06:27,800
当你数学能力变好的时候

182
00:06:27,800 --> 00:06:28,960
你的内存会大

183
00:06:29,000 --> 00:06:30,480
虽然你可能对深度学习

184
00:06:30,480 --> 00:06:31,920
可能没我们讲的东西

185
00:06:31,920 --> 00:06:33,560
可能不会有问题

186
00:06:33,560 --> 00:06:35,240
但是你对别的东西

187
00:06:35,280 --> 00:06:36,199
可能复杂一点的

188
00:06:36,199 --> 00:06:36,879
需要数学的

189
00:06:36,879 --> 00:06:37,800
你就不能做了

190
00:06:37,840 --> 00:06:38,800
所以我觉得

191
00:06:39,519 --> 00:06:40,439
数学

192
00:06:40,560 --> 00:06:42,000
一定是不能

193
00:06:42,040 --> 00:06:44,199
虽然现在还没以前那么重要

194
00:06:44,319 --> 00:06:45,120
对记忆学习

195
00:06:45,120 --> 00:06:46,159
但是我觉得

196
00:06:46,639 --> 00:06:47,800
大家是不能丢的

197
00:06:47,839 --> 00:06:49,279
所以你说看文字描述

198
00:06:49,279 --> 00:06:49,920
没什么感觉

199
00:06:49,920 --> 00:06:50,439
怎么突破

200
00:06:50,560 --> 00:06:52,439
就是上点数学课

201
00:06:54,000 --> 00:06:54,800
信息代数

202
00:06:55,000 --> 00:06:55,680
这东西学一学

203
00:06:56,039 --> 00:06:56,240
好

204
00:06:56,240 --> 00:06:57,599
符点概率论学一学

205
00:07:02,599 --> 00:07:03,319
问题6

206
00:07:03,360 --> 00:07:06,159
如果第16位辅典数

207
00:07:06,159 --> 00:07:07,399
影响比较严重

208
00:07:07,399 --> 00:07:09,759
32位或者64位就好了吗

209
00:07:09,800 --> 00:07:10,759
那就是说

210
00:07:11,680 --> 00:07:12,719
通过

211
00:07:14,439 --> 00:07:17,560
对是说

212
00:07:17,560 --> 00:07:19,319
你32位就会好一些

213
00:07:19,319 --> 00:07:20,639
64位当然就更好了

214
00:07:20,639 --> 00:07:23,039
所以传统的高性能计算

215
00:07:23,039 --> 00:07:24,759
他们都是用64位的

216
00:07:24,879 --> 00:07:25,639
就像Python

217
00:07:25,639 --> 00:07:27,959
它的默认的数据类型是64位

218
00:07:28,079 --> 00:07:30,000
32位是大家常用的

219
00:07:30,159 --> 00:07:31,039
但是

220
00:07:31,839 --> 00:07:33,279
就是说这是一个权衡

221
00:07:34,439 --> 00:07:35,519
你可认为

222
00:07:36,199 --> 00:07:37,680
对一个芯片来讲

223
00:07:37,719 --> 00:07:39,240
芯片它最重要是什么

224
00:07:39,240 --> 00:07:41,079
是你的芯片的面积

225
00:07:42,079 --> 00:07:43,879
举个最简单的例子

226
00:07:43,879 --> 00:07:44,439
就说

227
00:07:44,719 --> 00:07:46,519
我芯片面积是那么大的话

228
00:07:47,079 --> 00:07:49,439
那么64位一个辅典运算

229
00:07:49,439 --> 00:07:52,439
计算单元占的面积

230
00:07:52,680 --> 00:07:54,120
你可以近似的认为

231
00:07:54,120 --> 00:07:56,079
等于两个32位

232
00:07:56,079 --> 00:07:57,919
辅典运算器占的面积

233
00:07:57,959 --> 00:07:59,360
等于4个16位

234
00:07:59,360 --> 00:08:00,799
辅典运算占的面积

235
00:08:01,199 --> 00:08:02,439
所以意味着是说

236
00:08:02,479 --> 00:08:04,479
假设我芯片的面积是给定的

237
00:08:04,479 --> 00:08:05,519
一般是给定的

238
00:08:06,039 --> 00:08:06,959
在情况下

239
00:08:07,319 --> 00:08:08,199
我的

240
00:08:08,199 --> 00:08:10,759
如果我说我要做16位

241
00:08:10,759 --> 00:08:11,719
辅典数的话

242
00:08:12,079 --> 00:08:14,439
那么我就是比32位要快两倍

243
00:08:14,439 --> 00:08:15,879
比64位要快4倍

244
00:08:16,000 --> 00:08:17,240
所以为什么是说

245
00:08:17,240 --> 00:08:19,759
现在的芯片都是往小里做

246
00:08:19,800 --> 00:08:21,199
做16位做8位

247
00:08:21,199 --> 00:08:23,159
或者甚至做1位4位

248
00:08:23,800 --> 00:08:25,519
就是说硬件决定了

249
00:08:25,519 --> 00:08:30,159
我们想用比较短的那些辅典数

250
00:08:30,240 --> 00:08:31,360
这样子快一点

251
00:08:31,639 --> 00:08:34,720
但确实这些会给你带来很大问题

252
00:08:34,720 --> 00:08:36,240
给你的训练会带来问题

253
00:08:36,240 --> 00:08:36,919
就是说

254
00:08:37,399 --> 00:08:38,879
做预测问题不大

255
00:08:38,919 --> 00:08:39,960
主要是训练的时候

256
00:08:40,159 --> 00:08:40,799
梯度的问题

257
00:08:40,799 --> 00:08:42,439
做预测其实你就是往前跑了

258
00:08:42,439 --> 00:08:43,360
就问题不大的

259
00:08:43,720 --> 00:08:45,480
主要是在训练的时候

260
00:08:45,480 --> 00:08:47,200
会有各种问题

261
00:08:47,320 --> 00:08:49,600
我们其实有一些解决方案

262
00:08:49,720 --> 00:08:51,639
我们可能会之后会给大家讲一讲

263
00:08:52,759 --> 00:08:54,000
另外一块就是说

264
00:08:54,320 --> 00:08:57,279
FP16现在慢慢的会用的越来越少

265
00:08:57,279 --> 00:08:58,480
现在大家用

266
00:08:58,679 --> 00:08:59,960
Bfloat16

267
00:09:00,639 --> 00:09:03,200
Bfloat16就是指数为一个32位是一样的

268
00:09:03,200 --> 00:09:04,440
所以这个是我觉得

269
00:09:04,480 --> 00:09:07,519
新的一代的media应该是和新的CPU

270
00:09:07,519 --> 00:09:11,199
它都是慢慢的支持一个Bfloat16的指令

271
00:09:11,199 --> 00:09:14,240
是他和32位的区间尽可能相近

272
00:09:14,240 --> 00:09:17,039
所以可能没以前没现在那么严重

273
00:09:22,679 --> 00:09:23,919
就梯度消失

274
00:09:23,919 --> 00:09:27,600
可以说是因为使用了sigma的函数引起的吗

275
00:09:27,840 --> 00:09:30,199
我们可以使用redol来替代sigma

276
00:09:30,199 --> 00:09:31,600
解决梯度消失吗

277
00:09:34,799 --> 00:09:37,360
sigma容易引起梯度消失

278
00:09:37,360 --> 00:09:39,960
redol确实对于这个东西解决比较好

279
00:09:39,960 --> 00:09:41,399
但是梯度消失

280
00:09:41,399 --> 00:09:44,240
不一定是只有sigma的产生的

281
00:09:44,440 --> 00:09:47,080
梯度消失可能有别的地方产生的

282
00:09:49,279 --> 00:09:50,039
就是说

283
00:09:50,440 --> 00:09:52,320
sigma的能引起梯度消失

284
00:09:52,320 --> 00:09:53,519
但是它是一个

285
00:09:55,159 --> 00:09:56,240
不是一个

286
00:09:56,720 --> 00:09:57,840
充分必要的关系

287
00:09:58,080 --> 00:10:00,480
梯度消失不仅仅是有sigma的

288
00:10:00,560 --> 00:10:03,039
所以说你要redol来替代sigma的

289
00:10:03,039 --> 00:10:05,720
可以让梯度消失的概率变低

290
00:10:05,720 --> 00:10:08,720
但是我无法说你一定是可以解决这个问题

291
00:10:11,000 --> 00:10:13,040
所以梯度爆炸是有什么激活函数的

292
00:10:13,519 --> 00:10:15,320
爆炸不会有激活函数

293
00:10:15,320 --> 00:10:18,720
激活函数它的相对是它的梯度还行

294
00:10:18,879 --> 00:10:20,279
就是说激活函数它的梯度

295
00:10:20,279 --> 00:10:21,680
因为它是一个平滑的

296
00:10:21,680 --> 00:10:22,879
比较平滑的一个曲线

297
00:10:22,920 --> 00:10:24,920
它梯度都不会有太大问题

298
00:10:25,040 --> 00:10:27,480
梯度爆炸一般是就是因为你的

299
00:10:28,560 --> 00:10:31,040
你的那些每一个层的输出的值太大

300
00:10:31,080 --> 00:10:32,160
n个很大的值

301
00:10:32,160 --> 00:10:33,800
那层会导致梯度爆炸

302
00:10:36,040 --> 00:10:39,480
就LSTM里面

303
00:10:39,480 --> 00:10:43,720
就是说LSTM通过指数和log和这种

304
00:10:43,920 --> 00:10:44,960
操作单元

305
00:10:44,960 --> 00:10:49,160
使得里面的不再是一个累乘的一个东西

306
00:10:49,160 --> 00:10:51,639
我们之后会讲这个东西

307
00:10:51,639 --> 00:10:53,800
大家不一定要就提到这个事情

308
00:10:53,800 --> 00:10:55,720
我们之后肯定会讲为什么会这样子

309
00:11:01,240 --> 00:11:03,960
就是说为什么说乘法变加法

310
00:11:03,960 --> 00:11:05,480
可以让训练更稳定

311
00:11:05,920 --> 00:11:09,160
我们之前有讲过

312
00:11:09,160 --> 00:11:12,920
100个1.5乘在一起梯度会炸

313
00:11:13,040 --> 00:11:13,840
太大了

314
00:11:13,960 --> 00:11:15,800
但100个1.5加起来没问题

315
00:11:15,920 --> 00:11:16,480
就150

316
00:11:16,480 --> 00:11:21,800
就是说你100是等于你的乘的个数

317
00:11:22,480 --> 00:11:23,840
假设你100层的话

318
00:11:23,840 --> 00:11:25,480
你全用乘法来算梯子的话

319
00:11:25,480 --> 00:11:26,160
很容易就出

320
00:11:26,160 --> 00:11:26,840
要么就太大

321
00:11:26,840 --> 00:11:27,639
要么就太小

322
00:11:27,720 --> 00:11:28,720
假设你100层

323
00:11:28,720 --> 00:11:30,920
你的梯度是用加法来做的话

324
00:11:30,920 --> 00:11:33,120
那你不管你的

325
00:11:33,159 --> 00:11:35,759
你甚至你的输出都是1000

326
00:11:35,759 --> 00:11:36,080
1万

327
00:11:36,080 --> 00:11:37,080
你都没关系

328
00:11:37,080 --> 00:11:38,919
1万乘以100也没问题

329
00:11:38,919 --> 00:11:39,120
对吧

330
00:11:39,120 --> 00:11:39,840
100万

331
00:11:39,919 --> 00:11:41,080
对于计算机来讲

332
00:11:41,080 --> 00:11:42,080
100万不是问题

333
00:11:42,960 --> 00:11:45,759
所以就是说为什么说乘法变加法

334
00:11:45,759 --> 00:11:47,840
可以让数学更加稳定

335
00:11:50,560 --> 00:11:53,159
让每层的方差是一个常数

336
00:11:53,159 --> 00:11:56,720
是跟batchnode没有太多关系

337
00:11:56,879 --> 00:11:57,720
batchnode

338
00:11:58,080 --> 00:12:00,039
somewhat能够做一点东西

339
00:12:00,039 --> 00:12:01,080
但是确实

340
00:12:01,280 --> 00:12:08,160
我们之后会讲到batchnode到底是怎么回事

341
00:12:08,160 --> 00:12:10,400
batchnode可以让你的输出

342
00:12:10,400 --> 00:12:12,960
它确实变成一个均值为0

343
00:12:12,960 --> 00:12:16,000
方差为一个差不多是一个固定值的东西

344
00:12:16,000 --> 00:12:19,080
但是我想想它不一定能保证你的梯度

345
00:12:20,120 --> 00:12:21,920
所以batchnode就是说

346
00:12:22,520 --> 00:12:24,720
大家没有从这个角度来看过

347
00:12:24,720 --> 00:12:26,840
batchnode是不是干这个事情

348
00:12:26,840 --> 00:12:28,120
我们之后讲到batchnode的时候

349
00:12:28,120 --> 00:12:29,560
我们再回过头来看这个事情

350
00:12:32,080 --> 00:12:38,400
输出或者参数符合正态分布有利学习

351
00:12:38,440 --> 00:12:41,400
其实不是说你需要是一个正态分布

352
00:12:41,400 --> 00:12:45,280
从纯粹是说我需要你的输出值

353
00:12:45,280 --> 00:12:47,320
它在一个合理的区间里面

354
00:12:49,759 --> 00:12:52,800
就是说怎么你模拟怎么去

355
00:12:53,520 --> 00:12:57,720
怎么样去使得我的输出好

356
00:12:57,720 --> 00:12:59,280
去用公式推理

357
00:12:59,480 --> 00:13:03,120
就是说我假设它是一个随机分布

358
00:13:03,120 --> 00:13:07,039
因为随机分布比较容易算均值和期望

359
00:13:07,039 --> 00:13:08,360
所以我们就是说

360
00:13:08,399 --> 00:13:10,439
假设你是个随机分布

361
00:13:10,439 --> 00:13:12,399
但是你不一定需要正态没关系

362
00:13:12,399 --> 00:13:14,319
就是说只要你的均值为0

363
00:13:14,319 --> 00:13:15,759
方差为一个固定值就行了

364
00:13:15,759 --> 00:13:17,399
就随便什么分布都行

365
00:13:17,439 --> 00:13:20,399
大家之所以用正态分布或者均匀分布的话

366
00:13:20,399 --> 00:13:23,319
纯粹是因为那个东西算起来比较容易

367
00:13:23,319 --> 00:13:24,519
数学做比较容易

368
00:13:24,519 --> 00:13:26,360
但实际上来说你用什么都没关系

369
00:13:30,279 --> 00:13:32,079
就随机初始化

370
00:13:32,079 --> 00:13:35,639
XVR是一个不错的方案

371
00:13:35,639 --> 00:13:36,839
不能说是最好

372
00:13:36,839 --> 00:13:38,559
但是确实是很常用的方案

373
00:13:38,559 --> 00:13:42,399
就是说你我不知道有最好或者最推荐的是什么

374
00:13:42,399 --> 00:13:45,839
但我觉得大家如果没有更好的想法的时候

375
00:13:45,839 --> 00:13:46,919
就用XVR就行了

376
00:13:50,159 --> 00:13:52,480
梯度规划不是batch normalization

377
00:13:52,480 --> 00:13:54,480
这个其实是不一样的东西

378
00:13:54,480 --> 00:13:57,000
我们之后可以去再来讲这个事情

379
00:13:59,720 --> 00:14:06,279
问题15是说我们等高线是不是可以可视化

380
00:14:06,279 --> 00:14:08,039
这个东西也是个很好玩的问题

381
00:14:08,039 --> 00:14:11,079
就是说对一个损失函数可视化

382
00:14:11,079 --> 00:14:13,799
它面积是一个很难的问题

383
00:14:13,799 --> 00:14:16,199
确实真的有research来做这个事情

384
00:14:16,199 --> 00:14:17,480
大家如果感兴趣的话

385
00:14:17,480 --> 00:14:18,919
可以去找一点paper

386
00:14:18,959 --> 00:14:23,879
但我确实没看到特别好的方法

387
00:14:23,879 --> 00:14:27,319
真的能把一个很复杂的函数给你画出来

388
00:14:27,360 --> 00:14:28,439
最简单情况下

389
00:14:28,440 --> 00:14:30,200
我们就能画一个二维输入

390
00:14:30,200 --> 00:14:32,880
你三维输入你就比较难画了

391
00:14:32,880 --> 00:14:33,240
对吧

392
00:14:33,240 --> 00:14:35,080
你欧为就没戏了

393
00:14:35,080 --> 00:14:36,400
这是第一点

394
00:14:36,440 --> 00:14:40,240
所以这块没有特别好的方法

395
00:14:40,240 --> 00:14:43,560
但是确实有一些研究工作来可视化你

396
00:14:43,560 --> 00:14:46,920
损失函数的一个等高线曲线

397
00:14:46,960 --> 00:14:48,280
大家可以去搜一下

398
00:14:48,320 --> 00:14:50,280
但是也还比较原始

399
00:14:53,240 --> 00:14:54,920
对就是说问题16是说

400
00:14:54,920 --> 00:14:57,080
为什么我要假设独立同分布

401
00:14:57,080 --> 00:15:00,040
没就简单一点

402
00:15:00,040 --> 00:15:01,360
就是为了简单起见

403
00:15:05,360 --> 00:15:08,200
就是说如果你不是独立同分布会怎么样

404
00:15:08,240 --> 00:15:10,200
会不会相互影响

405
00:15:10,200 --> 00:15:15,639
对不是独立同分布的话

406
00:15:18,120 --> 00:15:19,840
一般来说还真的是

407
00:15:20,000 --> 00:15:20,720
嗯

408
00:15:21,960 --> 00:15:23,720
就是说它是条件

409
00:15:24,000 --> 00:15:26,879
独立同分布还真的可以这么假设

410
00:15:26,879 --> 00:15:28,559
这个地方虽然你就是说

411
00:15:28,600 --> 00:15:30,399
内部协变量偏移

412
00:15:30,399 --> 00:15:32,840
当然是说你其实之间有一定关系

413
00:15:32,879 --> 00:15:33,960
但我觉得没必要

414
00:15:33,960 --> 00:15:35,240
在这个简单情况下

415
00:15:35,240 --> 00:15:36,360
没必要做这个假设

416
00:15:36,399 --> 00:15:38,200
反过来说一句

417
00:15:38,200 --> 00:15:39,480
就是batch normalization

418
00:15:39,480 --> 00:15:40,480
其实是说

419
00:15:40,519 --> 00:15:42,879
就是有一个他就是想说

420
00:15:42,879 --> 00:15:45,519
我里面神经网络里面有一个类

421
00:15:45,519 --> 00:15:47,080
就是说类变的

422
00:15:47,200 --> 00:15:48,200
有一个我和性

423
00:15:48,200 --> 00:15:49,080
我想把你解我

424
00:15:49,080 --> 00:15:50,559
就是batch normalization要干的事情

425
00:15:53,799 --> 00:15:56,639
为什么假设每一层的权重是一个

426
00:15:56,679 --> 00:15:58,120
服从一个独立同分布了

427
00:15:58,120 --> 00:15:59,679
因为这个是我们初始化

428
00:15:59,840 --> 00:16:00,960
就我们其实说白了

429
00:16:00,960 --> 00:16:02,240
刚刚说的就是说

430
00:16:02,279 --> 00:16:04,879
我在权重一开始的时候

431
00:16:05,000 --> 00:16:06,000
应该怎么做

432
00:16:06,639 --> 00:16:07,759
就假设我是

433
00:16:07,759 --> 00:16:10,399
所以我这个是手动初始化出来的结果

434
00:16:10,399 --> 00:16:11,480
就是说刚刚那个分析

435
00:16:11,480 --> 00:16:12,840
只能分析到

436
00:16:12,840 --> 00:16:15,199
在权重一开始的时候怎么样

437
00:16:15,240 --> 00:16:16,600
中间当然是不能

438
00:16:16,639 --> 00:16:18,919
不能说是一个独立同分布了

439
00:16:20,960 --> 00:16:22,759
正常分布的假设有什么缺陷

440
00:16:22,759 --> 00:16:24,240
为什么看上去万能的

441
00:16:24,319 --> 00:16:25,159
嗯

442
00:16:25,639 --> 00:16:26,480
也不是万能的

443
00:16:26,600 --> 00:16:29,000
正常分布做推导比较容易点

444
00:16:29,000 --> 00:16:29,920
就这么点事情

445
00:16:31,200 --> 00:16:32,080
而且大树定理

446
00:16:32,960 --> 00:16:33,279
对吧

447
00:16:33,279 --> 00:16:33,759
大树定理

448
00:16:33,759 --> 00:16:36,759
最后一切的一切都变成一个正常分布

449
00:16:40,080 --> 00:16:41,519
就是说有另外问题

450
00:16:41,519 --> 00:16:42,480
二是说

451
00:16:42,519 --> 00:16:42,759
啊

452
00:16:42,759 --> 00:16:46,399
有用一些很复杂的算法来进行

453
00:16:46,480 --> 00:16:49,039
初始化权重或预值

454
00:16:49,120 --> 00:16:50,279
其实我不知道

455
00:16:50,279 --> 00:16:51,680
我不知道这个是什么样的东西

456
00:16:51,680 --> 00:16:52,920
就是你可以研究

457
00:16:53,080 --> 00:16:54,759
这一块有很多工作

458
00:16:55,639 --> 00:16:57,159
我们就不再展开了

459
00:17:01,360 --> 00:17:02,960
就是说问题21是说

460
00:17:02,960 --> 00:17:05,879
强制使得每一层的输出均值为0

461
00:17:05,879 --> 00:17:06,480
方差为1

462
00:17:06,480 --> 00:17:08,680
是不是损失了网络的表达能力

463
00:17:08,720 --> 00:17:09,960
改变了数据的特征

464
00:17:09,960 --> 00:17:11,519
降低了科学性的准确率

465
00:17:11,559 --> 00:17:11,720
啊

466
00:17:11,720 --> 00:17:12,839
这也没有啊

467
00:17:12,839 --> 00:17:14,440
要其实也没有啊

468
00:17:14,480 --> 00:17:17,160
就是说数值就是说是一个区间

469
00:17:17,440 --> 00:17:19,559
你把它拉到什么地方都没关系

470
00:17:20,839 --> 00:17:21,879
就是说理解吗

471
00:17:21,879 --> 00:17:22,559
就是说

472
00:17:23,559 --> 00:17:24,839
我一个神经网络

473
00:17:24,839 --> 00:17:25,759
我想让就是说

474
00:17:25,759 --> 00:17:27,359
我只是让那个数值

475
00:17:27,359 --> 00:17:29,240
在一个合理的区间里面表达

476
00:17:29,240 --> 00:17:29,960
还使数嘛

477
00:17:29,960 --> 00:17:31,480
反正你你压小一点

478
00:17:31,480 --> 00:17:32,679
压大一点都没关系

479
00:17:32,720 --> 00:17:35,200
这个区间是使得我的硬件处理起来

480
00:17:35,200 --> 00:17:36,000
比较容易

481
00:17:36,319 --> 00:17:37,839
就是合适硬件的区间

482
00:17:37,879 --> 00:17:38,919
从数学上来讲

483
00:17:38,919 --> 00:17:40,079
我不管用什么区间

484
00:17:40,079 --> 00:17:41,319
我做任何变化

485
00:17:41,359 --> 00:17:42,559
都不会影响我的

486
00:17:42,559 --> 00:17:44,159
算我的模型的可表达性

487
00:17:46,960 --> 00:17:49,159
问题22为什么他可以这个变化

488
00:17:49,159 --> 00:17:50,679
可以提高稳定性

489
00:17:50,799 --> 00:17:52,759
他和就他其实没有

490
00:17:52,759 --> 00:17:54,960
他其实是说做完这个变化之后

491
00:17:55,000 --> 00:17:56,360
在零点附近

492
00:17:56,360 --> 00:17:58,000
他近似于他的区间

493
00:17:58,000 --> 00:18:00,400
近似于fx等于x这个函数

494
00:18:05,880 --> 00:18:08,600
我们来另外一个是要具体讲解

495
00:18:08,600 --> 00:18:10,279
是xv的初始化

496
00:18:10,320 --> 00:18:11,080
啊

497
00:18:11,240 --> 00:18:12,360
针对于

498
00:18:12,759 --> 00:18:13,000
啊

499
00:18:13,000 --> 00:18:13,799
这个是个好问题

500
00:18:13,920 --> 00:18:16,240
这个这个是说你其实我理解

501
00:18:16,240 --> 00:18:16,960
你是想说

502
00:18:16,960 --> 00:18:18,759
我们要实现一下这个函数吧

503
00:18:18,839 --> 00:18:20,879
我们今天肯定没有空来实现

504
00:18:20,879 --> 00:18:22,440
我可以考虑一下哪一天

505
00:18:22,440 --> 00:18:24,559
我们来回过头来讲一下

506
00:18:24,559 --> 00:18:27,119
我们啊这个怎么实现啊

507
00:18:27,400 --> 00:18:28,119
就反过来讲

508
00:18:28,119 --> 00:18:29,799
如果我们就算不实现

509
00:18:29,799 --> 00:18:30,480
你可以去看一下

510
00:18:30,480 --> 00:18:32,279
实啊框架的时间

511
00:18:32,279 --> 00:18:32,960
其实挺简单

512
00:18:32,960 --> 00:18:34,400
就就你几句话

513
00:18:37,119 --> 00:18:39,039
24几乎函数有什么选择

514
00:18:39,079 --> 00:18:40,960
啊几乎函数你叫relu吧

515
00:18:40,960 --> 00:18:42,079
就是简单

516
00:18:44,319 --> 00:18:46,480
一般权重是在每个epoc之后

517
00:18:46,480 --> 00:18:46,920
更新的吗

518
00:18:46,920 --> 00:18:48,279
权重是每一次迭代

519
00:18:48,279 --> 00:18:49,240
每个batch更新

520
00:18:49,240 --> 00:18:50,480
每个iterate更新

521
00:18:50,519 --> 00:18:53,039
epoc是说每次扫完数据

522
00:18:53,079 --> 00:18:54,799
那个是已经更新过很多次了

523
00:18:55,599 --> 00:18:56,720
啊我用的是resnet

524
00:18:56,720 --> 00:18:57,680
为什么还是会出

525
00:18:57,680 --> 00:18:59,359
还是会出现数字稳定性问题

526
00:18:59,359 --> 00:19:00,359
当然会出现了

527
00:19:00,399 --> 00:19:03,240
就是说所谓的这些技术是来缓解

528
00:19:03,680 --> 00:19:05,319
他从来不是解决

529
00:19:05,519 --> 00:19:06,839
就是resnet你把能力

530
00:19:06,839 --> 00:19:07,519
rate调到很大

531
00:19:07,519 --> 00:19:08,480
一样的会出问题

532
00:19:08,639 --> 00:19:10,119
就我们有很多很多的方法

533
00:19:10,119 --> 00:19:11,879
来缓解数字稳定性的问题

534
00:19:11,920 --> 00:19:14,160
你可以认为整个深度学习

535
00:19:14,160 --> 00:19:15,480
这个进展都

536
00:19:15,960 --> 00:19:16,599
啊

537
00:19:17,000 --> 00:19:19,880
都是在让数值更加稳定

538
00:19:19,920 --> 00:19:22,160
你可以从这个很简单的

539
00:19:22,200 --> 00:19:22,840
啊

540
00:19:23,200 --> 00:19:24,960
啊观点来考虑

541
00:19:25,560 --> 00:19:27,040
所以啊

542
00:19:28,160 --> 00:19:29,760
resnet没有解决

543
00:19:30,120 --> 00:19:31,200
数值稳定性的问题

544
00:19:31,200 --> 00:19:33,560
只是说他确实比别的稳定性

545
00:19:33,560 --> 00:19:34,440
要好一点点

546
00:19:37,760 --> 00:19:39,920
就是27是说啊

547
00:19:39,920 --> 00:19:40,760
数值稳定性

548
00:19:40,760 --> 00:19:42,560
可能是模型结构引起的

549
00:19:42,600 --> 00:19:44,400
如果觉得啊

550
00:19:44,400 --> 00:19:45,080
孪生网络

551
00:19:45,080 --> 00:19:45,880
孪生网络是吗

552
00:19:45,880 --> 00:19:47,680
是那个to tower

553
00:19:47,840 --> 00:19:48,240
那个吧

554
00:19:48,240 --> 00:19:49,160
就两个嘛

555
00:19:49,280 --> 00:19:50,360
两路输入不一样

556
00:19:50,360 --> 00:19:52,040
会不会引起数值稳定性

557
00:19:52,240 --> 00:19:52,840
啊

558
00:19:52,840 --> 00:19:53,880
会的就是说

559
00:19:54,120 --> 00:19:55,080
这也是个很好问题

560
00:19:55,080 --> 00:19:55,720
我们没讲啊

561
00:19:55,720 --> 00:19:58,040
就是说你有两类不一样的数据

562
00:19:58,320 --> 00:19:58,800
然后呢

563
00:19:58,800 --> 00:20:00,880
你一类比如说文本加图片

564
00:20:00,880 --> 00:20:01,240
嘛

565
00:20:01,360 --> 00:20:03,200
文本进一个神经网络

566
00:20:03,200 --> 00:20:04,640
图片进另外一个神经网络

567
00:20:04,640 --> 00:20:06,040
最后我们要把它合起来

568
00:20:06,240 --> 00:20:07,880
这里这里面最容易的问题

569
00:20:07,880 --> 00:20:09,520
是说你的文本和图片

570
00:20:09,520 --> 00:20:11,280
你的数值区间不一样

571
00:20:11,320 --> 00:20:11,880
你怎么做

572
00:20:11,880 --> 00:20:12,360
对吧

573
00:20:12,520 --> 00:20:14,120
可能文本的输出很大

574
00:20:14,120 --> 00:20:15,680
数图片输出很小

575
00:20:16,480 --> 00:20:18,960
这时候你有很多种办法

576
00:20:18,960 --> 00:20:19,279
啊

577
00:20:19,279 --> 00:20:21,320
bashlon是我们可以做的事情

578
00:20:21,440 --> 00:20:21,960
啊

579
00:20:21,960 --> 00:20:24,120
但是确实这一块

580
00:20:24,160 --> 00:20:24,759
啊

581
00:20:24,759 --> 00:20:27,039
通常来做的方法是通过于

582
00:20:27,080 --> 00:20:27,279
啊

583
00:20:27,279 --> 00:20:30,080
两个两头通过一个权重

584
00:20:30,080 --> 00:20:33,120
就是说这一个的权重加一个权重

585
00:20:33,120 --> 00:20:34,400
就是说文本成一个权重

586
00:20:34,400 --> 00:20:35,320
加上图片

587
00:20:35,320 --> 00:20:37,039
可能图片就不用权重没关系

588
00:20:37,080 --> 00:20:40,400
然后调这个权重使得这两块比较一样

589
00:20:40,440 --> 00:20:43,320
我们可能会在呃style transformation

590
00:20:43,320 --> 00:20:45,200
就是那个样式迁移里面

591
00:20:45,200 --> 00:20:46,600
会大概会讲到这一块

592
00:20:46,600 --> 00:20:48,360
就是它里面也是有两路输入

593
00:20:48,400 --> 00:20:49,200
两路输入的话

594
00:20:49,200 --> 00:20:51,120
一般是通过一个权重来使得

595
00:20:51,160 --> 00:20:52,480
每两类啊

596
00:20:52,480 --> 00:20:53,720
两路的那个数值

597
00:20:53,720 --> 00:20:55,799
都在差不多的范围里面

598
00:20:58,319 --> 00:20:58,559
啊

599
00:20:58,559 --> 00:20:59,759
问题28是说

600
00:20:59,799 --> 00:21:02,920
我们主要做的是算法移植

601
00:21:02,920 --> 00:21:03,880
这种工程化

602
00:21:03,880 --> 00:21:06,480
怎么样在模型设计和模型精度方面

603
00:21:06,480 --> 00:21:07,559
有所突破呢

604
00:21:07,759 --> 00:21:08,080
啊

605
00:21:08,080 --> 00:21:10,240
模型设计和模型精度啊

606
00:21:10,480 --> 00:21:11,240
嗯

607
00:21:11,240 --> 00:21:15,559
这我觉得模型的精啊

608
00:21:15,559 --> 00:21:17,039
设计这是一个很大的问题

609
00:21:17,079 --> 00:21:19,720
我觉得我在这里肯定是解决不了

610
00:21:19,720 --> 00:21:21,200
讲不了太多事情啊

611
00:21:21,359 --> 00:21:23,240
然后嗯

612
00:21:23,440 --> 00:21:24,599
我们来

613
00:21:24,720 --> 00:21:25,799
嗯

614
00:21:25,920 --> 00:21:27,240
我们今后会来讲

615
00:21:27,240 --> 00:21:28,519
就是说不同的

616
00:21:28,559 --> 00:21:29,000
啊

617
00:21:29,000 --> 00:21:30,120
这个这个很大的问题

618
00:21:30,120 --> 00:21:32,480
我们只能说今后再碰到实际的

619
00:21:32,480 --> 00:21:33,519
我们再讲resnet

620
00:21:33,519 --> 00:21:34,680
要讲transformer

621
00:21:34,680 --> 00:21:35,519
讲RN也好

622
00:21:35,519 --> 00:21:37,599
就是说我们尽量去解释

623
00:21:37,599 --> 00:21:40,559
它背后的设计思路会怎么样

624
00:21:40,720 --> 00:21:42,039
就尽量去解释

625
00:21:42,079 --> 00:21:42,440
啊

626
00:21:42,440 --> 00:21:43,679
但是说啊

627
00:21:43,679 --> 00:21:45,799
希望给大家会带来一些

628
00:21:45,960 --> 00:21:46,919
想法

629
00:21:46,960 --> 00:21:49,679
但是说你问我说要要要怎么设计

630
00:21:49,679 --> 00:21:50,720
这个太大了

631
00:21:50,720 --> 00:21:52,480
我觉得这里回答不了

632
00:21:57,000 --> 00:21:58,079
就是说啊

633
00:21:58,079 --> 00:21:58,799
问题29

634
00:21:58,799 --> 00:22:01,919
把每一层的输入的均值方差做限制

635
00:22:01,919 --> 00:22:04,679
是不是可以理解成限制各层输入

636
00:22:04,759 --> 00:22:07,480
只出现极大或极小的异常值

637
00:22:07,639 --> 00:22:08,279
啊

638
00:22:08,440 --> 00:22:11,039
他其实你也可以这么认为

639
00:22:11,039 --> 00:22:14,119
就是说我把均值和方差做限制

640
00:22:14,119 --> 00:22:15,200
可以理解成

641
00:22:15,240 --> 00:22:15,920
啊

642
00:22:15,920 --> 00:22:18,720
如果你的方差确实在一个很小的区间

643
00:22:18,720 --> 00:22:19,160
里面

644
00:22:19,160 --> 00:22:21,359
那么出现极大值的概率就会变低

645
00:22:21,399 --> 00:22:22,279
但是还是会有

646
00:22:22,279 --> 00:22:23,119
但是会变低

647
00:22:23,160 --> 00:22:25,759
通常这些一两额外出现的极大值

648
00:22:25,759 --> 00:22:26,879
不会影响太多

649
00:22:26,879 --> 00:22:28,240
确实可以这么认为

650
00:22:28,279 --> 00:22:28,720
啊

