1
00:00:00,000 --> 00:00:02,240
seq to seq的实现

2
00:00:02,240 --> 00:00:03,960
这基本上import也没什么

3
00:00:04,080 --> 00:00:05,879
就是直接就进去了

4
00:00:07,040 --> 00:00:07,520
OK

5
00:00:07,520 --> 00:00:09,679
所以首先来看一下encoder

6
00:00:10,359 --> 00:00:12,240
就encoder其实比较简单

7
00:00:12,359 --> 00:00:14,320
就是一个很正常的RN

8
00:00:15,359 --> 00:00:16,199
就是说你看到

9
00:00:17,640 --> 00:00:18,800
vocab的size

10
00:00:19,359 --> 00:00:21,120
你的embedding的size

11
00:00:21,480 --> 00:00:24,879
然后你的最后你RN的隐藏大小

12
00:00:25,120 --> 00:00:27,240
你要多少个RN的层

13
00:00:27,760 --> 00:00:29,199
然后你要不要dropout

14
00:00:29,679 --> 00:00:30,079
对吧

15
00:00:30,320 --> 00:00:31,719
然后它其实就两个东西

16
00:00:31,719 --> 00:00:32,960
一个是一个embedding层

17
00:00:33,960 --> 00:00:34,640
就是embedding

18
00:00:34,920 --> 00:00:37,119
反正就是你的vocab的size

19
00:00:37,119 --> 00:00:37,479
对吧

20
00:00:37,479 --> 00:00:39,159
对每个词做一个embedding

21
00:00:39,240 --> 00:00:41,439
然后你出来是你的embedding的

22
00:00:41,439 --> 00:00:42,159
那个长度

23
00:00:42,840 --> 00:00:43,759
embedding出来之后

24
00:00:43,960 --> 00:00:45,000
就是接下来就是RN

25
00:00:45,159 --> 00:00:46,159
我们用的是GRU

26
00:00:46,359 --> 00:00:47,480
你改成LSTM

27
00:00:47,679 --> 00:00:49,200
改成别的都差不多

28
00:00:49,359 --> 00:00:50,480
没什么太多区别

29
00:00:50,840 --> 00:00:52,439
然后当然就是初始化的

30
00:00:52,600 --> 00:00:55,600
就是基本上就是我们上次讲过的

31
00:00:55,719 --> 00:00:57,560
就是输入的是embedding size

32
00:00:57,560 --> 00:00:58,440
然后你的输出

33
00:00:58,600 --> 00:00:59,400
基本上就是

34
00:01:00,400 --> 00:01:01,240
number of hindrances

35
00:01:01,240 --> 00:01:03,920
然后你的RN的层数

36
00:01:04,200 --> 00:01:05,760
你之间要不要dropout

37
00:01:07,000 --> 00:01:07,560
注意到这里

38
00:01:07,560 --> 00:01:08,600
我们没有输出层

39
00:01:09,120 --> 00:01:10,200
没有用一个dense

40
00:01:10,200 --> 00:01:13,159
因为encoder是不需要输出到

41
00:01:13,159 --> 00:01:14,359
最终的标号

42
00:01:14,359 --> 00:01:15,719
所以只要拿到RN

43
00:01:15,719 --> 00:01:17,159
最后一层的输出就行了

44
00:01:18,240 --> 00:01:18,560
好

45
00:01:18,599 --> 00:01:20,079
然后我们看一下它的forward

46
00:01:20,680 --> 00:01:21,840
forward其实也很简单

47
00:01:21,840 --> 00:01:23,719
就是把所有东西都丢进去

48
00:01:23,719 --> 00:01:24,520
然后拿出来

49
00:01:25,240 --> 00:01:27,280
所以看到x进去到embedding

50
00:01:27,680 --> 00:01:28,320
我们有讲过

51
00:01:28,439 --> 00:01:29,920
就是把后面10g

52
00:01:30,400 --> 00:01:31,680
就把两个换一下

53
00:01:31,680 --> 00:01:33,519
就是说把batch size换到中间

54
00:01:33,519 --> 00:01:35,120
然后把10g

55
00:01:35,120 --> 00:01:36,920
number of steps放到前面

56
00:01:37,600 --> 00:01:38,400
这样子的话

57
00:01:38,400 --> 00:01:39,159
丢给RN

58
00:01:39,159 --> 00:01:39,719
RN

59
00:01:39,719 --> 00:01:42,040
它的RN就是要这样子的输入的格式

60
00:01:42,320 --> 00:01:45,040
然后拿到它的输出和拿到的状态

61
00:01:46,879 --> 00:01:49,680
所以输出就是每一个时刻的

62
00:01:51,320 --> 00:01:53,480
每一个时间步的最后一层的输出

63
00:01:53,599 --> 00:01:55,120
最后一个RN的输出

64
00:01:55,719 --> 00:01:57,400
然后state基本上就是

65
00:01:57,400 --> 00:01:58,840
在最后一个时刻

66
00:01:58,840 --> 00:02:01,480
所有的层的输出

67
00:02:01,680 --> 00:02:02,719
就是放在state里面

68
00:02:02,719 --> 00:02:03,760
基本上你看到

69
00:02:04,120 --> 00:02:04,840
是一个

70
00:02:04,960 --> 00:02:06,840
它RN是一个矩阵

71
00:02:06,960 --> 00:02:08,159
这种展开之后

72
00:02:08,159 --> 00:02:10,240
所以output就是最上面的一个东西

73
00:02:10,240 --> 00:02:12,879
就每一个时刻最上面一层的输出

74
00:02:13,080 --> 00:02:15,840
state就是在那边一侧的东西

75
00:02:15,840 --> 00:02:17,120
就是在最后一个时刻

76
00:02:17,240 --> 00:02:18,599
每一个层的输出

77
00:02:18,599 --> 00:02:19,400
就state

78
00:02:20,040 --> 00:02:20,480
OK

79
00:02:20,480 --> 00:02:21,800
这是RN

80
00:02:22,040 --> 00:02:23,040
就incode

81
00:02:23,040 --> 00:02:24,280
incode就是丢进去

82
00:02:24,280 --> 00:02:24,840
然后拿出来

83
00:02:24,840 --> 00:02:25,280
对吧

84
00:02:25,280 --> 00:02:28,280
然后看一下

85
00:02:28,280 --> 00:02:29,479
这是怎么回事

86
00:02:29,919 --> 00:02:31,479
反正我们实现一个incoder

87
00:02:31,840 --> 00:02:34,280
然后反正放在EVA的模式

88
00:02:34,280 --> 00:02:35,479
就是说这样子的话

89
00:02:35,479 --> 00:02:37,599
dropout就不会生效

90
00:02:37,759 --> 00:02:39,479
你写不写的EVA都没关系

91
00:02:39,479 --> 00:02:42,080
然后你说我给一个X

92
00:02:42,080 --> 00:02:43,400
X是4×7

93
00:02:43,400 --> 00:02:44,639
4是batch size

94
00:02:44,639 --> 00:02:45,879
7就是你的

95
00:02:46,639 --> 00:02:47,800
你那个句子的长度

96
00:02:48,680 --> 00:02:51,439
然后你拿到我的output和state

97
00:02:51,560 --> 00:02:54,520
output就是一个746

98
00:02:54,639 --> 00:02:55,120
对吧

99
00:02:55,120 --> 00:02:58,240
7就是你的每一个时刻

100
00:02:58,280 --> 00:03:00,560
然后4就是你的batch size

101
00:03:00,560 --> 00:03:02,800
然后你的16就是你的隐藏层

102
00:03:03,000 --> 00:03:03,480
隐藏层

103
00:03:03,480 --> 00:03:04,719
这有个16的这个地方

104
00:03:05,159 --> 00:03:06,319
这个地方有个16

105
00:03:06,360 --> 00:03:07,120
就隐藏层

106
00:03:08,560 --> 00:03:12,280
然后你的state是一个2416

107
00:03:12,280 --> 00:03:12,840
2是什么

108
00:03:13,319 --> 00:03:15,560
2就是你的有两层

109
00:03:15,920 --> 00:03:20,480
就每一层的在最后一个时刻的输出

110
00:03:20,480 --> 00:03:22,080
就是4就是你的batch size

111
00:03:22,080 --> 00:03:24,599
然后你的16就是你的隐藏层的大小

112
00:03:24,919 --> 00:03:25,239
Ok

113
00:03:25,239 --> 00:03:26,599
这就是大家记住

114
00:03:26,719 --> 00:03:29,159
state和你的output是长出这个样子的

115
00:03:32,919 --> 00:03:33,240
好

116
00:03:33,240 --> 00:03:34,159
然后我们就看

117
00:03:34,719 --> 00:03:36,479
decode所谓的代码

118
00:03:36,479 --> 00:03:37,680
基本上就在这个地方

119
00:03:37,680 --> 00:03:38,359
主要是

120
00:03:39,359 --> 00:03:40,519
我们仔细来看一下

121
00:03:41,959 --> 00:03:43,719
decode在模型上

122
00:03:43,719 --> 00:03:44,879
跟encode是一样的

123
00:03:44,879 --> 00:03:46,159
它有个embedding层

124
00:03:46,599 --> 00:03:46,879
对吧

125
00:03:46,879 --> 00:03:48,159
它有个自己的embedding层

126
00:03:48,159 --> 00:03:50,560
它不能跟你的encode share

127
00:03:50,560 --> 00:03:51,919
因为你的词汇的

128
00:03:51,919 --> 00:03:53,120
你的workhab都不一样

129
00:03:53,479 --> 00:03:54,319
这也是法语

130
00:03:54,519 --> 00:03:55,039
比如说

131
00:03:55,799 --> 00:03:57,439
rn的话

132
00:03:57,439 --> 00:03:57,919
你会看到

133
00:04:00,759 --> 00:04:03,799
它是一个embedding的size

134
00:04:03,799 --> 00:04:06,079
加上一个number of hindrance

135
00:04:06,560 --> 00:04:07,879
就这里我们假设

136
00:04:07,959 --> 00:04:09,799
比如说你的encode的

137
00:04:10,120 --> 00:04:11,280
就是隐藏层大小

138
00:04:11,280 --> 00:04:13,639
和decode的隐藏层大小是一样的

139
00:04:14,000 --> 00:04:14,199
好

140
00:04:14,199 --> 00:04:15,000
我们等会来解释

141
00:04:15,000 --> 00:04:16,480
它为什么要加各种东西进来

142
00:04:16,519 --> 00:04:18,319
但我们有假设在这个地方

143
00:04:18,800 --> 00:04:19,639
后面是一样的

144
00:04:19,639 --> 00:04:21,639
后面就是反正你有多少层

145
00:04:21,879 --> 00:04:23,920
然后你要用dropout

146
00:04:24,839 --> 00:04:26,439
另外一个要注意的是

147
00:04:26,439 --> 00:04:27,519
你的decode

148
00:04:27,519 --> 00:04:29,120
它是有个输出层的

149
00:04:29,399 --> 00:04:31,120
因为最后你要输出到你的

150
00:04:32,240 --> 00:04:33,519
你要去预测每一个词

151
00:04:33,519 --> 00:04:33,959
对吧

152
00:04:33,959 --> 00:04:35,240
每一个词就是你做一个

153
00:04:35,240 --> 00:04:36,959
我们知道做一个workabsize的

154
00:04:36,959 --> 00:04:38,159
一个分类

155
00:04:38,439 --> 00:04:40,480
所以你的输出就是一个

156
00:04:40,480 --> 00:04:42,800
workabsize的一个大小

157
00:04:44,399 --> 00:04:46,639
然后输入程度就是每一个

158
00:04:47,079 --> 00:04:47,680
时刻

159
00:04:47,800 --> 00:04:49,279
你的输入就是一个number of hindrance

160
00:04:49,279 --> 00:04:49,680
对吧

161
00:04:49,719 --> 00:04:50,920
因为是rn的输出

162
00:04:50,920 --> 00:04:52,039
就是一个number of hindrance

163
00:04:54,319 --> 00:04:56,360
好init state

164
00:04:57,560 --> 00:05:00,120
init state我们拿的是谁呢

165
00:05:00,800 --> 00:05:03,319
我们拿的是它的

166
00:05:04,680 --> 00:05:07,879
encoder的output

167
00:05:09,360 --> 00:05:10,240
e是谁

168
00:05:10,439 --> 00:05:11,759
encoder output是什么东西

169
00:05:11,800 --> 00:05:13,959
它有它只output词

170
00:05:14,159 --> 00:05:14,839
看一下这个东西

171
00:05:14,920 --> 00:05:16,680
这时候有点confuse的地方

172
00:05:16,719 --> 00:05:18,120
它其实里面是

173
00:05:19,360 --> 00:05:20,560
里面是两个东西

174
00:05:21,000 --> 00:05:22,240
它其实是两个东西

175
00:05:22,240 --> 00:05:24,199
它是这个东西

176
00:05:24,199 --> 00:05:25,920
它有个output一个state

177
00:05:26,159 --> 00:05:28,479
它的e就是它把state拿出来

178
00:05:29,519 --> 00:05:31,399
它就把自己的state拿出来

179
00:05:31,439 --> 00:05:32,079
OK

180
00:05:32,360 --> 00:05:36,360
它就要做成自己的初始的encode

181
00:05:36,360 --> 00:05:37,759
就是说你的

182
00:05:37,759 --> 00:05:39,560
就是说你的encode

183
00:05:39,599 --> 00:05:41,639
最后一个时间步的state

184
00:05:41,680 --> 00:05:43,199
做成decode的state

185
00:05:43,199 --> 00:05:43,959
这个很好理解

186
00:05:43,959 --> 00:05:44,199
对吧

187
00:05:44,199 --> 00:05:45,079
就等于是

188
00:05:45,120 --> 00:05:46,560
反正你是每一层都对着

189
00:05:46,599 --> 00:05:47,560
然后一路就过去了

190
00:05:47,560 --> 00:05:48,000
对吧

191
00:05:50,079 --> 00:05:50,759
好

192
00:05:52,039 --> 00:05:53,719
但接下来他还干了一点点不一样的

193
00:05:53,719 --> 00:05:54,000
东西

194
00:05:54,000 --> 00:05:54,319
就是说

195
00:05:54,319 --> 00:05:55,639
如果你就是这么干的话

196
00:05:55,759 --> 00:05:56,920
你就是一个RN对吧

197
00:05:56,920 --> 00:05:58,160
你就是一个很简单的RN

198
00:05:58,199 --> 00:05:59,079
没区别

199
00:05:59,079 --> 00:06:00,120
就是我就一个RN

200
00:06:00,120 --> 00:06:02,439
然后就不需要encode decode了

201
00:06:02,439 --> 00:06:02,720
对吧

202
00:06:02,720 --> 00:06:04,480
我反正就接过去了都行

203
00:06:04,920 --> 00:06:06,920
然后就是说这里一点点区别

204
00:06:07,120 --> 00:06:08,160
等会我们来看一下

205
00:06:08,199 --> 00:06:08,879
区别是什么

206
00:06:09,240 --> 00:06:12,000
首先我们看看decode怎么输出

207
00:06:12,639 --> 00:06:13,279
x是什么

208
00:06:13,279 --> 00:06:15,360
x就是你的target句子

209
00:06:15,360 --> 00:06:15,800
那些东西

210
00:06:16,000 --> 00:06:16,600
state

211
00:06:17,839 --> 00:06:18,360
所以一样的

212
00:06:18,519 --> 00:06:19,680
进入我的embedding层

213
00:06:19,680 --> 00:06:22,600
然后我们把时间步放到前面

214
00:06:23,240 --> 00:06:23,839
好

215
00:06:24,199 --> 00:06:25,680
接下来一个context

216
00:06:27,160 --> 00:06:28,560
context是什么东西

217
00:06:29,560 --> 00:06:31,120
context是

218
00:06:31,920 --> 00:06:33,439
这是你的上下文

219
00:06:33,839 --> 00:06:34,439
它是个什么

220
00:06:34,639 --> 00:06:37,240
它是把state的-1

221
00:06:37,879 --> 00:06:39,480
state-1是什么意思

222
00:06:40,399 --> 00:06:42,240
state的-1是说

223
00:06:42,240 --> 00:06:42,959
是

224
00:06:45,439 --> 00:06:47,040
state是最后一个时刻

225
00:06:47,040 --> 00:06:51,000
所有的那些RN层的隐藏状态

226
00:06:51,519 --> 00:06:55,000
它-1就是在最后一个时刻的

227
00:06:55,399 --> 00:06:57,560
最后一层它的输出

228
00:06:58,959 --> 00:06:59,600
OK

229
00:06:59,920 --> 00:07:00,879
就是把那个东西

230
00:07:01,120 --> 00:07:03,040
我们觉得那个东西是

231
00:07:03,759 --> 00:07:06,639
包括了基本上所有的浓缩的信息

232
00:07:06,639 --> 00:07:07,519
都在那里

233
00:07:08,199 --> 00:07:09,480
我们把它拿出来

234
00:07:09,959 --> 00:07:11,439
然后把它repeat一下

235
00:07:11,439 --> 00:07:12,560
repeat是一个什么东西

236
00:07:12,560 --> 00:07:13,759
repeat是一个

237
00:07:13,800 --> 00:07:15,000
就是说跟你的输入

238
00:07:15,000 --> 00:07:16,639
你的batch size是一样

239
00:07:17,959 --> 00:07:18,560
不是batch size

240
00:07:18,839 --> 00:07:20,319
这个已经是你的时间步了

241
00:07:20,439 --> 00:07:21,560
就是把

242
00:07:21,560 --> 00:07:22,360
就是说

243
00:07:23,680 --> 00:07:24,319
第一

244
00:07:24,319 --> 00:07:25,439
encoder

245
00:07:25,759 --> 00:07:27,519
把encoder的

246
00:07:28,599 --> 00:07:29,680
最后一层的

247
00:07:29,680 --> 00:07:30,519
最后一个时间

248
00:07:30,519 --> 00:07:32,240
那个输出拿出来

249
00:07:32,240 --> 00:07:33,839
然后把它重复几次

250
00:07:33,839 --> 00:07:35,079
重复成你的

251
00:07:36,519 --> 00:07:38,360
decode你输入的长度

252
00:07:38,480 --> 00:07:40,000
每个时刻都重复一次

253
00:07:41,159 --> 00:07:44,079
然后把它concat起来

254
00:07:45,399 --> 00:07:46,240
看一看这个东西

255
00:07:46,240 --> 00:07:47,600
在concat是什么意思

256
00:07:47,639 --> 00:07:48,719
concat就是说

257
00:07:48,720 --> 00:07:50,680
我的RN的输入

258
00:07:51,000 --> 00:07:52,440
我的decode RN输入

259
00:07:52,560 --> 00:07:54,960
是当前的embedding的输出

260
00:07:55,000 --> 00:07:56,120
加上

261
00:07:57,080 --> 00:07:58,080
解码

262
00:07:58,120 --> 00:08:00,000
编码器最后一个时刻的东西

263
00:08:00,200 --> 00:08:01,560
context上下文

264
00:08:01,560 --> 00:08:02,560
就是说我觉得

265
00:08:02,560 --> 00:08:04,360
虽然你的state已经传过来了

266
00:08:04,360 --> 00:08:05,240
但是我觉得不够

267
00:08:05,240 --> 00:08:06,480
我还要去看

268
00:08:06,640 --> 00:08:09,080
还要把你的最后时刻

269
00:08:09,080 --> 00:08:11,120
和我的输入拼在一起做进去

270
00:08:11,720 --> 00:08:12,760
所以这就是为什么

271
00:08:12,760 --> 00:08:13,680
我们的RN

272
00:08:13,680 --> 00:08:15,560
它的输入是一个embedding size

273
00:08:15,560 --> 00:08:17,280
加上numbered hindrance的原因

274
00:08:17,520 --> 00:08:18,120
OK

275
00:08:18,720 --> 00:08:19,720
然后拿出来之后

276
00:08:19,720 --> 00:08:21,480
就会拿到我的output的state

277
00:08:22,160 --> 00:08:23,240
state我们就不用了

278
00:08:23,240 --> 00:08:23,600
对吧

279
00:08:23,600 --> 00:08:24,760
我们就拿output

280
00:08:24,880 --> 00:08:25,640
output的话

281
00:08:25,640 --> 00:08:27,280
反正就是把你的

282
00:08:28,360 --> 00:08:30,280
然后我是output的

283
00:08:30,280 --> 00:08:31,200
每个时刻都有

284
00:08:31,360 --> 00:08:33,120
然后把你的做一个dense

285
00:08:33,120 --> 00:08:34,399
就是说做一个输出

286
00:08:34,480 --> 00:08:36,480
然后最后你搞成我们要的

287
00:08:36,480 --> 00:08:37,080
需要的形式

288
00:08:37,160 --> 00:08:38,120
就应该是batch size

289
00:08:38,120 --> 00:08:39,879
在前面的样子

290
00:08:40,000 --> 00:08:42,120
我们本来是batch size在中间

291
00:08:42,200 --> 00:08:43,440
我们现在换到前面

292
00:08:43,639 --> 00:08:45,000
最后我们输出就行了

293
00:08:45,440 --> 00:08:45,800
OK

294
00:08:45,800 --> 00:08:47,000
基本可以看到是说

295
00:08:47,000 --> 00:08:49,559
它跟RN的一点点区别

296
00:08:49,720 --> 00:08:50,480
就是一是

297
00:08:50,480 --> 00:08:51,600
影像状态怎么过来的

298
00:08:51,600 --> 00:08:52,879
二是说我们把

299
00:08:53,120 --> 00:08:54,200
做一个context

300
00:08:54,200 --> 00:08:55,159
和我的输入

301
00:08:55,799 --> 00:08:57,799
就解码器的输

302
00:08:57,799 --> 00:08:59,320
并在一起输出的

303
00:08:59,440 --> 00:08:59,960
OK

304
00:09:01,720 --> 00:09:03,679
这就是我们的decoder

305
00:09:06,039 --> 00:09:06,679
所以看一下

306
00:09:06,679 --> 00:09:07,679
这个decoder长什么样子

307
00:09:07,840 --> 00:09:08,559
就是说

308
00:09:08,600 --> 00:09:10,279
我们实现一个decoder

309
00:09:10,279 --> 00:09:12,679
然后说这样

310
00:09:12,759 --> 00:09:14,039
然后就是把

311
00:09:14,879 --> 00:09:16,320
刚刚那个把

312
00:09:16,320 --> 00:09:17,280
还是那个x

313
00:09:17,760 --> 00:09:19,040
把encoder的输出

314
00:09:19,320 --> 00:09:20,400
放到decoder

315
00:09:20,400 --> 00:09:21,320
init state里面

316
00:09:21,320 --> 00:09:22,200
拿到state

317
00:09:22,640 --> 00:09:23,879
把state和你的

318
00:09:23,879 --> 00:09:24,840
我们就偷个懒

319
00:09:24,960 --> 00:09:25,840
就是把

320
00:09:26,040 --> 00:09:27,560
同样的x又传进来

321
00:09:27,560 --> 00:09:28,600
反正就看一下大小

322
00:09:29,120 --> 00:09:30,520
拿到output的state

323
00:09:30,879 --> 00:09:31,640
output的话

324
00:09:31,640 --> 00:09:33,120
你可以看到是什么东西

325
00:09:33,600 --> 00:09:35,200
4是你的

326
00:09:36,560 --> 00:09:37,520
batch size

327
00:09:37,520 --> 00:09:38,840
最后我们有个permute

328
00:09:39,240 --> 00:09:40,960
把batch size又放回来了

329
00:09:41,400 --> 00:09:42,880
然后7是你的长度

330
00:09:42,880 --> 00:09:44,000
就是你的序列的长

331
00:09:44,000 --> 00:09:45,000
句子的长度

332
00:09:45,159 --> 00:09:47,399
10是你的vocab的size

333
00:09:47,440 --> 00:09:47,960
对吧

334
00:09:48,200 --> 00:09:49,240
这是你的输出

335
00:09:49,279 --> 00:09:52,000
就是说等于是对每一个时刻

336
00:09:52,000 --> 00:09:55,320
然后对每个样本

337
00:09:55,360 --> 00:09:56,320
对每个时刻

338
00:09:56,320 --> 00:09:57,360
我都做一个输出

339
00:09:58,879 --> 00:09:59,559
state

340
00:09:59,559 --> 00:10:01,960
state反正就是2就是乘数

341
00:10:02,159 --> 00:10:04,200
4就是你的batch size

342
00:10:04,200 --> 00:10:05,600
16就是你的number of hindrances

343
00:10:05,600 --> 00:10:08,200
就是对number of hindrances

344
00:10:08,200 --> 00:10:09,000
那就16

345
00:10:09,559 --> 00:10:10,120
OK

346
00:10:10,159 --> 00:10:12,919
所以这就是解码器

347
00:10:16,000 --> 00:10:18,559
另外一个我们要讲的是说

348
00:10:18,600 --> 00:10:20,080
通过灵值化

349
00:10:20,080 --> 00:10:21,159
通过一个mask

350
00:10:21,159 --> 00:10:23,000
来屏蔽的不相关的东西

351
00:10:24,279 --> 00:10:26,039
就记得我们是有个valid length

352
00:10:26,039 --> 00:10:26,559
对吧

353
00:10:26,720 --> 00:10:27,679
就valid length

354
00:10:27,679 --> 00:10:28,440
就是说

355
00:10:28,480 --> 00:10:30,840
我们一个序列里面

356
00:10:30,960 --> 00:10:32,080
我们说

357
00:10:32,120 --> 00:10:32,879
你有

358
00:10:33,559 --> 00:10:34,879
你比如说我有两个序列

359
00:10:35,000 --> 00:10:36,519
123和456

360
00:10:36,519 --> 00:10:37,679
但是我说

361
00:10:37,799 --> 00:10:40,200
假设我的后面两个是

362
00:10:40,240 --> 00:10:41,080
不要的东西

363
00:10:41,080 --> 00:10:41,799
是panic的东西

364
00:10:41,799 --> 00:10:42,200
我怎么办

365
00:10:42,200 --> 00:10:43,000
我要把它过滤掉

366
00:10:43,039 --> 00:10:43,519
对吧

367
00:10:43,720 --> 00:10:44,600
所以这个东西干嘛

368
00:10:44,800 --> 00:10:46,000
就是给你一个x

369
00:10:46,040 --> 00:10:47,399
给你一个valid length

370
00:10:47,399 --> 00:10:47,720
然后

371
00:10:48,560 --> 00:10:49,720
对每一个

372
00:10:49,720 --> 00:10:51,480
就x是有n行的话

373
00:10:51,680 --> 00:10:52,840
有n个样本的话

374
00:10:52,840 --> 00:10:54,560
那么valid length也是常为n

375
00:10:54,680 --> 00:10:56,159
然后就是说

376
00:10:56,159 --> 00:10:56,879
每一次

377
00:10:57,399 --> 00:10:58,840
比如说举个具体例子

378
00:10:59,000 --> 00:10:59,759
就先不讲代码

379
00:10:59,759 --> 00:11:00,159
举个例子

380
00:11:00,159 --> 00:11:00,639
就是说

381
00:11:00,639 --> 00:11:01,720
我告诉你有两个样本

382
00:11:01,720 --> 00:11:03,200
123和456

383
00:11:03,360 --> 00:11:05,080
两个常为3的句子

384
00:11:05,200 --> 00:11:05,759
我告诉你

385
00:11:05,759 --> 00:11:07,360
第一个句子的长度是1

386
00:11:07,920 --> 00:11:08,879
这valid是1

387
00:11:08,879 --> 00:11:09,560
第二个句子

388
00:11:09,560 --> 00:11:10,600
valid是2

389
00:11:11,800 --> 00:11:13,040
那么我调用它的话

390
00:11:13,039 --> 00:11:15,079
就会把第一个句子

391
00:11:15,079 --> 00:11:16,679
第一个东西保留下来

392
00:11:16,719 --> 00:11:18,799
后面的全部填成0

393
00:11:18,799 --> 00:11:19,879
就是我们填的value

394
00:11:20,000 --> 00:11:21,000
你可以填任何东西

395
00:11:21,839 --> 00:11:22,279
然后

396
00:11:22,959 --> 00:11:23,879
第二个就是说

397
00:11:23,879 --> 00:11:24,559
第二个句子

398
00:11:24,559 --> 00:11:25,879
就把第二个

399
00:11:25,879 --> 00:11:27,799
然后保留下来

400
00:11:27,799 --> 00:11:28,919
前面两个保留下来

401
00:11:28,919 --> 00:11:29,959
后面填上0

402
00:11:30,759 --> 00:11:31,360
就说白了

403
00:11:31,360 --> 00:11:32,000
就是说

404
00:11:32,039 --> 00:11:34,399
我把你那些填充东西

405
00:11:34,399 --> 00:11:35,279
给标出来

406
00:11:35,279 --> 00:11:36,799
就是我们等会可以看到

407
00:11:37,319 --> 00:11:39,000
所以基本上就是sequence mask

408
00:11:39,000 --> 00:11:39,759
可以干的事情

409
00:11:40,199 --> 00:11:42,039
这东西是用来处理

410
00:11:42,039 --> 00:11:44,599
我们填的东西的一个操作

411
00:11:44,719 --> 00:11:47,159
mask在NLP处理变敞的东西

412
00:11:47,159 --> 00:11:48,919
是一个非常常见的操作

413
00:11:51,919 --> 00:11:52,079
好

414
00:11:52,079 --> 00:11:54,279
然后我们要拓展一下

415
00:11:54,279 --> 00:11:55,599
我们的softmax

416
00:11:56,240 --> 00:11:57,199
cross entropy

417
00:11:57,759 --> 00:11:58,159
为什么

418
00:11:58,159 --> 00:11:59,839
是因为我们算

419
00:11:59,879 --> 00:12:01,120
我们现在是什么

420
00:12:01,240 --> 00:12:02,360
我们是在

421
00:12:02,399 --> 00:12:03,120
我们可以看到

422
00:12:03,120 --> 00:12:06,439
刚刚我们的解码器的输出

423
00:12:06,599 --> 00:12:09,519
是在每一个样本

424
00:12:09,559 --> 00:12:10,679
每一个时刻

425
00:12:10,680 --> 00:12:11,560
我都输出一个

426
00:12:11,560 --> 00:12:13,320
我cap size的一个预测

427
00:12:13,800 --> 00:12:15,960
但实际上里面很多是填充的

428
00:12:15,960 --> 00:12:16,520
填充的

429
00:12:16,520 --> 00:12:18,520
我们不需要去算softmax

430
00:12:19,720 --> 00:12:20,320
OK

431
00:12:20,440 --> 00:12:21,600
就是我说填充的东西

432
00:12:21,600 --> 00:12:22,680
我不参加计算

433
00:12:22,680 --> 00:12:24,000
因为预测对预测错

434
00:12:24,000 --> 00:12:24,520
没意义

435
00:12:24,520 --> 00:12:25,800
我只要预测出那个

436
00:12:25,800 --> 00:12:27,880
end of send叫EOS就行了

437
00:12:27,880 --> 00:12:28,320
对吧

438
00:12:29,680 --> 00:12:30,480
所以我们怎么做

439
00:12:30,920 --> 00:12:32,200
就是说我们说白了

440
00:12:32,200 --> 00:12:33,560
就是给cross

441
00:12:33,560 --> 00:12:34,960
cross entropy loss

442
00:12:35,120 --> 00:12:36,160
加一个weight

443
00:12:36,520 --> 00:12:38,200
就是说要的weight是1

444
00:12:38,200 --> 00:12:39,440
不要的weight是0

445
00:12:39,760 --> 00:12:40,880
所以你可以看到是怎么干的

446
00:12:41,000 --> 00:12:41,600
就是说

447
00:12:41,640 --> 00:12:43,080
weights就是我们生成一个

448
00:12:43,080 --> 00:12:43,880
跟label一样的

449
00:12:43,880 --> 00:12:45,040
全一的东西

450
00:12:45,640 --> 00:12:47,120
然后我们用valid length

451
00:12:47,120 --> 00:12:48,800
因为我们知道每个句子

452
00:12:48,800 --> 00:12:50,160
到底多少个是有效的

453
00:12:50,160 --> 00:12:51,600
多少个是有效的

454
00:12:51,600 --> 00:12:52,360
剩下是padding

455
00:12:52,800 --> 00:12:53,840
就把每一个句子

456
00:12:54,240 --> 00:12:55,680
有效的那一些东西

457
00:12:55,880 --> 00:12:57,040
一保留下来

458
00:12:57,080 --> 00:12:58,880
其他东西全部变成0

459
00:12:59,960 --> 00:13:01,240
然后我们算loss的时候

460
00:13:01,240 --> 00:13:02,240
我们就调负累的

461
00:13:03,200 --> 00:13:03,840
loss函数

462
00:13:04,080 --> 00:13:04,640
就是说

463
00:13:06,280 --> 00:13:07,880
其实我是不需要调

464
00:13:08,000 --> 00:13:09,439
super不需要写那么麻烦

465
00:13:09,639 --> 00:13:10,360
super就是

466
00:13:11,039 --> 00:13:12,720
其实就是

467
00:13:13,399 --> 00:13:14,679
就这么写就可以了

468
00:13:15,240 --> 00:13:17,039
然后就是predict permute

469
00:13:17,039 --> 00:13:17,480
我们

470
00:13:18,480 --> 00:13:19,039
就是把

471
00:13:20,559 --> 00:13:21,519
这个是PyTorch的

472
00:13:21,960 --> 00:13:23,399
他要把预测的

473
00:13:23,559 --> 00:13:24,519
纬度放到中间

474
00:13:24,519 --> 00:13:26,439
就是说他要把留在中间

475
00:13:26,439 --> 00:13:28,480
这是他的一个要求

476
00:13:29,240 --> 00:13:31,159
所以因为我们把最后纬度

477
00:13:31,159 --> 00:13:31,919
挪到中间来

478
00:13:31,919 --> 00:13:34,279
因为他需要在第二个纬度

479
00:13:34,279 --> 00:13:36,240
作为他是输出的纬度

480
00:13:36,879 --> 00:13:37,200
然后

481
00:13:39,960 --> 00:13:41,560
我们就是说做loss的时候

482
00:13:41,560 --> 00:13:42,560
对每一项做loss

483
00:13:42,560 --> 00:13:44,799
但是你不要做reduction

484
00:13:45,879 --> 00:13:46,799
就不要求命

485
00:13:46,919 --> 00:13:47,759
不要求什么东西

486
00:13:48,080 --> 00:13:49,200
然后这样子的话

487
00:13:49,200 --> 00:13:50,840
我们对每一个

488
00:13:51,799 --> 00:13:53,360
时间步和每个样本

489
00:13:53,360 --> 00:13:53,919
他的预测

490
00:13:53,919 --> 00:13:55,399
他的loss拿出来之后

491
00:13:56,159 --> 00:13:58,320
就跟我weight乘一下

492
00:13:58,960 --> 00:13:59,480
对吧

493
00:14:00,480 --> 00:14:01,039
weight乘一下

494
00:14:01,039 --> 00:14:01,720
就是说

495
00:14:03,600 --> 00:14:05,639
只有有效的地方留下来

496
00:14:05,639 --> 00:14:07,199
没下的地方全部变成零

497
00:14:07,879 --> 00:14:08,480
OK

498
00:14:08,639 --> 00:14:09,199
这就是我

499
00:14:10,080 --> 00:14:10,519
Mask

500
00:14:11,360 --> 00:14:12,600
最后我们就返回

501
00:14:12,600 --> 00:14:14,720
我们被乘过之后的那个东西

502
00:14:15,319 --> 00:14:16,319
当我们做了一个命

503
00:14:16,439 --> 00:14:17,480
命就是说

504
00:14:17,759 --> 00:14:18,879
这个地方就是

505
00:14:19,159 --> 00:14:20,360
dimension等于一个命

506
00:14:20,480 --> 00:14:21,840
就是对每一个句子

507
00:14:21,879 --> 00:14:22,840
我们取个平均

508
00:14:23,000 --> 00:14:23,559
这样子的话

509
00:14:23,559 --> 00:14:25,879
我们对每个样本返回一个loss

510
00:14:26,759 --> 00:14:27,399
这就是我们

511
00:14:28,240 --> 00:14:29,120
Mask loss

512
00:14:29,120 --> 00:14:30,319
他的干的事情

513
00:14:34,319 --> 00:14:35,000
最后看一下

514
00:14:35,399 --> 00:14:36,679
你可以看到长什么样子

515
00:14:36,679 --> 00:14:37,279
就是说

516
00:14:37,440 --> 00:14:38,679
假设我给一个

517
00:14:39,159 --> 00:14:40,639
3是10

518
00:14:40,639 --> 00:14:41,879
就30batch size

519
00:14:42,159 --> 00:14:44,080
4是你的时间长度

520
00:14:44,080 --> 00:14:46,120
然后10是你的cap size

521
00:14:46,360 --> 00:14:47,159
我这边标号

522
00:14:47,159 --> 00:14:48,480
假设是34的话

523
00:14:48,639 --> 00:14:49,600
然后我的

524
00:14:49,600 --> 00:14:50,559
假设我的

525
00:14:50,919 --> 00:14:51,559
valid length

526
00:14:51,559 --> 00:14:52,279
第一个是说

527
00:14:52,279 --> 00:14:53,639
所有的都是valid

528
00:14:53,639 --> 00:14:54,759
就是说它长度是4

529
00:14:55,240 --> 00:14:56,879
第一个我说第一个样本

530
00:14:57,080 --> 00:14:58,679
他的所有的都是valid

531
00:14:58,840 --> 00:14:59,600
第二个样本

532
00:15:01,120 --> 00:15:02,759
就是只有两个是valid

533
00:15:02,840 --> 00:15:03,679
第三个样本

534
00:15:04,360 --> 00:15:05,280
都不是valid

535
00:15:05,280 --> 00:15:06,120
就是都是0

536
00:15:06,120 --> 00:15:07,480
所以你看到就是说

537
00:15:07,520 --> 00:15:09,200
返回的就是一个3

538
00:15:09,240 --> 00:15:10,120
3乘以1的

539
00:15:10,120 --> 00:15:11,600
就是一个长为3的向量

540
00:15:12,040 --> 00:15:12,600
每一个向量

541
00:15:12,600 --> 00:15:14,200
就是你一个对应的是一个句子

542
00:15:14,680 --> 00:15:15,800
所以他是求均值

543
00:15:16,480 --> 00:15:18,120
然后第一个是大一点

544
00:15:18,120 --> 00:15:18,960
第二个是小一点

545
00:15:18,960 --> 00:15:19,200
对吧

546
00:15:19,200 --> 00:15:20,320
第二个有两个是0

547
00:15:21,520 --> 00:15:22,800
第三个就是全0

548
00:15:22,800 --> 00:15:23,360
OK

549
00:15:23,640 --> 00:15:25,760
这就是我的loss

550
00:15:28,400 --> 00:15:28,640
好

551
00:15:28,640 --> 00:15:30,000
最后可以看到训练

552
00:15:30,200 --> 00:15:31,680
训练其实就是

553
00:15:32,000 --> 00:15:33,320
没什么特别

554
00:15:33,440 --> 00:15:34,520
跟前面是一样的

555
00:15:34,520 --> 00:15:35,240
就是说

556
00:15:35,720 --> 00:15:37,560
你可以看到我们它是一个

557
00:15:38,760 --> 00:15:40,160
net就是encoder

558
00:15:40,400 --> 00:15:41,240
它encoder decoder

559
00:15:41,240 --> 00:15:42,120
虽然encoder decoder

560
00:15:42,120 --> 00:15:44,040
中间是有点state什么东西

561
00:15:44,040 --> 00:15:45,080
但是你包完之后

562
00:15:45,080 --> 00:15:46,320
它其实就是一个

563
00:15:46,320 --> 00:15:48,480
很正常的一个network

564
00:15:48,920 --> 00:15:50,240
前面基本上都可以不看

565
00:15:50,240 --> 00:15:52,240
就唯一的就是说我们loss

566
00:15:52,240 --> 00:15:53,880
是用到我们的mask的loss

567
00:15:54,680 --> 00:15:56,760
然后可以看到我们是怎么样做

568
00:15:56,960 --> 00:15:57,560
可以看一下

569
00:15:57,560 --> 00:15:58,560
这里是怎么输出的

570
00:15:59,480 --> 00:16:01,200
主要的精华在于

571
00:16:02,080 --> 00:16:02,800
这一块

572
00:16:03,720 --> 00:16:05,040
首先我们

573
00:16:05,400 --> 00:16:06,360
batch里面是什么

574
00:16:06,360 --> 00:16:08,560
batch里面是有你的原句子

575
00:16:08,680 --> 00:16:10,400
原句子的valid length

576
00:16:10,600 --> 00:16:12,320
你的target的句子

577
00:16:12,320 --> 00:16:13,000
target的句子

578
00:16:13,000 --> 00:16:13,800
valid length

579
00:16:13,840 --> 00:16:14,480
对吧

580
00:16:16,280 --> 00:16:16,720
然后

581
00:16:18,320 --> 00:16:19,280
我们干了一个事情

582
00:16:19,280 --> 00:16:20,560
就是我们在

583
00:16:21,240 --> 00:16:22,480
加了一个

584
00:16:22,640 --> 00:16:24,080
begin of sentence进去

585
00:16:24,080 --> 00:16:25,280
就是说我们因为原句子

586
00:16:25,280 --> 00:16:25,920
你要翻译的时候

587
00:16:25,920 --> 00:16:27,040
有一个begin of sentence

588
00:16:27,040 --> 00:16:27,760
就是说

589
00:16:27,800 --> 00:16:29,800
我们在我们的

590
00:16:30,480 --> 00:16:31,160
input

591
00:16:31,559 --> 00:16:32,879
我们给decode的input

592
00:16:32,919 --> 00:16:33,839
就是原句子

593
00:16:34,360 --> 00:16:35,279
你把

594
00:16:35,759 --> 00:16:37,159
把最后一个东西拿掉

595
00:16:37,199 --> 00:16:38,159
最后一个东西拿掉

596
00:16:38,159 --> 00:16:38,879
然后在前面

597
00:16:39,000 --> 00:16:40,959
贴加一个begin of sentence

598
00:16:40,959 --> 00:16:42,600
作为它的input

599
00:16:42,919 --> 00:16:43,480
对吧

600
00:16:43,639 --> 00:16:45,039
它的output需要去match

601
00:16:45,039 --> 00:16:45,480
这个y

602
00:16:45,480 --> 00:16:46,199
但是它的input

603
00:16:46,319 --> 00:16:47,199
就是要把

604
00:16:47,519 --> 00:16:48,439
贴一个

605
00:16:48,559 --> 00:16:49,279
句子开始

606
00:16:49,279 --> 00:16:50,839
然后把所有东西都后移一下

607
00:16:50,839 --> 00:16:51,839
这样子我能够

608
00:16:51,879 --> 00:16:53,879
给定现在去预测下一个词

609
00:16:53,879 --> 00:16:54,399
对吧

610
00:16:56,000 --> 00:16:56,600
然后

611
00:16:58,039 --> 00:16:58,759
x

612
00:16:59,319 --> 00:17:00,199
就是

613
00:17:01,000 --> 00:17:01,920
你的

614
00:17:03,800 --> 00:17:04,840
它的input

615
00:17:04,840 --> 00:17:06,080
然后它的valid length

616
00:17:06,080 --> 00:17:06,799
拿进去

617
00:17:06,960 --> 00:17:08,319
可以拿到我的net

618
00:17:08,319 --> 00:17:09,559
然后就是y hat

619
00:17:09,720 --> 00:17:11,440
y hat就是跟你的y

620
00:17:12,240 --> 00:17:12,840
就这个地方

621
00:17:12,840 --> 00:17:14,440
我们不需要把y的valid length

622
00:17:14,440 --> 00:17:14,799
传进去

623
00:17:14,799 --> 00:17:17,400
就是因为我们只是在算lost

624
00:17:17,400 --> 00:17:18,039
需要用

625
00:17:18,080 --> 00:17:19,080
算lost的时候

626
00:17:19,200 --> 00:17:19,880
就是

627
00:17:20,120 --> 00:17:21,080
其实我们这个地方

628
00:17:21,080 --> 00:17:22,559
还没有用到这个东西

629
00:17:22,720 --> 00:17:23,759
我们之后会用

630
00:17:23,759 --> 00:17:25,319
之所以这么写在这个地方

631
00:17:26,319 --> 00:17:27,680
之后我们在做tension的时候

632
00:17:27,680 --> 00:17:28,400
需要用到它

633
00:17:28,519 --> 00:17:29,840
但其实我们现在用的时候

634
00:17:29,839 --> 00:17:31,079
主要是用到这个东西

635
00:17:31,079 --> 00:17:31,879
和这两个东西

636
00:17:31,879 --> 00:17:32,319
对吧

637
00:17:32,919 --> 00:17:34,319
然后算lost的时候

638
00:17:34,439 --> 00:17:35,079
就是说

639
00:17:35,319 --> 00:17:36,319
那些y的填充

640
00:17:36,319 --> 00:17:37,559
我就不算lost了

641
00:17:37,879 --> 00:17:39,599
最后我们就是

642
00:17:40,959 --> 00:17:41,720
求个t度

643
00:17:41,720 --> 00:17:42,799
然后更新了

644
00:17:43,119 --> 00:17:44,000
所以基本上就是

645
00:17:44,000 --> 00:17:44,720
你可以看到

646
00:17:44,919 --> 00:17:45,759
核心的区别

647
00:17:45,759 --> 00:17:46,879
就是这么几句话

648
00:17:47,119 --> 00:17:48,079
基本上就是

649
00:17:48,599 --> 00:17:50,679
怎么样找你的

650
00:17:50,720 --> 00:17:51,519
decoder

651
00:17:51,519 --> 00:17:52,240
你的

652
00:17:52,639 --> 00:17:54,039
输入和输出

653
00:17:54,039 --> 00:17:56,000
以及说你怎么算lost的时候

654
00:17:56,000 --> 00:17:58,599
把你那些padding给你去掉

655
00:17:58,799 --> 00:17:59,319
OK

656
00:17:59,319 --> 00:18:00,599
所以这个就是整个

657
00:18:00,919 --> 00:18:02,159
训练算法的逻辑

658
00:18:02,240 --> 00:18:02,679
别的东西

659
00:18:02,679 --> 00:18:04,359
都是跟之前是一样的东西

660
00:18:07,759 --> 00:18:08,079
好

661
00:18:08,079 --> 00:18:08,399
这样子

662
00:18:08,399 --> 00:18:09,679
我们就可以训练我们的东西了

663
00:18:09,720 --> 00:18:10,279
训练东西

664
00:18:10,279 --> 00:18:11,679
当然这个东西

665
00:18:11,720 --> 00:18:12,319
就是

666
00:18:12,399 --> 00:18:13,399
相对来说

667
00:18:13,679 --> 00:18:14,319
比较多一点

668
00:18:15,480 --> 00:18:16,720
就是说超强是多一点

669
00:18:16,720 --> 00:18:17,919
超强是可以看到

670
00:18:18,079 --> 00:18:19,639
embedding size是32

671
00:18:19,679 --> 00:18:20,839
然后number of hindrances

672
00:18:21,079 --> 00:18:22,359
就是都很小

673
00:18:22,480 --> 00:18:22,919
你可以看到

674
00:18:22,919 --> 00:18:24,200
就是32

675
00:18:24,319 --> 00:18:27,000
然后你的两层dropout

676
00:18:27,039 --> 00:18:27,599
用了

677
00:18:28,160 --> 00:18:29,040
一个0.1

678
00:18:29,280 --> 00:18:30,480
就是dropout的0.1

679
00:18:30,480 --> 00:18:32,560
就是应该丢掉10%的东西

680
00:18:33,240 --> 00:18:34,680
然后batch size又是4

681
00:18:34,680 --> 00:18:35,240
number of steps

682
00:18:35,240 --> 00:18:36,600
就是句子长度是10

683
00:18:36,640 --> 00:18:37,720
我们刚刚有看到

684
00:18:37,840 --> 00:18:38,360
我们的句子

685
00:18:38,360 --> 00:18:39,520
基本上都是在

686
00:18:39,560 --> 00:18:41,000
长度为10以内

687
00:18:43,120 --> 00:18:43,920
learning rate

688
00:18:44,640 --> 00:18:46,360
跑300个epoch

689
00:18:46,360 --> 00:18:47,920
因为我们这个数据比较小

690
00:18:47,920 --> 00:18:48,880
所以我们可以跑多一点

691
00:18:48,880 --> 00:18:49,960
然后用GPU

692
00:18:50,800 --> 00:18:52,120
然后这些东西都是

693
00:18:52,400 --> 00:18:53,640
怎么load进来

694
00:18:53,760 --> 00:18:54,760
然后把

695
00:18:54,960 --> 00:18:57,000
encode decode生成出来

696
00:18:57,039 --> 00:18:59,160
然后用encode和decode

697
00:18:59,160 --> 00:19:02,000
创建一个encode decode的net

698
00:19:02,160 --> 00:19:02,480
对吧

699
00:19:02,480 --> 00:19:04,079
它就有两个东西在里面了

700
00:19:04,240 --> 00:19:05,480
然后就把net丢进去

701
00:19:05,480 --> 00:19:06,279
可以训练了

702
00:19:06,720 --> 00:19:07,559
基本上就是

703
00:19:07,559 --> 00:19:09,039
我们没有做validation

704
00:19:09,160 --> 00:19:10,759
就是就看到training loss

705
00:19:10,759 --> 00:19:11,720
看到training loss

706
00:19:11,720 --> 00:19:12,599
咣咣咣的往下降

707
00:19:12,599 --> 00:19:13,000
对吧

708
00:19:13,200 --> 00:19:14,279
然后可以看到

709
00:19:14,319 --> 00:19:15,519
跑起来还挺快的

710
00:19:15,720 --> 00:19:16,839
因为说白了

711
00:19:16,839 --> 00:19:17,839
它就是两个RM

712
00:19:18,000 --> 00:19:19,359
就是或者

713
00:19:19,359 --> 00:19:19,799
你可以认为

714
00:19:19,799 --> 00:19:20,559
就是一个RM

715
00:19:20,720 --> 00:19:21,720
一个RM

716
00:19:21,720 --> 00:19:22,680
然后长度是20

717
00:19:22,680 --> 00:19:23,119
对吧

718
00:19:24,440 --> 00:19:25,559
所以我们训练

719
00:19:26,440 --> 00:19:27,559
大概1万个token

720
00:19:27,799 --> 00:19:29,039
可以每秒可以训练到

721
00:19:29,440 --> 00:19:30,240
基本上可以看到

722
00:19:30,480 --> 00:19:31,599
这个东西到

723
00:19:32,839 --> 00:19:34,480
到后面就比较平了

724
00:19:34,679 --> 00:19:36,960
这基本上就over fitting了

725
00:19:36,960 --> 00:19:38,319
是吧

726
00:19:38,319 --> 00:19:39,799
你的句子都给你记住了

727
00:19:41,319 --> 00:19:41,839
OK

728
00:19:42,559 --> 00:19:42,960
好

729
00:19:42,960 --> 00:19:44,200
我们刚才训练

730
00:19:44,399 --> 00:19:45,599
训练我们看到是

731
00:19:45,599 --> 00:19:46,759
我们在做

732
00:19:46,799 --> 00:19:47,279
输入

733
00:19:47,279 --> 00:19:49,359
就解码器的输入输出的时候

734
00:19:49,720 --> 00:19:50,839
输入用的是

735
00:19:50,879 --> 00:19:52,759
真实的target的句子

736
00:19:53,160 --> 00:19:56,839
用了它加上一个BOS做的输入

737
00:19:56,879 --> 00:19:57,640
在预测的时候

738
00:19:57,640 --> 00:19:58,519
没有真实句子

739
00:19:59,079 --> 00:20:00,960
所以这要做一点点别的区别

740
00:20:01,119 --> 00:20:02,279
所以这一堆代码

741
00:20:02,279 --> 00:20:03,400
都是干这个事情

742
00:20:05,839 --> 00:20:06,759
就是说这个函数

743
00:20:06,920 --> 00:20:07,839
跟我们之前的

744
00:20:07,839 --> 00:20:09,559
那个language model

745
00:20:09,759 --> 00:20:11,079
就做预测是一样的

746
00:20:11,240 --> 00:20:12,440
就给我一个句子

747
00:20:12,799 --> 00:20:14,119
给我一个原句子

748
00:20:14,359 --> 00:20:15,440
它的vocab

749
00:20:15,640 --> 00:20:17,440
和的目标的vocab

750
00:20:17,640 --> 00:20:18,319
你给我预测

751
00:20:18,480 --> 00:20:20,079
你给我把那个句子翻译出来

752
00:20:20,079 --> 00:20:21,960
就说说最大长度是多少

753
00:20:22,440 --> 00:20:23,400
就是长度是多少

754
00:20:23,640 --> 00:20:25,160
就是说之前的10

755
00:20:26,559 --> 00:20:27,759
这个东西你可以先不看

756
00:20:27,759 --> 00:20:28,160
这个东西

757
00:20:28,160 --> 00:20:28,960
都是我们之后

758
00:20:28,960 --> 00:20:30,160
attention要用的东西

759
00:20:32,160 --> 00:20:32,799
OK

760
00:20:33,240 --> 00:20:33,799
可以看一下

761
00:20:34,079 --> 00:20:35,360
就怎么办呢

762
00:20:35,880 --> 00:20:37,120
encode比较好理解

763
00:20:37,120 --> 00:20:37,360
对吧

764
00:20:37,360 --> 00:20:38,960
encode我们就把token

765
00:20:39,120 --> 00:20:41,680
把它tokenized出来

766
00:20:41,680 --> 00:20:42,759
然后把它的

767
00:20:42,799 --> 00:20:44,079
wide length

768
00:20:44,079 --> 00:20:45,039
我们现在其实不用

769
00:20:45,039 --> 00:20:45,759
但我们之后用

770
00:20:45,759 --> 00:20:46,799
所以我们存在那里

771
00:20:46,960 --> 00:20:48,120
然后这个token

772
00:20:48,240 --> 00:20:49,880
这个东西都是encode

773
00:20:50,079 --> 00:20:50,640
就是说

774
00:20:50,640 --> 00:20:51,800
这东西都是把encode

775
00:20:51,800 --> 00:20:52,600
给表示出来

776
00:20:52,960 --> 00:20:56,440
然后我们从encode

777
00:20:56,560 --> 00:20:58,560
拿到我的encode output

778
00:20:58,880 --> 00:21:00,440
就基本上跟之前没区别

779
00:21:00,480 --> 00:21:00,840
这些东西

780
00:21:00,840 --> 00:21:01,720
就是把我的encode

781
00:21:01,720 --> 00:21:03,520
表示成tokenized

782
00:21:03,560 --> 00:21:05,080
然后vocab到index

783
00:21:05,120 --> 00:21:06,280
丢到我的encoder

784
00:21:06,280 --> 00:21:07,759
把我的encode的东西拿出来

785
00:21:09,440 --> 00:21:10,480
接下来是decode

786
00:21:10,480 --> 00:21:11,600
decode把state拿出来

787
00:21:11,640 --> 00:21:13,080
这个跟我们之前是没区别的

788
00:21:14,000 --> 00:21:14,360
好

789
00:21:14,400 --> 00:21:15,840
接下来主要是在这个地方

790
00:21:17,800 --> 00:21:18,840
decoder

791
00:21:18,839 --> 00:21:20,720
这时候我们就不能用RN的

792
00:21:20,720 --> 00:21:22,799
就是把一个序列丢进去了

793
00:21:22,879 --> 00:21:23,799
我们刚刚是怎么样

794
00:21:23,799 --> 00:21:25,439
我们刚刚是把整个target的

795
00:21:25,439 --> 00:21:26,240
目标句子

796
00:21:26,319 --> 00:21:27,039
整个句子

797
00:21:27,039 --> 00:21:28,480
那个东西丢进去

798
00:21:29,319 --> 00:21:30,359
现在我们不能这么干了

799
00:21:30,359 --> 00:21:32,159
因为我们要一步一步的

800
00:21:32,159 --> 00:21:33,519
往前去预测

801
00:21:33,959 --> 00:21:35,319
所以我们先看第一步

802
00:21:35,559 --> 00:21:36,359
第一步是干嘛

803
00:21:36,359 --> 00:21:37,039
第一步就是说

804
00:21:37,039 --> 00:21:38,079
你可以忽略到这些

805
00:21:38,079 --> 00:21:39,679
onscreeze这种东西

806
00:21:39,679 --> 00:21:41,799
说白了就是把BOS这个东西

807
00:21:42,319 --> 00:21:44,359
第一步就是给你的decoder

808
00:21:44,439 --> 00:21:45,799
编码器给一个

809
00:21:45,919 --> 00:21:46,839
句子开始了

810
00:21:46,839 --> 00:21:48,319
你可以开始往前做了

811
00:21:49,279 --> 00:21:52,359
就等于就是把BOS包成一个tensor

812
00:21:52,359 --> 00:21:53,279
就看这个事情

813
00:21:53,279 --> 00:21:54,559
但是你要把纬度设对

814
00:21:54,559 --> 00:21:55,519
因为你有batchsize

815
00:21:56,039 --> 00:21:57,519
你要把batch搞出来

816
00:21:57,599 --> 00:21:58,599
number step搞出来

817
00:21:58,919 --> 00:21:59,439
对吧

818
00:22:00,119 --> 00:22:00,439
好

819
00:22:00,439 --> 00:22:02,720
然后我们就fold起来

820
00:22:02,959 --> 00:22:04,599
那是预测n步

821
00:22:06,199 --> 00:22:08,519
每一步把输入进去

822
00:22:08,559 --> 00:22:10,679
就是一开始输就是BOS

823
00:22:10,799 --> 00:22:11,720
句子开始

824
00:22:11,839 --> 00:22:13,079
它state进去

825
00:22:13,199 --> 00:22:15,879
输出一个y和它的更新的state

826
00:22:16,880 --> 00:22:18,760
y我们就可以

827
00:22:18,920 --> 00:22:20,280
把它的argmax

828
00:22:20,280 --> 00:22:21,040
dim2

829
00:22:21,040 --> 00:22:21,400
就是

830
00:22:23,000 --> 00:22:25,360
最后dim2是最后

831
00:22:25,360 --> 00:22:26,320
就是workheavway

832
00:22:26,720 --> 00:22:27,600
把那个东西

833
00:22:28,840 --> 00:22:30,640
argmax就是预测的

834
00:22:30,640 --> 00:22:32,160
那一个token

835
00:22:32,160 --> 00:22:33,240
就预测的一个词

836
00:22:33,920 --> 00:22:34,640
拿出来

837
00:22:35,240 --> 00:22:37,400
做成decoded x

838
00:22:37,400 --> 00:22:38,520
就是把它的预测

839
00:22:38,520 --> 00:22:39,920
作为下一步的输入

840
00:22:40,840 --> 00:22:42,600
当然之后我们要把这个东西

841
00:22:42,600 --> 00:22:43,760
做成predict出来

842
00:22:43,760 --> 00:22:44,840
变成一个integer

843
00:22:45,000 --> 00:22:46,439
这个东西就把它变成一个integer

844
00:22:46,439 --> 00:22:47,359
要把那些

845
00:22:47,519 --> 00:22:49,039
因为你的纬度比较高

846
00:22:49,159 --> 00:22:50,319
要把那些纬度去掉

847
00:22:50,319 --> 00:22:51,919
就把batchsize纬度去掉

848
00:22:51,919 --> 00:22:53,799
要把它换成一个int

849
00:22:53,959 --> 00:22:55,919
然后这个东西你可先不管

850
00:22:56,919 --> 00:22:58,919
这个东西是我们之后要去

851
00:22:59,279 --> 00:23:00,399
看tension

852
00:23:00,399 --> 00:23:01,000
那个weight

853
00:23:01,000 --> 00:23:02,079
怎么样的东西用

854
00:23:03,399 --> 00:23:04,919
如果predict

855
00:23:05,119 --> 00:23:08,119
它等于我的end of sentence

856
00:23:08,599 --> 00:23:09,119
就预测

857
00:23:09,319 --> 00:23:10,240
我预测结束了

858
00:23:10,240 --> 00:23:11,119
那就break

859
00:23:12,199 --> 00:23:13,199
不然的话我就继续

860
00:23:13,199 --> 00:23:13,480
对吧

861
00:23:13,480 --> 00:23:15,319
我就把它放在我doppler里面

862
00:23:15,319 --> 00:23:15,960
就继续

863
00:23:16,720 --> 00:23:17,680
如果实在不行

864
00:23:17,680 --> 00:23:18,360
就是说

865
00:23:18,400 --> 00:23:20,240
一直没有看到句子结束

866
00:23:21,039 --> 00:23:22,400
那就是跑10下

867
00:23:22,559 --> 00:23:24,240
就我们这里设的是10

868
00:23:25,160 --> 00:23:25,400
好

869
00:23:25,400 --> 00:23:28,000
最后就是把那些预测东西

870
00:23:28,079 --> 00:23:28,880
到我cap里面

871
00:23:28,880 --> 00:23:30,120
把token查出来

872
00:23:30,240 --> 00:23:31,640
把它用空格

873
00:23:31,640 --> 00:23:32,440
把它变成虚化

874
00:23:32,440 --> 00:23:33,400
就可以输出了

875
00:23:33,640 --> 00:23:34,240
OK

876
00:23:34,240 --> 00:23:36,720
所以这就是预测干的事情

877
00:23:37,240 --> 00:23:37,880
预测干的事情

878
00:23:37,880 --> 00:23:38,640
基本上就是

879
00:23:38,640 --> 00:23:42,039
你看主要是你需要一步一步的

880
00:23:42,039 --> 00:23:42,759
往前走

881
00:23:42,759 --> 00:23:44,839
然后把上一步的预测

882
00:23:45,200 --> 00:23:46,519
作为下一步的输出

883
00:23:46,680 --> 00:23:47,400
拉进去

884
00:23:47,400 --> 00:23:48,680
就是一个拆解

885
00:23:48,680 --> 00:23:50,039
OK

886
00:23:50,039 --> 00:23:52,119
虽然就carry代码比较多

887
00:23:52,240 --> 00:23:53,440
实际上就干这个事情

888
00:23:56,599 --> 00:24:00,359
然后我们来看一下

889
00:24:03,519 --> 00:24:05,079
blue这个东西

890
00:24:05,839 --> 00:24:07,000
我就不给大家

891
00:24:07,000 --> 00:24:07,640
一定要讲

892
00:24:07,640 --> 00:24:08,559
这长什么样

893
00:24:08,599 --> 00:24:09,400
基本上你可以看到

894
00:24:09,759 --> 00:24:11,319
可以大概看到是干嘛

895
00:24:12,279 --> 00:24:12,879
就score

896
00:24:12,879 --> 00:24:15,279
score就是我们叫label的sense

897
00:24:15,279 --> 00:24:18,359
是除以你的predict sentence

898
00:24:18,359 --> 00:24:19,559
然后一减去mean

899
00:24:19,559 --> 00:24:20,200
一下exp

900
00:24:20,319 --> 00:24:21,200
就是我们score

901
00:24:21,200 --> 00:24:23,079
就是我们数学前面那一坨

902
00:24:23,439 --> 00:24:24,319
后面那一坨

903
00:24:24,559 --> 00:24:26,240
就是反正是n就是n-grand

904
00:24:26,559 --> 00:24:27,960
从1一直到k加1

905
00:24:27,960 --> 00:24:30,039
然后去算是说

906
00:24:30,439 --> 00:24:31,200
基本上你看到

907
00:24:31,200 --> 00:24:32,159
他干什么事情

908
00:24:32,559 --> 00:24:33,639
他就把label

909
00:24:34,319 --> 00:24:35,639
就是说把所谓的label

910
00:24:35,639 --> 00:24:36,439
那一些东西

911
00:24:36,720 --> 00:24:38,279
那些长的

912
00:24:38,279 --> 00:24:38,919
就n-grand

913
00:24:38,919 --> 00:24:39,799
label里面的n-grand

914
00:24:39,799 --> 00:24:41,359
全部放在一个dictionary里面

915
00:24:42,680 --> 00:24:44,599
每一次我就去

916
00:24:45,799 --> 00:24:47,480
那么我就去看有多少match

917
00:24:47,480 --> 00:24:47,919
对吧

918
00:24:48,200 --> 00:24:49,440
我的预测里面

919
00:24:49,440 --> 00:24:50,119
就预测里面

920
00:24:50,119 --> 00:24:52,039
把每一个他名的n-grand拿出来

921
00:24:52,039 --> 00:24:52,799
去查一下

922
00:24:52,799 --> 00:24:55,200
你label里面有没有

923
00:24:55,919 --> 00:24:56,919
如果有的话

924
00:24:57,079 --> 00:24:58,359
我就预测加1

925
00:24:58,359 --> 00:24:59,799
但是把他的东西减掉1

926
00:24:59,799 --> 00:25:00,279
对吧

927
00:25:00,599 --> 00:25:01,799
就是说你看number

928
00:25:01,799 --> 00:25:03,200
就是说他要加

929
00:25:03,200 --> 00:25:03,839
就是说match

930
00:25:03,839 --> 00:25:05,119
如果发现有

931
00:25:05,119 --> 00:25:06,680
那就是加等于1

932
00:25:06,680 --> 00:25:08,200
然后他要就是说

933
00:25:08,200 --> 00:25:09,159
label里面的东西

934
00:25:09,279 --> 00:25:10,200
要减掉1个

935
00:25:10,560 --> 00:25:10,680
对

936
00:25:10,680 --> 00:25:11,160
不然的话

937
00:25:11,160 --> 00:25:13,120
我就一个AB这一个

938
00:25:13,160 --> 00:25:14,640
n-grand

939
00:25:14,640 --> 00:25:16,400
然后你给我

940
00:25:16,720 --> 00:25:18,160
预测了很多ABABAB

941
00:25:18,160 --> 00:25:18,800
那就不对了

942
00:25:18,800 --> 00:25:19,040
对吧

943
00:25:19,040 --> 00:25:19,920
所以要减掉

944
00:25:19,960 --> 00:25:21,200
最后就是把他

945
00:25:21,240 --> 00:25:22,480
这东西应该看到是

946
00:25:22,519 --> 00:25:23,680
number of match

947
00:25:23,680 --> 00:25:25,360
和你n-grand的长度

948
00:25:25,720 --> 00:25:26,680
取一个power

949
00:25:26,680 --> 00:25:27,880
然后乘一下

950
00:25:27,880 --> 00:25:29,600
就是最后就返回那个score

951
00:25:30,600 --> 00:25:31,880
这具体细节

952
00:25:31,960 --> 00:25:32,720
大家可以去看一下

953
00:25:32,720 --> 00:25:33,920
就是说基本上就是

954
00:25:34,279 --> 00:25:35,240
这样一个逻辑

955
00:25:35,840 --> 00:25:36,560
好

956
00:25:36,720 --> 00:25:37,960
然后最后的最后

957
00:25:37,960 --> 00:25:38,759
我们就可以

958
00:25:38,759 --> 00:25:40,000
我们刚刚不是训练一个模型

959
00:25:40,359 --> 00:25:41,960
然后大家可以看一下

960
00:25:42,119 --> 00:25:43,839
就是翻译的是这么做

961
00:25:44,000 --> 00:25:44,960
就是我要

962
00:25:45,119 --> 00:25:46,200
这是我的英语句子

963
00:25:47,000 --> 00:25:48,559
这是我的法语的句子

964
00:25:48,559 --> 00:25:49,319
我去翻译一下

965
00:25:49,480 --> 00:25:50,079
就翻译一下

966
00:25:50,079 --> 00:25:50,799
就是说

967
00:25:52,200 --> 00:25:53,839
我们翻译的是

968
00:25:54,759 --> 00:25:55,720
我们会打一下

969
00:25:55,799 --> 00:25:57,000
这是英语的句子

970
00:25:57,000 --> 00:25:58,799
和我们翻译出来的句子

971
00:25:58,799 --> 00:26:00,039
和他的blue

972
00:26:00,039 --> 00:26:00,960
适得于己

973
00:26:02,599 --> 00:26:04,000
这基本上看到是说

974
00:26:05,200 --> 00:26:06,440
我们做的不行

975
00:26:06,640 --> 00:26:07,400
这一次

976
00:26:07,920 --> 00:26:08,560
就是

977
00:26:08,759 --> 00:26:10,120
go没有翻对

978
00:26:10,120 --> 00:26:11,640
就blue是等于0的

979
00:26:11,640 --> 00:26:13,000
就go应该是等于

980
00:26:16,120 --> 00:26:16,680
wa

981
00:26:16,680 --> 00:26:17,360
然后感叹号

982
00:26:17,400 --> 00:26:18,800
就是说基本上虽然

983
00:26:18,800 --> 00:26:19,160
就是说

984
00:26:19,160 --> 00:26:20,120
虽然你是翻对了

985
00:26:20,120 --> 00:26:21,120
wa和感叹号

986
00:26:21,519 --> 00:26:22,600
但中间两个没翻对

987
00:26:22,600 --> 00:26:23,560
而且你少很多

988
00:26:23,560 --> 00:26:25,400
最后blue score就算了算去

989
00:26:25,400 --> 00:26:26,280
就变成0了

990
00:26:26,280 --> 00:26:27,759
就是他就零点零零几

991
00:26:28,519 --> 00:26:29,200
然后这个东西

992
00:26:29,360 --> 00:26:30,160
就是你看到

993
00:26:30,280 --> 00:26:31,680
这是他的翻译

994
00:26:31,680 --> 00:26:32,680
这是翻译的

995
00:26:32,880 --> 00:26:34,360
然后这个是你的真实的

996
00:26:34,360 --> 00:26:35,720
真实的和

997
00:26:36,720 --> 00:26:37,960
让我

998
00:26:38,319 --> 00:26:40,880
主要是问号没有翻对

999
00:26:41,039 --> 00:26:41,720
就感叹号

1000
00:26:41,720 --> 00:26:42,279
就是说

1001
00:26:42,559 --> 00:26:44,039
你句号翻成了问号

1002
00:26:44,240 --> 00:26:45,720
所以就是损失掉一点

1003
00:26:46,120 --> 00:26:47,279
这个就是你另外翻的

1004
00:26:48,759 --> 00:26:50,160
就基本上看到还行

1005
00:26:50,319 --> 00:26:51,160
就一般

1006
00:26:51,160 --> 00:26:51,880
就是说

1007
00:26:52,000 --> 00:26:53,440
你可能法国人可能看一眼

1008
00:26:53,440 --> 00:26:54,120
就觉得

1009
00:26:54,120 --> 00:26:55,600
你看就目测这个句子

1010
00:26:55,600 --> 00:26:57,799
还跟目测这个句子

1011
00:26:57,799 --> 00:26:59,160
跟你的句子长得差不多

1012
00:26:59,319 --> 00:27:00,360
就词作差不多

1013
00:27:00,360 --> 00:27:01,000
但是

1014
00:27:01,920 --> 00:27:03,279
法国人可能也许能看懂

1015
00:27:03,360 --> 00:27:04,880
但是从机器角度来讲

1016
00:27:04,880 --> 00:27:07,280
他的blue score不是那么高

1017
00:27:07,920 --> 00:27:08,360
OK

1018
00:27:08,360 --> 00:27:10,080
这也是很多原因

1019
00:27:10,520 --> 00:27:12,600
一个是说我们数据真的很小

1020
00:27:14,200 --> 00:27:14,960
第二个是说

1021
00:27:14,960 --> 00:27:18,040
我们也是用的是最原始的方法

1022
00:27:18,440 --> 00:27:20,120
我们之后会给大家用

1023
00:27:20,120 --> 00:27:21,520
稍微复杂一点点

1024
00:27:21,520 --> 00:27:22,720
带有tension的

1025
00:27:23,200 --> 00:27:24,880
给大家再回过头来看

1026
00:27:24,880 --> 00:27:26,040
sequence to sequence

