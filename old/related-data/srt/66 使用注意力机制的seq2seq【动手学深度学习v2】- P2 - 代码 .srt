1
00:00:00,000 --> 00:00:03,160
我们来看一下加入Attention之后

2
00:00:03,160 --> 00:00:04,799
seek的实现怎么做

3
00:00:04,799 --> 00:00:06,600
我们之所以要做这个名字

4
00:00:06,600 --> 00:00:08,480
是因为这个人是那个一座的名字

5
00:00:08,480 --> 00:00:11,880
所以你说

6
00:00:11,880 --> 00:00:15,200
所以那个文章他也没给自己的paper取好名字

7
00:00:15,200 --> 00:00:17,359
所以我们就把他的名字放在这里面

8
00:00:17,359 --> 00:00:18,600
就跟AlexNet一样

9
00:00:18,600 --> 00:00:20,199
AlexNet其实不叫AlexNet

10
00:00:20,199 --> 00:00:22,559
他其实就叫说我提出了一个方法

11
00:00:22,559 --> 00:00:24,120
但是因为比较有名

12
00:00:24,120 --> 00:00:26,640
所以干脆就把他的名字放在里面叫AlexNet

13
00:00:26,640 --> 00:00:30,480
好我们来看一下就是说

14
00:00:30,480 --> 00:00:34,200
带有注意力机制的解码器的基本结合

15
00:00:34,200 --> 00:00:35,719
其实跟前面没什么区别

16
00:00:35,719 --> 00:00:36,920
唯一的区别是说

17
00:00:36,920 --> 00:00:40,719
加了一个tension weight这个函数

18
00:00:40,719 --> 00:00:41,520
是一个property

19
00:00:41,520 --> 00:00:44,240
然后就是这样子

20
00:00:44,240 --> 00:00:45,439
我们可以给你画东西

21
00:00:45,439 --> 00:00:47,320
就是说有没有都没关系

22
00:00:47,320 --> 00:00:48,920
主要是用来画东西用的

23
00:00:48,920 --> 00:00:51,600
好

24
00:00:51,600 --> 00:00:54,480
我们核心实现就这么一个函数

25
00:00:54,480 --> 00:00:58,240
啊

26
00:00:58,240 --> 00:00:59,720
核心实现就在这个地方

27
00:00:59,720 --> 00:01:02,080
所以别的东西其实跟前面没什么区别

28
00:01:02,080 --> 00:01:06,200
我们今天的最后的任务就是主要给讲一下

29
00:01:06,200 --> 00:01:08,400
讲一下这个函数是怎么回事

30
00:01:08,400 --> 00:01:10,000
OK

31
00:01:10,000 --> 00:01:13,280
首先我们来仔细看一下这个函数

32
00:01:13,280 --> 00:01:16,520
首先我们的编码其实没变的

33
00:01:16,520 --> 00:01:16,800
对吧

34
00:01:16,800 --> 00:01:17,840
第一

35
00:01:17,840 --> 00:01:19,000
encoder是不变的

36
00:01:19,000 --> 00:01:21,359
因为tension只作用在decoder上面

37
00:01:21,359 --> 00:01:23,599
所以encoder我们直接用seq

38
00:01:23,599 --> 00:01:24,920
seq的encoder

39
00:01:24,920 --> 00:01:26,840
但是decoder我们改成了seq

40
00:01:26,840 --> 00:01:30,480
to seq的tension decoder

41
00:01:30,480 --> 00:01:33,159
然后这些东西都没变

42
00:01:33,159 --> 00:01:34,200
就我cap size

43
00:01:34,200 --> 00:01:34,959
embedding size

44
00:01:34,959 --> 00:01:36,000
number of hints

45
00:01:36,000 --> 00:01:36,920
number of layers

46
00:01:36,920 --> 00:01:38,760
这种东西dropout都没变

47
00:01:38,760 --> 00:01:40,879
就跟之前是一样的

48
00:01:40,879 --> 00:01:44,000
模型下面的三个是没变的

49
00:01:44,000 --> 00:01:45,560
就是我一个embedding层

50
00:01:45,560 --> 00:01:47,079
有一个rn层

51
00:01:47,079 --> 00:01:48,519
然后这个东西都长一样

52
00:01:48,640 --> 00:01:50,480
就是都是一个embedding size

53
00:01:50,480 --> 00:01:52,160
加上一个number of hints

54
00:01:52,160 --> 00:01:54,079
就是说这三行

55
00:01:54,079 --> 00:01:54,800
大家回一下

56
00:01:54,800 --> 00:01:56,159
我们讲seq to seq里面

57
00:01:56,439 --> 00:01:58,240
这三行是没有变的

58
00:01:58,240 --> 00:01:59,359
一句话都没有变

59
00:01:59,400 --> 00:02:00,920
唯一的不一样是说

60
00:02:00,920 --> 00:02:02,520
加了一个tension在里面

61
00:02:03,480 --> 00:02:06,920
我们就是加性的tension

62
00:02:07,280 --> 00:02:11,840
然后我们虽然我们在的key

63
00:02:11,840 --> 00:02:12,560
value

64
00:02:12,560 --> 00:02:13,159
query

65
00:02:13,319 --> 00:02:15,319
长度都是number of hints

66
00:02:15,319 --> 00:02:16,400
都是h

67
00:02:16,960 --> 00:02:20,000
我们这可以用点程的tension

68
00:02:20,080 --> 00:02:22,439
但我们这里用的是加性的tension

69
00:02:22,439 --> 00:02:24,759
是因为他可以学参数

70
00:02:24,759 --> 00:02:27,599
所以相对来说效果通常会好一点

71
00:02:27,800 --> 00:02:30,240
就算他都是长一样的话

72
00:02:30,240 --> 00:02:31,319
我们也是用了它

73
00:02:32,240 --> 00:02:32,520
OK

74
00:02:32,520 --> 00:02:33,039
所以用的是

75
00:02:33,039 --> 00:02:35,680
所以这三个都是number of hints

76
00:02:35,680 --> 00:02:37,360
然后dropout对吧

77
00:02:37,360 --> 00:02:38,759
这还有一个dropout的东西在里面

78
00:02:40,439 --> 00:02:40,840
OK

79
00:02:40,840 --> 00:02:42,680
这就是我们的那个东西

80
00:02:43,039 --> 00:02:46,039
另外init state也应该是没变了

81
00:02:46,039 --> 00:02:47,319
init state多了一个东西

82
00:02:47,400 --> 00:02:51,079
叫做encoder valid length

83
00:02:53,000 --> 00:02:55,159
就encoder valid length要什么东西

84
00:02:55,280 --> 00:02:56,519
就是说你得告诉我

85
00:02:56,519 --> 00:02:59,039
说你的英语句子里面

86
00:02:59,039 --> 00:02:59,479
每一句

87
00:02:59,479 --> 00:03:01,599
你到底哪些东西是pad的

88
00:03:02,039 --> 00:03:03,280
之前我们是不要的

89
00:03:03,280 --> 00:03:05,000
这个地方我们是需要的

90
00:03:05,359 --> 00:03:06,439
我们等会再来讲

91
00:03:06,599 --> 00:03:07,519
为什么需要它

92
00:03:08,759 --> 00:03:09,680
这种东西都没变

93
00:03:09,759 --> 00:03:10,519
就是把output的

94
00:03:10,519 --> 00:03:12,799
就是把你的batch size放到第一轮的

95
00:03:12,799 --> 00:03:13,799
放到第一个维度

96
00:03:13,799 --> 00:03:15,879
然后中间维度又变成了

97
00:03:15,879 --> 00:03:17,079
你那个sequence length

98
00:03:17,080 --> 00:03:17,880
就句子长度

99
00:03:17,880 --> 00:03:19,640
最后一个是你的h

100
00:03:20,040 --> 00:03:20,760
hint state

101
00:03:20,760 --> 00:03:22,080
然后我还要把你的

102
00:03:22,080 --> 00:03:24,080
valid length也传一传

103
00:03:24,080 --> 00:03:24,600
也传过来

104
00:03:24,600 --> 00:03:25,480
也放到state里面

105
00:03:25,720 --> 00:03:26,240
OK

106
00:03:26,439 --> 00:03:28,400
就这句话就到这句话展开

107
00:03:28,960 --> 00:03:30,400
然后这个没变

108
00:03:30,400 --> 00:03:30,760
对吧

109
00:03:30,760 --> 00:03:31,439
x进来

110
00:03:31,439 --> 00:03:32,439
formal embedding

111
00:03:32,880 --> 00:03:35,040
然后就把你那个时间维度

112
00:03:35,040 --> 00:03:37,400
就是number of steps放回第一位

113
00:03:37,600 --> 00:03:39,280
就我们的换来换去换个不停

114
00:03:40,280 --> 00:03:40,680
好

115
00:03:40,680 --> 00:03:41,760
接下来就是

116
00:03:43,960 --> 00:03:45,640
接下来跟前面不一样的是

117
00:03:45,640 --> 00:03:46,280
接下来这个东西

118
00:03:46,360 --> 00:03:48,640
有点像我们在做seq to seq的

119
00:03:48,640 --> 00:03:49,719
prediction里面

120
00:03:49,879 --> 00:03:51,240
就要一步一步来

121
00:03:51,759 --> 00:03:54,000
我们之前就是直接一句话就过去了

122
00:03:54,000 --> 00:03:54,280
对吧

123
00:03:54,280 --> 00:03:55,159
因为反正

124
00:03:55,680 --> 00:03:57,800
交给RN直接给你迭代了

125
00:03:57,800 --> 00:03:58,520
现在这里不行

126
00:03:58,520 --> 00:03:59,920
因为每一步你

127
00:04:00,400 --> 00:04:03,240
因为你每一步你context东西会变

128
00:04:03,280 --> 00:04:05,080
所以我们需要真的是

129
00:04:06,199 --> 00:04:07,000
一个一个来

130
00:04:07,000 --> 00:04:07,800
就是

131
00:04:08,840 --> 00:04:11,680
对于decoder的输入

132
00:04:12,280 --> 00:04:14,159
的每一个x

133
00:04:14,159 --> 00:04:16,519
query

134
00:04:16,519 --> 00:04:18,959
首先把hidden state

135
00:04:20,719 --> 00:04:22,360
就是hidden state

136
00:04:22,360 --> 00:04:24,360
就是这个东西是什么东西

137
00:04:24,480 --> 00:04:26,480
hidden state就是上一个

138
00:04:27,159 --> 00:04:29,240
时间的

139
00:04:29,560 --> 00:04:31,079
RN的输出

140
00:04:31,079 --> 00:04:31,800
就是-1

141
00:04:32,040 --> 00:04:34,040
-1就是最后一层的输出

142
00:04:35,319 --> 00:04:37,120
一开始当然是来自于上一次

143
00:04:37,319 --> 00:04:39,480
上一次来自于encoder

144
00:04:39,480 --> 00:04:40,759
现在当然是decoder

145
00:04:40,759 --> 00:04:42,120
就是说你每次我们会

146
00:04:42,120 --> 00:04:43,480
当然这个东西会变会更新

147
00:04:44,240 --> 00:04:46,720
就是说上一个时刻的

148
00:04:46,720 --> 00:04:50,040
最后一层的RN的输出

149
00:04:50,480 --> 00:04:52,240
然后加一个dimension

150
00:04:52,240 --> 00:04:52,800
意味什么

151
00:04:53,000 --> 00:04:53,720
是因为它

152
00:04:54,200 --> 00:04:55,400
我要加一个number of

153
00:04:55,400 --> 00:04:56,600
query这个维度进去

154
00:04:56,600 --> 00:04:57,760
虽然这query只有一个

155
00:04:57,760 --> 00:04:59,280
但是我把这个维度给加进去

156
00:04:59,280 --> 00:04:59,560
对吧

157
00:04:59,560 --> 00:05:00,760
所以要加一个

158
00:05:00,800 --> 00:05:02,280
在1那里加进去

159
00:05:05,960 --> 00:05:07,000
context

160
00:05:07,680 --> 00:05:09,040
context就唯一的

161
00:05:09,040 --> 00:05:09,560
其实看一下

162
00:05:09,560 --> 00:05:10,360
就在这个地方

163
00:05:11,600 --> 00:05:13,560
query就是上一个时刻的

164
00:05:13,560 --> 00:05:14,720
最后一层的输出

165
00:05:15,800 --> 00:05:16,600
T是什么

166
00:05:16,600 --> 00:05:18,519
T就是encoder的output

167
00:05:18,519 --> 00:05:19,480
encoder的output是什么

168
00:05:19,480 --> 00:05:20,519
就是常为

169
00:05:23,560 --> 00:05:25,160
第一个维度是batch size

170
00:05:25,319 --> 00:05:27,800
第二个维度是你的时间步

171
00:05:27,840 --> 00:05:29,439
第三个维度是h

172
00:05:30,079 --> 00:05:30,560
对吧

173
00:05:31,920 --> 00:05:33,720
一样的就是batch size

174
00:05:34,120 --> 00:05:35,240
多少个

175
00:05:36,120 --> 00:05:37,319
原句子有多长

176
00:05:37,480 --> 00:05:38,879
就是你多少个key value pair

177
00:05:38,879 --> 00:05:40,319
然后最后你是h

178
00:05:40,319 --> 00:05:45,079
然后我们这里要把encoder的

179
00:05:45,079 --> 00:05:46,000
value的num

180
00:05:46,000 --> 00:05:46,680
放进去

181
00:05:47,079 --> 00:05:47,959
这是干嘛

182
00:05:48,199 --> 00:05:50,480
这东西是一个常为batch size的

183
00:05:50,480 --> 00:05:51,439
一个向量

184
00:05:51,639 --> 00:05:54,159
其中的第一个元素

185
00:05:54,159 --> 00:05:54,680
表示

186
00:05:54,680 --> 00:05:56,079
第一个样本里面

187
00:05:56,199 --> 00:05:57,319
在第二个原句子

188
00:05:57,319 --> 00:05:58,600
就英文句子里面

189
00:05:58,639 --> 00:06:00,319
它的原始长度是几

190
00:06:00,319 --> 00:06:02,039
是4还是5还是6

191
00:06:02,279 --> 00:06:02,959
不是

192
00:06:02,959 --> 00:06:03,360
就是说

193
00:06:03,360 --> 00:06:05,000
因为它已经怪全是10

194
00:06:05,199 --> 00:06:06,399
记得我们用的是10

195
00:06:06,399 --> 00:06:07,480
派得到10了

196
00:06:07,879 --> 00:06:09,439
所以它的意思就是说

197
00:06:09,719 --> 00:06:12,000
我在看第一个样本

198
00:06:12,000 --> 00:06:13,480
就这里都是有小皮量的

199
00:06:13,600 --> 00:06:15,279
所以query它不是一个是

200
00:06:15,279 --> 00:06:17,279
是有一个query

201
00:06:17,279 --> 00:06:18,319
就是有一个样本

202
00:06:19,399 --> 00:06:20,319
第一个样本

203
00:06:20,319 --> 00:06:22,000
就是第一个句子里面

204
00:06:22,759 --> 00:06:24,160
的query的时候

205
00:06:24,199 --> 00:06:26,519
我不应该去看

206
00:06:26,519 --> 00:06:28,879
它对应的原句子里面

207
00:06:28,879 --> 00:06:30,480
所有的RNN的输出

208
00:06:30,480 --> 00:06:31,199
是因为

209
00:06:31,240 --> 00:06:32,159
最后那些东西

210
00:06:32,159 --> 00:06:34,159
很有可能是被派的掉的

211
00:06:34,399 --> 00:06:35,800
所以我告诉你说

212
00:06:35,800 --> 00:06:36,480
OK

213
00:06:36,480 --> 00:06:37,560
我知道这里面

214
00:06:38,000 --> 00:06:39,560
只有前4个

215
00:06:39,600 --> 00:06:42,120
那些东西是真正的有意义的

216
00:06:42,120 --> 00:06:44,079
后面都是派的那些符号

217
00:06:44,079 --> 00:06:45,079
所以我告诉你说

218
00:06:45,079 --> 00:06:45,399
告诉你

219
00:06:45,600 --> 00:06:47,040
只看前4个

220
00:06:47,319 --> 00:06:49,600
只去对前4个key value pair

221
00:06:49,600 --> 00:06:50,199
算

222
00:06:50,240 --> 00:06:51,519
tension weight

223
00:06:51,800 --> 00:06:52,959
也对它做加强平均

224
00:06:52,959 --> 00:06:54,439
后面的我就不管了

225
00:06:55,519 --> 00:06:57,040
这个也这个make sense

226
00:06:57,040 --> 00:06:57,560
对吧

227
00:06:57,600 --> 00:07:00,040
因为我去看的时候

228
00:07:00,040 --> 00:07:02,199
我算how low word

229
00:07:02,639 --> 00:07:04,720
都句号三个词

230
00:07:04,879 --> 00:07:06,000
如果但是我训练时候

231
00:07:06,000 --> 00:07:06,920
我可能是成为10

232
00:07:06,920 --> 00:07:08,640
因为后面我还派的了7个

233
00:07:08,640 --> 00:07:10,199
当然还有个句号的句子结尾

234
00:07:10,280 --> 00:07:11,080
句子结尾就算上了

235
00:07:11,080 --> 00:07:11,720
也就4个词

236
00:07:12,439 --> 00:07:14,800
最后有6个派的符号

237
00:07:14,840 --> 00:07:16,120
那么我要去

238
00:07:16,400 --> 00:07:18,400
算我的tension的时候

239
00:07:18,400 --> 00:07:20,640
我应该只看前4个词

240
00:07:20,800 --> 00:07:22,120
后面那些词我都不要

241
00:07:22,120 --> 00:07:22,600
对吧

242
00:07:23,400 --> 00:07:24,800
这就是value length

243
00:07:24,800 --> 00:07:25,520
这种事情

244
00:07:25,560 --> 00:07:26,480
所以为什么我要把

245
00:07:26,480 --> 00:07:27,720
encoded weight length

246
00:07:27,720 --> 00:07:28,720
传进来的原因

247
00:07:28,720 --> 00:07:29,480
就在这个地方

248
00:07:30,080 --> 00:07:31,280
RNN是不用管

249
00:07:31,280 --> 00:07:33,720
RNN反正这东西你也可以管

250
00:07:33,720 --> 00:07:35,200
就是说当然你也可以管

251
00:07:35,200 --> 00:07:36,560
但实际上RNN反正就是

252
00:07:36,560 --> 00:07:37,280
觉得无所谓了

253
00:07:37,280 --> 00:07:40,000
就是就直接拿最后一个没关系

254
00:07:41,319 --> 00:07:41,839
OK

255
00:07:41,839 --> 00:07:43,040
所以后面就是差不多了

256
00:07:43,240 --> 00:07:43,879
就是这样子

257
00:07:43,879 --> 00:07:46,000
我们就知道context是怎么来的

258
00:07:46,199 --> 00:07:51,920
然后就跟你x做并起来

259
00:07:52,040 --> 00:07:53,000
并起来就是说我们

260
00:07:53,000 --> 00:07:54,560
因为context里面

261
00:07:54,560 --> 00:07:56,399
多了一个常规1的一个维度

262
00:07:56,519 --> 00:07:57,480
就是key的维度

263
00:07:57,480 --> 00:07:59,199
所以他也加了一个维度进来

264
00:07:59,360 --> 00:08:00,360
然后在最后一个维度

265
00:08:00,360 --> 00:08:01,319
concate起来

266
00:08:02,120 --> 00:08:04,079
然后把它做RNN进去

267
00:08:04,279 --> 00:08:05,040
别的都一样了

268
00:08:05,079 --> 00:08:06,120
别的都是一样的

269
00:08:06,879 --> 00:08:09,680
然后我们当然把attention的weight

270
00:08:09,680 --> 00:08:10,560
给你存一存

271
00:08:11,160 --> 00:08:12,240
最后我们就输出了

272
00:08:12,280 --> 00:08:13,199
这个东西都是一样的

273
00:08:13,199 --> 00:08:15,000
我们就不用再去讲了

274
00:08:15,160 --> 00:08:17,120
所以核心的核心就是

275
00:08:17,360 --> 00:08:18,560
context怎么算

276
00:08:18,759 --> 00:08:20,759
上一次seq里面

277
00:08:20,759 --> 00:08:25,680
我们就直接把encoded的

278
00:08:26,079 --> 00:08:26,759
state里面

279
00:08:26,759 --> 00:08:27,959
那个东西拿出来就行了

280
00:08:28,199 --> 00:08:29,720
这里是每一步

281
00:08:29,759 --> 00:08:31,680
他根据我们上一步的query

282
00:08:31,959 --> 00:08:33,320
就query是在不断变的

283
00:08:33,320 --> 00:08:34,720
就是key value是不变的

284
00:08:34,720 --> 00:08:35,800
key value就是你的

285
00:08:36,080 --> 00:08:37,399
原句子每一个词

286
00:08:37,399 --> 00:08:38,360
它对应的输出

287
00:08:38,720 --> 00:08:40,840
但是query会每次会变

288
00:08:40,840 --> 00:08:43,160
所以我们要重新算到context进去

289
00:08:44,240 --> 00:08:44,800
OK

290
00:08:46,040 --> 00:08:47,759
所以最后就是这个东西

291
00:08:47,759 --> 00:08:48,720
就返回我的weight

292
00:08:49,320 --> 00:08:50,960
我们主要是给画图用的

293
00:08:51,200 --> 00:08:52,639
所以主要区别在这个地方

294
00:08:53,080 --> 00:08:54,000
训练我们就

295
00:08:54,440 --> 00:08:55,960
我就不给大家讲了

296
00:08:56,000 --> 00:08:56,560
就是这个东西

297
00:08:56,560 --> 00:08:57,519
因为改的是里面

298
00:08:57,680 --> 00:08:58,759
就外面没有改

299
00:08:59,040 --> 00:09:00,240
就外面都是一样的

300
00:09:00,279 --> 00:09:02,759
所以你把这东西放进去

301
00:09:02,759 --> 00:09:03,800
放回来都长一样

302
00:09:04,479 --> 00:09:06,240
就output什么state

303
00:09:06,479 --> 00:09:07,759
state0这个东西

304
00:09:08,199 --> 00:09:09,479
就是跟我们昨天

305
00:09:09,479 --> 00:09:10,919
应该是没有本质区别的

306
00:09:10,959 --> 00:09:12,759
而且这个东西训练起来

307
00:09:13,839 --> 00:09:15,639
其实是区别不大

308
00:09:15,959 --> 00:09:16,599
但看一看

309
00:09:18,359 --> 00:09:20,479
应该是我记得参数都是一样的

310
00:09:20,679 --> 00:09:21,959
就是这个跟我们做

311
00:09:22,319 --> 00:09:23,079
前面的seq

312
00:09:23,079 --> 00:09:24,439
就seq的参数是一样

313
00:09:24,559 --> 00:09:25,759
但是我们的

314
00:09:25,759 --> 00:09:26,159
记不记得

315
00:09:26,159 --> 00:09:27,719
我们之前可以训练到

316
00:09:28,159 --> 00:09:29,719
1万个token每秒

317
00:09:30,799 --> 00:09:32,159
现在变成了5000个token

318
00:09:33,159 --> 00:09:34,039
就是tension

319
00:09:34,039 --> 00:09:35,399
我们还是比较贵的

320
00:09:36,439 --> 00:09:38,199
当然是我们实现的不优

321
00:09:38,399 --> 00:09:40,159
我们里面各种转来转去

322
00:09:40,319 --> 00:09:41,799
然后各种小矩阵成法

323
00:09:41,799 --> 00:09:42,559
使得我们实现

324
00:09:42,559 --> 00:09:43,600
确实效率不高

325
00:09:43,719 --> 00:09:44,480
但是可以看到

326
00:09:44,480 --> 00:09:46,279
就是我们的性能降了一倍

327
00:09:47,000 --> 00:09:47,559
OK

328
00:09:48,879 --> 00:09:49,679
所以这个训练

329
00:09:49,679 --> 00:09:52,000
但训练从本质上角度来讲

330
00:09:52,000 --> 00:09:53,240
跟我们之前的

331
00:09:53,240 --> 00:09:54,600
include decode的模型没区别

332
00:09:54,600 --> 00:09:57,559
因为只是在传递信息的时候

333
00:09:57,559 --> 00:09:58,600
用了一个tension

334
00:09:58,639 --> 00:09:59,720
但从外面角度来看

335
00:09:59,720 --> 00:10:00,879
那个模型没有变

336
00:10:01,039 --> 00:10:01,519
OK

337
00:10:03,159 --> 00:10:04,919
然后所以我们代码

338
00:10:04,919 --> 00:10:06,000
就不给大家过了

339
00:10:06,559 --> 00:10:06,959
好

340
00:10:06,959 --> 00:10:07,519
最后看一下

341
00:10:07,519 --> 00:10:08,639
就是说

342
00:10:09,399 --> 00:10:11,120
就是说还是翻那么几个句子

343
00:10:11,120 --> 00:10:11,959
看一下效果

344
00:10:14,480 --> 00:10:15,959
首先大家记不记得

345
00:10:15,959 --> 00:10:16,839
我们昨天的翻译

346
00:10:16,919 --> 00:10:18,360
昨天翻译是说

347
00:10:23,000 --> 00:10:23,879
就是说

348
00:10:24,000 --> 00:10:24,959
我

349
00:10:26,839 --> 00:10:27,959
觉得

350
00:10:29,679 --> 00:10:29,919
对

351
00:10:30,240 --> 00:10:30,719
就是说

352
00:10:30,719 --> 00:10:32,159
昨天我们的blue score

353
00:10:32,279 --> 00:10:32,879
就是说

354
00:10:32,879 --> 00:10:33,919
就是比较低

355
00:10:34,039 --> 00:10:34,839
就0.2

356
00:10:34,879 --> 00:10:35,399
0.3

357
00:10:35,399 --> 00:10:35,959
0.4

358
00:10:36,159 --> 00:10:38,319
现在我们基本上就挺高了

359
00:10:38,439 --> 00:10:39,079
对吧

360
00:10:39,199 --> 00:10:40,639
你看我们的现在的翻译

361
00:10:40,639 --> 00:10:42,279
就基本上都是变成了1.0了

362
00:10:42,279 --> 00:10:42,959
1.0了

363
00:10:42,959 --> 00:10:43,479
1.0了

364
00:10:43,479 --> 00:10:45,000
0.658了

365
00:10:46,399 --> 00:10:47,279
就当然这个东西

366
00:10:47,279 --> 00:10:48,319
你不能说明太多问题

367
00:10:48,439 --> 00:10:49,399
这个东西你得

368
00:10:49,399 --> 00:10:50,439
我们没有用一个

369
00:10:50,439 --> 00:10:52,240
真正的一个validation的set

370
00:10:52,240 --> 00:10:52,879
去验证

371
00:10:53,000 --> 00:10:53,719
所以我们就是

372
00:10:53,719 --> 00:10:54,919
实际就拿这个东西

373
00:10:54,919 --> 00:10:56,120
就眼睛看一看

374
00:10:56,559 --> 00:10:57,879
但整体来讲很有可能

375
00:10:57,879 --> 00:11:00,279
因为我们模型的复杂度

376
00:11:00,279 --> 00:11:00,639
增加了

377
00:11:00,639 --> 00:11:01,080
对吧

378
00:11:01,120 --> 00:11:04,039
所以我们可能记住的东西

379
00:11:04,039 --> 00:11:05,399
记的东西更多了

380
00:11:05,559 --> 00:11:06,240
所以导致说

381
00:11:06,240 --> 00:11:08,159
你看上去好像效果更好了

382
00:11:08,320 --> 00:11:10,840
但实际上在实际中

383
00:11:11,399 --> 00:11:14,120
还是会有效果的明显的增加

384
00:11:15,600 --> 00:11:16,840
当然这个东西

385
00:11:16,840 --> 00:11:19,399
我们这里不能反映真实的问题

386
00:11:19,480 --> 00:11:21,519
真实大家有很多研究表明

387
00:11:21,639 --> 00:11:24,080
所以大家加这东西还是有用的

388
00:11:24,519 --> 00:11:26,159
从直觉上来也说

389
00:11:26,159 --> 00:11:26,679
对吧

390
00:11:26,680 --> 00:11:27,720
你现在可以

391
00:11:27,960 --> 00:11:29,840
之前我只能看最后一个状态的输出

392
00:11:29,840 --> 00:11:30,480
作为context

393
00:11:30,480 --> 00:11:31,440
现在我能看到

394
00:11:31,440 --> 00:11:32,520
往回历史上

395
00:11:32,520 --> 00:11:33,640
随便找谁都行

396
00:11:34,600 --> 00:11:35,200
OK

397
00:11:35,800 --> 00:11:37,200
另外一个是说

398
00:11:37,480 --> 00:11:38,600
我们来看一下

399
00:11:38,600 --> 00:11:39,840
就是说我们其中看一下

400
00:11:40,200 --> 00:11:40,960
翻译的时候

401
00:11:41,120 --> 00:11:41,920
做预测的时候

402
00:11:42,920 --> 00:11:43,680
Attention weight

403
00:11:43,680 --> 00:11:44,240
是什么样子

404
00:11:44,960 --> 00:11:47,040
就基本上就是说

405
00:11:48,160 --> 00:11:49,360
这个是你的query

406
00:11:50,080 --> 00:11:51,720
就query我在翻译的时候

407
00:11:52,240 --> 00:11:53,160
就是说

408
00:11:53,240 --> 00:11:55,760
在翻译每一个词的时候

409
00:11:55,799 --> 00:11:58,039
我去对应的原句子里面

410
00:11:58,039 --> 00:11:58,600
就是key

411
00:11:58,600 --> 00:12:00,360
就是原句子1234

412
00:12:01,279 --> 00:12:02,240
可能就是Hello world

413
00:12:02,439 --> 00:12:03,759
可能就是说

414
00:12:04,039 --> 00:12:05,200
那么对应的

415
00:12:05,519 --> 00:12:06,399
它

416
00:12:06,919 --> 00:12:08,399
在每翻每个词的时候

417
00:12:08,399 --> 00:12:10,080
对应它的权重发生的变化

418
00:12:10,080 --> 00:12:11,200
可以看到还是

419
00:12:11,200 --> 00:12:12,639
虽然不能说明什么问题

420
00:12:12,720 --> 00:12:13,919
我觉得这个东西

421
00:12:14,159 --> 00:12:15,240
你得通过

422
00:12:15,679 --> 00:12:17,399
你要句子比较长一点

423
00:12:17,399 --> 00:12:17,919
效果好一点

424
00:12:17,919 --> 00:12:19,279
我们的短句子还效果

425
00:12:19,279 --> 00:12:20,439
还真不一定看得出来

426
00:12:20,439 --> 00:12:21,679
如果你翻比较长的句子

427
00:12:21,679 --> 00:12:22,720
你还是能看到

428
00:12:22,919 --> 00:12:24,279
比较好的效果

429
00:12:24,360 --> 00:12:25,679
但这里基本上可以看到

430
00:12:25,679 --> 00:12:26,319
是说

431
00:12:26,519 --> 00:12:27,559
翻译0的时候

432
00:12:27,720 --> 00:12:28,679
就反正你也不知道

433
00:12:28,679 --> 00:12:28,959
对吧

434
00:12:28,959 --> 00:12:29,839
多看一看

435
00:12:29,879 --> 00:12:31,720
然后翻译1的时候

436
00:12:31,720 --> 00:12:32,839
就看得比较厚

437
00:12:32,839 --> 00:12:35,240
然后翻译3的时候

438
00:12:35,360 --> 00:12:36,120
就看得比较浅

439
00:12:36,120 --> 00:12:37,039
反正就是说

440
00:12:37,279 --> 00:12:38,879
基本上在变化

441
00:12:38,919 --> 00:12:41,799
就只能说它有在发生变化

442
00:12:42,519 --> 00:12:43,720
就还不是说

443
00:12:43,720 --> 00:12:44,839
在seq2seq的时候

444
00:12:44,839 --> 00:12:45,600
你可认为

445
00:12:45,959 --> 00:12:48,399
它等价于seq2seq

446
00:12:48,399 --> 00:12:49,079
等价是说

447
00:12:49,079 --> 00:12:51,600
每一次把最后一列设置

448
00:12:51,600 --> 00:12:52,720
前面全部设置在0

449
00:12:52,920 --> 00:12:55,080
我们之前实现了seq2seq

450
00:12:55,200 --> 00:12:57,960
但现在说我们确实去

451
00:12:58,000 --> 00:13:00,240
根据每一次的翻译

452
00:13:00,240 --> 00:13:01,279
就每一次预测的

453
00:13:02,720 --> 00:13:03,840
target的句子词

454
00:13:04,200 --> 00:13:06,080
我们去看源据的不同的地方

455
00:13:06,080 --> 00:13:08,399
然后对它的东西做加权比均

456
00:13:08,800 --> 00:13:09,200
OK

457
00:13:09,200 --> 00:13:11,840
所以这就是注意力机制

458
00:13:11,879 --> 00:13:15,080
给seq2seq带来的变化

