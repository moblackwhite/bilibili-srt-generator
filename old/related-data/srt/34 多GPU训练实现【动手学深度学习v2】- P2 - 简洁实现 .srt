1
00:00:00,000 --> 00:00:00,920
好

2
00:00:00,920 --> 00:00:01,919
接下来我们来讲一下

3
00:00:02,080 --> 00:00:02,960
就是说

4
00:00:03,439 --> 00:00:04,839
我们之前提到两个问题

5
00:00:04,839 --> 00:00:06,960
一个是我们手写的

6
00:00:07,679 --> 00:00:09,080
不是很优

7
00:00:09,080 --> 00:00:13,200
第二个是我们的神经网络太小了

8
00:00:13,280 --> 00:00:15,240
所以我们在接下来这一个里面

9
00:00:15,240 --> 00:00:17,080
去尝试去改善这两个东西

10
00:00:17,080 --> 00:00:19,519
来看一下多GPU的性能怎么样

11
00:00:24,120 --> 00:00:24,440
OK

12
00:00:24,440 --> 00:00:25,879
我们之前要的import一些东西

13
00:00:27,519 --> 00:00:29,719
然后我们这里用一个resnet18

14
00:00:30,359 --> 00:00:30,960
就是说

15
00:00:30,960 --> 00:00:32,560
Nullet太小的话

16
00:00:32,640 --> 00:00:34,240
我们就用的基本上就是

17
00:00:34,240 --> 00:00:36,840
我们在之前讲resnet的时候

18
00:00:37,119 --> 00:00:37,960
用到那个模型

19
00:00:38,120 --> 00:00:40,719
我们说这是稍微修改的模型

20
00:00:40,719 --> 00:00:41,399
就是说

21
00:00:41,719 --> 00:00:44,000
跟我们的论文里面的有一点点不一样

22
00:00:44,240 --> 00:00:45,000
但是这个不一样

23
00:00:45,000 --> 00:00:45,880
大家可以忽略掉

24
00:00:46,600 --> 00:00:47,560
主要是说

25
00:00:47,879 --> 00:00:50,200
这是一个比Nullet慢个

26
00:00:50,799 --> 00:00:52,079
计算量大个

27
00:00:52,879 --> 00:00:53,719
50倍的样子

28
00:00:57,000 --> 00:00:57,280
好

29
00:00:57,280 --> 00:00:59,200
我们就跳过这个模型是定义

30
00:00:59,200 --> 00:01:00,160
是什么样定义的

31
00:01:00,600 --> 00:01:02,079
我们直接来看接下来

32
00:01:03,280 --> 00:01:04,159
就训练的时候

33
00:01:04,159 --> 00:01:06,200
我们现在用的想用的是

34
00:01:06,200 --> 00:01:07,920
PyTorch自己的

35
00:01:08,359 --> 00:01:09,560
PyAllet的高层的

36
00:01:11,280 --> 00:01:12,240
API是怎么用的

37
00:01:12,640 --> 00:01:13,200
我们看一下

38
00:01:13,200 --> 00:01:15,439
我们直接说我们的训练函数

39
00:01:16,400 --> 00:01:18,840
所以我们的参数跟之前差不多

40
00:01:19,079 --> 00:01:20,359
给定我们的network

41
00:01:21,159 --> 00:01:23,640
告诉我们的GPU是有多少个GPU

42
00:01:23,960 --> 00:01:25,079
然后我们的batch size

43
00:01:25,079 --> 00:01:26,200
我们的learning rate

44
00:01:27,200 --> 00:01:27,960
第一行是一样的

45
00:01:27,960 --> 00:01:28,799
第二行是一样的

46
00:01:29,600 --> 00:01:31,760
第三行就是说也是没什么区别

47
00:01:31,760 --> 00:01:32,359
就是怎么样

48
00:01:32,359 --> 00:01:33,079
You need to wait

49
00:01:36,320 --> 00:01:38,760
然后主要的区别在这个地方

50
00:01:38,760 --> 00:01:39,760
第一个地方

51
00:01:39,760 --> 00:01:43,600
第一个是用的NN.DataParallel这个API

52
00:01:44,079 --> 00:01:44,600
它干嘛

53
00:01:44,800 --> 00:01:47,760
就是说我给定一个network的话

54
00:01:48,240 --> 00:01:49,400
我然后告诉你说

55
00:01:49,400 --> 00:01:51,280
我需要用哪些device

56
00:01:51,280 --> 00:01:52,600
说给定一个网络

57
00:01:52,600 --> 00:01:54,079
我给定了所有的device

58
00:01:54,200 --> 00:01:56,679
那么它返回一个新的net

59
00:01:57,640 --> 00:01:58,759
那么系统就知道了

60
00:01:58,759 --> 00:02:00,560
说这个net我们会在

61
00:02:00,560 --> 00:02:03,039
我们会把它复制到每个GPU上

62
00:02:03,039 --> 00:02:05,439
就你可以等于是我们之前

63
00:02:06,079 --> 00:02:07,359
把所有的parameter

64
00:02:07,359 --> 00:02:09,759
手动的复制到各个GPU上的操作

65
00:02:11,439 --> 00:02:13,280
然后它在上面还包了一层

66
00:02:14,039 --> 00:02:15,560
剩下东西跟之前是一样的

67
00:02:15,879 --> 00:02:17,039
定义trainer

68
00:02:17,039 --> 00:02:18,280
定义我的loss

69
00:02:18,599 --> 00:02:20,560
定义我的timer

70
00:02:22,280 --> 00:02:23,879
然后接下来就是说

71
00:02:24,080 --> 00:02:28,600
我们看我们的训练逻辑

72
00:02:29,080 --> 00:02:30,200
训练逻辑应该看到

73
00:02:30,200 --> 00:02:31,560
跟之前几乎是一样的

74
00:02:31,560 --> 00:02:31,840
对吧

75
00:02:31,840 --> 00:02:33,920
跟单GPU是没区别的地方

76
00:02:34,320 --> 00:02:36,120
就是说我们单GPU怎么做

77
00:02:36,120 --> 00:02:39,040
单GPU把我们的XY复制到GPU0上

78
00:02:39,040 --> 00:02:42,200
然后我们把丢到net里面

79
00:02:42,200 --> 00:02:44,280
然后算跟Y算loss

80
00:02:44,280 --> 00:02:46,480
然后再算backward就行了

81
00:02:47,040 --> 00:02:47,640
就是说

82
00:02:48,480 --> 00:02:49,400
这个地方我们

83
00:02:49,880 --> 00:02:52,880
因为之前的DataParallel这个操作

84
00:02:53,879 --> 00:02:55,159
所以系统知道说

85
00:02:55,159 --> 00:02:57,919
这个net在做给X进去的时候

86
00:02:57,919 --> 00:03:02,400
我会把X给你切开到不同的GPU上

87
00:03:02,400 --> 00:03:04,159
然后并行的算T2

88
00:03:04,159 --> 00:03:06,319
然后再把你loss加起来

89
00:03:07,719 --> 00:03:08,319
OK

90
00:03:08,319 --> 00:03:10,439
就是把我们刚刚那一些东西

91
00:03:10,439 --> 00:03:11,560
包在这个里面了

92
00:03:11,560 --> 00:03:11,919
已经

93
00:03:12,560 --> 00:03:14,280
它就从等价于是说

94
00:03:14,280 --> 00:03:17,359
它重新定义了net的forward函数

95
00:03:18,319 --> 00:03:19,599
就把我们刚刚那一堆东西

96
00:03:19,599 --> 00:03:20,280
塞在里面

97
00:03:20,879 --> 00:03:21,240
OK

98
00:03:21,240 --> 00:03:23,600
所以说这个API还是挺好用的

99
00:03:23,600 --> 00:03:26,400
就是说你基本上要拓展到多GPU上的话

100
00:03:26,400 --> 00:03:28,320
你就是调用一个这样子的东西就行了

101
00:03:29,240 --> 00:03:29,600
OK

102
00:03:29,600 --> 00:03:31,640
这就是其实就是它的

103
00:03:33,040 --> 00:03:36,439
怎么样在PyTorch里面做多GPU的实现

104
00:03:37,080 --> 00:03:37,360
OK

105
00:03:37,360 --> 00:03:39,040
这是最简单的数据并行

106
00:03:39,040 --> 00:03:40,439
所以叫做DataParallel

107
00:03:41,879 --> 00:03:42,320
OK

108
00:03:43,080 --> 00:03:43,840
就我们看一下

109
00:03:43,840 --> 00:03:46,040
就是说同样的话

110
00:03:46,040 --> 00:03:48,560
在我们的上面做会怎么样

111
00:03:49,120 --> 00:03:50,120
现在是

112
00:03:50,439 --> 00:03:52,520
直接可以看一下精度是

113
00:03:54,000 --> 00:03:55,599
测试精度是0.92

114
00:03:55,599 --> 00:03:56,719
高比刚刚高一点

115
00:03:57,480 --> 00:03:59,800
然后我们每一个DataEpoch

116
00:03:59,800 --> 00:04:01,879
是要跑13.6秒

117
00:04:02,400 --> 00:04:03,840
这是GPU01

118
00:04:03,840 --> 00:04:04,920
唯一的情况

119
00:04:06,439 --> 00:04:08,319
那么接下来看到的是

120
00:04:08,640 --> 00:04:09,400
同样的道理

121
00:04:09,560 --> 00:04:12,159
我们没有把BatchSize增加

122
00:04:12,159 --> 00:04:13,240
就是说跟之前是一样

123
00:04:13,240 --> 00:04:13,680
BatchSize

124
00:04:13,680 --> 00:04:15,360
LearningRate应该是没有改变

125
00:04:15,480 --> 00:04:15,960
我们看一下

126
00:04:16,160 --> 00:04:17,560
都是256和0.1

127
00:04:18,560 --> 00:04:19,920
首先看一下是说

128
00:04:19,920 --> 00:04:20,560
精度不变

129
00:04:20,560 --> 00:04:21,040
对吧

130
00:04:22,240 --> 00:04:23,480
都是0.

131
00:04:23,480 --> 00:04:24,280
刚刚0.92

132
00:04:24,280 --> 00:04:25,000
0.91

133
00:04:25,800 --> 00:04:27,000
第二个是说

134
00:04:28,319 --> 00:04:31,560
我们的性能有所增加

135
00:04:31,600 --> 00:04:36,600
从13.6秒变成了10秒钟

136
00:04:36,600 --> 00:04:37,319
就是

137
00:04:37,879 --> 00:04:38,480
增

138
00:04:39,160 --> 00:04:40,480
提升了30%

139
00:04:40,480 --> 00:04:41,959
但没有我们想象那么好

140
00:04:41,959 --> 00:04:42,639
想象的是

141
00:04:42,639 --> 00:04:45,560
你应该是从13.6秒变成了

142
00:04:45,920 --> 00:04:46,920
6.8秒

143
00:04:46,920 --> 00:04:47,199
对吧

144
00:04:47,360 --> 00:04:47,920
就是Combine

145
00:04:47,920 --> 00:04:49,399
这是你的最好的情况

146
00:04:49,439 --> 00:04:50,879
但现在我们没有

147
00:04:51,000 --> 00:04:51,599
没有的话

148
00:04:51,599 --> 00:04:52,479
那么就是说

149
00:04:52,479 --> 00:04:53,399
我们再假设

150
00:04:53,519 --> 00:04:55,000
我们的Network已经够好了

151
00:04:55,000 --> 00:04:56,199
ResNet也不差了

152
00:04:56,199 --> 00:04:56,759
就18

153
00:04:56,759 --> 00:04:58,240
虽然是一个小的ResNet

154
00:04:58,279 --> 00:05:00,000
但是对于GPU来讲

155
00:05:00,000 --> 00:05:01,199
应该是问题不大了

156
00:05:02,480 --> 00:05:03,839
第二个是说

157
00:05:04,800 --> 00:05:06,839
我们用的是Pytorch的

158
00:05:06,839 --> 00:05:08,560
自己High-Level Data Parallel

159
00:05:08,800 --> 00:05:09,480
理论上说

160
00:05:09,480 --> 00:05:11,919
我们希望你做一个框架一样

161
00:05:12,240 --> 00:05:14,079
这种最简单的Data Parallel

162
00:05:14,079 --> 00:05:15,480
你应该是做的比较好的

163
00:05:15,520 --> 00:05:17,680
不需要我去关心太多性能的问题

164
00:05:17,840 --> 00:05:18,240
所以

165
00:05:18,920 --> 00:05:20,319
现在你假设那两个参数

166
00:05:20,319 --> 00:05:21,400
已经排除的话

167
00:05:21,400 --> 00:05:22,840
那么接下来就是说

168
00:05:22,879 --> 00:05:23,360
好

169
00:05:23,360 --> 00:05:24,720
那就是调我们的Batch Size

170
00:05:24,720 --> 00:05:25,280
对吧

171
00:05:25,319 --> 00:05:26,520
调我们的Learning Rate

172
00:05:27,600 --> 00:05:27,800
好

173
00:05:27,800 --> 00:05:28,879
给大家试一下

174
00:05:29,920 --> 00:05:30,680
跟之前一样

175
00:05:30,680 --> 00:05:33,600
我们先把Batch Size乘以2

176
00:05:34,319 --> 00:05:34,920
Learning Rate

177
00:05:34,920 --> 00:05:35,319
我们说

178
00:05:35,319 --> 00:05:37,240
我们也乘以2看看

179
00:05:38,439 --> 00:05:39,560
跑一跑看看

180
00:05:39,560 --> 00:05:47,079
但这个东西跑起来就比较慢了

181
00:05:47,680 --> 00:05:49,680
跑一下要6秒钟一个

182
00:05:49,879 --> 00:05:52,120
跑10下就是1分钟就过去了

183
00:05:53,000 --> 00:05:56,720
所以我们稍微等一下

184
00:05:57,279 --> 00:06:01,639
然后稍微看两个Data Epoch

185
00:06:04,160 --> 00:06:05,399
而且另外一个大家看到

186
00:06:05,399 --> 00:06:07,399
是说我们其实还是挺抖动的

187
00:06:07,560 --> 00:06:11,160
我们测试精度是抖动的比较厉害的

188
00:06:11,160 --> 00:06:12,920
是因为我们的Learning Rate

189
00:06:12,920 --> 00:06:14,840
用的相对来说是比较大的

190
00:06:14,840 --> 00:06:15,920
就是调的比较狠的

191
00:06:17,000 --> 00:06:18,360
所以调的比较狠的话

192
00:06:18,360 --> 00:06:20,600
很容易看到你跳来跳去

193
00:06:21,720 --> 00:06:22,520
通常来说

194
00:06:23,080 --> 00:06:25,120
你也不用太关心前面那一些

195
00:06:25,360 --> 00:06:27,720
只要后面相对来说比较稳定就行了

196
00:06:27,800 --> 00:06:30,720
所以你可以往大力调

197
00:06:30,720 --> 00:06:32,440
就是说前面的抖动不抖动

198
00:06:32,440 --> 00:06:35,160
其实不会特别影响后面

199
00:06:36,120 --> 00:06:36,840
但是反过来讲

200
00:06:36,840 --> 00:06:38,880
如果你调的太小的话

201
00:06:38,880 --> 00:06:40,320
Learning Rate太小的话

202
00:06:40,320 --> 00:06:43,400
其实后面很有可能后期就比较乏力了

203
00:06:46,040 --> 00:06:49,160
OK我们跑到第7个Epoch了

204
00:06:50,160 --> 00:06:52,200
Learning Rate可能是有点大

205
00:06:52,200 --> 00:06:55,480
可以看到是我们抖动的就比较严重

206
00:06:57,280 --> 00:07:01,000
很有可能Learning Rate似乎太大了一点

207
00:07:01,760 --> 00:07:04,360
基本上到了第8个Epoch的时候

208
00:07:04,480 --> 00:07:06,320
还是在0.85

209
00:07:07,320 --> 00:07:09,000
另外一个我们要看的是说

210
00:07:09,000 --> 00:07:10,040
我们得看一下

211
00:07:10,040 --> 00:07:11,280
我们在测性能的时候

212
00:07:11,280 --> 00:07:12,280
最关键你要看一下

213
00:07:12,280 --> 00:07:13,720
有没有别人在用你的机器

214
00:07:16,920 --> 00:07:18,080
来我来看一眼

215
00:07:21,120 --> 00:07:22,160
我已经跑完了吗

216
00:07:23,160 --> 00:07:24,280
OK我已经跑完了

217
00:07:24,800 --> 00:07:26,080
所以应该目前来看到

218
00:07:26,080 --> 00:07:27,800
没有别人在用这个机器

219
00:07:28,520 --> 00:07:31,840
所以一般来说你性能不行的时候

220
00:07:31,840 --> 00:07:32,640
先去看一下

221
00:07:32,640 --> 00:07:34,840
你的GPU是不是还有别人在用

222
00:07:35,360 --> 00:07:37,160
经常会遇到这个问题

223
00:07:37,440 --> 00:07:39,000
所以我们看到是说

224
00:07:39,480 --> 00:07:40,720
我们先不看精度

225
00:07:40,880 --> 00:07:42,400
精度是Learning Rate决定了

226
00:07:43,480 --> 00:07:44,560
我们性能还是有提升

227
00:07:44,560 --> 00:07:45,160
对吧

228
00:07:45,280 --> 00:07:46,600
从10秒钟

229
00:07:46,600 --> 00:07:48,240
刚刚变成了8秒钟

230
00:07:48,760 --> 00:07:50,760
比我们的最优还有一点距离

231
00:07:51,560 --> 00:07:52,840
就是说我们是理论上

232
00:07:52,840 --> 00:07:54,680
我们觉得应该要到6.8秒

233
00:07:55,040 --> 00:07:56,840
13.6秒减半

234
00:07:57,240 --> 00:07:59,160
但现在是8.3秒

235
00:07:59,240 --> 00:08:01,320
还差那么一点点

236
00:08:03,160 --> 00:08:04,040
还差那么一点点

237
00:08:05,840 --> 00:08:07,920
你一般来说

238
00:08:07,920 --> 00:08:08,600
就是说

239
00:08:10,200 --> 00:08:11,360
我已经保证了

240
00:08:11,360 --> 00:08:13,080
每一个GPU上用的

241
00:08:14,280 --> 00:08:15,920
Batch Size已经是一样的

242
00:08:15,920 --> 00:08:18,480
跟就是说GPU为0和GPU为1的时候

243
00:08:18,480 --> 00:08:19,600
那么唯一的问题

244
00:08:19,600 --> 00:08:20,880
我们都等会会讲

245
00:08:21,040 --> 00:08:22,920
就是整体来讲

246
00:08:22,920 --> 00:08:24,320
这个Batch Size还是偏小

247
00:08:24,320 --> 00:08:25,920
这是我的通讯的开销

248
00:08:25,920 --> 00:08:28,440
大于我的计算的开销

249
00:08:28,440 --> 00:08:29,720
我们等会来讲这个事情

250
00:08:31,000 --> 00:08:31,920
所以但是还不错

251
00:08:31,920 --> 00:08:32,320
对吧

252
00:08:32,320 --> 00:08:34,080
就是你看到增加Batch Size

253
00:08:34,080 --> 00:08:36,560
能够帮你的性能提升

254
00:08:37,800 --> 00:08:38,480
但反过来讲

255
00:08:38,480 --> 00:08:38,920
很多时候

256
00:08:38,920 --> 00:08:41,000
你也不用那么追求说完美并行

257
00:08:41,160 --> 00:08:43,480
就是说我说我两个GPU能够快一点

258
00:08:43,480 --> 00:08:44,680
4个GPU还能快一点

259
00:08:44,680 --> 00:08:46,320
8个GPU能更快也行

260
00:08:46,400 --> 00:08:47,560
虽然就是说

261
00:08:49,000 --> 00:08:50,960
节省的是我的时间

262
00:08:50,960 --> 00:08:51,440
对吧

263
00:08:52,920 --> 00:08:55,840
所以如果你的资源允许的话

264
00:08:56,200 --> 00:08:57,280
尽量多用GPU

265
00:08:58,000 --> 00:08:59,920
因为省下的是你的时间

266
00:09:00,240 --> 00:09:01,440
具体真的

267
00:09:01,960 --> 00:09:03,400
是不是并行度

268
00:09:03,399 --> 00:09:04,159
是不是完美

269
00:09:04,399 --> 00:09:06,199
就是你的Scalability是什么样子

270
00:09:06,279 --> 00:09:08,399
其实大家都还行

271
00:09:09,039 --> 00:09:10,240
就快一点

272
00:09:10,240 --> 00:09:10,959
总比不快好

273
00:09:10,959 --> 00:09:11,399
对吧

274
00:09:12,000 --> 00:09:13,319
另外一个是说

275
00:09:13,360 --> 00:09:15,240
Learning Rate和Batch Size

276
00:09:15,240 --> 00:09:17,399
两个控制了我的测试精度

277
00:09:18,199 --> 00:09:18,679
测试精度

278
00:09:18,679 --> 00:09:19,919
我们看到这里还是有问题的

279
00:09:20,199 --> 00:09:22,759
就是说我们刚刚是0.92

280
00:09:23,079 --> 00:09:24,759
现在变成了0.84

281
00:09:24,799 --> 00:09:26,079
就是说意味着说

282
00:09:26,079 --> 00:09:27,360
Batch Size变大的时候

283
00:09:27,360 --> 00:09:30,120
我的在跑同样个Data Epoch的

284
00:09:30,120 --> 00:09:30,679
情况下

285
00:09:30,679 --> 00:09:32,399
我得到的测试精度变低了

286
00:09:34,199 --> 00:09:36,879
通常来说

287
00:09:38,039 --> 00:09:39,559
很多情况下是

288
00:09:39,679 --> 00:09:41,319
可能Batch Size

289
00:09:41,559 --> 00:09:42,840
对512

290
00:09:42,840 --> 00:09:46,120
对于我们简单的Fashion List

291
00:09:46,120 --> 00:09:47,279
来说太大了

292
00:09:50,799 --> 00:09:51,799
所以就是说

293
00:09:51,799 --> 00:09:53,000
你有很你可以

294
00:09:53,000 --> 00:09:53,840
大家可以去玩一下

295
00:09:53,840 --> 00:09:54,959
去调一下参数

296
00:09:54,959 --> 00:09:56,799
看看你能不能在500以上的

297
00:09:56,799 --> 00:09:57,159
情况下

298
00:09:57,159 --> 00:09:59,360
还是能不能跑10个Epoch

299
00:09:59,360 --> 00:10:01,159
能够Match到0.92

300
00:10:01,199 --> 00:10:02,279
如果不能的话

301
00:10:02,279 --> 00:10:04,399
很有可能Batch Size太大了

302
00:10:04,879 --> 00:10:06,039
通常来说

303
00:10:06,399 --> 00:10:07,240
最简单来说

304
00:10:07,240 --> 00:10:08,000
就是说

305
00:10:08,519 --> 00:10:11,000
假设你一个很简单数据机

306
00:10:11,319 --> 00:10:12,959
你用个特别大的Batch Size

307
00:10:13,399 --> 00:10:14,519
用512

308
00:10:14,519 --> 00:10:16,559
那么每一次采样的时候

309
00:10:16,559 --> 00:10:17,879
里面有很多样本

310
00:10:17,879 --> 00:10:19,319
可能是重复的

311
00:10:19,319 --> 00:10:20,839
跟大家长得差不多的

312
00:10:22,120 --> 00:10:22,399
对吧

313
00:10:22,399 --> 00:10:23,279
就是说你就10类

314
00:10:23,799 --> 00:10:26,199
但是你就每一类里面抽了52

315
00:10:26,759 --> 00:10:29,240
基本上是51个样本放在里面

316
00:10:29,879 --> 00:10:30,679
你在类里面

317
00:10:30,680 --> 00:10:32,440
如果样本的长得差不多

318
00:10:32,440 --> 00:10:33,800
那么差不多的样本

319
00:10:33,800 --> 00:10:35,000
就算T图是浪费

320
00:10:36,280 --> 00:10:37,000
极端情况下

321
00:10:37,000 --> 00:10:38,080
两个完全一样的样本

322
00:10:38,080 --> 00:10:39,560
算T图是没意义的

323
00:10:39,560 --> 00:10:39,800
对吧

324
00:10:39,800 --> 00:10:41,080
因为T图都精致了

325
00:10:41,080 --> 00:10:43,200
所以当你的数据级的多元性

326
00:10:43,200 --> 00:10:44,400
不够大的情况下

327
00:10:44,400 --> 00:10:46,440
你是无法用特别大的Batch Size

328
00:10:47,920 --> 00:10:49,560
当然我们是说

329
00:10:49,800 --> 00:10:51,200
当然还有很多别的东西

330
00:10:51,200 --> 00:10:52,920
技术让你做得更好

331
00:10:53,080 --> 00:10:54,680
但是直观上来说

332
00:10:54,960 --> 00:10:57,040
这个Data Set用512

333
00:10:57,040 --> 00:10:58,720
可能还是确实比较悬

334
00:10:59,240 --> 00:11:00,000
但你碰到

335
00:11:00,639 --> 00:11:02,519
AmazonNet那种1000类的话

336
00:11:02,519 --> 00:11:03,879
你有1000个类

337
00:11:03,879 --> 00:11:05,800
你每次采用512个样本

338
00:11:05,800 --> 00:11:06,720
那没事

339
00:11:06,759 --> 00:11:07,320
对吧

340
00:11:07,960 --> 00:11:09,720
基本上你都你一个类

341
00:11:09,720 --> 00:11:11,480
在一个样本里面出现的

342
00:11:12,080 --> 00:11:12,639
概率都

343
00:11:12,639 --> 00:11:14,720
一个某一个类出现一个样本

344
00:11:14,720 --> 00:11:16,600
就有1 2的概率的情况下

345
00:11:16,600 --> 00:11:18,879
所以你就不要太担心重复了

346
00:11:19,120 --> 00:11:19,639
OK

347
00:11:20,720 --> 00:11:21,040
OK

348
00:11:21,040 --> 00:11:22,399
这就是我们的

349
00:11:23,040 --> 00:11:26,639
单机多GPU的实现

350
00:11:26,960 --> 00:11:28,600
给大家主要是看一下

351
00:11:28,600 --> 00:11:30,240
具体我们讲的Data Parallel

352
00:11:30,240 --> 00:11:31,200
是怎么实现的

353
00:11:31,519 --> 00:11:32,200
第二点是说

354
00:11:32,200 --> 00:11:33,040
给大家看一下

355
00:11:33,040 --> 00:11:35,040
实际运行的时候

356
00:11:35,560 --> 00:11:36,960
性能会长什么样子

357
00:11:36,960 --> 00:11:38,200
以及你调参

358
00:11:38,200 --> 00:11:39,879
会给你带来的各种影响

359
00:11:40,159 --> 00:11:43,159
我们在等会儿讲分布式的时候

360
00:11:43,159 --> 00:11:44,720
再给大家再讲一讲

361
00:11:45,320 --> 00:11:48,639
细讲一下这些性能瘦脸的问题

362
00:11:48,879 --> 00:11:51,240
我们接下来先回答一下问题

