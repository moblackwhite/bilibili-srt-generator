1
00:00:00,000 --> 00:00:02,960
我们是权重衰退的实相

2
00:00:02,960 --> 00:00:05,520
我们首先导入我们的包

3
00:00:05,520 --> 00:00:08,800
就是我们跟之前没什么区别

4
00:00:11,599 --> 00:00:13,919
那么我们要干这里干的事情是说

5
00:00:13,919 --> 00:00:16,399
我们生成一个人工的数据集

6
00:00:16,640 --> 00:00:18,160
人工数据的好处是

7
00:00:18,160 --> 00:00:19,280
我们很容易的看到

8
00:00:19,280 --> 00:00:21,440
我们跟真实的之间的区别

9
00:00:21,640 --> 00:00:24,120
我们这里的人工数据集也挺简单的

10
00:00:24,120 --> 00:00:27,199
就是一个还是一个现行回归问题

11
00:00:27,359 --> 00:00:29,560
我们加了一个偏差0.05

12
00:00:29,560 --> 00:00:32,400
然后乘以加上一个0.01

13
00:00:32,400 --> 00:00:33,960
就是0.01就是我们的权重

14
00:00:33,960 --> 00:00:38,680
Wxi就是我们的一些随机的一些输入

15
00:00:38,920 --> 00:00:41,000
然后加上一个噪音

16
00:00:41,000 --> 00:00:43,079
噪音就是一个均值为0

17
00:00:43,079 --> 00:00:45,800
方差为0.01的一个正常分布

18
00:00:46,680 --> 00:00:48,680
所以我们可以看到是说

19
00:00:48,800 --> 00:00:51,400
这里一段其实也写的挺简单的

20
00:00:51,439 --> 00:00:54,600
我们特意的加我们的训练数据做的比较小

21
00:00:55,079 --> 00:00:56,200
就是这样子

22
00:00:56,200 --> 00:00:58,800
你们训练数据越小越容易过拟核

23
00:00:59,520 --> 00:01:00,440
大家应该回忆一下

24
00:01:00,440 --> 00:01:02,240
就是当你的数据越简单

25
00:01:02,240 --> 00:01:03,440
你的模型越复杂的时候

26
00:01:03,440 --> 00:01:05,280
你过拟核会越容易发生

27
00:01:05,600 --> 00:01:08,800
所以我们把训练样本选了一个很小的20

28
00:01:09,920 --> 00:01:11,760
接下来就是测试

29
00:01:11,760 --> 00:01:14,159
测试我们稍微选大一点没关系

30
00:01:14,520 --> 00:01:16,480
然后我们的Number of Inputs

31
00:01:16,480 --> 00:01:18,560
就是我们的特征的维度

32
00:01:18,560 --> 00:01:19,520
我们选的比较大

33
00:01:19,520 --> 00:01:20,560
就是选了个200

34
00:01:21,159 --> 00:01:22,439
所以基本上大家可以认为

35
00:01:22,439 --> 00:01:24,480
这个很容易就发生了过拟核

36
00:01:24,920 --> 00:01:27,480
然后我们的Batch Size取了个5

37
00:01:28,200 --> 00:01:29,079
然后我们的

38
00:01:29,079 --> 00:01:31,079
当然这个是我们真实的W和B

39
00:01:31,079 --> 00:01:31,960
就W的话

40
00:01:32,079 --> 00:01:35,320
就是一个0.01×1的一个向量

41
00:01:35,520 --> 00:01:37,200
然后我们的B就是0.05

42
00:01:38,040 --> 00:01:38,680
同样的话

43
00:01:38,680 --> 00:01:42,200
我们这里就不再多说了

44
00:01:42,520 --> 00:01:45,480
我们是之前存了一些函数

45
00:01:45,480 --> 00:01:46,320
在之前讲的是

46
00:01:46,320 --> 00:01:48,760
我们如何生成一个人工数据集

47
00:01:48,960 --> 00:01:52,760
我们怎么读取一个内存里面的一个

48
00:01:53,439 --> 00:01:55,800
数组变成一个iterator

49
00:01:57,520 --> 00:01:59,480
好

50
00:01:59,480 --> 00:02:00,880
我们接下来就是说

51
00:02:01,079 --> 00:02:03,640
这里也是我们之前是讲过的

52
00:02:03,640 --> 00:02:06,800
我们怎么对W来进行初始化

53
00:02:06,800 --> 00:02:08,599
我们W就直接是一个

54
00:02:08,840 --> 00:02:11,360
很简单的一个均值为0

55
00:02:11,360 --> 00:02:12,360
方差为1

56
00:02:12,360 --> 00:02:14,520
然后我们的长度就是

57
00:02:14,520 --> 00:02:16,400
你200×1的一个向量

58
00:02:16,759 --> 00:02:17,599
我们需要T2

59
00:02:17,599 --> 00:02:18,840
因为我们要做计算

60
00:02:18,879 --> 00:02:21,039
B就是一个全零的一个标量

61
00:02:21,039 --> 00:02:22,920
这就是我们的参数了

62
00:02:23,599 --> 00:02:24,640
接下来就是核心

63
00:02:24,640 --> 00:02:25,280
就是

64
00:02:25,760 --> 00:02:28,040
怎么定义我们的L2的

65
00:02:28,080 --> 00:02:29,840
犯书惩罚

66
00:02:30,320 --> 00:02:31,600
这是L2 penalty

67
00:02:32,200 --> 00:02:33,600
那就给定W的话

68
00:02:33,600 --> 00:02:35,480
其实我们就是把W

69
00:02:35,880 --> 00:02:38,360
做一次二次计算

70
00:02:38,360 --> 00:02:38,960
就是

71
00:02:39,400 --> 00:02:40,360
power2

72
00:02:40,360 --> 00:02:42,600
或者你可以写W×2

73
00:02:43,000 --> 00:02:45,240
然后再除核

74
00:02:46,080 --> 00:02:48,040
因为这就是我们的

75
00:02:48,080 --> 00:02:50,480
L2的犯式的定义

76
00:02:50,480 --> 00:02:53,080
然后我们不要开根号

77
00:02:53,080 --> 00:02:55,560
因为我们是一个L2犯式的平方

78
00:02:55,760 --> 00:02:57,800
然后我们最后乘出了一个2

79
00:02:57,960 --> 00:02:59,920
注意到我们这里没有把num

80
00:02:59,920 --> 00:03:00,360
写进来

81
00:03:00,520 --> 00:03:02,800
因为num我们会之前写在外面

82
00:03:02,960 --> 00:03:04,640
那么说白了这个函数就是

83
00:03:04,680 --> 00:03:05,520
给定个W

84
00:03:05,520 --> 00:03:06,600
我们就算了一下

85
00:03:06,600 --> 00:03:09,200
L2的平凡犯数

86
00:03:10,520 --> 00:03:12,080
所以这个其实说白了

87
00:03:12,080 --> 00:03:14,600
这个就是我们这一节讲的核心

88
00:03:14,600 --> 00:03:15,320
就在这个地方

89
00:03:15,320 --> 00:03:16,640
就是一个那么小的函数

90
00:03:21,120 --> 00:03:21,640
OK

91
00:03:23,080 --> 00:03:24,280
这个是一个训练函数

92
00:03:24,480 --> 00:03:26,520
训练函数跟我们之前没本质区别

93
00:03:26,840 --> 00:03:27,600
就是

94
00:03:27,960 --> 00:03:29,000
只所谓的是说

95
00:03:29,000 --> 00:03:32,240
我接受一个输入参数是num

96
00:03:32,320 --> 00:03:33,480
这是我们的超参数

97
00:03:33,480 --> 00:03:34,920
我们一会会指定

98
00:03:35,280 --> 00:03:37,200
然后我们先来初始化

99
00:03:37,200 --> 00:03:39,280
我们的权重W和B

100
00:03:39,800 --> 00:03:41,040
然后我们的

101
00:03:41,439 --> 00:03:43,360
我们就做了一个很简单的信息回归

102
00:03:43,400 --> 00:03:46,200
还是用我们之前定义的信息回归

103
00:03:46,520 --> 00:03:48,960
然后我们用的平方损失函数

104
00:03:49,120 --> 00:03:50,200
因为这是一个回归问题

105
00:03:50,960 --> 00:03:53,680
然后我们选训练稍微长一点

106
00:03:53,680 --> 00:03:54,920
因为这个数据比较小

107
00:03:54,920 --> 00:03:55,920
就20个样本

108
00:03:56,120 --> 00:03:56,760
那么

109
00:03:57,160 --> 00:03:58,800
就是迭代100次

110
00:03:59,720 --> 00:04:02,000
然后我们的学习率属于0.03

111
00:04:02,240 --> 00:04:03,880
这个其实大家也可以改

112
00:04:04,080 --> 00:04:06,640
就是说我们之前用的是0.03

113
00:04:06,760 --> 00:04:08,040
现在也用个0.03

114
00:04:08,080 --> 00:04:09,120
其实问题不大了

115
00:04:09,280 --> 00:04:11,640
就你可以往稍微往大调一点

116
00:04:11,760 --> 00:04:13,480
那么你num of epoch可以变小

117
00:04:13,520 --> 00:04:15,400
你稍微往小调一点

118
00:04:15,760 --> 00:04:18,080
你num of epoch就变大一点

119
00:04:18,280 --> 00:04:19,840
但是因为我们这个数据很简单

120
00:04:19,840 --> 00:04:21,320
所以我们就不用调这个东西了

121
00:04:22,080 --> 00:04:23,800
这个就是说我们要

122
00:04:24,240 --> 00:04:25,800
要动画一下我们的效果

123
00:04:27,800 --> 00:04:31,600
然后接下来也是个很标准的一个训练

124
00:04:31,640 --> 00:04:34,480
首先我们对每一次外面的for loop

125
00:04:34,480 --> 00:04:36,360
就是每次数据迭代

126
00:04:36,440 --> 00:04:37,960
里面for loop就是

127
00:04:38,000 --> 00:04:40,560
每一次从我们的迭代器里面

128
00:04:40,560 --> 00:04:42,080
拿出一个x和y

129
00:04:43,200 --> 00:04:44,240
其实这个函数是

130
00:04:44,520 --> 00:04:45,680
这个其实不需要的

131
00:04:46,040 --> 00:04:47,000
我可以注释掉

132
00:04:47,000 --> 00:04:48,400
应该我新版本里面

133
00:04:48,400 --> 00:04:50,080
我已经删掉这个函数了

134
00:04:50,720 --> 00:04:53,520
然后给定x

135
00:04:53,560 --> 00:04:55,560
输入到我们的信息模型里面

136
00:04:55,560 --> 00:04:56,560
算损失

137
00:04:56,680 --> 00:04:57,840
唯一的区别在这个地方

138
00:04:57,920 --> 00:04:59,520
这是唯一的不一样

139
00:04:59,960 --> 00:05:02,920
我们要加上一个我们的损失

140
00:05:02,960 --> 00:05:04,680
要加上一个num

141
00:05:04,880 --> 00:05:06,840
就是我们的超参数

142
00:05:06,840 --> 00:05:08,040
是传进来的num

143
00:05:08,040 --> 00:05:08,960
是一个标量

144
00:05:09,200 --> 00:05:12,680
乘以我们之前定义的L2 penalty

145
00:05:13,920 --> 00:05:16,320
这就是我们唯一的区别

146
00:05:16,600 --> 00:05:17,680
大家可以看到是

147
00:05:17,680 --> 00:05:20,400
我们在我们的损失L那个地方

148
00:05:20,600 --> 00:05:22,480
这个是来自于我们原始的损失

149
00:05:22,480 --> 00:05:22,800
函数

150
00:05:22,800 --> 00:05:24,040
就是我们的神经网络

151
00:05:24,199 --> 00:05:26,759
现在我们加上了一个新的

152
00:05:27,040 --> 00:05:27,759
在这个地方

153
00:05:28,519 --> 00:05:29,800
然后别的你可以不用变

154
00:05:29,800 --> 00:05:30,879
因为他自动求导

155
00:05:30,879 --> 00:05:32,160
然后自动都做完了

156
00:05:32,400 --> 00:05:33,840
所以剩下都是一样的

157
00:05:34,280 --> 00:05:36,399
最后我们来打印一下

158
00:05:36,840 --> 00:05:38,639
就是他的L2犯数

159
00:05:38,639 --> 00:05:41,199
就是当然Patch有L2的实现了

160
00:05:41,199 --> 00:05:42,600
就是我们这一只

161
00:05:42,600 --> 00:05:43,280
所以这么实现

162
00:05:43,280 --> 00:05:44,160
就给大家看一下

163
00:05:44,160 --> 00:05:45,439
这个是实现起来

164
00:05:45,439 --> 00:05:46,680
其实非常简单

165
00:05:46,680 --> 00:05:50,360
不一定要用框架提供的东西

166
00:05:51,480 --> 00:05:51,800
OK

167
00:05:51,800 --> 00:05:52,600
最后我们打印一下

168
00:05:52,600 --> 00:05:53,680
他的L2犯数

169
00:05:53,920 --> 00:05:55,319
注意的是说

170
00:05:55,319 --> 00:05:56,280
这是L2犯数

171
00:05:56,280 --> 00:05:57,360
当然没有平方了

172
00:05:57,360 --> 00:06:00,879
OK

173
00:06:00,879 --> 00:06:01,759
这就是我们

174
00:06:02,000 --> 00:06:06,079
接下来我们我来重新回到头来训练一次

175
00:06:06,079 --> 00:06:13,800
我们第一次启动会慢一点

176
00:06:17,000 --> 00:06:18,439
我们稍等一下

177
00:06:20,840 --> 00:06:21,639
因为

178
00:06:26,120 --> 00:06:28,040
没有启动起来

179
00:06:28,040 --> 00:06:29,519
我来重新来启动一下

180
00:06:29,519 --> 00:06:31,280
因为不小心

181
00:06:34,199 --> 00:06:36,439
不小心今天上午重启了一下机器

182
00:06:43,800 --> 00:06:44,360
OK

183
00:06:46,680 --> 00:06:52,199
给大家看一下我们的动画效果

184
00:06:57,199 --> 00:06:59,199
这就是我们的训练动画效果

185
00:06:59,439 --> 00:07:01,920
首先我们如果number等于0的话

186
00:07:01,920 --> 00:07:05,040
就是完全没有L2的犯数

187
00:07:05,240 --> 00:07:06,079
可以看到这个

188
00:07:06,079 --> 00:07:06,759
我们可以

189
00:07:06,800 --> 00:07:10,000
这个是number等于3的时候的一个效果

190
00:07:10,040 --> 00:07:12,079
就是说他其实动画效果

191
00:07:12,079 --> 00:07:14,240
就是每一次你扫完一个数据

192
00:07:14,240 --> 00:07:15,840
就在你的图面打印一下

193
00:07:15,880 --> 00:07:18,440
因为之后我们的训练每一个周期比较长

194
00:07:18,440 --> 00:07:19,480
所以这样子的话

195
00:07:19,480 --> 00:07:22,480
大家能前期能看到一个效果

196
00:07:22,640 --> 00:07:23,800
我们还是回到之前

197
00:07:23,800 --> 00:07:25,600
可以看到是说

198
00:07:25,600 --> 00:07:28,960
当你没有规约的时候

199
00:07:29,320 --> 00:07:30,920
就是没有罚的时候

200
00:07:30,920 --> 00:07:32,080
number等于0

201
00:07:32,240 --> 00:07:33,720
那么你可以看到是说

202
00:07:33,720 --> 00:07:37,120
W的L2的犯数是12.56

203
00:07:37,120 --> 00:07:39,240
记住这个我们可以之后比较

204
00:07:40,280 --> 00:07:41,600
另外一个是说

205
00:07:41,720 --> 00:07:47,720
你看到训练的损失是一直在往下降

206
00:07:48,280 --> 00:07:51,120
但是我的测试基本上是平的

207
00:07:52,560 --> 00:07:54,960
中间有个特别大的一个差

208
00:07:55,439 --> 00:07:58,240
那么这是一个非常明显的一个过离合

209
00:07:58,240 --> 00:07:59,720
就是说我的训练

210
00:07:59,720 --> 00:08:01,520
我一直去离合我的训练函数

211
00:08:01,520 --> 00:08:02,840
离合他的噪音

212
00:08:02,879 --> 00:08:04,200
但是在新的数据上

213
00:08:04,200 --> 00:08:05,080
在我的测试级

214
00:08:05,080 --> 00:08:08,280
也就是之前我们讲的验证级的上面

215
00:08:08,280 --> 00:08:10,120
其实你看不到我们有任何进展

216
00:08:10,240 --> 00:08:11,879
这是非常严重的过离合

217
00:08:11,879 --> 00:08:15,040
就是他中间 gap

218
00:08:15,040 --> 00:08:17,759
就是这个勾

219
00:08:18,040 --> 00:08:19,959
比之前的一直在扩大

220
00:08:21,160 --> 00:08:22,879
那么碰到这个情况

221
00:08:22,879 --> 00:08:24,079
我们第二个就是说

222
00:08:24,079 --> 00:08:26,439
我们把选了一个

223
00:08:26,480 --> 00:08:30,480
也还算行的一个参数

224
00:08:30,840 --> 00:08:32,799
可以看到有一定的效果

225
00:08:33,320 --> 00:08:36,680
就是说他在往下降的时候

226
00:08:36,680 --> 00:08:39,120
我的测试也还是在往下降的

227
00:08:39,399 --> 00:08:41,879
虽然他之间还是有一定过离合

228
00:08:41,919 --> 00:08:43,159
但是你可以看到是说

229
00:08:43,159 --> 00:08:45,679
在迭代了50轮之后

230
00:08:46,360 --> 00:08:48,840
基本上他的就比较平了

231
00:08:48,840 --> 00:08:51,120
就是我的法使的

232
00:08:51,120 --> 00:08:52,799
你w不会变得特别大

233
00:08:52,799 --> 00:08:54,039
那不会变得特别大的话

234
00:08:54,039 --> 00:08:55,399
那么对他的过离合

235
00:08:55,399 --> 00:08:57,799
基本上在这个点就差不多

236
00:08:57,799 --> 00:08:59,440
就不会再往下走了

237
00:08:59,440 --> 00:09:01,279
就全只不会变得更大

238
00:09:01,600 --> 00:09:04,320
然后我的测试还是在往下

239
00:09:04,320 --> 00:09:05,480
继续往下降的

240
00:09:05,519 --> 00:09:07,480
所以你如果再多迭代迭代

241
00:09:07,480 --> 00:09:09,440
还是有可能会赶上一点

242
00:09:10,080 --> 00:09:12,039
但是因为我们这是一个非常

243
00:09:13,159 --> 00:09:14,560
小的一个训练机

244
00:09:14,560 --> 00:09:15,680
因为我们就20个样板

245
00:09:15,960 --> 00:09:18,759
所以过离合可能是无法避免的

246
00:09:18,759 --> 00:09:22,039
大家可以在课后可以去调一下

247
00:09:22,120 --> 00:09:25,920
你可以选择我们再多迭代一些轮

248
00:09:25,960 --> 00:09:27,960
看看他是不是会变得更小

249
00:09:28,519 --> 00:09:29,399
这是第一点

250
00:09:29,440 --> 00:09:30,440
第二点是说

251
00:09:30,440 --> 00:09:32,240
你可以把lambda调的更大一点

252
00:09:32,840 --> 00:09:34,200
就是说你可以认为说

253
00:09:34,200 --> 00:09:35,759
这里面还有一定的过离合

254
00:09:35,799 --> 00:09:38,600
意味着这个范数其实还是太大了

255
00:09:38,960 --> 00:09:40,080
记得我们之前

256
00:09:40,080 --> 00:09:43,080
其实我的w是取的是0.01

257
00:09:43,080 --> 00:09:46,240
所以我们学到的其实还是偏大

258
00:09:46,279 --> 00:09:48,519
所以大家可以尝试去把lambda调的

259
00:09:48,519 --> 00:09:49,159
更大一点

260
00:09:49,960 --> 00:09:52,799
那来通过调大lambda

261
00:09:52,799 --> 00:09:53,600
来看一下效果

262
00:09:53,600 --> 00:09:55,159
当你可以调的很大很大的话

263
00:09:55,159 --> 00:09:57,360
那就是基本上是欠离合了

264
00:09:57,480 --> 00:09:58,759
就是基本上就不会动

265
00:09:59,200 --> 00:09:59,679
OK

266
00:10:03,039 --> 00:10:03,360
好

267
00:10:03,360 --> 00:10:05,639
我们这个是讲的是从零开始实现

268
00:10:05,840 --> 00:10:07,200
最简单实现

269
00:10:07,399 --> 00:10:08,879
其实也挺容易的

270
00:10:09,080 --> 00:10:11,360
我们之前就不给大家过了

271
00:10:11,360 --> 00:10:13,200
其实唯一的区别是

272
00:10:13,360 --> 00:10:14,560
我们在

273
00:10:15,279 --> 00:10:17,159
我们唯一的区别是在这个地方

274
00:10:19,080 --> 00:10:20,600
就记不记得是说

275
00:10:20,600 --> 00:10:23,519
我们其实那个要处

276
00:10:23,840 --> 00:10:24,680
就是我们那个法

277
00:10:25,240 --> 00:10:27,800
既可以写在我们的目标函数里面

278
00:10:28,519 --> 00:10:30,920
也可以做在我们的训练算法里面

279
00:10:30,920 --> 00:10:31,399
说白了

280
00:10:31,399 --> 00:10:33,799
就是每一次把你的在更新之前

281
00:10:33,799 --> 00:10:35,199
把你当前的w

282
00:10:35,360 --> 00:10:37,639
成一个这样子的小的值就行了

283
00:10:38,559 --> 00:10:39,079
对吧

284
00:10:39,360 --> 00:10:39,720
所以

285
00:10:41,039 --> 00:10:42,759
如果这样子写还方便点

286
00:10:42,759 --> 00:10:44,919
这样子我在做自动求道的时候

287
00:10:44,919 --> 00:10:46,319
我还真不要少算一点

288
00:10:46,319 --> 00:10:48,720
所以对深度学习框架来讲

289
00:10:48,759 --> 00:10:50,919
它其实绝大部分框架

290
00:10:50,919 --> 00:10:52,720
包括pytorch在里面

291
00:10:52,759 --> 00:10:54,319
其实提供在你的

292
00:10:54,480 --> 00:10:55,960
SGD你的trainer

293
00:10:55,960 --> 00:10:57,279
就是你的优化算法里面

294
00:10:57,399 --> 00:10:58,600
基本上所有的优化算法

295
00:10:58,600 --> 00:10:59,879
都提供一个weight

296
00:10:59,879 --> 00:11:01,079
decay的一个选项

297
00:11:02,079 --> 00:11:02,679
就是

298
00:11:03,120 --> 00:11:04,279
weight decay就是我们之前

299
00:11:04,279 --> 00:11:05,360
那么大

300
00:11:05,360 --> 00:11:06,399
就是一个值

301
00:11:06,600 --> 00:11:08,799
所以你现在叫做wd叫weight decay

302
00:11:08,799 --> 00:11:10,439
但实际上是一一对应的

303
00:11:10,639 --> 00:11:12,319
所以别的值都别的地方都一样

304
00:11:12,319 --> 00:11:13,879
我就不仔细讲了

305
00:11:15,319 --> 00:11:17,279
可以看到一个效果是说

306
00:11:17,600 --> 00:11:19,639
你可以看到跟之前应该是差不多的

307
00:11:20,000 --> 00:11:20,799
等于0的时候

308
00:11:20,799 --> 00:11:21,919
你基本上就是

309
00:11:22,399 --> 00:11:24,000
L2的范数比较大

310
00:11:24,120 --> 00:11:25,960
然后之间的差距比较大

311
00:11:26,000 --> 00:11:27,079
当你等于3的时候

312
00:11:27,079 --> 00:11:28,159
你会往回拉一点

313
00:11:28,159 --> 00:11:29,120
来看一下

314
00:11:29,159 --> 00:11:30,559
放小一点看一下效果

315
00:11:30,880 --> 00:11:33,000
但是还是这一次

316
00:11:33,000 --> 00:11:34,160
因为随机的原因

317
00:11:34,160 --> 00:11:35,880
所以我们相对来说

318
00:11:35,880 --> 00:11:38,280
在这个点就开始差不多了

319
00:11:38,360 --> 00:11:40,080
然后接下来可以看到

320
00:11:40,080 --> 00:11:41,720
是说我们还是比较大的

321
00:11:41,760 --> 00:11:43,400
所以3其实是不够的

322
00:11:43,840 --> 00:11:46,160
大家具体什么样的值比较好

323
00:11:46,160 --> 00:11:47,720
我留给大家去试一下

324
00:11:47,720 --> 00:11:50,400
因为运气特别简单

325
00:11:50,400 --> 00:11:51,200
也特别快

326
00:11:51,200 --> 00:11:53,560
所以就给大家自己手动试一下了

327
00:11:54,400 --> 00:11:54,640
好

328
00:11:54,640 --> 00:11:58,560
这个就是我们的L2 penalty

329
00:11:58,560 --> 00:11:59,880
也叫做weight decay

330
00:11:59,879 --> 00:12:02,159
或者权重衰退的一个实现

331
00:12:02,519 --> 00:12:04,000
我们给大家讲到这里

332
00:12:04,159 --> 00:12:05,200
就是一个很简单

333
00:12:05,759 --> 00:12:06,919
如果手动做的话

334
00:12:06,960 --> 00:12:09,600
那就是在我的损失函数里面

335
00:12:09,600 --> 00:12:11,080
加上一个这样子的罚

336
00:12:11,240 --> 00:12:12,799
如果你用参数的话

337
00:12:12,840 --> 00:12:15,600
那么一个办法是说

338
00:12:15,600 --> 00:12:17,799
我们就在我们的trainer

339
00:12:17,799 --> 00:12:18,799
就是STD里面

340
00:12:18,799 --> 00:12:20,480
加入一个weight decay的选项

341
00:12:20,679 --> 00:12:21,360
通常来说

342
00:12:21,360 --> 00:12:22,159
大家会选一个

343
00:12:22,159 --> 00:12:23,000
比如说

344
00:12:24,000 --> 00:12:25,120
1-3

345
00:12:25,399 --> 00:12:26,559
就0.0001

346
00:12:26,879 --> 00:12:28,120
这个是一个常用的

347
00:12:28,159 --> 00:12:29,120
但你可以选小一点

348
00:12:29,120 --> 00:12:29,799
选大一点

349
00:12:29,879 --> 00:12:30,799
但不会选的很大

350
00:12:30,919 --> 00:12:33,519
不会选到1或者那么大的值

