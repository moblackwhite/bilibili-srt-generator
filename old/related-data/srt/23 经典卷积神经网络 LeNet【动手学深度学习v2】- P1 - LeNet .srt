1
00:00:00,000 --> 00:00:02,000
好 接下来是Nernet

2
00:00:02,000 --> 00:00:09,320
这也是卷积神纪网络里面最为有名的一个网络

3
00:00:09,320 --> 00:00:12,800
是80年代末

4
00:00:12,800 --> 00:00:16,120
Nernet就是杨乐坤

5
00:00:16,120 --> 00:00:19,559
他的LE提出来的

6
00:00:19,559 --> 00:00:21,760
他最早是干嘛呢

7
00:00:21,760 --> 00:00:28,359
最早是要去做一个手写数字识别的一个应用

8
00:00:29,359 --> 00:00:31,320
就是说大家都知道

9
00:00:31,320 --> 00:00:33,759
就是你写信的时候

10
00:00:33,759 --> 00:00:35,399
你得有个邮编

11
00:00:35,399 --> 00:00:35,960
对吧

12
00:00:35,960 --> 00:00:37,560
而且在美国

13
00:00:37,560 --> 00:00:39,640
美国我给别人付钱

14
00:00:39,640 --> 00:00:41,519
我就算现在去交学费

15
00:00:41,519 --> 00:00:42,879
我也是写个check

16
00:00:42,879 --> 00:00:44,200
就下面这个东西

17
00:00:44,200 --> 00:00:48,280
在美国各种支付不如中国厉害

18
00:00:48,280 --> 00:00:53,519
就是说不管是你上面的信还是下面的check

19
00:00:53,519 --> 00:00:56,719
然后就是说它里面信就会有一个邮政编码

20
00:00:57,359 --> 00:00:59,359
所以邮政局就是说

21
00:00:59,359 --> 00:01:02,079
他希望你能够信往里面放

22
00:01:02,079 --> 00:01:05,280
然后就把邮政编码给你扫出来

23
00:01:05,280 --> 00:01:07,039
然后就是把信就是扫出来之后

24
00:01:07,039 --> 00:01:09,519
他就会啪啪啪放到一个对应的一个地方

25
00:01:09,519 --> 00:01:12,560
不同的每个邮政编码世界各地都有

26
00:01:12,560 --> 00:01:14,640
邮政编码就是放到不一样的地方

27
00:01:14,640 --> 00:01:16,359
有不一样的车运出去

28
00:01:16,359 --> 00:01:19,400
这个是邮政局想干的事情

29
00:01:21,239 --> 00:01:26,120
在美国邮政局是一个非常大的一个机构

30
00:01:26,120 --> 00:01:27,760
但现在的地位越来越低了

31
00:01:29,200 --> 00:01:33,079
邮政局反正这个是算政府部门

32
00:01:33,079 --> 00:01:36,040
然后是一个特别重要的一个政府部门

33
00:01:36,040 --> 00:01:39,160
所以当年他确实是有很大的话语权的

34
00:01:39,320 --> 00:01:41,480
你想美国选举

35
00:01:41,480 --> 00:01:47,439
大家都是要记投票用东西记的

36
00:01:48,439 --> 00:01:51,080
然后反正这样

37
00:01:51,080 --> 00:01:53,439
现在我每天我们家门口

38
00:01:53,560 --> 00:01:56,159
每天我得去看我的信箱里面一堆信

39
00:01:56,159 --> 00:01:58,359
什么重要的事情不重要的事情

40
00:01:58,359 --> 00:02:01,599
广告都是用都是给写信给你

41
00:02:01,599 --> 00:02:05,439
所以我们家每天有的人要来一次

42
00:02:07,000 --> 00:02:09,120
接下来一个check也是一样的

43
00:02:09,120 --> 00:02:11,759
check是说现在美国都用check

44
00:02:11,759 --> 00:02:15,000
我每个月至少用几张

45
00:02:15,000 --> 00:02:17,639
所以check就是说他得识别数字

46
00:02:17,639 --> 00:02:21,199
就是说你在美国check的话

47
00:02:21,200 --> 00:02:24,160
你可以丢放到一个自动区块机里面

48
00:02:24,160 --> 00:02:25,240
就塞进去

49
00:02:25,240 --> 00:02:27,280
塞进去就是说他就帮你识别出来

50
00:02:27,280 --> 00:02:29,680
因为check很多人就是拿check去取钱的

51
00:02:29,960 --> 00:02:32,800
就是比如说我找你帮我干个事情

52
00:02:33,080 --> 00:02:34,720
干完之后我给你写个check给你

53
00:02:34,840 --> 00:02:35,880
你就拿这个check去

54
00:02:35,880 --> 00:02:38,800
放到自动区块机里面就能取到钱

55
00:02:39,640 --> 00:02:43,640
所以这一块肯定是要自动识别的

56
00:02:44,200 --> 00:02:45,920
所以这就是80年代

57
00:02:45,960 --> 00:02:48,400
当时候杨冷坤他们应该是ATM的T

58
00:02:48,439 --> 00:02:52,319
就是当年的一个非常大的电信局

59
00:02:52,680 --> 00:02:55,520
应该跟中国电信有点像

60
00:02:55,719 --> 00:02:56,920
但是他大很多

61
00:02:56,960 --> 00:02:58,360
当时他们做了一个应用

62
00:02:58,800 --> 00:03:01,480
然后这个模型确实在80年代末期

63
00:03:01,480 --> 00:03:03,599
在银行行业

64
00:03:03,840 --> 00:03:07,560
在邮递行业确实是一直被补书了的

65
00:03:07,759 --> 00:03:09,400
所以这也是为什么他很有名

66
00:03:11,400 --> 00:03:17,159
所以当然这个事情最后留下来的

67
00:03:17,159 --> 00:03:21,199
其实Nernet模型在很长一段时间

68
00:03:21,199 --> 00:03:23,520
大家都不觉得

69
00:03:23,520 --> 00:03:27,879
就是说因为神经网络有一段时间不流行

70
00:03:27,879 --> 00:03:30,439
Nernet大家其实知名度

71
00:03:30,439 --> 00:03:31,800
那时候没那么高

72
00:03:31,919 --> 00:03:35,639
但是知名度最高的就是M list

73
00:03:35,639 --> 00:03:37,000
这一个数据集

74
00:03:37,039 --> 00:03:42,359
就是说如果大家学机器学习的话

75
00:03:42,400 --> 00:03:44,319
很有可能是第一个数据集

76
00:03:44,319 --> 00:03:45,840
就是M list这个数据集

77
00:03:46,800 --> 00:03:49,960
这也是Nernet网络提出的时候

78
00:03:50,000 --> 00:03:51,200
附带的一个数据集

79
00:03:52,200 --> 00:03:53,719
所以可以看到这个数据集

80
00:03:53,719 --> 00:03:56,080
就是有5万个训练数据

81
00:03:56,400 --> 00:03:58,240
在当年是挺大的一个数据集

82
00:03:58,240 --> 00:03:59,480
80年代

83
00:03:59,480 --> 00:04:01,280
80年代末期

84
00:04:01,280 --> 00:04:02,599
内存就几兆了

85
00:04:03,400 --> 00:04:06,560
然后你有1万个测试数据集

86
00:04:07,000 --> 00:04:09,960
数据的图片大小是28x28

87
00:04:11,159 --> 00:04:13,319
可以看到它是一个非常简单数据集

88
00:04:13,319 --> 00:04:14,840
是因为他的数字

89
00:04:14,840 --> 00:04:16,240
他已经帮你scale好了

90
00:04:16,240 --> 00:04:17,560
就是放在正中间了

91
00:04:17,720 --> 00:04:19,160
然后它每个是一个手写的

92
00:04:19,160 --> 00:04:20,160
而且它是个灰度图

93
00:04:20,160 --> 00:04:21,439
基本上就是个黑白图

94
00:04:22,000 --> 00:04:24,079
它有10类就是0到9了

95
00:04:24,560 --> 00:04:26,959
就是说这是当年的大数据

96
00:04:27,959 --> 00:04:29,240
当年就是神经网络说

97
00:04:29,360 --> 00:04:31,319
我在这个大数据上效果很好

98
00:04:32,079 --> 00:04:34,759
比你们SVM什么东西

99
00:04:34,759 --> 00:04:36,040
要是别的效果要好

100
00:04:36,079 --> 00:04:38,519
因为我神经网络能跑动

101
00:04:38,680 --> 00:04:41,560
然后确实在大数据上

102
00:04:41,560 --> 00:04:42,280
我效果很好

103
00:04:42,280 --> 00:04:44,639
比你们什么random forest什么东西要好

104
00:04:45,440 --> 00:04:47,920
这就是当年的big data

105
00:04:49,760 --> 00:04:50,200
可以看一下

106
00:04:50,200 --> 00:04:53,240
这个是一个他当年的demo

107
00:04:53,880 --> 00:04:56,240
是你把信放进去

108
00:04:56,240 --> 00:04:57,680
他就慢慢给你扫

109
00:04:58,200 --> 00:04:59,480
扫扫

110
00:04:59,480 --> 00:05:00,680
然后就是说跟你说

111
00:05:00,680 --> 00:05:03,880
这个是当前是1 3什么的

112
00:05:04,200 --> 00:05:05,840
就是你把信放进去

113
00:05:05,840 --> 00:05:06,640
他就扫过去

114
00:05:06,640 --> 00:05:07,760
然后给你处置

115
00:05:08,040 --> 00:05:09,760
这个就是当年他们做的demo

116
00:05:12,520 --> 00:05:12,840
OK

117
00:05:12,840 --> 00:05:16,600
接下来我们来讲一下

118
00:05:16,600 --> 00:05:18,680
Nernet是个什么样的概念

119
00:05:18,680 --> 00:05:19,760
其实它就是你

120
00:05:19,760 --> 00:05:21,280
我们用一个很简单的图

121
00:05:21,280 --> 00:05:22,040
这一来表示

122
00:05:22,040 --> 00:05:25,440
我们之后会干嘛给大家实现一下

123
00:05:25,440 --> 00:05:26,560
我们就给一个简单图

124
00:05:27,920 --> 00:05:29,480
简单图的话可以看到是说

125
00:05:29,480 --> 00:05:31,480
它其实是一个挺简单的网络了

126
00:05:31,760 --> 00:05:35,440
他输入的是一个32x32的image

127
00:05:35,440 --> 00:05:37,160
就是说你加了一点piling在里面

128
00:05:37,160 --> 00:05:38,640
32x32

129
00:05:39,639 --> 00:05:42,159
然后接下来放到一个

130
00:05:42,159 --> 00:05:46,959
应该是一个5x5的卷积层里面

131
00:05:47,319 --> 00:05:50,079
然后它的输出通道数是6

132
00:05:50,759 --> 00:05:52,599
所以可以看到它的输出是一个

133
00:05:52,599 --> 00:05:54,879
这张图是当年他的paper里面来的

134
00:05:54,879 --> 00:05:56,319
我们就直接copy过来了

135
00:05:56,479 --> 00:06:00,959
所以它的输出是一个6个通道数

136
00:06:00,959 --> 00:06:03,439
宽高宽度是28的一个输出

137
00:06:04,319 --> 00:06:06,479
他的东西叫做feature map

138
00:06:07,200 --> 00:06:09,319
大家之前有问过什么是feature map

139
00:06:09,319 --> 00:06:11,759
就是说这一坨东西的输出就叫feature map

140
00:06:12,920 --> 00:06:14,720
接下来他用一个pooling层

141
00:06:15,360 --> 00:06:17,120
就是一个2x2的pooling层

142
00:06:17,240 --> 00:06:20,640
他就把28和28就变成了14x14了

143
00:06:20,840 --> 00:06:22,439
然后你的通道数没变对吧

144
00:06:23,120 --> 00:06:25,240
他接下来又是一个卷积层

145
00:06:25,920 --> 00:06:27,680
这个卷积层仍然是一个

146
00:06:27,680 --> 00:06:30,759
我觉得应该仍然是一个5x5的

147
00:06:30,759 --> 00:06:32,720
所以它的输入就变成10x10了

148
00:06:33,680 --> 00:06:35,120
然后通道数会增加

149
00:06:35,160 --> 00:06:36,720
变成6变到16

150
00:06:38,160 --> 00:06:39,959
之后再建一个pooling层

151
00:06:40,519 --> 00:06:41,800
那就是同样的pooling层

152
00:06:41,920 --> 00:06:43,519
就是高宽减半

153
00:06:43,519 --> 00:06:45,720
输入通道数和输出不变

154
00:06:46,399 --> 00:06:48,319
最后是一个16个通道数

155
00:06:48,319 --> 00:06:50,840
5x5的一个东西

156
00:06:51,240 --> 00:06:54,399
然后我再把它拉成一个向量

157
00:06:55,120 --> 00:06:57,399
输入到一个全连接层

158
00:06:57,759 --> 00:06:59,280
全连接是一个

159
00:06:59,319 --> 00:07:01,639
第一个全连接层是输出是120

160
00:07:02,040 --> 00:07:03,560
最后一个是64

161
00:07:04,560 --> 00:07:06,040
到最后还有一个

162
00:07:06,079 --> 00:07:08,280
他这个叫高4层

163
00:07:08,280 --> 00:07:10,199
就我们其实现在也不用那个层了

164
00:07:10,199 --> 00:07:11,600
所以基本上就是你可以认为

165
00:07:11,600 --> 00:07:13,000
最后也是个全连接层

166
00:07:13,160 --> 00:07:14,399
就是一个softmax

167
00:07:14,600 --> 00:07:16,840
就是一个10层的输出

168
00:07:17,000 --> 00:07:18,240
得到我的10个数字

169
00:07:18,399 --> 00:07:21,840
然后再做softmax得到一个概率

170
00:07:22,000 --> 00:07:22,360
OK

171
00:07:22,360 --> 00:07:24,399
这就是两个卷积层

172
00:07:24,399 --> 00:07:26,319
两个持花层

173
00:07:26,319 --> 00:07:28,360
再加两个全连接层

174
00:07:28,399 --> 00:07:29,720
最后一个输出层

175
00:07:29,720 --> 00:07:33,120
就是就会得到我的nerdnet这个函数

176
00:07:34,360 --> 00:07:34,639
好

177
00:07:34,639 --> 00:07:37,040
我们等会来代码来仔细讲这个函数

178
00:07:37,040 --> 00:07:39,240
我们先给大家总结一下

179
00:07:39,240 --> 00:07:39,879
就是

180
00:07:40,680 --> 00:07:44,920
Nerdnet是早期的成功的神经网络

181
00:07:44,920 --> 00:07:47,199
他也是奠定了说

182
00:07:47,199 --> 00:07:49,959
神经网络确实在图片上效果挺好的

183
00:07:50,639 --> 00:07:52,879
然后他先用卷积层

184
00:07:52,879 --> 00:07:55,040
来学习图片的空间信息

185
00:07:55,399 --> 00:07:58,199
通过持花层来降低图片敏感度

186
00:07:58,920 --> 00:08:00,519
最后使用全连接层

187
00:08:00,560 --> 00:08:03,560
来转换到类别的空间

188
00:08:03,560 --> 00:08:04,439
得到10类

189
00:08:04,639 --> 00:08:07,319
这基本就是两个卷积层

190
00:08:07,319 --> 00:08:10,359
再加一个多层感知器

191
00:08:10,359 --> 00:08:11,519
得到我们的

192
00:08:12,959 --> 00:08:16,120
整个的一个从图片到类别的映射

193
00:08:16,560 --> 00:08:19,879
当然这个思想在未来很多年

194
00:08:19,879 --> 00:08:21,000
都一直是流行的

195
00:08:21,079 --> 00:08:23,560
当然现在已经完全不一样了

196
00:08:23,799 --> 00:08:27,120
但是他确实影响了之后的几个

197
00:08:27,199 --> 00:08:28,120
比如说AlexNet

198
00:08:28,120 --> 00:08:30,800
最著名的深度神经网络

