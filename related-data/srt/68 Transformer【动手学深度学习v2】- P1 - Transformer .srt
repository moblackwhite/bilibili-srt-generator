1
00:00:00,000 --> 00:00:03,640
好,我们终于轮到讲我们的变形金刚了

2
00:00:03,640 --> 00:00:08,919
大家很多同学在留言说什么是讲变形金刚

3
00:00:08,919 --> 00:00:12,200
我们今天终于等到了Transformers

4
00:00:12,200 --> 00:00:14,880
大家知道Transformers

5
00:00:14,880 --> 00:00:18,760
它就是一个在英文中变形金刚

6
00:00:18,760 --> 00:00:24,240
就是擎天柱他们那一套东西的名字叫Transformers

7
00:00:24,240 --> 00:00:25,560
所以从Transformers开始

8
00:00:25,560 --> 00:00:28,120
大家对取名字取得越来越fancy了

9
00:00:28,519 --> 00:00:30,440
就之前我们说取名字ResNet

10
00:00:30,440 --> 00:00:31,039
RN

11
00:00:31,039 --> 00:00:32,280
就Nonet

12
00:00:32,280 --> 00:00:33,120
就都很简单

13
00:00:33,120 --> 00:00:33,560
对吧

14
00:00:33,640 --> 00:00:36,880
就是很朴实无华的XXNet

15
00:00:37,079 --> 00:00:38,480
要么就是人民Net

16
00:00:38,520 --> 00:00:40,480
要么就是什么东西

17
00:00:40,600 --> 00:00:44,320
或者VGG甚至是说是他们组的名字

18
00:00:46,079 --> 00:00:47,400
但是从Transformers开始

19
00:00:47,640 --> 00:00:49,000
就Transformers这个paper

20
00:00:49,120 --> 00:00:51,000
它的title反正就很fancy

21
00:00:51,000 --> 00:00:52,680
就是说从Transformers开始

22
00:00:52,680 --> 00:00:54,880
大家走的越来越娱乐了

23
00:00:55,320 --> 00:00:57,000
它的是机器人

24
00:00:57,039 --> 00:00:57,880
等到BIRTH的话

25
00:00:57,880 --> 00:00:59,719
就BIRTH是芝麻街那一套东西

26
00:00:59,719 --> 00:01:01,280
就是整个BIRTH后面那一系列

27
00:01:01,280 --> 00:01:02,359
都是芝麻街里面的

28
00:01:02,359 --> 00:01:02,880
UpperT

29
00:01:02,880 --> 00:01:03,640
什么什么

30
00:01:03,640 --> 00:01:06,799
反正那一套东西

31
00:01:07,000 --> 00:01:09,159
所以今天我刚好在做slides的时候

32
00:01:09,159 --> 00:01:12,799
我的老大小孩就四岁过来

33
00:01:12,799 --> 00:01:13,240
同学

34
00:01:13,240 --> 00:01:14,879
这不是芝麻街吗

35
00:01:14,879 --> 00:01:15,280
这个东西

36
00:01:15,280 --> 00:01:17,039
看来我在做BIRTH的时候

37
00:01:17,039 --> 00:01:18,039
他说这芝麻街吗

38
00:01:18,039 --> 00:01:18,680
不是

39
00:01:18,680 --> 00:01:21,200
所以就是说从Transformers开始

40
00:01:21,200 --> 00:01:23,319
我们的名字变得越来越fancy了

41
00:01:24,120 --> 00:01:27,760
所以你中文叫他变形金刚也没事

42
00:01:27,960 --> 00:01:30,400
然后当然我们还是用Transformers这个名字

43
00:01:32,280 --> 00:01:34,360
所以Transformers这个架构

44
00:01:34,560 --> 00:01:36,600
其实挺简单的

45
00:01:36,600 --> 00:01:39,040
就是说你可以一张slides就能讲完

46
00:01:40,440 --> 00:01:46,360
它是一个基于编码器解码器的架构

47
00:01:46,400 --> 00:01:49,120
就跟刚之前我们讲过的seq2seq

48
00:01:49,120 --> 00:01:50,280
是一回事

49
00:01:50,920 --> 00:01:53,599
他用一个编码器解码器的架构

50
00:01:53,599 --> 00:01:55,280
来处理一个序列队

51
00:01:55,960 --> 00:01:59,920
但是它跟使用注意力的seq2seq

52
00:01:59,920 --> 00:02:01,920
或者说原始的seq2seq

53
00:02:01,960 --> 00:02:03,359
它的不同的在于是说

54
00:02:03,480 --> 00:02:07,560
Transformers它是一个纯基于注意力的

55
00:02:07,560 --> 00:02:08,680
或者具体来讲

56
00:02:08,680 --> 00:02:13,039
它是一个纯基于self attention的架构

57
00:02:13,439 --> 00:02:14,479
就它里面没有RN

58
00:02:15,159 --> 00:02:17,159
就是说我们知道seq2seq加了

59
00:02:17,159 --> 00:02:17,479
attention

60
00:02:17,479 --> 00:02:18,840
我们在上周有讲过

61
00:02:18,840 --> 00:02:20,280
就加了一个tension进来

62
00:02:20,840 --> 00:02:22,599
但现在是说我把里面的RN

63
00:02:22,840 --> 00:02:24,719
那个东西全部换成一个Transformer

64
00:02:24,719 --> 00:02:26,280
就纯Transformer架构

65
00:02:27,599 --> 00:02:29,560
然后可以看一下它的架构是这样子

66
00:02:29,560 --> 00:02:32,360
就是说你有一个source

67
00:02:32,560 --> 00:02:34,319
就跟还是一个encode decode

68
00:02:34,480 --> 00:02:35,680
跟我们之前的图是一样的

69
00:02:35,680 --> 00:02:36,560
就是说你的

70
00:02:36,599 --> 00:02:38,280
比如说你的英文句子过来

71
00:02:38,280 --> 00:02:40,319
你要法语句子在这个地方

72
00:02:40,560 --> 00:02:42,960
然后你先进来进入embedding

73
00:02:43,000 --> 00:02:44,319
embedding之后

74
00:02:44,360 --> 00:02:45,560
因为它没有未知信息

75
00:02:45,719 --> 00:02:47,680
就后面这一块的不看未知信息的

76
00:02:47,760 --> 00:02:49,280
所以我们把刚刚那个P

77
00:02:49,480 --> 00:02:50,520
那个position embedding

78
00:02:50,520 --> 00:02:52,040
就是未知编码加进来

79
00:02:52,800 --> 00:02:53,520
然后进入

80
00:02:53,520 --> 00:02:55,360
这个就是一个self attention

81
00:02:56,319 --> 00:02:57,439
然后最后有加一个东西

82
00:02:57,439 --> 00:02:58,560
然后再有一个东西

83
00:02:58,599 --> 00:02:59,520
再加一个东西

84
00:02:59,680 --> 00:03:01,360
然后它把它持续N次

85
00:03:01,400 --> 00:03:03,760
所以这个东西叫做一个Transformer

86
00:03:03,760 --> 00:03:04,200
quay

87
00:03:05,280 --> 00:03:07,600
你可认为或者我们讲resnet

88
00:03:07,600 --> 00:03:08,560
也有quay一样

89
00:03:08,719 --> 00:03:10,439
然后它里面有点架构

90
00:03:10,439 --> 00:03:11,680
我们等会来仔细讲

91
00:03:11,680 --> 00:03:13,520
但是这个东西就是self attention

92
00:03:13,520 --> 00:03:14,159
这个东西

93
00:03:14,400 --> 00:03:16,120
这个东西是一个叫做

94
00:03:16,520 --> 00:03:17,319
名字很fancy

95
00:03:17,479 --> 00:03:19,759
其实就是个全连接

96
00:03:20,079 --> 00:03:22,319
然后它的信息传递

97
00:03:22,439 --> 00:03:23,079
跟

98
00:03:23,359 --> 00:03:24,519
也是说

99
00:03:25,079 --> 00:03:25,959
这个地方过来

100
00:03:26,159 --> 00:03:27,120
等我们仔细讲

101
00:03:27,519 --> 00:03:29,280
就是说每个细节是什么样子

102
00:03:29,439 --> 00:03:31,039
但是你可以看到

103
00:03:31,159 --> 00:03:32,719
跟我们之前的

104
00:03:33,079 --> 00:03:34,680
encode decode的架构

105
00:03:34,799 --> 00:03:36,639
就sig to sig没本质太多区别

106
00:03:37,120 --> 00:03:38,359
就是说架构都是一样的

107
00:03:38,359 --> 00:03:39,599
然后它也N层

108
00:03:39,799 --> 00:03:41,599
就是说之前你每一个层是RNN

109
00:03:41,840 --> 00:03:44,680
现在我一个层叫做我的transformerquay

110
00:03:44,799 --> 00:03:46,319
然后信息传递也是一样

111
00:03:46,359 --> 00:03:47,879
就最后一层的输出

112
00:03:48,280 --> 00:03:49,079
作为

113
00:03:49,680 --> 00:03:51,560
之后的层的一些输入

114
00:03:51,560 --> 00:03:54,000
来完成信息的传递

115
00:03:54,879 --> 00:03:55,239
Ok

116
00:03:55,239 --> 00:03:56,639
接下来我们来看一下说

117
00:03:56,680 --> 00:03:59,959
具体它是怎么样的细节

118
00:04:01,400 --> 00:04:03,199
首先一个东西叫做

119
00:04:03,240 --> 00:04:05,000
Multi head attention

120
00:04:05,000 --> 00:04:05,919
叫做多

121
00:04:05,919 --> 00:04:08,039
我们翻译成

122
00:04:08,240 --> 00:04:09,959
多头注意力

123
00:04:10,400 --> 00:04:11,599
就是说这两个东西

124
00:04:12,120 --> 00:04:12,879
就在你

125
00:04:13,799 --> 00:04:15,400
里面就这两个块

126
00:04:15,439 --> 00:04:16,519
叫做多头

127
00:04:17,439 --> 00:04:18,480
多头的意思是什么意思

128
00:04:18,639 --> 00:04:19,360
就是说

129
00:04:20,000 --> 00:04:21,840
对同一个key value query

130
00:04:22,040 --> 00:04:24,240
我们可能要抽取不同的信息

131
00:04:24,240 --> 00:04:25,519
就是说比如说

132
00:04:26,040 --> 00:04:27,240
我们知道我们的tension

133
00:04:27,240 --> 00:04:28,720
就是说通过一个softmax

134
00:04:28,720 --> 00:04:30,280
通过一个可学习的W

135
00:04:30,480 --> 00:04:32,519
通过一个softmax来做事情

136
00:04:33,000 --> 00:04:35,360
但是可学习的就是一个全连接

137
00:04:35,439 --> 00:04:36,519
说白了一个全连接

138
00:04:36,800 --> 00:04:38,639
全连接就是我们希望

139
00:04:39,639 --> 00:04:41,600
我们真的需要有不同的版本

140
00:04:41,600 --> 00:04:42,600
就是说我可能说

141
00:04:42,600 --> 00:04:45,000
我想做N个这样的注意力

142
00:04:45,000 --> 00:04:45,480
机制

143
00:04:46,240 --> 00:04:47,840
然后有一些注意力

144
00:04:48,280 --> 00:04:49,800
才能是能够

145
00:04:49,800 --> 00:04:52,000
比如说抽比较长的信息

146
00:04:52,000 --> 00:04:53,200
有些注意力

147
00:04:53,200 --> 00:04:55,160
可以比如说抽比较短的信息

148
00:04:55,520 --> 00:04:56,480
有点像什么

149
00:04:56,520 --> 00:04:59,480
有点像我们做卷机的时候

150
00:04:59,840 --> 00:05:00,600
多通道

151
00:05:00,800 --> 00:05:02,360
多输出通道

152
00:05:03,560 --> 00:05:05,280
但是你这个地方已经有多通道了

153
00:05:05,280 --> 00:05:06,600
因为你的输出dimension

154
00:05:06,600 --> 00:05:07,520
D是一个通道

155
00:05:07,720 --> 00:05:08,640
理论上来讲

156
00:05:08,800 --> 00:05:10,560
所以你就是

157
00:05:10,560 --> 00:05:14,639
所以你的做法叫multi head

158
00:05:14,639 --> 00:05:16,240
它就做N个不同的

159
00:05:16,920 --> 00:05:18,000
具体来看怎么回事

160
00:05:19,720 --> 00:05:20,399
具体来看

161
00:05:20,399 --> 00:05:22,959
就是说我给一个query key value

162
00:05:23,840 --> 00:05:26,000
假设我这里画了两个头

163
00:05:26,000 --> 00:05:27,360
就两个头的话

164
00:05:27,360 --> 00:05:29,279
就是当你中间可以画人影多头

165
00:05:29,360 --> 00:05:29,959
都没关系

166
00:05:30,480 --> 00:05:31,800
它把你的每一个

167
00:05:31,800 --> 00:05:32,560
反正你的query

168
00:05:32,639 --> 00:05:33,199
key和value

169
00:05:33,199 --> 00:05:34,680
都是一个常规D的一个向量

170
00:05:35,319 --> 00:05:37,120
它通过一个全连接层

171
00:05:37,399 --> 00:05:40,360
把你比如映射到一个低一点的dimension

172
00:05:41,560 --> 00:05:44,920
然后就进入了我们刚刚自主义的

173
00:05:44,920 --> 00:05:45,720
multi head

174
00:05:45,720 --> 00:05:46,879
就self attention

175
00:05:46,879 --> 00:05:49,040
或者你是别的tension都没关系

176
00:05:49,160 --> 00:05:50,319
就是说多头注意力

177
00:05:50,319 --> 00:05:52,319
跟self attention是不一样

178
00:05:52,319 --> 00:05:53,720
self attention是说你query

179
00:05:53,720 --> 00:05:55,120
key和value是怎么选的

180
00:05:55,240 --> 00:05:57,279
多头是说我可以做多个tension

181
00:05:57,519 --> 00:05:58,600
然后它进去之后

182
00:05:58,920 --> 00:05:59,840
它当然会有输出

183
00:05:59,840 --> 00:06:00,279
对吧

184
00:06:01,279 --> 00:06:03,360
所以它对每一个tension

185
00:06:03,519 --> 00:06:06,240
就是说这之间的weight是不一样的

186
00:06:06,319 --> 00:06:08,160
它就是N个tension这个东西

187
00:06:08,840 --> 00:06:10,040
就是N个全重

188
00:06:10,120 --> 00:06:12,080
然后对它每一个都做了一个

189
00:06:12,080 --> 00:06:13,680
对应的全连接

190
00:06:14,240 --> 00:06:15,000
然后进去

191
00:06:15,520 --> 00:06:16,439
进去之后

192
00:06:16,439 --> 00:06:17,680
做tension出来

193
00:06:18,080 --> 00:06:18,680
出来之后

194
00:06:18,800 --> 00:06:20,960
它把每一个东西给你concate起来

195
00:06:20,960 --> 00:06:23,240
在你的特征维度concate起来

196
00:06:24,800 --> 00:06:25,200
OK

197
00:06:25,640 --> 00:06:26,439
concate起来之后

198
00:06:26,439 --> 00:06:27,600
再通过一个全连接

199
00:06:27,600 --> 00:06:32,520
拿到我要的输出的长度

200
00:06:32,520 --> 00:06:33,480
就是说维度

201
00:06:34,360 --> 00:06:34,920
这样子的话

202
00:06:34,920 --> 00:06:36,240
我就说不管你再做多少

203
00:06:36,439 --> 00:06:39,000
我知道最后全连接层的输出的

204
00:06:39,000 --> 00:06:40,399
维度是长于某个样子

205
00:06:40,639 --> 00:06:42,560
我的最后的输出的维度是这样子

206
00:06:43,199 --> 00:06:43,519
OK

207
00:06:43,519 --> 00:06:45,920
这就是multi head tension

208
00:06:46,160 --> 00:06:47,360
说白了就是说

209
00:06:48,120 --> 00:06:49,360
它就是用了

210
00:06:49,360 --> 00:06:50,560
做了N个东西在里面

211
00:06:50,720 --> 00:06:51,160
做了

212
00:06:51,600 --> 00:06:52,279
比如说

213
00:06:53,800 --> 00:06:55,199
N个版本在这个地方

214
00:06:55,199 --> 00:06:56,759
然后加了一堆全连接层

215
00:06:56,759 --> 00:06:58,639
但中间这个东西没有发生变化的

216
00:06:59,279 --> 00:06:59,920
然后我们可以看一下

217
00:06:59,920 --> 00:07:01,199
数学上长什么样子

218
00:07:02,240 --> 00:07:03,319
数学上来说

219
00:07:03,519 --> 00:07:05,480
假设你的query是一个

220
00:07:05,480 --> 00:07:06,759
常为d的一个东西

221
00:07:07,560 --> 00:07:08,240
dq

222
00:07:08,240 --> 00:07:09,199
k是dk

223
00:07:09,199 --> 00:07:10,920
然后你的v是一个dv的东西

224
00:07:12,600 --> 00:07:13,560
那么你的头

225
00:07:13,560 --> 00:07:15,759
i就是你的dI个tension

226
00:07:15,759 --> 00:07:16,240
那个东西

227
00:07:16,800 --> 00:07:18,759
它的可学习参数有一堆

228
00:07:20,319 --> 00:07:21,960
就是说这是额外的可学习参数

229
00:07:22,079 --> 00:07:23,519
就是它自己本身是有的

230
00:07:23,519 --> 00:07:25,079
本身有的先不管

231
00:07:25,319 --> 00:07:26,519
就是说它额外加的

232
00:07:26,519 --> 00:07:27,160
就是说

233
00:07:27,160 --> 00:07:28,800
对于query的投影

234
00:07:28,800 --> 00:07:29,560
对于key的投影

235
00:07:29,560 --> 00:07:30,960
对于w的投影

236
00:07:30,960 --> 00:07:34,759
就把你dq投影到一个pq上面

237
00:07:35,040 --> 00:07:36,280
就通常会减少

238
00:07:36,280 --> 00:07:38,560
然后你的输出

239
00:07:38,560 --> 00:07:40,280
就是说你的头

240
00:07:40,280 --> 00:07:40,840
i的输出

241
00:07:40,840 --> 00:07:42,240
就是说你就是tension

242
00:07:42,360 --> 00:07:43,080
就那个f

243
00:07:43,520 --> 00:07:44,640
就里面是怎么用的

244
00:07:44,640 --> 00:07:45,960
我们说用什么

245
00:07:46,240 --> 00:07:47,600
我们讲过两个tension版本

246
00:07:47,600 --> 00:07:48,440
用什么先不管

247
00:07:48,440 --> 00:07:49,920
就是反正算在f里面

248
00:07:50,160 --> 00:07:51,920
它输出来是一个pv的

249
00:07:53,160 --> 00:07:56,600
然后所有的这些hi空开了起来

250
00:07:56,960 --> 00:07:57,840
空开了起来

251
00:07:57,840 --> 00:07:59,360
然后再丢到一个w

252
00:07:59,360 --> 00:08:00,760
就输出了w里面

253
00:08:01,360 --> 00:08:02,720
这样子就会拿到

254
00:08:02,720 --> 00:08:05,640
我要的最重要的po的输出

255
00:08:06,600 --> 00:08:07,240
Ok

256
00:08:07,240 --> 00:08:08,160
所以就是说

257
00:08:08,160 --> 00:08:08,880
等于就是说

258
00:08:08,880 --> 00:08:12,480
在这个f是我们的正常的

259
00:08:12,480 --> 00:08:13,600
tension的情况下

260
00:08:13,640 --> 00:08:15,560
再加了这一堆

261
00:08:15,560 --> 00:08:16,920
两一个两个三个

262
00:08:17,280 --> 00:08:18,160
三个

263
00:08:18,280 --> 00:08:19,400
就是你有多少个头

264
00:08:19,400 --> 00:08:21,080
就有三个乘以多少个头

265
00:08:21,240 --> 00:08:23,360
就是说头如果是n的话

266
00:08:23,360 --> 00:08:25,320
就是3n个w

267
00:08:25,440 --> 00:08:27,400
然后最后输出还有一个w

268
00:08:28,040 --> 00:08:28,440
Ok

269
00:08:28,440 --> 00:08:30,160
这就是多头的注意力

270
00:08:30,400 --> 00:08:31,760
等会我们看它怎么实现

271
00:08:33,480 --> 00:08:34,800
另外一个是说

272
00:08:35,599 --> 00:08:37,959
里面还有一个带有野马的多头

273
00:08:37,959 --> 00:08:38,559
注意力

274
00:08:39,159 --> 00:08:40,199
带野马是什么意思

275
00:08:40,359 --> 00:08:42,719
就是说当你在解码器的时候

276
00:08:42,799 --> 00:08:45,199
因为我们知道 attention

277
00:08:45,319 --> 00:08:47,799
这个东西是没有实际信息的

278
00:08:47,919 --> 00:08:48,839
就是说你输出

279
00:08:48,839 --> 00:08:50,879
你在中间第r一个的输出的时候

280
00:08:50,879 --> 00:08:52,399
可以看到后面所有东西

281
00:08:53,919 --> 00:08:56,240
在编码是没关系的

282
00:08:56,359 --> 00:08:57,799
但解码不行

283
00:08:58,199 --> 00:08:59,000
解码东西

284
00:08:59,000 --> 00:09:00,759
你不应该去考虑

285
00:09:00,759 --> 00:09:02,839
该元素本身

286
00:09:02,879 --> 00:09:04,879
或者和它之后的元素

287
00:09:06,440 --> 00:09:07,920
就是说这个东西怎么办

288
00:09:08,399 --> 00:09:09,600
就是我们有讲过

289
00:09:09,639 --> 00:09:10,280
通过野马

290
00:09:10,320 --> 00:09:12,600
就是valid length来实现

291
00:09:13,080 --> 00:09:13,840
就是说

292
00:09:13,879 --> 00:09:16,399
当你计算xi的输出的时候

293
00:09:16,960 --> 00:09:19,840
就假设你当前的序列长度是xi

294
00:09:20,000 --> 00:09:21,519
就把你的valid length

295
00:09:21,519 --> 00:09:21,960
长度

296
00:09:22,000 --> 00:09:23,759
就合法的长度设成i

297
00:09:23,920 --> 00:09:25,320
就把后面所有东西

298
00:09:25,840 --> 00:09:27,120
把它演掉

299
00:09:27,120 --> 00:09:29,160
就是在你算softmax的时候

300
00:09:29,680 --> 00:09:31,080
在你算softmax的时候

301
00:09:31,080 --> 00:09:33,240
就不会去算它的权重

302
00:09:33,240 --> 00:09:33,600
这样子

303
00:09:33,600 --> 00:09:36,400
就不会对后面的key value给权重

304
00:09:36,520 --> 00:09:36,920
这样子

305
00:09:36,920 --> 00:09:40,560
就会当假装我算xi的时候

306
00:09:40,680 --> 00:09:42,600
不会去看到后面的输入的情况

307
00:09:42,639 --> 00:09:43,560
因为你输入的时候

308
00:09:43,560 --> 00:09:44,480
你得一次性给我

309
00:09:44,480 --> 00:09:44,960
对吧

310
00:09:45,680 --> 00:09:47,000
所以这个就是说

311
00:09:47,240 --> 00:09:48,360
为什么他这个地方

312
00:09:48,360 --> 00:09:49,759
有个带野马的东西

313
00:09:49,759 --> 00:09:50,520
在这个地方

314
00:09:53,639 --> 00:09:54,200
另外一个

315
00:09:54,520 --> 00:09:54,960
另外一个

316
00:09:54,960 --> 00:09:56,080
就是我们讲这三个

317
00:09:56,320 --> 00:09:57,320
一个两个三个

318
00:09:57,560 --> 00:10:00,240
这三个都是multi head self attention

319
00:10:00,279 --> 00:10:01,720
就其中一个带野马的

320
00:10:02,279 --> 00:10:04,320
另外还有两个难快快是什么

321
00:10:04,840 --> 00:10:05,519
它的难快快

322
00:10:05,519 --> 00:10:06,360
就是说

323
00:10:06,799 --> 00:10:08,320
它其实就是一个全连接层

324
00:10:08,600 --> 00:10:12,080
叫做基于位置的潜窥网络

325
00:10:12,080 --> 00:10:14,200
就是说叫feedforward

326
00:10:14,519 --> 00:10:15,840
说白了就是一个全连接

327
00:10:17,600 --> 00:10:18,720
怎么什么意思

328
00:10:18,799 --> 00:10:19,519
就是说

329
00:10:19,519 --> 00:10:22,759
他说我要在之后再加一个全连接

330
00:10:23,000 --> 00:10:24,320
全连接正常加的是吗

331
00:10:24,320 --> 00:10:25,720
现在我的维度是有问题的

332
00:10:25,799 --> 00:10:27,200
现在我的维度是三维

333
00:10:27,720 --> 00:10:29,039
b batch size

334
00:10:29,240 --> 00:10:30,919
n序列的长度

335
00:10:31,159 --> 00:10:32,839
d你的dimension

336
00:10:34,559 --> 00:10:35,799
之前我们做怎么做

337
00:10:35,799 --> 00:10:37,879
之前我们在卷机的时候

338
00:10:37,879 --> 00:10:38,159
的时候

339
00:10:38,159 --> 00:10:39,519
就把n d换成一位

340
00:10:39,559 --> 00:10:42,519
就变成一个b乘以n乘以d的一个东西

341
00:10:42,799 --> 00:10:43,799
然后丢进去

342
00:10:44,199 --> 00:10:45,199
但现在不想

343
00:10:45,199 --> 00:10:46,879
为什么是n会变

344
00:10:48,079 --> 00:10:50,439
n这个东西是序列的长度

345
00:10:50,439 --> 00:10:51,879
理论上跟我模型无关

346
00:10:51,879 --> 00:10:52,639
模型我可以

347
00:10:52,639 --> 00:10:54,559
应该可以处理任意的长度

348
00:10:54,599 --> 00:10:57,000
所以你不能把这个东西做成一个特征

349
00:10:57,039 --> 00:10:57,799
所以他怎么做

350
00:10:58,120 --> 00:10:59,160
他就是说

351
00:10:59,200 --> 00:11:01,640
对每一个序列中的每个元素

352
00:11:01,720 --> 00:11:04,680
我给你做一个全连接

353
00:11:06,080 --> 00:11:06,520
说白了

354
00:11:06,520 --> 00:11:08,040
就是说把一个序列里面

355
00:11:08,040 --> 00:11:08,800
每一个

356
00:11:09,920 --> 00:11:11,600
x i当一个样本

357
00:11:12,480 --> 00:11:13,600
这个东西我们也看过很多次

358
00:11:13,720 --> 00:11:14,760
就是一乘一的卷机

359
00:11:14,760 --> 00:11:15,320
就干这个事情

360
00:11:15,320 --> 00:11:15,800
对吧

361
00:11:16,040 --> 00:11:17,440
就一乘一的卷机

362
00:11:18,480 --> 00:11:19,640
所以他就是说

363
00:11:19,640 --> 00:11:22,920
把这个东西换成一个b乘以n和d的东西

364
00:11:22,960 --> 00:11:24,480
然后把它转成这个样子

365
00:11:24,480 --> 00:11:24,720
之后

366
00:11:24,800 --> 00:11:26,160
作为两个全连接层

367
00:11:26,559 --> 00:11:28,159
输出了再换回来

368
00:11:28,399 --> 00:11:28,879
对吧

369
00:11:29,399 --> 00:11:30,039
说白了

370
00:11:30,039 --> 00:11:31,759
它就等价于两层

371
00:11:31,799 --> 00:11:34,079
和窗口为一的一个

372
00:11:34,079 --> 00:11:35,439
一维卷机层

373
00:11:36,319 --> 00:11:38,120
就是一乘一的卷机层

374
00:11:38,199 --> 00:11:39,559
说白了就等价那个东西

375
00:11:40,159 --> 00:11:40,719
我们也讲过

376
00:11:40,719 --> 00:11:41,519
一乘一的卷机层

377
00:11:41,519 --> 00:11:42,480
等价于全连接

378
00:11:42,519 --> 00:11:44,439
所以这个东西等价于一个全连接

379
00:11:44,759 --> 00:11:45,240
OK

380
00:11:45,240 --> 00:11:46,240
所以这就是说

381
00:11:46,279 --> 00:11:48,600
后面那个就说ffn

382
00:11:48,600 --> 00:11:50,279
ffn是干这个事情

383
00:11:50,600 --> 00:11:52,000
实际上就是个全连接

384
00:11:52,159 --> 00:11:53,600
大家不要被名字吓到

385
00:11:56,159 --> 00:11:57,439
然后还有个什么东西

386
00:11:57,559 --> 00:11:58,839
还有个add和non

387
00:11:58,839 --> 00:11:59,839
add就好理解

388
00:11:59,839 --> 00:12:01,519
add就是一个resnet

389
00:12:01,519 --> 00:12:02,519
residual block

390
00:12:03,039 --> 00:12:04,919
就是说他说一个东西进来

391
00:12:05,199 --> 00:12:06,079
进到这个地方

392
00:12:06,199 --> 00:12:08,279
这个block可以是这一块

393
00:12:08,279 --> 00:12:09,120
也可以是这一块

394
00:12:09,559 --> 00:12:10,600
然后进来之后

395
00:12:10,600 --> 00:12:12,120
我加一个residual connection

396
00:12:12,360 --> 00:12:13,439
这个很正常

397
00:12:13,439 --> 00:12:13,839
对吧

398
00:12:14,039 --> 00:12:15,480
就是说这样子

399
00:12:15,480 --> 00:12:16,559
我就可以做得很深

400
00:12:17,679 --> 00:12:18,319
接下来

401
00:12:19,079 --> 00:12:19,879
接下来他说

402
00:12:19,879 --> 00:12:21,360
我要加一个normalization在里面

403
00:12:21,360 --> 00:12:22,919
这样子我能够训练的

404
00:12:23,199 --> 00:12:24,319
做比较长的

405
00:12:24,319 --> 00:12:25,639
比较深的网络训练

406
00:12:25,639 --> 00:12:26,519
训练起来比较好

407
00:12:27,399 --> 00:12:28,679
但是他这个地方

408
00:12:28,679 --> 00:12:30,480
不能用batch normalization

409
00:12:31,199 --> 00:12:32,279
为什么是因为

410
00:12:32,519 --> 00:12:34,600
batch normalization是对

411
00:12:36,319 --> 00:12:38,600
每一个通道

412
00:12:38,679 --> 00:12:40,240
或者每一个特征的向量

413
00:12:40,240 --> 00:12:41,199
做规划

414
00:12:42,000 --> 00:12:42,799
特征是什么

415
00:12:42,799 --> 00:12:44,439
特征就是你每个序列

416
00:12:44,439 --> 00:12:45,639
里面的d中的e位

417
00:12:47,000 --> 00:12:49,039
所以他做规划的时候

418
00:12:49,039 --> 00:12:50,399
说把你整个里面的东西

419
00:12:50,600 --> 00:12:55,080
就是方差变1均值变0

420
00:12:55,480 --> 00:12:56,680
这地方的问题是什么

421
00:12:56,960 --> 00:12:58,000
问题是说

422
00:12:58,360 --> 00:13:00,000
我做NOP的时候

423
00:13:00,160 --> 00:13:02,440
如果我选的是d的话

424
00:13:03,160 --> 00:13:03,759
e位的话

425
00:13:03,759 --> 00:13:05,639
那么你的输入给batch normalization

426
00:13:05,639 --> 00:13:07,520
是一个n乘以一个b的东西

427
00:13:07,520 --> 00:13:09,080
b是你的p站大小的话

428
00:13:09,120 --> 00:13:11,240
n是你的序列的长度

429
00:13:11,720 --> 00:13:14,320
这个长度会变

430
00:13:14,520 --> 00:13:15,400
这个长度

431
00:13:15,520 --> 00:13:17,360
根据你的序列长度会变

432
00:13:17,400 --> 00:13:18,160
所以就是说

433
00:13:18,160 --> 00:13:19,600
每次给batch normalization

434
00:13:19,600 --> 00:13:20,400
输入一个东西的

435
00:13:20,400 --> 00:13:21,320
这个东西的

436
00:13:22,840 --> 00:13:23,480
元素的

437
00:13:23,480 --> 00:13:25,279
那里面长度会非常变化

438
00:13:25,279 --> 00:13:27,159
所以导致说它会不稳定

439
00:13:27,560 --> 00:13:29,639
就导致你的做prediction的时候

440
00:13:29,840 --> 00:13:30,720
或者做什么时候

441
00:13:30,720 --> 00:13:31,960
你的长度

442
00:13:31,960 --> 00:13:32,440
就是说

443
00:13:32,440 --> 00:13:33,600
训练和预测的长度

444
00:13:33,600 --> 00:13:35,120
就本来就不一样

445
00:13:35,120 --> 00:13:35,360
对吧

446
00:13:35,360 --> 00:13:36,279
预测它是

447
00:13:36,399 --> 00:13:38,360
慢慢的长度会变得越来越长

448
00:13:39,039 --> 00:13:40,399
所以就不适合做

449
00:13:40,399 --> 00:13:41,639
你的batch normalization

450
00:13:42,600 --> 00:13:44,000
反过来做的就是说

451
00:13:44,039 --> 00:13:47,840
我对于每个样本的元素做规划

452
00:13:48,519 --> 00:13:49,320
就是说

453
00:13:51,560 --> 00:13:52,600
可以看一下区别

454
00:13:52,600 --> 00:13:53,800
这个区别是说

455
00:13:54,920 --> 00:13:56,639
假设是一个3D的东西

456
00:13:57,560 --> 00:13:59,879
这个维度是你的

457
00:14:00,720 --> 00:14:02,639
这个维度是你的batch size

458
00:14:04,440 --> 00:14:06,399
这个维度是你的特征

459
00:14:06,399 --> 00:14:07,840
就是每一个元素特征

460
00:14:07,840 --> 00:14:08,920
那个hidden size

461
00:14:09,600 --> 00:14:11,159
这个是你的序列长度

462
00:14:11,920 --> 00:14:12,960
batch normalization

463
00:14:13,080 --> 00:14:13,800
就是每一次

464
00:14:13,960 --> 00:14:15,840
就是说在d这个维度

465
00:14:16,480 --> 00:14:18,160
找出一个这样子的矩阵过来

466
00:14:19,639 --> 00:14:21,000
然后这样子说

467
00:14:21,320 --> 00:14:23,240
你这个d的东西

468
00:14:25,279 --> 00:14:26,120
把它来

469
00:14:26,519 --> 00:14:27,799
里面方差

470
00:14:28,799 --> 00:14:30,080
均值变0方差变1

471
00:14:30,879 --> 00:14:31,559
layer

472
00:14:31,720 --> 00:14:32,279
layer就是说

473
00:14:32,279 --> 00:14:33,720
我选的时候不一样一点

474
00:14:33,960 --> 00:14:34,960
每次选个元素

475
00:14:34,960 --> 00:14:36,639
就是选一个batch里面的

476
00:14:36,639 --> 00:14:37,200
就是说

477
00:14:37,240 --> 00:14:38,279
每个batch里面

478
00:14:38,399 --> 00:14:40,200
就是说给一个样本

479
00:14:41,480 --> 00:14:42,480
这里面做一下

480
00:14:43,480 --> 00:14:44,919
虽然这个长度还是变化

481
00:14:44,919 --> 00:14:46,679
但是说至少我是在一个

482
00:14:46,879 --> 00:14:48,759
单样本里面了

483
00:14:49,360 --> 00:14:50,159
就不再是说

484
00:14:50,159 --> 00:14:50,919
我是说

485
00:14:50,919 --> 00:14:52,200
我不管你p量多少

486
00:14:52,200 --> 00:14:52,600
我不管

487
00:14:52,600 --> 00:14:53,759
我就给一个特征

488
00:14:53,759 --> 00:14:55,080
就是说在那个地方

489
00:14:55,080 --> 00:14:56,559
所以大家发现

490
00:14:56,720 --> 00:14:57,399
就这个东西

491
00:14:57,639 --> 00:14:59,399
在对变化强度的时候

492
00:14:59,399 --> 00:15:00,319
稍微稳定一点

493
00:15:00,319 --> 00:15:02,000
就不会因为你的长度

494
00:15:02,000 --> 00:15:02,919
发生太多变化

495
00:15:03,039 --> 00:15:06,480
导致说它稳定性会发生很大变化

496
00:15:06,480 --> 00:15:07,759
就是学的东西

497
00:15:08,840 --> 00:15:10,960
所以就是用的是层规划

498
00:15:11,279 --> 00:15:12,879
但大家知道这个东西

499
00:15:12,879 --> 00:15:14,399
我们在讲batch normalization

500
00:15:14,399 --> 00:15:15,240
所以有给大家讲过

501
00:15:15,240 --> 00:15:18,120
就是说我们有很多xx

502
00:15:19,080 --> 00:15:19,919
叫什么

503
00:15:20,360 --> 00:15:23,639
我们还最近中了一篇ccp paper

504
00:15:23,639 --> 00:15:24,120
讲什么

505
00:15:24,799 --> 00:15:25,919
说白了就是说

506
00:15:25,960 --> 00:15:27,200
你反正一个矩阵给你

507
00:15:27,200 --> 00:15:29,480
就是说你LP是个三维的矩阵

508
00:15:29,480 --> 00:15:31,200
你图片是一个四维矩阵

509
00:15:31,320 --> 00:15:32,240
就看你怎么切

510
00:15:32,759 --> 00:15:35,039
就每一次你拿哪一块出来

511
00:15:35,360 --> 00:15:36,360
你可以group long

512
00:15:36,360 --> 00:15:36,919
就group long

513
00:15:36,919 --> 00:15:37,960
就是说我可以切

514
00:15:38,039 --> 00:15:40,440
就是说把中间切成几块出来

515
00:15:40,440 --> 00:15:41,360
切小一点

516
00:15:41,560 --> 00:15:42,639
就是说你可以这么切

517
00:15:42,639 --> 00:15:43,080
横着切

518
00:15:43,080 --> 00:15:43,759
还可以

519
00:15:43,759 --> 00:15:44,480
你可以这么切

520
00:15:44,480 --> 00:15:44,720
对吧

521
00:15:44,720 --> 00:15:45,240
可以这么切

522
00:15:45,240 --> 00:15:45,840
可以对吧

523
00:15:45,960 --> 00:15:48,440
所以真的就是说

524
00:15:49,200 --> 00:15:49,920
给你一个矩阵

525
00:15:49,920 --> 00:15:53,120
每一次看你怎么样去挖一块出来

526
00:15:53,120 --> 00:15:55,600
然后把里面的均值变领

527
00:15:55,600 --> 00:15:56,160
放上变异

528
00:15:56,160 --> 00:15:57,519
然后让他可以学一下

529
00:15:57,720 --> 00:15:58,240
就说白了

530
00:15:58,399 --> 00:15:59,600
有一堆这样的东西

531
00:16:00,200 --> 00:16:02,320
这个地方我们用的是layer long

532
00:16:03,200 --> 00:16:03,720
OK

533
00:16:05,320 --> 00:16:05,720
好

534
00:16:05,720 --> 00:16:07,759
最后然后看一下信息传递

535
00:16:08,399 --> 00:16:09,680
就是说我们基本上讲完了

536
00:16:09,680 --> 00:16:10,480
就是说

537
00:16:10,800 --> 00:16:11,879
这里面这个block

538
00:16:11,879 --> 00:16:12,480
长什么样子

539
00:16:12,639 --> 00:16:14,480
就是说这里面那一块

540
00:16:14,480 --> 00:16:15,040
长什么样子

541
00:16:15,240 --> 00:16:16,120
这个我们也讲过了

542
00:16:16,480 --> 00:16:17,159
最后讲一下

543
00:16:17,159 --> 00:16:18,439
这一根线是什么样子

544
00:16:19,959 --> 00:16:21,240
这个线是说

545
00:16:22,279 --> 00:16:25,120
假设编码器最终的输出

546
00:16:25,120 --> 00:16:27,000
是y1到yn的情况下

547
00:16:28,000 --> 00:16:28,879
将其

548
00:16:28,919 --> 00:16:30,599
然后他这个东西

549
00:16:32,519 --> 00:16:33,039
怎么用

550
00:16:33,159 --> 00:16:35,039
就有点像我们的sequence

551
00:16:35,039 --> 00:16:35,839
to sequence

552
00:16:35,839 --> 00:16:36,719
sequence to sequence

553
00:16:36,719 --> 00:16:37,719
加个tension的做法

554
00:16:37,839 --> 00:16:38,759
就编码器的输出

555
00:16:38,759 --> 00:16:41,279
我们输出作为我们的context

556
00:16:42,199 --> 00:16:43,679
它的具体做法是说

557
00:16:43,680 --> 00:16:44,520
他把这个东西

558
00:16:45,600 --> 00:16:47,920
做成解码器中

559
00:16:48,680 --> 00:16:51,040
第i个每一个transformer

560
00:16:51,080 --> 00:16:56,080
它的中间的multi head的

561
00:16:56,280 --> 00:16:57,360
attention

562
00:16:57,440 --> 00:16:59,080
它的key和value

563
00:16:59,840 --> 00:17:00,520
就说

564
00:17:00,720 --> 00:17:01,480
这个东西

565
00:17:01,480 --> 00:17:02,800
这两个东西是self

566
00:17:02,800 --> 00:17:04,000
都是multi head

567
00:17:04,000 --> 00:17:04,519
首先

568
00:17:04,600 --> 00:17:06,080
但是这两个是self

569
00:17:06,080 --> 00:17:06,440
attention

570
00:17:06,440 --> 00:17:07,440
就是说key value

571
00:17:07,440 --> 00:17:08,400
query都是自己

572
00:17:08,440 --> 00:17:10,320
但是这个东西是正常的

573
00:17:10,320 --> 00:17:10,720
attention

574
00:17:10,720 --> 00:17:12,200
它的key和value

575
00:17:12,200 --> 00:17:14,519
是来自于你编码器的输出

576
00:17:15,240 --> 00:17:15,840
OK

577
00:17:16,600 --> 00:17:17,799
所以本来就是说这个东西

578
00:17:17,799 --> 00:17:20,120
你我们在sequence to sequence

579
00:17:20,319 --> 00:17:21,319
加个tension的里面

580
00:17:21,319 --> 00:17:23,039
其实这个东西就是tension

581
00:17:23,319 --> 00:17:24,400
就是干这个事情的

582
00:17:25,360 --> 00:17:26,200
你说那个东西

583
00:17:26,200 --> 00:17:27,240
是不是每一层都加

584
00:17:27,240 --> 00:17:29,120
现在我是每一层都加这个东西进来

585
00:17:29,160 --> 00:17:31,319
所以我就把它塞在这个地方

586
00:17:32,840 --> 00:17:33,880
所以当然这意味着说

587
00:17:33,880 --> 00:17:34,600
你的编码器

588
00:17:34,600 --> 00:17:36,319
解码器的快的个数

589
00:17:36,319 --> 00:17:38,840
就是快的个数

590
00:17:38,840 --> 00:17:39,720
一般我们设一下

591
00:17:39,839 --> 00:17:42,559
主要是输出的维度是一样的

592
00:17:43,279 --> 00:17:43,759
OK

593
00:17:45,000 --> 00:17:46,160
这个就是因为

594
00:17:47,039 --> 00:17:49,079
这个就是你带来的一个

595
00:17:49,640 --> 00:17:51,480
维度上我们一般来说

596
00:17:51,559 --> 00:17:52,360
编码器解码器

597
00:17:52,360 --> 00:17:52,920
一般来说

598
00:17:52,960 --> 00:17:54,200
乘数维度数

599
00:17:54,279 --> 00:17:55,640
都一般是取一样

600
00:17:55,799 --> 00:17:56,720
一是为了简单

601
00:17:56,720 --> 00:17:57,720
二是为了对称

602
00:17:58,759 --> 00:17:59,360
OK

603
00:18:01,680 --> 00:18:02,039
好

604
00:18:02,039 --> 00:18:03,559
然后最后讲一下预测

605
00:18:03,920 --> 00:18:04,799
预测怎么回事

606
00:18:05,319 --> 00:18:05,960
就预测

607
00:18:06,600 --> 00:18:07,400
就是说

608
00:18:07,400 --> 00:18:08,560
当你在预测

609
00:18:08,560 --> 00:18:10,800
第t加1个输出的时候

610
00:18:11,640 --> 00:18:12,960
那么我们有什么

611
00:18:13,120 --> 00:18:14,200
我们有

612
00:18:15,040 --> 00:18:16,880
前t个的预测值

613
00:18:16,880 --> 00:18:17,640
就跟我们的

614
00:18:19,440 --> 00:18:20,519
RNN的预测有点像

615
00:18:20,680 --> 00:18:21,840
RNN的低后的预测

616
00:18:21,840 --> 00:18:22,200
就是说

617
00:18:22,200 --> 00:18:22,800
我在预测

618
00:18:22,800 --> 00:18:23,800
第t加1个的时候

619
00:18:24,120 --> 00:18:26,800
我知道前面t个的预测值

620
00:18:27,680 --> 00:18:30,000
然后这个地方在自注意力中间

621
00:18:30,120 --> 00:18:32,280
这前t个的预测值

622
00:18:32,680 --> 00:18:35,360
它是作为key和value都出现的

623
00:18:36,360 --> 00:18:39,800
而且第t个的预测值

624
00:18:39,800 --> 00:18:41,640
还作为query进来

625
00:18:42,280 --> 00:18:43,640
这样子对它来

626
00:18:43,640 --> 00:18:44,560
它的输出

627
00:18:44,560 --> 00:18:47,400
就是对t加1时刻的预测

628
00:18:48,800 --> 00:18:49,320
OK

629
00:18:49,520 --> 00:18:50,560
所以你看到这个图

630
00:18:50,560 --> 00:18:51,200
就是说

631
00:18:51,880 --> 00:18:54,200
假设我前t的都有了

632
00:18:54,200 --> 00:18:55,800
就是1到t-1

633
00:18:56,040 --> 00:18:57,200
它的那些输出

634
00:18:57,320 --> 00:18:58,920
都是作为key和value进来

635
00:18:59,200 --> 00:19:01,160
它最后t它就作为

636
00:19:01,160 --> 00:19:03,200
还能还作为一个query

637
00:19:03,400 --> 00:19:05,600
这样子我就拿到对它的预测

638
00:19:05,600 --> 00:19:06,279
这个output

639
00:19:06,279 --> 00:19:07,840
output就是对于

640
00:19:07,840 --> 00:19:10,080
第t时刻的预测

641
00:19:10,080 --> 00:19:11,440
就是第就是说

642
00:19:11,440 --> 00:19:12,680
就是第

643
00:19:12,960 --> 00:19:14,640
t加1个词的预测

644
00:19:14,880 --> 00:19:16,480
所以它就是又作为

645
00:19:16,480 --> 00:19:18,279
这我t加1时刻的

646
00:19:18,279 --> 00:19:19,200
输入在这个地方

647
00:19:19,400 --> 00:19:19,920
OK

648
00:19:19,920 --> 00:19:21,559
所以这就是它是怎么做的

649
00:19:21,840 --> 00:19:22,799
意味着是说

650
00:19:22,799 --> 00:19:24,279
在做训练的时候

651
00:19:24,279 --> 00:19:25,000
做attention的时候

652
00:19:25,000 --> 00:19:26,960
我可以一框就给你丢进来

653
00:19:26,960 --> 00:19:27,960
但是预测的时候

654
00:19:28,360 --> 00:19:30,680
它就是一个顺序的东西

655
00:19:31,640 --> 00:19:32,559
就它不再是一个

656
00:19:32,559 --> 00:19:33,399
并且都是on

657
00:19:33,799 --> 00:19:35,480
它还是得一个一个往前走

658
00:19:35,879 --> 00:19:36,119
OK

659
00:19:36,119 --> 00:19:37,039
这就是预测

660
00:19:38,200 --> 00:19:38,720
好

661
00:19:38,919 --> 00:19:40,119
最后总结一下

662
00:19:41,960 --> 00:19:42,519
总结一下

663
00:19:42,519 --> 00:19:43,240
就是说

664
00:19:43,319 --> 00:19:45,480
transformer首先是一个

665
00:19:45,519 --> 00:19:47,119
incode decode的架构

666
00:19:47,399 --> 00:19:49,000
跟我们之前讲的seq2seq

667
00:19:49,200 --> 00:19:51,720
在大致上是长得有点像的

668
00:19:52,159 --> 00:19:54,159
但是它一个比较大的不一样

669
00:19:54,159 --> 00:19:55,240
是说它是一个

670
00:19:55,240 --> 00:19:57,519
纯使用注意力的

671
00:19:57,679 --> 00:19:59,000
incode decode

672
00:19:59,559 --> 00:19:59,759
好

673
00:19:59,960 --> 00:20:02,039
它之前加了一个全连接层

674
00:20:02,159 --> 00:20:03,200
它还加了一些别的东西

675
00:20:03,279 --> 00:20:04,159
全连接

676
00:20:04,599 --> 00:20:05,039
这东西

677
00:20:06,799 --> 00:20:07,639
就每个块里面

678
00:20:07,680 --> 00:20:10,039
它有multi head

679
00:20:10,799 --> 00:20:11,599
的

680
00:20:12,200 --> 00:20:13,720
两个是自注意力

681
00:20:13,759 --> 00:20:14,079
但是

682
00:20:16,480 --> 00:20:19,159
去传递incode decode之间

683
00:20:19,279 --> 00:20:20,720
就是一个正常的一个attention

684
00:20:20,720 --> 00:20:22,759
就是说用来跟seq2seq

685
00:20:22,759 --> 00:20:23,559
加attention里面

686
00:20:23,559 --> 00:20:24,720
那个做的有点像

687
00:20:25,119 --> 00:20:26,519
另外一个它用了

688
00:20:26,799 --> 00:20:28,319
基于位置的前快网络

689
00:20:28,319 --> 00:20:29,039
叫ffn

690
00:20:29,559 --> 00:20:31,680
叫pointwise ffn

691
00:20:31,880 --> 00:20:33,160
说白了就是一个

692
00:20:33,480 --> 00:20:34,000
全连接

693
00:20:34,680 --> 00:20:35,799
或者说你可以叫做

694
00:20:36,039 --> 00:20:37,400
一乘一的卷机都可以

695
00:20:38,039 --> 00:20:39,759
然后另外它用的是

696
00:20:40,480 --> 00:20:41,600
resizional connection

697
00:20:41,720 --> 00:20:42,680
这样子做更深

698
00:20:42,680 --> 00:20:44,920
以及用的是层规划

699
00:20:44,920 --> 00:20:46,120
使得相对来说

700
00:20:46,120 --> 00:20:47,320
训练起来更加容易点

701
00:20:48,160 --> 00:20:49,360
然后这是每个块

702
00:20:49,360 --> 00:20:50,960
然后它的编码器解码器

703
00:20:51,160 --> 00:20:52,680
都是有n个这样子的

704
00:20:52,680 --> 00:20:53,640
transformer块

705
00:20:54,240 --> 00:20:55,960
再加上我们在之前讲过的

706
00:20:55,960 --> 00:20:56,920
position encoding

707
00:20:57,240 --> 00:20:58,200
这样子就基本上

708
00:20:58,200 --> 00:20:59,400
就整个的

709
00:20:59,880 --> 00:21:01,279
transformer的架构

710
00:21:01,279 --> 00:21:03,039
就是长成这个样子的

