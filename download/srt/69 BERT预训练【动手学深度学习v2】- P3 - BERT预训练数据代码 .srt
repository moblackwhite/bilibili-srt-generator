1
00:00:00,000 --> 00:00:04,320
好 我们接下来就是说先讲一下BERT的实现

2
00:00:04,320 --> 00:00:05,679
整个模型的实现

3
00:00:05,679 --> 00:00:08,839
再讲说它那个数据是怎么做出来的

4
00:00:08,839 --> 00:00:11,200
它用来做预训练的数据是怎么做出来的

5
00:00:11,200 --> 00:00:14,160
最后再来讲说这个东西怎么训练的

6
00:00:14,160 --> 00:00:17,240
所以我们把这个东西拆成了三个机制版

7
00:00:17,240 --> 00:00:19,800
因为这也是我们一直干这个事情

8
00:00:19,800 --> 00:00:22,640
就数据 模型 训练的三块

9
00:00:22,640 --> 00:00:25,000
确实是全部放在一起太长

10
00:00:25,000 --> 00:00:27,640
所以我们拆开来讲相对来说更加独立一点

11
00:00:28,839 --> 00:00:29,519
OK

12
00:00:30,120 --> 00:00:31,920
第一个我们看到是说

13
00:00:31,920 --> 00:00:33,640
我给你一些token

14
00:00:33,799 --> 00:00:34,640
给你一个list

15
00:00:34,640 --> 00:00:36,359
就是说第一个句子的那些token

16
00:00:36,359 --> 00:00:37,880
第二个句子的token

17
00:00:37,880 --> 00:00:40,159
第二个句子可以是没有

18
00:00:40,159 --> 00:00:43,799
然后我怎么表示成一个BERT的一个东西

19
00:00:44,399 --> 00:00:45,480
可以看到就是说很简单

20
00:00:45,600 --> 00:00:49,000
就是加一个class的special token

21
00:00:49,200 --> 00:00:50,359
把第一个句子

22
00:00:50,400 --> 00:00:53,640
然后拼上一个分割符

23
00:00:54,000 --> 00:00:55,200
然后第二个就是说

24
00:00:55,200 --> 00:00:56,320
然后就是说

25
00:00:58,000 --> 00:00:59,359
它的segment是零

26
00:00:59,519 --> 00:01:00,839
所以它用的零来表示

27
00:01:00,839 --> 00:01:02,399
这是第零段

28
00:01:02,920 --> 00:01:04,239
然后它就是说

29
00:01:04,239 --> 00:01:04,840
再加二

30
00:01:05,000 --> 00:01:07,239
这个二就是这两个特殊的符号

31
00:01:07,519 --> 00:01:09,239
如果你有第二个句子的话

32
00:01:09,400 --> 00:01:10,799
我就是再加上第二个句子

33
00:01:10,799 --> 00:01:12,079
再加上一个分割符

34
00:01:12,319 --> 00:01:13,359
然后第二个句子

35
00:01:13,359 --> 00:01:14,120
那个segment

36
00:01:14,359 --> 00:01:16,000
就是用的是一来表示的

37
00:01:16,480 --> 00:01:17,519
所以反馈的就是

38
00:01:17,519 --> 00:01:18,920
你把这些句子拼好的

39
00:01:18,920 --> 00:01:20,760
和对应的segment这个东西

40
00:01:20,840 --> 00:01:22,000
所以这就是说

41
00:01:22,000 --> 00:01:23,560
给两个句子

42
00:01:23,560 --> 00:01:26,359
我怎么样变成BERT的输入

43
00:01:29,760 --> 00:01:31,400
第二个就是BERT的encoder

44
00:01:31,640 --> 00:01:32,840
就是整个就是BERT

45
00:01:32,840 --> 00:01:34,159
是怎么做encoder的

46
00:01:34,439 --> 00:01:35,280
BERT encoder

47
00:01:35,280 --> 00:01:37,120
其实跟我们之前看的

48
00:01:37,120 --> 00:01:38,200
transformer encoder

49
00:01:38,200 --> 00:01:39,239
是非常像的

50
00:01:39,239 --> 00:01:40,439
给大家看一下

51
00:01:40,439 --> 00:01:42,480
就是说大家如果有认真听课的话

52
00:01:42,799 --> 00:01:44,039
这里就毫无压力

53
00:01:44,760 --> 00:01:46,560
第一个是token的embedding

54
00:01:46,680 --> 00:01:47,239
大家都有

55
00:01:47,400 --> 00:01:50,000
就是从cap size

56
00:01:50,000 --> 00:01:51,960
变成number of hints的一个embedding

57
00:01:52,799 --> 00:01:54,120
第二个是BERT新加的

58
00:01:54,120 --> 00:01:55,039
叫segment embedding

59
00:01:55,039 --> 00:01:55,879
它就是二

60
00:01:56,439 --> 00:01:56,879
对吧

61
00:01:56,879 --> 00:01:57,799
因为你的输入

62
00:01:57,799 --> 00:01:58,640
就是要么是零

63
00:01:58,640 --> 00:01:59,320
要么是一

64
00:01:59,320 --> 00:02:01,200
所以你还是变成number of hints

65
00:02:02,320 --> 00:02:03,799
之后就是

66
00:02:04,239 --> 00:02:07,239
多少个transformer的block

67
00:02:07,799 --> 00:02:09,639
就transformer的encoder block

68
00:02:09,759 --> 00:02:12,000
然后每次就是加上我们

69
00:02:12,520 --> 00:02:14,719
上节课实现的encoder block

70
00:02:15,439 --> 00:02:16,680
反正是那些key size

71
00:02:16,680 --> 00:02:18,039
那东西都是照搬过来

72
00:02:18,479 --> 00:02:19,319
所以它的主体

73
00:02:19,360 --> 00:02:21,319
就是把你transformer encoder block

74
00:02:21,319 --> 00:02:21,960
搬过来

75
00:02:23,039 --> 00:02:24,920
然后再下面就是一个position

76
00:02:24,920 --> 00:02:25,520
embedding

77
00:02:26,000 --> 00:02:28,000
你怎么样辨识它可学呢

78
00:02:28,319 --> 00:02:29,759
你就不要用一个embedding

79
00:02:29,919 --> 00:02:31,199
你就把它变成一个

80
00:02:31,199 --> 00:02:31,960
就把它随机

81
00:02:31,960 --> 00:02:32,680
就最简单

82
00:02:32,680 --> 00:02:34,840
就把它随机初始化一下

83
00:02:35,080 --> 00:02:36,000
它就是by size

84
00:02:36,000 --> 00:02:37,240
以max length

85
00:02:37,240 --> 00:02:38,280
就最长的长度

86
00:02:38,280 --> 00:02:39,240
然后number of hints

87
00:02:39,240 --> 00:02:41,120
它就是一个这样子的东西

88
00:02:41,280 --> 00:02:43,840
然后你包在一个parameter里面

89
00:02:43,919 --> 00:02:44,479
它就可以

90
00:02:44,800 --> 00:02:45,879
这就随机初始化

91
00:02:46,199 --> 00:02:48,280
然后让它变成可以学习

92
00:02:48,639 --> 00:02:49,840
所以这就是

93
00:02:50,280 --> 00:02:53,439
让它什么叫可学习的位置编码

94
00:02:54,319 --> 00:02:56,199
所以你跟你transformer encoder

95
00:02:56,199 --> 00:02:56,680
比的话

96
00:02:56,680 --> 00:02:59,560
其实区别就是多了一个

97
00:02:59,719 --> 00:03:00,240
对吧

98
00:03:00,599 --> 00:03:02,280
和这个东西变得可学了

99
00:03:03,080 --> 00:03:05,439
所以这就是它的区别

100
00:03:05,719 --> 00:03:06,560
其实也很小了

101
00:03:06,560 --> 00:03:07,040
对吧

102
00:03:08,240 --> 00:03:09,480
做forward的时候

103
00:03:09,680 --> 00:03:12,120
当然也多了一个segment在里面

104
00:03:12,360 --> 00:03:14,200
因为两个就是我们刚刚函数

105
00:03:14,200 --> 00:03:14,800
返回的东西

106
00:03:15,319 --> 00:03:16,520
width and length是一样的

107
00:03:16,520 --> 00:03:17,200
就是说

108
00:03:17,520 --> 00:03:19,280
你的句子有多少是合法的

109
00:03:20,280 --> 00:03:21,159
那么你做的话

110
00:03:21,360 --> 00:03:23,400
先是进token embedding

111
00:03:24,120 --> 00:03:26,640
然后再加上

112
00:03:27,719 --> 00:03:30,120
再加上谁

113
00:03:30,120 --> 00:03:30,560
我来

114
00:03:31,080 --> 00:03:33,439
再加上你的segmented embedding

115
00:03:33,719 --> 00:03:34,120
OK

116
00:03:34,120 --> 00:03:35,960
这样子就达到我的x

117
00:03:36,120 --> 00:03:38,520
然后再去position embedding里面

118
00:03:38,520 --> 00:03:39,759
找出它一样的shape

119
00:03:39,759 --> 00:03:40,560
然后加回去

120
00:03:41,120 --> 00:03:41,400
OK

121
00:03:41,400 --> 00:03:43,560
这就是前面的embedding的输入

122
00:03:44,040 --> 00:03:46,560
接下来就是你把它全部输入到

123
00:03:46,560 --> 00:03:47,439
那些block里面

124
00:03:47,439 --> 00:03:48,319
就完事了

125
00:03:48,400 --> 00:03:48,879
就没了

126
00:03:48,879 --> 00:03:52,040
就是你把整个东西

127
00:03:52,599 --> 00:03:54,840
那些transformer的那些block

128
00:03:55,000 --> 00:03:56,280
就一个过一遍

129
00:03:56,280 --> 00:03:57,560
然后就返回x

130
00:03:57,919 --> 00:03:59,159
你就birth

131
00:04:00,280 --> 00:04:02,519
这个模型的输出就搞定了

132
00:04:03,560 --> 00:04:04,079
OK

133
00:04:05,399 --> 00:04:06,799
然后看一下这个东西

134
00:04:06,919 --> 00:04:08,919
就是说我也不给大家仔细讲

135
00:04:08,919 --> 00:04:09,799
这个参数怎么回事

136
00:04:09,799 --> 00:04:10,280
就是说白了

137
00:04:10,280 --> 00:04:10,759
就是

138
00:04:10,879 --> 00:04:13,239
如果capsize给你是1万的话

139
00:04:13,280 --> 00:04:16,039
那么number hindrance给你的是768

140
00:04:16,079 --> 00:04:17,879
然后你ffn的number hindrance

141
00:04:18,039 --> 00:04:18,879
就稍微大一点点

142
00:04:19,039 --> 00:04:19,839
给的是

143
00:04:20,399 --> 00:04:21,199
我再换个号

144
00:04:21,759 --> 00:04:23,000
给的是1024

145
00:04:23,319 --> 00:04:25,199
然后你的number head给的是4

146
00:04:25,839 --> 00:04:28,199
然后你就出示化一个encoder出来

147
00:04:28,639 --> 00:04:29,599
然后可以看一下

148
00:04:29,599 --> 00:04:30,319
你的token

149
00:04:30,319 --> 00:04:31,479
如果是一个

150
00:04:32,759 --> 00:04:34,079
batch size是2

151
00:04:34,079 --> 00:04:35,800
如果长度是8的话

152
00:04:36,120 --> 00:04:36,839
segment当然是

153
00:04:36,839 --> 00:04:38,240
你可以构造一个东西出来

154
00:04:38,279 --> 00:04:39,199
那么进去的话

155
00:04:39,199 --> 00:04:41,399
就得到一个birth出来

156
00:04:41,439 --> 00:04:42,279
结果就是

157
00:04:42,319 --> 00:04:43,839
batch size2

158
00:04:44,079 --> 00:04:45,800
乘以你这个句子有多长

159
00:04:45,800 --> 00:04:46,480
8

160
00:04:46,560 --> 00:04:47,919
再乘以768

161
00:04:47,919 --> 00:04:49,039
就是你这个东西

162
00:04:49,039 --> 00:04:50,159
就number of hindrance

163
00:04:50,880 --> 00:04:51,439
Ok

164
00:04:51,439 --> 00:04:52,360
所以看到是说

165
00:04:52,360 --> 00:04:55,240
跟transformer的encoder是一样

166
00:04:55,680 --> 00:04:57,480
给一个句子

167
00:04:57,800 --> 00:05:01,520
那么它把每个句子里面的词

168
00:05:01,800 --> 00:05:04,360
抽出一个常为number of hindrance的

169
00:05:04,360 --> 00:05:05,000
项量出来

170
00:05:05,000 --> 00:05:06,400
这里是768

171
00:05:08,160 --> 00:05:08,800
Ok

172
00:05:09,680 --> 00:05:11,840
然后接下来就是说

173
00:05:11,840 --> 00:05:12,600
你要算

174
00:05:13,440 --> 00:05:14,520
你有两个loss

175
00:05:14,720 --> 00:05:17,800
第一个就是mask的language model

176
00:05:17,960 --> 00:05:18,800
是怎么做

177
00:05:19,000 --> 00:05:20,920
这个东西其实就是

178
00:05:20,960 --> 00:05:22,759
它用的是一个

179
00:05:22,800 --> 00:05:24,840
先用一个全连接层

180
00:05:25,319 --> 00:05:27,000
加一个reload activation

181
00:05:27,040 --> 00:05:28,600
再加一个layer long在后面

182
00:05:28,720 --> 00:05:30,120
再用一个全连接层

183
00:05:30,480 --> 00:05:32,439
它的输出的大小就是

184
00:05:33,040 --> 00:05:33,800
我换个行

185
00:05:34,040 --> 00:05:35,360
就是说这样子的话

186
00:05:39,840 --> 00:05:40,560
这样子的话

187
00:05:40,560 --> 00:05:42,800
就是说到workable size

188
00:05:43,000 --> 00:05:44,240
因为对每一个词

189
00:05:44,439 --> 00:05:45,680
你要预测

190
00:05:46,000 --> 00:05:47,199
它是哪个词

191
00:05:47,240 --> 00:05:48,959
就是你要workable size的预测

192
00:05:50,279 --> 00:05:52,039
所以它就是在你的burd上面

193
00:05:52,039 --> 00:05:54,560
再接一个MLP

194
00:05:54,680 --> 00:05:57,079
一个有单影长程的MLP

195
00:05:58,000 --> 00:05:59,360
然后在预测的时候是怎么做

196
00:05:59,560 --> 00:06:00,639
预测的时候就是说

197
00:06:00,680 --> 00:06:01,399
你告诉我说

198
00:06:01,399 --> 00:06:02,920
我要预测哪些position

199
00:06:03,240 --> 00:06:04,240
就你的x是什么

200
00:06:04,240 --> 00:06:06,719
x是说我给的所有的句子里面

201
00:06:06,919 --> 00:06:08,519
所以那些那个burd的输出

202
00:06:08,680 --> 00:06:10,560
就刚刚那个burd encoder的输出

203
00:06:10,879 --> 00:06:11,680
然后告诉你说

204
00:06:11,680 --> 00:06:12,639
你哪些句子

205
00:06:12,639 --> 00:06:14,560
哪些地方我要去预测

206
00:06:14,599 --> 00:06:16,839
就哪些词是加了mask

207
00:06:17,800 --> 00:06:19,639
所以这个东西在干嘛

208
00:06:19,800 --> 00:06:20,800
这个东西就是说

209
00:06:21,000 --> 00:06:21,719
这些句子

210
00:06:21,719 --> 00:06:22,959
这一段代码在干嘛

211
00:06:22,959 --> 00:06:24,079
我就不仔细过了

212
00:06:24,079 --> 00:06:26,120
它就是把你

213
00:06:28,240 --> 00:06:30,480
每一个predict position

214
00:06:30,480 --> 00:06:31,079
就是说

215
00:06:31,079 --> 00:06:32,360
每一个句子里面说

216
00:06:32,360 --> 00:06:33,719
我第1个第0个句子

217
00:06:33,719 --> 00:06:35,759
要预测第0个token

218
00:06:35,759 --> 00:06:36,480
第3个token

219
00:06:36,480 --> 00:06:37,439
第7个token

220
00:06:37,560 --> 00:06:39,360
那么我要我这个东西要干嘛

221
00:06:39,360 --> 00:06:40,399
我这个东西要把

222
00:06:40,480 --> 00:06:41,839
这一个第0个

223
00:06:41,839 --> 00:06:42,399
第3个

224
00:06:42,399 --> 00:06:43,439
第7个token

225
00:06:43,439 --> 00:06:44,719
它对应的burd

226
00:06:44,719 --> 00:06:46,439
给我输出来特征

227
00:06:46,920 --> 00:06:48,439
给我放在这个地方

228
00:06:49,480 --> 00:06:50,879
Ok这样子的话

229
00:06:50,879 --> 00:06:51,800
那就是

230
00:06:51,839 --> 00:06:54,000
我就把每一个要predict

231
00:06:54,000 --> 00:06:55,480
要预测的那些位置

232
00:06:55,480 --> 00:06:56,639
那些burd的特征

233
00:06:56,639 --> 00:06:57,719
给你拎出来

234
00:06:58,120 --> 00:06:58,879
拎出来之后

235
00:06:58,879 --> 00:06:59,639
就变成了一个

236
00:06:59,639 --> 00:07:01,480
就丢到我的mlp里面

237
00:07:01,480 --> 00:07:01,920
对吧

238
00:07:02,439 --> 00:07:03,879
就丢到我的mlp里面

239
00:07:03,879 --> 00:07:05,120
那么对每个位置

240
00:07:05,120 --> 00:07:06,519
我都会做一个预测

241
00:07:06,560 --> 00:07:07,439
那么它就变成一个

242
00:07:07,439 --> 00:07:09,319
很正常的一个分类问题了

243
00:07:10,480 --> 00:07:11,159
你看一下

244
00:07:11,159 --> 00:07:11,519
也就是说

245
00:07:11,519 --> 00:07:12,759
这个东西长什么样子

246
00:07:13,240 --> 00:07:14,839
就是说假设我构造一个

247
00:07:14,880 --> 00:07:17,400
我的模型的话

248
00:07:17,440 --> 00:07:18,600
我的vocab size

249
00:07:18,600 --> 00:07:19,520
number of intents

250
00:07:19,800 --> 00:07:21,720
假设我说我的预测的是说

251
00:07:21,720 --> 00:07:22,560
第0个样本

252
00:07:22,560 --> 00:07:23,480
我要预测

253
00:07:23,680 --> 00:07:24,360
第1个

254
00:07:24,360 --> 00:07:24,960
第5个

255
00:07:24,960 --> 00:07:25,840
第2个

256
00:07:26,520 --> 00:07:27,200
第2个句子

257
00:07:27,200 --> 00:07:28,040
我要预测的

258
00:07:28,040 --> 00:07:29,400
那些词的位置

259
00:07:29,520 --> 00:07:30,360
是第6个

260
00:07:30,360 --> 00:07:30,680
第1个

261
00:07:30,680 --> 00:07:31,440
第5个

262
00:07:31,480 --> 00:07:32,720
那么丢进去之后

263
00:07:32,720 --> 00:07:33,880
它会出个什么东西

264
00:07:34,040 --> 00:07:36,680
那出的是2是batch size

265
00:07:36,960 --> 00:07:38,240
每个batch size

266
00:07:38,240 --> 00:07:39,720
我要预测三个词

267
00:07:39,720 --> 00:07:40,600
三个位置

268
00:07:40,600 --> 00:07:41,120
对吧

269
00:07:41,320 --> 00:07:42,240
对每个位置

270
00:07:42,240 --> 00:07:43,280
我要输出的

271
00:07:43,640 --> 00:07:45,120
是1万

272
00:07:45,120 --> 00:07:46,080
1万这个地方

273
00:07:46,080 --> 00:07:49,040
就是因为我的vocab size是1万的

274
00:07:49,920 --> 00:07:50,200
OK

275
00:07:50,200 --> 00:07:51,200
就是一个说白了

276
00:07:51,200 --> 00:07:52,560
就有点像是

277
00:07:53,160 --> 00:07:53,760
语言模型

278
00:07:53,760 --> 00:07:54,400
我们就这么干的

279
00:07:54,400 --> 00:07:54,880
对吧

280
00:07:56,080 --> 00:07:56,760
另外一个时候

281
00:07:56,760 --> 00:07:57,760
你要上loss的时候

282
00:07:57,760 --> 00:07:59,200
它就是一个cross entropy loss

283
00:07:59,640 --> 00:08:00,880
出来的时候就是6

284
00:08:00,880 --> 00:08:02,080
因为它就是6个样本

285
00:08:02,080 --> 00:08:02,560
对吧

286
00:08:03,040 --> 00:08:04,760
就是你每个两个句子

287
00:08:04,760 --> 00:08:06,040
每个句子里面做三个预测

288
00:08:06,040 --> 00:08:07,240
那么就一共做6个预测

289
00:08:07,240 --> 00:08:08,240
所以它是有

290
00:08:09,680 --> 00:08:11,320
你的loss是有6个

291
00:08:12,319 --> 00:08:12,879
OK

292
00:08:12,879 --> 00:08:15,839
这就是mask的lm

293
00:08:16,319 --> 00:08:17,000
另外一个是说

294
00:08:17,000 --> 00:08:18,360
你要做下一个句子预测

295
00:08:18,360 --> 00:08:19,360
下一个句子预测

296
00:08:19,480 --> 00:08:20,439
就是一个

297
00:08:20,719 --> 00:08:21,839
就是一个单分的问题

298
00:08:22,879 --> 00:08:23,560
单分的问题

299
00:08:23,560 --> 00:08:25,439
我们一般就是在神经网络里面

300
00:08:25,439 --> 00:08:25,879
就偷懒

301
00:08:25,879 --> 00:08:27,480
就把它做成两个

302
00:08:27,600 --> 00:08:28,519
output为2

303
00:08:28,519 --> 00:08:30,079
当然实际上你做一个也没关系

304
00:08:30,079 --> 00:08:31,680
就是logistic regression也没关系

305
00:08:31,800 --> 00:08:32,879
但我们用的是softmax

306
00:08:33,159 --> 00:08:34,399
干脆我们就输出2

307
00:08:35,159 --> 00:08:37,840
然后给一个句子出来

308
00:08:37,840 --> 00:08:38,759
我就输出

309
00:08:39,480 --> 00:08:41,480
就丢进我的output就行了

310
00:08:41,559 --> 00:08:43,639
就一个output类

311
00:08:44,039 --> 00:08:44,600
OK

312
00:08:45,799 --> 00:08:46,039
好

313
00:08:46,039 --> 00:08:47,159
然后我们就不

314
00:08:47,159 --> 00:08:47,840
这个就不看了

315
00:08:47,960 --> 00:08:48,919
这个就太简单了

316
00:08:48,919 --> 00:08:50,679
就是给一个句子预测

317
00:08:50,679 --> 00:08:51,639
预测那个东西

318
00:08:52,159 --> 00:08:52,799
好

319
00:08:53,399 --> 00:08:53,960
这样的话

320
00:08:54,159 --> 00:08:55,439
我们有知道

321
00:08:56,480 --> 00:08:57,639
编码器是谁

322
00:08:57,639 --> 00:08:58,840
两个loss是谁

323
00:08:59,039 --> 00:09:00,519
我们就可以写整个模型了

324
00:09:01,559 --> 00:09:02,360
整个模型是什么

325
00:09:02,879 --> 00:09:04,639
整个模型就是我的encoder

326
00:09:05,840 --> 00:09:07,000
这个hidden

327
00:09:07,120 --> 00:09:09,080
我觉得应该不应该写在这个地方

328
00:09:09,399 --> 00:09:10,639
hidden就应该写在这个里面

329
00:09:10,879 --> 00:09:11,639
hidden layer

330
00:09:12,000 --> 00:09:13,360
这个东西是给他用的

331
00:09:13,960 --> 00:09:15,679
等下来就说我有一个encoder

332
00:09:15,679 --> 00:09:16,559
就刚刚我们讲到

333
00:09:17,120 --> 00:09:18,080
BERT的encoder

334
00:09:18,080 --> 00:09:20,080
然后有一个做mask

335
00:09:20,679 --> 00:09:23,399
就是说语言模型的模型

336
00:09:23,399 --> 00:09:25,159
第二个是做一个next sentence

337
00:09:25,159 --> 00:09:26,120
predict的模型

338
00:09:26,679 --> 00:09:27,240
OK

339
00:09:27,240 --> 00:09:32,039
就是一个主干两个头

340
00:09:32,039 --> 00:09:32,480
对吧

341
00:09:33,799 --> 00:09:34,639
那么forward

342
00:09:35,439 --> 00:09:37,399
forward就是给你token

343
00:09:37,600 --> 00:09:38,679
给你segment

344
00:09:39,399 --> 00:09:40,519
然后valid length

345
00:09:40,519 --> 00:09:41,159
当然你可以给

346
00:09:41,679 --> 00:09:42,879
然后predict position

347
00:09:42,879 --> 00:09:44,240
然后当然是你要给的

348
00:09:44,240 --> 00:09:45,759
就是说我要预测哪一些东西

349
00:09:45,799 --> 00:09:46,639
你可以给的none

350
00:09:46,759 --> 00:09:47,319
如果none的话

351
00:09:47,319 --> 00:09:48,559
我就不做语言模型了

352
00:09:49,759 --> 00:09:50,879
如果你不是none的话

353
00:09:51,039 --> 00:09:52,039
就是说我就

354
00:09:52,039 --> 00:09:53,960
当我先把我的token和segment

355
00:09:54,159 --> 00:09:55,639
这个东西丢到我的encoder里面

356
00:09:55,639 --> 00:09:56,519
能拿到

357
00:09:56,559 --> 00:10:00,159
对每一个词都拿到我的特征

358
00:10:01,559 --> 00:10:03,039
如果你要预测位置的话

359
00:10:03,159 --> 00:10:05,039
我就丢到我的语言模型里面

360
00:10:06,159 --> 00:10:07,279
不然的话我就丢到

361
00:10:08,120 --> 00:10:10,519
我先把这个东西放到一个hidden层里面

362
00:10:11,159 --> 00:10:13,719
然后就是放到hidden层里面

363
00:10:14,240 --> 00:10:15,599
这个地方看到是说

364
00:10:16,159 --> 00:10:17,480
我这一次怎么在干嘛

365
00:10:17,639 --> 00:10:18,279
就是

366
00:10:18,879 --> 00:10:20,240
第一个维度是batch size

367
00:10:20,439 --> 00:10:21,120
0是什么

368
00:10:21,360 --> 00:10:23,199
0是sequence那个维度

369
00:10:23,360 --> 00:10:26,559
就0这个东西是把class

370
00:10:26,559 --> 00:10:29,039
就是每一个输进去的东西

371
00:10:29,120 --> 00:10:31,480
第一个特殊字符分类的token

372
00:10:31,680 --> 00:10:33,039
它对应的向量拎出来

373
00:10:33,600 --> 00:10:35,120
OK就对每一个句子

374
00:10:35,120 --> 00:10:35,840
一个句子对

375
00:10:36,200 --> 00:10:39,440
我将第一个token的向量抽出来

376
00:10:39,600 --> 00:10:42,680
然后用来做一个offer内问题

377
00:10:43,000 --> 00:10:44,600
我现在当然放在一个hidden里面

378
00:10:44,800 --> 00:10:46,399
所以我说我这个hidden应该写在

379
00:10:47,360 --> 00:10:48,080
nsp里面

380
00:10:48,159 --> 00:10:49,240
不用写在这个地方

381
00:10:49,399 --> 00:10:49,960
所以说白了

382
00:10:49,960 --> 00:10:51,159
然后再做一个

383
00:10:51,639 --> 00:10:53,240
VR的一个输出

384
00:10:53,279 --> 00:10:55,399
就得到我的预测

385
00:10:56,320 --> 00:10:56,920
最后的话

386
00:10:56,920 --> 00:10:59,600
我就把我的所有的encoder的输出

387
00:10:59,800 --> 00:11:02,920
我的两个东西的预测

388
00:11:02,920 --> 00:11:04,040
都输出回去

389
00:11:04,720 --> 00:11:05,680
我的模型了

390
00:11:06,759 --> 00:11:07,279
OK

391
00:11:08,440 --> 00:11:09,920
所以这个就是我们的

392
00:11:11,120 --> 00:11:12,040
BERT的模型

393
00:11:12,840 --> 00:11:15,240
所以整个模型就在这个地方

394
00:11:15,440 --> 00:11:16,399
所以看到

395
00:11:16,759 --> 00:11:17,639
稍微回顾一下

396
00:11:18,519 --> 00:11:19,720
就是一个transformer的encoder

397
00:11:19,720 --> 00:11:20,240
对吧

398
00:11:20,639 --> 00:11:21,680
再加了两个

399
00:11:21,680 --> 00:11:25,360
一个做mask language的一个输出

400
00:11:25,360 --> 00:11:26,279
一个是做

401
00:11:26,600 --> 00:11:28,800
句子对式不是相邻的一个输出

402
00:11:29,360 --> 00:11:29,960
就没事了

403
00:11:29,960 --> 00:11:30,920
这两个东西说白了

404
00:11:30,920 --> 00:11:31,680
就是一个

405
00:11:32,160 --> 00:11:32,800
MLP

406
00:11:33,160 --> 00:11:34,400
就是一个很简单的ML

407
00:11:34,400 --> 00:11:35,880
单隐藏层的MLP

