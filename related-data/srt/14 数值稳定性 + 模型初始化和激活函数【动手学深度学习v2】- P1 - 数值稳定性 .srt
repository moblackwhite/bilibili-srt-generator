1
00:00:00,000 --> 00:00:03,560
第一个内容是讲数值的稳定性

2
00:00:03,560 --> 00:00:08,640
这个是机器学习里面比较重要的一点

3
00:00:08,640 --> 00:00:11,359
特别是当你的神经网络变得很深的时候

4
00:00:11,359 --> 00:00:13,439
你的数值非常容易不稳定

5
00:00:15,000 --> 00:00:17,800
好 我们来看一个具体的例子

6
00:00:18,920 --> 00:00:23,039
我们假设有一个低层的神经网络

7
00:00:24,560 --> 00:00:28,320
然后我们的层我们记在T

8
00:00:28,320 --> 00:00:30,879
T不要大家搞成时间

9
00:00:30,879 --> 00:00:32,640
我们之前用T表示时间

10
00:00:32,640 --> 00:00:34,039
这里表示层

11
00:00:35,240 --> 00:00:37,320
然后我们就是说

12
00:00:37,320 --> 00:00:42,799
假设我的T-1是低T-1层的隐藏层的输出

13
00:00:43,079 --> 00:00:48,439
然后经过一个FT得到我们的低T层的输出HT

14
00:00:49,320 --> 00:00:51,640
对吧 这个是我们之前的表示

15
00:00:51,960 --> 00:00:54,519
然后我们的Y可以表示成为

16
00:00:55,120 --> 00:00:58,840
就是X进来低层一直到低层

17
00:00:59,080 --> 00:01:01,440
最后到一个损失函数

18
00:01:01,480 --> 00:01:03,160
就是我们的预测的

19
00:01:03,240 --> 00:01:05,600
我们要进行优化的目标函数

20
00:01:05,879 --> 00:01:07,159
但Y这里不是预测

21
00:01:07,200 --> 00:01:09,240
Y我们还包括了损失函数

22
00:01:10,079 --> 00:01:10,960
我们回忆一下

23
00:01:10,960 --> 00:01:14,200
就是我们之前有讲过T度怎么算

24
00:01:14,760 --> 00:01:17,320
就如果我们计算损失L

25
00:01:17,719 --> 00:01:21,840
关于我们某一个层WT的一个T度的话

26
00:01:22,159 --> 00:01:24,120
就是说那么你可以认为是说

27
00:01:24,120 --> 00:01:26,200
我们先把它一直写开

28
00:01:26,200 --> 00:01:28,040
就是说有链式法则

29
00:01:29,040 --> 00:01:31,880
就是说我们对L损失函数

30
00:01:31,880 --> 00:01:33,880
对最后一层的球道

31
00:01:34,040 --> 00:01:36,800
然后最后一层的隐藏层

32
00:01:36,800 --> 00:01:39,960
对于第二层的隐藏层球道

33
00:01:39,960 --> 00:01:44,080
一直一直求求求到低T层的输出

34
00:01:44,760 --> 00:01:46,560
然后低T层的输出

35
00:01:46,560 --> 00:01:49,000
关于低T层的权重的球道

36
00:01:50,120 --> 00:01:52,120
就是说我们就是一直累积下来

37
00:01:52,760 --> 00:01:55,320
就注意到一点是说我们有讲过

38
00:01:55,840 --> 00:01:57,120
所有的edge

39
00:01:57,120 --> 00:01:58,760
这都是一些向量

40
00:02:00,040 --> 00:02:03,520
向量关于向量的导数是一个矩阵

41
00:02:05,080 --> 00:02:07,480
所以这里的其实你可以看到是一个

42
00:02:07,719 --> 00:02:10,040
D减去T次的矩阵乘法

43
00:02:10,040 --> 00:02:14,800
这有这里一共有D减到T次的矩阵

44
00:02:14,800 --> 00:02:16,319
所以我们是对它相乘

45
00:02:17,640 --> 00:02:19,400
所以我们主要问题来自于这个地方

46
00:02:20,400 --> 00:02:22,920
因为我们做了太多的矩阵乘法

47
00:02:26,200 --> 00:02:29,120
就是说这样子的矩阵乘法带来的

48
00:02:29,120 --> 00:02:32,280
主要的两个问题是一个叫做T度爆炸

49
00:02:32,560 --> 00:02:34,200
一个叫做T度消失

50
00:02:35,040 --> 00:02:37,120
T度爆炸举个例子

51
00:02:37,400 --> 00:02:41,920
假设我的T度是都是一些比1大一点的数

52
00:02:43,040 --> 00:02:46,040
然后我对一个就是1.5

53
00:02:46,200 --> 00:02:47,000
做100次

54
00:02:47,000 --> 00:02:48,439
假如说我有一百层的话

55
00:02:48,840 --> 00:02:51,199
做100次就会得到一个4乘以10的

56
00:02:51,199 --> 00:02:52,280
是7的一个数

57
00:02:53,319 --> 00:02:55,680
当然这个数浮点是能表示的

58
00:02:56,000 --> 00:02:59,479
但是很容易这个数字会带来我们的浮点的

59
00:02:59,479 --> 00:03:00,840
上限的一些问题

60
00:03:00,879 --> 00:03:03,319
因为我们浮点其实是有一个合理的范围的

61
00:03:04,719 --> 00:03:05,960
同样的话

62
00:03:06,000 --> 00:03:08,960
如果我的T度的值是小于1的话

63
00:03:09,159 --> 00:03:10,400
就算不会太小

64
00:03:10,400 --> 00:03:11,560
就算是0.8

65
00:03:11,680 --> 00:03:13,039
如果是100层的话

66
00:03:13,079 --> 00:03:14,680
那么0.8的100次方

67
00:03:14,719 --> 00:03:16,599
那也是2的-10次方

68
00:03:16,759 --> 00:03:18,280
也是个非常小的数了

69
00:03:18,479 --> 00:03:23,319
就表示说我们这里基本上就T度就不见了

70
00:03:25,079 --> 00:03:27,800
OK我们来接下来看一下说

71
00:03:29,479 --> 00:03:31,319
我们来举一个简单的例子

72
00:03:31,359 --> 00:03:32,759
来到底看一下

73
00:03:32,879 --> 00:03:35,439
这两个是如何产生的

74
00:03:38,599 --> 00:03:42,319
就是说我们考虑一个如下的MLP

75
00:03:42,319 --> 00:03:44,840
就是一个多层感知器的模型

76
00:03:45,439 --> 00:03:47,199
我们为了简单

77
00:03:47,200 --> 00:03:50,120
我们就把偏移B给省略掉了

78
00:03:51,080 --> 00:03:52,680
如果我们写它的话

79
00:03:53,000 --> 00:03:57,200
就是说假设我们这是第T层的

80
00:03:57,680 --> 00:03:58,560
怎么计算的话

81
00:03:58,560 --> 00:04:01,320
我们这个是第T层的输入

82
00:04:01,360 --> 00:04:04,200
HT-1也就是T-1层的输出

83
00:04:04,880 --> 00:04:07,320
然后这个函数就会表示成一个

84
00:04:08,200 --> 00:04:10,680
第T层的权重WT

85
00:04:11,280 --> 00:04:13,160
乘以我的输入

86
00:04:13,280 --> 00:04:14,320
HT-1

87
00:04:15,280 --> 00:04:17,680
然后我们假设省略掉偏移的话

88
00:04:17,719 --> 00:04:20,639
然后我们就直接在输出上做激活函数

89
00:04:21,599 --> 00:04:22,800
就做Sigma

90
00:04:24,159 --> 00:04:25,560
然后我们求导

91
00:04:25,839 --> 00:04:27,040
大家回忆一下求导

92
00:04:28,120 --> 00:04:28,879
求导的话

93
00:04:28,879 --> 00:04:31,719
其实是一个这样子的一个函数

94
00:04:32,279 --> 00:04:35,079
就是说首先要对它进行求导

95
00:04:35,639 --> 00:04:38,159
对它是一个按元素的一个

96
00:04:39,759 --> 00:04:40,639
一个函数

97
00:04:40,680 --> 00:04:43,120
所以它的求导其实很简单

98
00:04:43,120 --> 00:04:44,160
它就变成一个

99
00:04:44,160 --> 00:04:45,600
因为它是一个对元素

100
00:04:45,600 --> 00:04:46,959
而且是一个向量的话

101
00:04:46,959 --> 00:04:50,079
它就变成一个对角矩阵

102
00:04:50,920 --> 00:04:52,399
里面的每一个项式

103
00:04:52,439 --> 00:04:54,399
假设我的Sigma一撇

104
00:04:54,399 --> 00:04:55,959
是我Sigma的导数

105
00:04:55,959 --> 00:04:57,920
就是激活函数的导数的话

106
00:04:57,959 --> 00:05:00,480
那就是把它的输入放进来

107
00:05:00,759 --> 00:05:01,879
它就输入是一个向量

108
00:05:02,040 --> 00:05:03,480
它的输出也是一个向量

109
00:05:03,480 --> 00:05:06,120
然后把它做成一个对角矩阵

110
00:05:07,240 --> 00:05:09,639
然后这个地方就是一个WT

111
00:05:09,639 --> 00:05:11,399
就是T还是说小T

112
00:05:11,519 --> 00:05:13,079
小T是用层的意思

113
00:05:13,240 --> 00:05:15,000
然后大T是它的转制

114
00:05:15,519 --> 00:05:15,800
OK

115
00:05:15,800 --> 00:05:17,800
这就是我们之前有讲过的

116
00:05:18,000 --> 00:05:18,720
就是说

117
00:05:19,759 --> 00:05:21,360
T层的输出

118
00:05:21,360 --> 00:05:22,600
关于输入的导数

119
00:05:22,600 --> 00:05:23,519
是个这样子

120
00:05:24,759 --> 00:05:25,800
然后回忆一下前面

121
00:05:25,959 --> 00:05:28,879
前面我们要对T减

122
00:05:28,879 --> 00:05:30,439
第一次这样子的乘法

123
00:05:30,439 --> 00:05:32,600
就是说要从最后一层开始

124
00:05:32,600 --> 00:05:34,800
一直乘到当前层

125
00:05:34,800 --> 00:05:35,480
就T

126
00:05:36,519 --> 00:05:38,079
如果我们把它累成的话

127
00:05:38,079 --> 00:05:39,319
我会发现是说

128
00:05:39,319 --> 00:05:42,680
那就是每一次就是一个对角矩阵

129
00:05:42,680 --> 00:05:44,160
乘以另外一个矩阵

130
00:05:44,160 --> 00:05:45,160
然后做

131
00:05:45,480 --> 00:05:49,439
这里是做D减T次这样子的乘法

132
00:05:50,759 --> 00:05:51,840
所以我们知道

133
00:05:52,000 --> 00:05:52,720
就是说

134
00:05:52,720 --> 00:05:55,000
假设我们是一个多层感知器的话

135
00:05:55,000 --> 00:05:55,879
我们会

136
00:05:56,240 --> 00:05:57,879
关于T层的导数

137
00:05:58,240 --> 00:05:59,759
会是个这样子的形状

138
00:06:02,600 --> 00:06:04,840
那么假设我们是使用一个

139
00:06:04,840 --> 00:06:07,960
做relu作为记录函数会怎么样呢

140
00:06:08,879 --> 00:06:09,639
而录回一下

141
00:06:09,639 --> 00:06:11,800
大家就是一个max0到x

142
00:06:12,280 --> 00:06:13,560
所以它的导数是说

143
00:06:13,560 --> 00:06:14,960
如果你的x大于0

144
00:06:14,960 --> 00:06:15,759
那就是1

145
00:06:16,439 --> 00:06:17,480
不然的话就是0

146
00:06:19,680 --> 00:06:21,120
所以你可以看到是说

147
00:06:21,120 --> 00:06:21,840
那么

148
00:06:22,400 --> 00:06:25,360
这里面就是一堆1或者0

149
00:06:26,240 --> 00:06:29,280
所以你一些1和0的一个

150
00:06:29,400 --> 00:06:31,439
对角圆做成的对角矩阵

151
00:06:31,439 --> 00:06:32,639
跟它一乘的话

152
00:06:33,240 --> 00:06:34,120
那么就意味着说

153
00:06:34,120 --> 00:06:35,480
你要么就把某一

154
00:06:36,319 --> 00:06:39,400
应该是某一列给留住了

155
00:06:39,400 --> 00:06:41,080
要么就是把它全变成0

156
00:06:42,800 --> 00:06:44,759
那么这个地方意味着是说

157
00:06:45,240 --> 00:06:47,800
因为这个函数里面全是一些0和1

158
00:06:48,520 --> 00:06:50,160
那么它的最后的值

159
00:06:50,520 --> 00:06:52,400
就这个的值的一些元素

160
00:06:52,400 --> 00:06:55,199
就是来自于那些没有被变成0的

161
00:06:55,199 --> 00:06:56,240
那一系列的乘法

162
00:06:56,240 --> 00:06:59,519
就是来自于你把所谓的wi

163
00:06:59,800 --> 00:07:02,720
就是第那个当前层的

164
00:07:02,960 --> 00:07:06,920
第i层的权重做导数

165
00:07:07,600 --> 00:07:08,879
然后做乘法

166
00:07:08,960 --> 00:07:10,680
就它的一些元素来自这里

167
00:07:10,680 --> 00:07:12,040
当然另外一些元素是0

168
00:07:13,920 --> 00:07:15,920
那么这里的问题是说

169
00:07:16,319 --> 00:07:18,920
如果d减掉t很大

170
00:07:18,920 --> 00:07:21,079
就是网络比较深的话

171
00:07:21,160 --> 00:07:22,680
那么它的值就会比较大

172
00:07:22,680 --> 00:07:24,759
因为里面这里全部是一些

173
00:07:24,920 --> 00:07:26,439
w的元素

174
00:07:26,439 --> 00:07:28,120
假设我每一个w的元素

175
00:07:28,120 --> 00:07:29,399
都是大于1的话

176
00:07:29,439 --> 00:07:30,840
而且乘数比较大的话

177
00:07:30,840 --> 00:07:33,120
那么这里面就会有非常大的值

178
00:07:33,920 --> 00:07:35,560
这就是t度爆炸

179
00:07:37,840 --> 00:07:39,399
那t度爆炸有什么问题

180
00:07:39,720 --> 00:07:40,560
就是说

181
00:07:41,200 --> 00:07:42,800
它的一些问题是说

182
00:07:42,840 --> 00:07:44,880
我的值可能太大了

183
00:07:45,080 --> 00:07:47,040
如果值超过我的浮点运算的话

184
00:07:47,040 --> 00:07:49,200
我就变成一个infinity

185
00:07:49,240 --> 00:07:50,320
就是一个无穷大

186
00:07:51,000 --> 00:07:52,800
这个东西对于浮点

187
00:07:52,800 --> 00:07:55,160
16位浮点数尤为严重

188
00:07:55,200 --> 00:07:57,360
我们现在很多时候用GPU的时候

189
00:07:57,360 --> 00:07:59,160
我们会使用16位浮点数

190
00:07:59,160 --> 00:08:00,480
这样子通常来说

191
00:08:00,680 --> 00:08:01,840
现在NVIDIA的GPU

192
00:08:01,840 --> 00:08:03,400
在16位浮点数

193
00:08:03,440 --> 00:08:05,560
比32位浮点数要快个两倍

194
00:08:05,600 --> 00:08:06,760
所以很多时候我们会采用

195
00:08:06,760 --> 00:08:07,760
16位浮点数

196
00:08:08,159 --> 00:08:10,920
16位浮点数的最大的问题是说

197
00:08:10,959 --> 00:08:13,519
它的数值区间其实是很低的

198
00:08:13,560 --> 00:08:15,480
它就是一个6亿-到

199
00:08:15,519 --> 00:08:19,199
6亿正4的一个区间

200
00:08:20,319 --> 00:08:21,399
那么就是说

201
00:08:21,439 --> 00:08:22,120
意味着说

202
00:08:22,120 --> 00:08:23,759
我这个区间其实很小

203
00:08:23,759 --> 00:08:26,159
如果你超出了我的值的区间

204
00:08:26,159 --> 00:08:27,839
那我就变成了无穷大

205
00:08:28,480 --> 00:08:30,079
所以这会给我带来问题

206
00:08:32,799 --> 00:08:36,000
那假设你没有到无穷大

207
00:08:36,039 --> 00:08:37,679
但是你还是会有很多问题

208
00:08:38,159 --> 00:08:39,720
最大的一个问题是说

209
00:08:39,720 --> 00:08:42,039
它对于学习率非常敏感

210
00:08:42,039 --> 00:08:43,840
就是我们学习率

211
00:08:43,840 --> 00:08:45,080
那个ATA比较敏感

212
00:08:46,639 --> 00:08:47,399
就是说

213
00:08:47,759 --> 00:08:51,200
如果我们的学习率调的太大

214
00:08:51,200 --> 00:08:52,679
或者说不是太大

215
00:08:52,679 --> 00:08:54,120
稍微大一点点

216
00:08:54,879 --> 00:08:57,240
那么就会带来比较大的参数的值

217
00:08:57,240 --> 00:08:58,559
因为我们一步走得比较远

218
00:08:58,559 --> 00:09:00,039
那么我对权重的更新

219
00:09:00,039 --> 00:09:01,559
就会权重会变得比较大

220
00:09:01,960 --> 00:09:03,159
那么权重一大

221
00:09:03,439 --> 00:09:04,159
对应的

222
00:09:04,159 --> 00:09:07,480
记得我们T字就是我们的权重的乘法

223
00:09:07,600 --> 00:09:09,240
那么就会带来更大的T度

224
00:09:09,639 --> 00:09:11,039
那么更大的T度

225
00:09:11,920 --> 00:09:13,560
会导致更大的参数值

226
00:09:13,560 --> 00:09:14,680
那就一直一直

227
00:09:14,680 --> 00:09:16,399
你迭代个几回

228
00:09:16,399 --> 00:09:18,480
你就会整个T度就炸掉了

229
00:09:18,680 --> 00:09:19,720
就变成无穷大了

230
00:09:21,560 --> 00:09:23,519
那么假设你的学习率太小

231
00:09:24,360 --> 00:09:26,680
就是说你说我不能调太大的学习率

232
00:09:26,960 --> 00:09:28,399
我得调很小的学习率

233
00:09:29,039 --> 00:09:30,159
因为我稍微大

234
00:09:30,159 --> 00:09:31,159
我就炸掉了

235
00:09:31,159 --> 00:09:32,639
那我就只能调非常小

236
00:09:33,240 --> 00:09:34,519
非常小的问题是说

237
00:09:34,519 --> 00:09:36,200
你就训练就没有进展了

238
00:09:37,160 --> 00:09:38,240
就是说你学习率太小

239
00:09:38,240 --> 00:09:43,000
每一次dw的增加就比较小

240
00:09:43,000 --> 00:09:45,280
这就导致我整个训练是跑不动了

241
00:09:46,360 --> 00:09:47,680
而且很有可能是说

242
00:09:47,680 --> 00:09:49,960
我们需要在训练过程中

243
00:09:49,960 --> 00:09:51,400
不断的调整学习率

244
00:09:51,759 --> 00:09:53,240
就是说在一开始

245
00:09:53,240 --> 00:09:57,280
我们可能整个权重都比较小的时候

246
00:09:57,280 --> 00:09:59,040
你可能学习率要稍微大一点点

247
00:09:59,280 --> 00:10:00,120
到后面的话

248
00:10:00,120 --> 00:10:01,480
你可能学习率的减少

249
00:10:01,480 --> 00:10:03,840
或者甚至你要做很动态的一些调整

250
00:10:03,840 --> 00:10:05,879
根据你的当前的T度来做

251
00:10:06,560 --> 00:10:09,560
所以整个这一块就导致说

252
00:10:09,560 --> 00:10:12,040
它不是说你真的

253
00:10:12,040 --> 00:10:13,520
假设你们要到infinity的话

254
00:10:13,520 --> 00:10:15,680
不是说它真的完全就不能训练

255
00:10:15,680 --> 00:10:18,560
只是说它会给你调学习率

256
00:10:18,560 --> 00:10:20,720
这个事情调的比较难调

257
00:10:21,680 --> 00:10:23,360
就是你的大一点点就炸掉了

258
00:10:23,360 --> 00:10:24,320
小一点点就不动

259
00:10:24,320 --> 00:10:26,360
所以在一个很小的一个学习率

260
00:10:26,360 --> 00:10:28,920
只有一个很小的范围是比较好的

261
00:10:29,280 --> 00:10:32,720
这个给你之后的模型训练调参

262
00:10:32,720 --> 00:10:33,759
带来很大的麻烦

263
00:10:34,000 --> 00:10:35,520
这是T度爆炸的问题

264
00:10:36,720 --> 00:10:38,640
我们知道之前还有个问题

265
00:10:38,640 --> 00:10:40,960
说T度如果会消失会怎么样

266
00:10:42,280 --> 00:10:45,440
就说我们举一个具体的例子

267
00:10:46,600 --> 00:10:49,879
假设我们用sigma的函数作为激活函数

268
00:10:51,080 --> 00:10:52,400
来回一下

269
00:10:52,400 --> 00:10:54,040
它就是1除以1

270
00:10:54,040 --> 00:10:57,759
加上指数1的负x是方

271
00:10:59,080 --> 00:11:01,080
那么它的导数大家可以算一下

272
00:11:01,080 --> 00:11:04,720
它就是它本身乘以1减去本身

273
00:11:05,680 --> 00:11:07,040
就是这个地方

274
00:11:08,399 --> 00:11:09,560
可以看到是说

275
00:11:09,840 --> 00:11:11,279
这个是你蓝色

276
00:11:11,279 --> 00:11:12,000
我们其实讲过

277
00:11:12,320 --> 00:11:15,960
蓝色是它的值的一个函数

278
00:11:15,960 --> 00:11:17,720
就是说它是在0和1之间

279
00:11:18,399 --> 00:11:20,720
它的T度是黄色这根线

280
00:11:22,080 --> 00:11:23,200
可以看到是说

281
00:11:23,200 --> 00:11:24,680
当你这个值很大

282
00:11:24,680 --> 00:11:26,800
就是说你的数是6的话

283
00:11:28,279 --> 00:11:29,279
它T度就很小

284
00:11:29,360 --> 00:11:30,879
基本上是到0了

285
00:11:32,120 --> 00:11:32,600
对吧

286
00:11:32,920 --> 00:11:33,680
也就是说

287
00:11:33,680 --> 00:11:35,920
当你的对于激活函数

288
00:11:35,920 --> 00:11:38,560
当你的输入稍微大一点点的时候

289
00:11:39,040 --> 00:11:41,440
那么它的输它的导数

290
00:11:41,440 --> 00:11:42,560
它就会变成0

291
00:11:43,840 --> 00:11:46,159
那么我们再来看一下会什么问题

292
00:11:47,320 --> 00:11:48,519
同样的话说

293
00:11:50,040 --> 00:11:52,000
你这一块就很有可能

294
00:11:52,000 --> 00:11:53,279
如果你这个输入

295
00:11:53,320 --> 00:11:55,240
相对来说稍微大一点的话

296
00:11:55,440 --> 00:11:56,960
那么这个东西就会变成0

297
00:11:57,960 --> 00:11:59,560
那么就意味着说

298
00:11:59,560 --> 00:12:00,960
你这里面可能有

299
00:12:01,279 --> 00:12:02,040
它也不是变成0

300
00:12:02,039 --> 00:12:03,159
它就变得很小

301
00:12:04,000 --> 00:12:05,480
那么这意味着说

302
00:12:05,480 --> 00:12:06,279
你可能会有

303
00:12:06,279 --> 00:12:08,599
低减T个小数值的成绩

304
00:12:09,639 --> 00:12:10,240
就是说

305
00:12:10,240 --> 00:12:12,079
那么每一个数可能都比较小的话

306
00:12:12,079 --> 00:12:13,439
如果你的输入比较大

307
00:12:13,439 --> 00:12:14,839
那么的T度就变得很小

308
00:12:14,879 --> 00:12:15,599
那么它的

309
00:12:15,599 --> 00:12:17,079
比如说我们之前讲过

310
00:12:17,159 --> 00:12:18,399
就算是0.8

311
00:12:18,599 --> 00:12:19,439
做100次

312
00:12:19,439 --> 00:12:21,079
那也变成2的-10次方

313
00:12:22,000 --> 00:12:23,759
那么你的T度就很小了

314
00:12:26,839 --> 00:12:28,559
T度消失有什么问题呢

315
00:12:29,439 --> 00:12:30,559
最严重的问题是说

316
00:12:30,559 --> 00:12:31,799
T度只变成0

317
00:12:32,639 --> 00:12:34,599
因为你16倍浮点数

318
00:12:34,959 --> 00:12:36,599
如果你是小于一个

319
00:12:36,639 --> 00:12:38,360
比如说5E-4的话

320
00:12:38,480 --> 00:12:40,079
基本上你可以把它当0了

321
00:12:40,079 --> 00:12:43,000
就是0.0005的时候

322
00:12:43,039 --> 00:12:43,919
那么你浮点数

323
00:12:43,919 --> 00:12:45,439
就基本上把它当0看

324
00:12:46,159 --> 00:12:47,919
如果你的T度是变成0

325
00:12:48,639 --> 00:12:50,639
那么你不管怎么学你的学习率

326
00:12:51,679 --> 00:12:52,839
你都不会有进展

327
00:12:53,519 --> 00:12:55,559
就是你学习率不管取决多大

328
00:12:55,559 --> 00:12:56,559
因为你的权重

329
00:12:56,599 --> 00:12:58,879
就是学习率乘以你的T度

330
00:12:58,919 --> 00:13:00,519
你的T度已经是0的话

331
00:13:00,799 --> 00:13:02,519
那么你就现在就不会有进展

332
00:13:04,399 --> 00:13:05,600
而且这个是说

333
00:13:05,600 --> 00:13:08,360
对于你的比较深的网络的时候

334
00:13:08,360 --> 00:13:12,199
对底层的尤为严重

335
00:13:13,000 --> 00:13:14,240
这是因为说

336
00:13:14,240 --> 00:13:16,240
当你的神经网络比较深的时候

337
00:13:16,279 --> 00:13:18,519
记得大家做T度反传的时候

338
00:13:18,519 --> 00:13:19,960
是从顶开始的

339
00:13:20,679 --> 00:13:21,439
那么顶部

340
00:13:21,439 --> 00:13:22,439
因为你就有一些

341
00:13:22,439 --> 00:13:23,679
比如说顶部第一层

342
00:13:23,679 --> 00:13:25,000
就是一次取得成反

343
00:13:25,039 --> 00:13:26,879
那么你的T度可能是正常的

344
00:13:26,919 --> 00:13:27,960
越到下面

345
00:13:28,279 --> 00:13:30,039
那么你就一直乘

346
00:13:30,039 --> 00:13:31,759
那么你的T度会变得特别小

347
00:13:32,000 --> 00:13:33,079
那么你的底部

348
00:13:33,319 --> 00:13:36,039
就无法让底部的层

349
00:13:36,039 --> 00:13:37,559
你拿到的T度就是很小了

350
00:13:37,559 --> 00:13:38,360
可能就是0

351
00:13:39,799 --> 00:13:41,559
那么底部那些层

352
00:13:41,559 --> 00:13:43,399
就是靠数据进的那些层

353
00:13:43,679 --> 00:13:45,240
如果你的T度是0的话

354
00:13:45,240 --> 00:13:48,759
那你不管怎么样做学习率

355
00:13:49,039 --> 00:13:50,120
你都不会有进展

356
00:13:51,360 --> 00:13:52,559
那么这个意味着什么问题

357
00:13:53,159 --> 00:13:54,199
意味着是说

358
00:13:54,240 --> 00:13:56,279
你不管把神经网络加的多深

359
00:13:56,959 --> 00:13:58,839
你底部那些层你跑不动

360
00:13:59,080 --> 00:14:00,759
你就把顶部那些层训练好

361
00:14:00,960 --> 00:14:01,759
那就意味着说

362
00:14:01,759 --> 00:14:03,600
你跟一个很浅的神经网络

363
00:14:03,600 --> 00:14:04,840
是没有本质区别的

364
00:14:06,399 --> 00:14:08,800
这就是T度消失的问题

365
00:14:09,800 --> 00:14:12,360
所以总结一下

366
00:14:12,360 --> 00:14:13,160
就是说

367
00:14:14,360 --> 00:14:18,519
当数值太大或者太小的时候

368
00:14:19,480 --> 00:14:21,040
都会导致数值问题

369
00:14:22,879 --> 00:14:26,200
这个常发生在深度神经网络里面

370
00:14:26,240 --> 00:14:27,800
因为你有很多层

371
00:14:27,800 --> 00:14:28,720
你的T度

372
00:14:28,760 --> 00:14:32,200
其实就是对n个层做累成

373
00:14:33,400 --> 00:14:35,080
如果你的权重稍微大一点点

374
00:14:35,080 --> 00:14:37,120
或者你的前项的输出

375
00:14:37,120 --> 00:14:38,120
稍微大一点点

376
00:14:38,160 --> 00:14:39,880
那么就会导致T度会炸掉

377
00:14:40,800 --> 00:14:42,360
如果你的值比较小

378
00:14:42,360 --> 00:14:43,320
或者你激活函数

379
00:14:43,320 --> 00:14:44,920
使得你的值变得比较小

380
00:14:45,200 --> 00:14:47,440
那就变成n个很小的数做惩罚

381
00:14:47,480 --> 00:14:49,080
那就导致很小

382
00:14:50,200 --> 00:14:50,880
就是说

383
00:14:50,920 --> 00:14:53,760
这个是我们常见的两类问题

384
00:14:53,760 --> 00:14:55,360
就是说我们要既要避免

385
00:14:55,360 --> 00:14:56,640
我们的T度不要太大

386
00:14:56,639 --> 00:14:59,080
我们也要避免我们的T度不能太小

387
00:15:01,000 --> 00:15:01,559
OK

388
00:15:02,159 --> 00:15:04,279
这个就是数值稳定性里面

389
00:15:04,279 --> 00:15:05,319
常见两个问题

390
00:15:06,240 --> 00:15:07,199
我们接下来

391
00:15:07,240 --> 00:15:08,120
我们就

392
00:15:09,319 --> 00:15:10,279
接下来我们先讲

393
00:15:10,279 --> 00:15:12,000
让训练如何更加稳定

394
00:15:12,000 --> 00:15:14,960
然后我们再来自己

395
00:15:15,039 --> 00:15:16,600
然后我们再来做一次QA

