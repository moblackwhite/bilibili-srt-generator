1
00:00:00,000 --> 00:00:00,720
好

2
00:00:00,720 --> 00:00:01,480
我们来看一下

3
00:00:01,500 --> 00:00:04,000
语言模型的

4
00:00:04,560 --> 00:00:06,600
就n边语法的一些实现

5
00:00:06,600 --> 00:00:13,200
以及说一个我们之后会用来训练语言模型的一个数据集

6
00:00:14,519 --> 00:00:18,199
首先我们之前有昨天有讲过

7
00:00:18,359 --> 00:00:22,120
就是把那个time machine数据集给你读出来

8
00:00:22,400 --> 00:00:24,000
然后读出来之后

9
00:00:24,000 --> 00:00:24,960
tokenize一下

10
00:00:24,960 --> 00:00:28,280
tokenize之后能够得到一个corpus

11
00:00:28,320 --> 00:00:29,320
corpus来

12
00:00:29,320 --> 00:00:32,799
然后我们当然是可以去做一个wkab

13
00:00:33,039 --> 00:00:34,439
wkab拿出来之后

14
00:00:34,600 --> 00:00:39,280
我们记得我们存过所谓的token的frequency

15
00:00:40,759 --> 00:00:42,159
我们昨天有讲过

16
00:00:42,159 --> 00:00:43,439
就基本上可以看到是说

17
00:00:43,960 --> 00:00:45,640
最常见的是the

18
00:00:45,960 --> 00:00:51,400
就是然后I end off a to a worse in that mine

19
00:00:51,480 --> 00:00:56,560
就是说这词这常见词基本上叫做stop words

20
00:00:56,799 --> 00:00:58,920
就是说我不知道停词

21
00:00:59,120 --> 00:01:02,120
就是说就这个词就是一些虚词

22
00:01:02,760 --> 00:01:06,719
在英语里面都是一些the end off这种东西

23
00:01:07,599 --> 00:01:08,840
叫做介词叫什么

24
00:01:08,840 --> 00:01:10,519
反正就是说你随便叫什么东西

25
00:01:10,640 --> 00:01:12,920
叫一般我们叫做stop words

26
00:01:12,960 --> 00:01:15,120
就是说他们就是大量的出现

27
00:01:15,159 --> 00:01:17,920
但是有没有对整个文本理解

28
00:01:17,920 --> 00:01:20,840
其实就是说没什么太多用

29
00:01:21,520 --> 00:01:25,439
就是说写上它主要是一个语法正不正确问题

30
00:01:25,599 --> 00:01:28,480
中国人写英文英语经常不用的这个东西

31
00:01:28,760 --> 00:01:30,719
所以你基本上也不会出什么问题

32
00:01:31,600 --> 00:01:34,079
你不用that也不会出什么问题

33
00:01:35,840 --> 00:01:38,240
然后主要看一下就是说

34
00:01:39,320 --> 00:01:42,800
那些或者中文叫停用词叫stop words

35
00:01:43,000 --> 00:01:44,560
然后我们看一下我们可以画一个图

36
00:01:44,760 --> 00:01:45,400
画一个图

37
00:01:45,400 --> 00:01:47,560
就是说看看每个token出现的概率

38
00:01:47,880 --> 00:01:49,960
就是说注意到这个地方

39
00:01:49,960 --> 00:01:55,400
我们的xscale和yscale都用的是lock

40
00:01:55,840 --> 00:01:57,359
就你看我们用的是lock

41
00:01:57,959 --> 00:02:01,120
我们的x也是用的lock

42
00:02:02,879 --> 00:02:04,359
就你这个地方看的是什么

43
00:02:04,920 --> 00:02:11,360
看的是一个token每个token它出现的频率

44
00:02:11,680 --> 00:02:13,080
然后这个token是排序的

45
00:02:13,120 --> 00:02:14,800
就是说出现最高的在最前面

46
00:02:14,919 --> 00:02:17,159
记得我们是排过序的

47
00:02:17,919 --> 00:02:21,159
然后可以看到是这个y是你出现的frequency

48
00:02:21,280 --> 00:02:23,120
就是说其实不叫frequency

49
00:02:23,240 --> 00:02:25,520
就是nx就出现的次数

50
00:02:26,480 --> 00:02:27,960
就如果你再取个log

51
00:02:28,240 --> 00:02:29,320
你y取个log的话

52
00:02:29,320 --> 00:02:31,960
基本上是一个power law distribution

53
00:02:32,080 --> 00:02:34,240
就是说一个这样子

54
00:02:34,240 --> 00:02:36,160
在log上面是一个线性

55
00:02:36,520 --> 00:02:37,719
log上面是个线性

56
00:02:37,719 --> 00:02:40,200
就是说这个东西就很好玩

57
00:02:40,200 --> 00:02:41,280
就是意味着说

58
00:02:41,280 --> 00:02:45,640
其实你可能80%出现的词

59
00:02:46,200 --> 00:02:48,800
都是在20%里面

60
00:02:50,080 --> 00:02:52,760
这个社会就是一个power law分布

61
00:02:52,920 --> 00:02:54,840
就是说比如说80%的财富

62
00:02:54,840 --> 00:02:57,520
居然在20%的人手里

63
00:02:58,000 --> 00:02:59,439
现在可能90%的财富

64
00:02:59,439 --> 00:03:02,120
在1%的人手里

65
00:03:03,080 --> 00:03:04,240
另外一块就是说

66
00:03:04,240 --> 00:03:06,840
你在一个网站上

67
00:03:07,039 --> 00:03:08,439
比如说电影网站

68
00:03:08,560 --> 00:03:10,280
或者卖货的网站上

69
00:03:10,280 --> 00:03:11,879
你可能80%人买的

70
00:03:11,879 --> 00:03:13,879
就算20%的货物

71
00:03:15,319 --> 00:03:19,439
所以自然语言也符合这个模型

72
00:03:19,439 --> 00:03:21,879
就是说基本上可以看到是说

73
00:03:21,879 --> 00:03:26,960
你前面这一块是那些停用词

74
00:03:26,960 --> 00:03:27,879
就是特别频繁

75
00:03:28,039 --> 00:03:29,960
接下来就词一直往下降

76
00:03:30,000 --> 00:03:32,039
最后那些词可能就没出现几次

77
00:03:32,039 --> 00:03:33,199
比如说这个是10次

78
00:03:33,759 --> 00:03:36,560
这一个是最多出现10次

79
00:03:36,560 --> 00:03:39,000
然后你看到大概100

80
00:03:39,000 --> 00:03:40,719
就这个地方大概是这个地方

81
00:03:40,719 --> 00:03:42,120
大概两三百的样子

82
00:03:42,560 --> 00:03:45,000
就两三百个词是出现了

83
00:03:45,000 --> 00:03:46,319
超过10次的

84
00:03:46,359 --> 00:03:48,840
然后后面现在我们大概有一千多个词

85
00:03:48,919 --> 00:03:50,879
这个地方就我们用的word

86
00:03:51,599 --> 00:03:57,039
所以大概是将近一千个词

87
00:03:57,159 --> 00:03:59,439
是没有出现超过10次的

88
00:04:00,759 --> 00:04:02,439
所以这种词就是比较

89
00:04:03,439 --> 00:04:04,519
我们文本不大

90
00:04:05,039 --> 00:04:06,159
所以就几千个词

91
00:04:06,159 --> 00:04:07,960
我们的就是一本书

92
00:04:08,159 --> 00:04:10,519
但是你看到你可以把它弄大一点

93
00:04:10,519 --> 00:04:11,039
弄大一点

94
00:04:11,039 --> 00:04:11,960
其实还是一样的

95
00:04:11,960 --> 00:04:13,359
就是说你有很多生批词

96
00:04:13,639 --> 00:04:16,079
或者是比如说GIE词汇

97
00:04:16,319 --> 00:04:18,480
是几乎是不怎么出现的

98
00:04:19,200 --> 00:04:20,360
所以意味着说

99
00:04:21,200 --> 00:04:23,360
它可能对你整个模型的贡献

100
00:04:23,360 --> 00:04:25,080
真的可能是训练

101
00:04:25,080 --> 00:04:27,160
通常是不充分的

102
00:04:28,480 --> 00:04:30,520
所以经常我们会把这些词评

103
00:04:30,560 --> 00:04:32,120
特别小的词过滤掉

104
00:04:32,400 --> 00:04:32,960
这样子后

105
00:04:32,960 --> 00:04:35,520
我们其实可以过滤掉很多词出去

106
00:04:35,560 --> 00:04:39,120
假设我要把出现少于10次的词

107
00:04:39,120 --> 00:04:40,280
全部过滤掉的话

108
00:04:40,319 --> 00:04:42,439
那么可能就只剩个几百个词了

109
00:04:42,439 --> 00:04:43,480
而不是我们

110
00:04:44,000 --> 00:04:45,080
如果你什么都不过滤

111
00:04:45,080 --> 00:04:47,200
可能是现在是两三千的样子

112
00:04:49,000 --> 00:04:52,200
另外一个是我们做二元语法

113
00:04:53,280 --> 00:04:54,759
就background

114
00:04:55,360 --> 00:04:57,200
就是说background这个事情

115
00:04:57,200 --> 00:04:58,319
其实很容易算

116
00:04:58,319 --> 00:04:58,759
对吧

117
00:04:58,800 --> 00:04:59,360
你怎么算

118
00:04:59,520 --> 00:05:01,280
就是你的Corpus里面

119
00:05:01,759 --> 00:05:03,160
每一次就是

120
00:05:03,439 --> 00:05:05,840
你就把所有的一个

121
00:05:05,840 --> 00:05:07,360
就是说做个zip

122
00:05:07,680 --> 00:05:08,480
Python里面的zip

123
00:05:08,480 --> 00:05:12,400
就是说你从0一直到把最后一个拿掉

124
00:05:12,400 --> 00:05:14,720
和从1开始

125
00:05:14,759 --> 00:05:15,439
这样子的话

126
00:05:15,439 --> 00:05:16,720
每zip出来之后

127
00:05:16,880 --> 00:05:17,800
就每一次拿到了

128
00:05:17,800 --> 00:05:22,160
就是一个元素和它的元素后面一个

129
00:05:23,080 --> 00:05:24,360
就拿了一个pair

130
00:05:24,760 --> 00:05:26,440
然后拿到所谓的pair的话

131
00:05:26,560 --> 00:05:28,280
我们就丢到我的vocab里面

132
00:05:28,280 --> 00:05:30,080
就会创起一个vocab

133
00:05:30,880 --> 00:05:32,400
这样子的我们的token

134
00:05:32,560 --> 00:05:33,120
我们的token

135
00:05:33,160 --> 00:05:35,080
这就是一个两个words

136
00:05:36,240 --> 00:05:36,760
OK

137
00:05:36,800 --> 00:05:40,080
然后我们去看top的frequency

138
00:05:40,280 --> 00:05:41,800
就是可以看到是

139
00:05:41,840 --> 00:05:43,720
of the

140
00:05:43,720 --> 00:05:44,560
in the

141
00:05:44,560 --> 00:05:45,920
然后I had

142
00:05:45,920 --> 00:05:47,040
I was

143
00:05:47,040 --> 00:05:48,240
就是说这种东西

144
00:05:48,800 --> 00:05:50,960
然后另外一个

145
00:05:50,960 --> 00:05:53,040
你可以做

146
00:05:55,120 --> 00:05:56,000
当你做了两个

147
00:05:56,000 --> 00:05:57,000
你可以做tryground

148
00:05:57,160 --> 00:05:58,160
就三元

149
00:05:58,560 --> 00:06:00,439
三元的话就是一样的道理

150
00:06:00,439 --> 00:06:01,840
就是说你还是自己铺一下

151
00:06:01,840 --> 00:06:03,040
然后出一个triple

152
00:06:03,040 --> 00:06:06,040
就是每一次拿到是一个这样子的triple

153
00:06:06,319 --> 00:06:07,439
然后可以看一下

154
00:06:07,439 --> 00:06:08,800
最常见的triple是谁

155
00:06:09,560 --> 00:06:12,759
就是the time traveler

156
00:06:12,759 --> 00:06:15,160
就是说这个时间旅行人

157
00:06:15,280 --> 00:06:16,320
the time machine

158
00:06:16,800 --> 00:06:18,160
就是说你可以看到

159
00:06:18,480 --> 00:06:20,720
就一元词二元词

160
00:06:20,720 --> 00:06:23,520
它的就是常见的东西

161
00:06:23,760 --> 00:06:25,480
就是还是挺stop words

162
00:06:25,480 --> 00:06:28,200
就是说可能哪个文本都一样

163
00:06:28,440 --> 00:06:30,160
但是你到三元的话

164
00:06:30,400 --> 00:06:31,240
出现三元

165
00:06:31,240 --> 00:06:32,440
就英语里面很少

166
00:06:32,440 --> 00:06:35,640
我加三个这种虚词放在一起

167
00:06:35,920 --> 00:06:38,040
但是所以你碰到三元的话

168
00:06:38,120 --> 00:06:39,440
你这边看到还是

169
00:06:39,480 --> 00:06:41,879
the medical man

170
00:06:42,960 --> 00:06:44,080
这个是很常见的

171
00:06:45,160 --> 00:06:49,200
这三个还是挺反映文章的趋势

172
00:06:49,720 --> 00:06:50,760
当然你可以做到更长

173
00:06:50,840 --> 00:06:55,760
就是可以做到更长的四元五元

174
00:06:55,880 --> 00:06:57,600
通常你越长的话

175
00:06:57,600 --> 00:07:00,400
你可以看到那些经常出现的词

176
00:07:00,400 --> 00:07:02,560
可能就是跟你整个文本

177
00:07:02,560 --> 00:07:04,000
它是什么类型相关的

178
00:07:05,680 --> 00:07:08,080
然后接下来我们可以说

179
00:07:08,080 --> 00:07:09,240
我们就刚刚一样

180
00:07:09,240 --> 00:07:12,360
我们把所有的这种frequency画出来

181
00:07:12,600 --> 00:07:14,360
就可以看到我们

182
00:07:14,400 --> 00:07:18,680
Uni-Gram就是我们的一元词

183
00:07:18,680 --> 00:07:19,759
Bi-Gram是二元

184
00:07:19,759 --> 00:07:21,360
Tri-Gram是三元

185
00:07:22,040 --> 00:07:23,680
就是这个我们之前看过了

186
00:07:24,560 --> 00:07:27,000
有意思的是说你的Bi-Gram

187
00:07:27,000 --> 00:07:28,759
也是一个Power Law Distribution

188
00:07:28,879 --> 00:07:31,600
也就是一个按照指数的Decay

189
00:07:32,040 --> 00:07:35,000
然后你的Tri-Gram也是这样子的

190
00:07:35,560 --> 00:07:37,360
就是说很好理解

191
00:07:37,520 --> 00:07:42,439
就是说你绝大部分的三元那种序列

192
00:07:42,840 --> 00:07:44,199
就常规三的序列

193
00:07:44,439 --> 00:07:46,080
它出现的概率真的很低

194
00:07:46,840 --> 00:07:48,199
就可能就出现个一两次

195
00:07:48,800 --> 00:07:49,920
你看绝大部分词

196
00:07:51,160 --> 00:07:52,120
出现是一次

197
00:07:52,120 --> 00:07:53,520
这十的两次方就一次

198
00:07:53,680 --> 00:07:56,720
就是说可能一半以上的

199
00:07:57,400 --> 00:07:59,879
是出现的是这样子

200
00:08:00,920 --> 00:08:02,560
所以这个东西就挺好的

201
00:08:02,720 --> 00:08:03,280
是吧

202
00:08:03,319 --> 00:08:06,560
就是说你做恩怨语法的时候

203
00:08:06,560 --> 00:08:08,079
我们之前有算过复杂度

204
00:08:08,240 --> 00:08:09,040
复杂度就是说

205
00:08:09,040 --> 00:08:11,800
我们说这是跟恩成一个指数关系

206
00:08:12,439 --> 00:08:15,639
假设你的整个我开谱的大小是1000的话

207
00:08:15,680 --> 00:08:17,240
那么你算二元的话

208
00:08:17,240 --> 00:08:18,879
那就是100万

209
00:08:18,879 --> 00:08:19,680
你三元的话

210
00:08:19,800 --> 00:08:20,439
就是一个billion

211
00:08:20,439 --> 00:08:21,319
就10个亿了

212
00:08:22,160 --> 00:08:23,720
所以4元就算不动了

213
00:08:24,439 --> 00:08:26,199
但是你实际上来说

214
00:08:26,199 --> 00:08:27,519
是一个Power Law Distribution

215
00:08:27,519 --> 00:08:29,040
就是说意味着你可以砍一刀

216
00:08:29,040 --> 00:08:31,600
就是说如果一个东西出现的概率

217
00:08:31,600 --> 00:08:33,840
出现的次数少于某一个频率的话

218
00:08:33,840 --> 00:08:35,279
我就不管它的话

219
00:08:35,319 --> 00:08:36,959
那么留下的是这一块

220
00:08:37,120 --> 00:08:38,159
这块你看上去

221
00:08:38,159 --> 00:08:39,199
其实三元词

222
00:08:39,399 --> 00:08:41,319
其实比你想象的要小很多

223
00:08:41,319 --> 00:08:43,199
它比Unigram还要小

224
00:08:44,120 --> 00:08:45,679
因为你看你砍一刀之后

225
00:08:45,839 --> 00:08:48,919
剩下的词大概就是几百的样子

226
00:08:48,959 --> 00:08:53,879
所以根本就不是我们所谓的一个billion

227
00:08:54,879 --> 00:08:57,759
因为这也是因为我们文本特别小

228
00:08:57,759 --> 00:08:59,480
我们就是一本很小的书

229
00:08:59,519 --> 00:09:02,199
当然你做到一个比较大的文本库的话

230
00:09:02,199 --> 00:09:05,159
如果说几百个G的数据的话

231
00:09:05,159 --> 00:09:08,120
那么当然你可能三元词还是会很多

232
00:09:08,159 --> 00:09:10,719
但是很有可能它不是一个

233
00:09:10,839 --> 00:09:12,480
确实是一个指数的关系

234
00:09:13,559 --> 00:09:15,879
如果你做一次低频词过滤的话

235
00:09:16,439 --> 00:09:18,039
那么你可能留下的东西

236
00:09:18,039 --> 00:09:20,199
还真是可以存下来的

237
00:09:20,879 --> 00:09:22,879
这也是说为什么在实际中

238
00:09:23,039 --> 00:09:23,879
我们说

239
00:09:24,559 --> 00:09:27,639
我记得以前读过一篇诺本

240
00:09:27,639 --> 00:09:28,719
说一个广告系统

241
00:09:28,839 --> 00:09:29,959
他用的是

242
00:09:31,199 --> 00:09:32,600
应该是用了7元

243
00:09:34,000 --> 00:09:34,879
用的是7

244
00:09:34,879 --> 00:09:37,919
所以他对所有的7元组的抽特征

245
00:09:37,919 --> 00:09:39,200
把它拿出来

246
00:09:39,240 --> 00:09:39,840
也不大

247
00:09:39,840 --> 00:09:40,879
也大概7元组

248
00:09:40,879 --> 00:09:44,000
可能当时他们的广告也挺大的

249
00:09:44,120 --> 00:09:45,200
就几个T的数据

250
00:09:45,600 --> 00:09:47,240
抽出来大概也可能就

251
00:09:47,799 --> 00:09:48,439
可能

252
00:09:48,840 --> 00:09:49,200
不大

253
00:09:49,320 --> 00:09:51,200
我觉得就几千万的样子

254
00:09:51,360 --> 00:09:53,360
所以就是说实际中

255
00:09:53,360 --> 00:09:55,399
因为你绝大部分词

256
00:09:55,840 --> 00:09:58,080
很长的序列出现

257
00:09:58,439 --> 00:09:59,840
真的是有特定意义的

258
00:09:59,840 --> 00:10:01,600
才会重复出现

259
00:10:01,600 --> 00:10:04,000
所以绝大部分出现可能是很少

260
00:10:04,000 --> 00:10:05,440
所以你基本可以过滤掉

261
00:10:05,440 --> 00:10:07,360
以至于说在实际中

262
00:10:07,720 --> 00:10:09,360
用n-grand

263
00:10:09,360 --> 00:10:10,159
就n取

264
00:10:10,159 --> 00:10:11,120
你可以取得比较大

265
00:10:11,120 --> 00:10:12,240
也是能做的

266
00:10:13,120 --> 00:10:13,679
OK

267
00:10:14,279 --> 00:10:15,559
这就是从实际上

268
00:10:15,559 --> 00:10:16,440
所以为什么

269
00:10:16,720 --> 00:10:18,120
虽然n-grand说

270
00:10:18,440 --> 00:10:19,960
是一个指数级的增加

271
00:10:19,960 --> 00:10:21,840
但是实际中也用的比较多

272
00:10:23,960 --> 00:10:24,240
好

273
00:10:24,240 --> 00:10:25,879
接下来我们要看的是

274
00:10:26,000 --> 00:10:27,480
另外一个比较重要的话题

275
00:10:28,720 --> 00:10:31,039
我们没有用slice来讲这个东西

276
00:10:32,240 --> 00:10:33,159
是说

277
00:10:34,120 --> 00:10:35,720
假设我已经

278
00:10:35,720 --> 00:10:37,000
我们上次已经讲过了

279
00:10:37,000 --> 00:10:38,720
假设我们把一个文本

280
00:10:39,279 --> 00:10:40,039
每一个词

281
00:10:40,039 --> 00:10:40,600
每个token

282
00:10:40,600 --> 00:10:42,000
也表示成一个

283
00:10:43,639 --> 00:10:44,519
表示成一个

284
00:10:45,039 --> 00:10:47,399
数标的数字的下标的话

285
00:10:47,600 --> 00:10:49,320
我们叫做corpus的话

286
00:10:50,399 --> 00:10:52,399
接下来我们是一个序列

287
00:10:52,480 --> 00:10:55,399
接下来我们要把它变成一个mini-batch

288
00:10:57,080 --> 00:10:59,720
我们在讲序列模型的时候

289
00:10:59,720 --> 00:11:00,679
也给大家讲过

290
00:11:00,679 --> 00:11:01,440
就是说

291
00:11:01,840 --> 00:11:02,279
说白了

292
00:11:03,079 --> 00:11:03,639
语言模型

293
00:11:03,759 --> 00:11:04,799
也是个语言模型

294
00:11:04,919 --> 00:11:08,000
就是给一个前面常为

295
00:11:08,360 --> 00:11:10,039
tau的一个序列

296
00:11:10,240 --> 00:11:11,839
去预测下一个词

297
00:11:13,000 --> 00:11:13,279
对吧

298
00:11:13,360 --> 00:11:14,759
前面常为tau的序列

299
00:11:14,759 --> 00:11:15,639
是我的样本

300
00:11:16,039 --> 00:11:16,919
我的x

301
00:11:16,959 --> 00:11:19,000
我下一个词是我的y

302
00:11:20,199 --> 00:11:22,240
现在是我们之前

303
00:11:22,439 --> 00:11:22,879
最简单

304
00:11:22,959 --> 00:11:24,360
就是一次滑过去了

305
00:11:24,639 --> 00:11:27,399
现在我们再来详细讲一下

306
00:11:27,439 --> 00:11:27,959
说

307
00:11:28,000 --> 00:11:30,480
我们怎么样来生成这样子的序列

308
00:11:30,560 --> 00:11:32,680
就跟之前是其实还挺像的

309
00:11:32,960 --> 00:11:34,399
但是我们这里就是说

310
00:11:35,279 --> 00:11:38,000
怎么样做得更通用一点

311
00:11:39,480 --> 00:11:42,840
所以我们这里会讲两种算法

312
00:11:44,120 --> 00:11:44,879
每一个算法

313
00:11:44,960 --> 00:11:45,639
就是说

314
00:11:45,920 --> 00:11:47,560
每一次给我一个corpus

315
00:11:47,560 --> 00:11:49,680
就是说给我一个很长的一个序列

316
00:11:49,720 --> 00:11:51,519
然后给我batch size

317
00:11:51,680 --> 00:11:53,680
number of steps

318
00:11:53,680 --> 00:11:55,600
就是说时间长度

319
00:11:55,600 --> 00:11:56,639
就是t

320
00:11:58,840 --> 00:11:59,120
t

321
00:11:59,120 --> 00:12:00,200
就是说

322
00:12:00,840 --> 00:12:01,960
你可以认为

323
00:12:02,840 --> 00:12:04,080
也就是说

324
00:12:04,080 --> 00:12:04,639
你可以认为

325
00:12:04,639 --> 00:12:06,679
就是我们的markov假设那个tau

326
00:12:06,919 --> 00:12:08,799
就是实际上是差不多一个东西

327
00:12:09,519 --> 00:12:13,240
每一次我取多少个长的序列

328
00:12:13,279 --> 00:12:14,200
来预测下一个

329
00:12:14,200 --> 00:12:16,399
就是那个t

330
00:12:17,480 --> 00:12:17,759
OK

331
00:12:17,759 --> 00:12:18,600
我们先看一下

332
00:12:18,600 --> 00:12:19,759
第一个想法

333
00:12:19,759 --> 00:12:20,560
就是说

334
00:12:21,360 --> 00:12:22,320
怎么做呢

335
00:12:22,919 --> 00:12:23,799
就是说

336
00:12:24,320 --> 00:12:25,680
第一个想法就是最简单

337
00:12:25,680 --> 00:12:27,120
就是sequence data

338
00:12:27,120 --> 00:12:27,680
iterate

339
00:12:27,680 --> 00:12:28,320
random

340
00:12:28,320 --> 00:12:29,200
就sequence data

341
00:12:29,200 --> 00:12:30,720
就是我在一个sequence data上

342
00:12:30,720 --> 00:12:31,680
做一个iterator

343
00:12:31,680 --> 00:12:32,720
然后是一个random

344
00:12:33,280 --> 00:12:34,960
就是说做法很简单

345
00:12:36,320 --> 00:12:39,440
每一次我们在一个样本中间

346
00:12:39,760 --> 00:12:42,200
随意随机采样一个

347
00:12:42,200 --> 00:12:44,800
常为t的一个序列

348
00:12:45,680 --> 00:12:47,920
然后把它后面那个东西做成y

349
00:12:48,480 --> 00:12:51,280
然后这样子做batch size的东西

350
00:12:51,320 --> 00:12:53,000
这样子获得我们的

351
00:12:53,600 --> 00:12:54,640
一个batch size

352
00:12:55,639 --> 00:12:58,519
然后这里稍微有一点点不一样

353
00:12:58,679 --> 00:13:00,840
不一样的地方是说

354
00:13:01,039 --> 00:13:04,199
我们在你可以回忆一下

355
00:13:04,199 --> 00:13:05,639
我们在sequence data里面

356
00:13:05,639 --> 00:13:06,360
是怎么做的

357
00:13:06,360 --> 00:13:06,879
sequence

358
00:13:06,879 --> 00:13:08,960
就序列模型里面是说

359
00:13:08,960 --> 00:13:10,199
我们真的就把序列

360
00:13:10,199 --> 00:13:11,960
就是一个排下去

361
00:13:13,240 --> 00:13:14,120
一个排下去

362
00:13:14,120 --> 00:13:15,399
最大的问题是什么

363
00:13:15,679 --> 00:13:16,559
最大的问题

364
00:13:16,720 --> 00:13:17,639
我们干脆用

365
00:13:19,840 --> 00:13:21,399
用这个讲一下

366
00:13:24,919 --> 00:13:27,840
就假设我有一个常为一个很长的序列

367
00:13:27,960 --> 00:13:28,519
一个

368
00:13:29,240 --> 00:13:30,120
n的序列

369
00:13:31,000 --> 00:13:33,000
然后我们之前讲的

370
00:13:33,000 --> 00:13:35,279
其实是说

371
00:13:35,399 --> 00:13:39,559
我们要在随机的一个地方采样

372
00:13:39,559 --> 00:13:40,399
然后

373
00:13:41,439 --> 00:13:42,360
这个作为x

374
00:13:42,360 --> 00:13:43,360
这个作为y

375
00:13:45,039 --> 00:13:46,919
然后我们这里一个想法是说

376
00:13:46,919 --> 00:13:48,439
我们之前讲的是说

377
00:13:48,439 --> 00:13:51,000
我们就把所有东西抽出来

378
00:13:51,000 --> 00:13:52,600
就在序列模型里面

379
00:13:52,600 --> 00:13:53,799
现在我们要讲一个东西

380
00:13:53,800 --> 00:13:54,920
就说那个东西太贵了一点

381
00:13:54,920 --> 00:13:56,240
那个东西就是说你

382
00:13:57,240 --> 00:13:58,400
每变了一次数据

383
00:13:58,560 --> 00:14:00,680
其实每一个数据被用过很多次

384
00:14:00,680 --> 00:14:02,800
就你比如说同样一个数据

385
00:14:02,800 --> 00:14:03,600
在这个地方

386
00:14:04,200 --> 00:14:05,360
它用在这个序列里面

387
00:14:05,360 --> 00:14:07,160
它也可能会用在下一个序列里面

388
00:14:07,160 --> 00:14:07,600
对吧

389
00:14:09,080 --> 00:14:10,560
所以我们这里是说

390
00:14:10,680 --> 00:14:12,840
我们这个地方扫一遍数据

391
00:14:12,840 --> 00:14:15,000
就所谓的数据只用过一次

392
00:14:16,400 --> 00:14:17,040
OK

393
00:14:17,560 --> 00:14:19,040
所有数据只用过一次的话

394
00:14:19,040 --> 00:14:19,560
你怎么做

395
00:14:19,560 --> 00:14:20,720
最简单就是说

396
00:14:20,720 --> 00:14:21,600
我们

397
00:14:22,080 --> 00:14:23,759
就是说把它切成

398
00:14:23,920 --> 00:14:26,399
假设把整个序列切成

399
00:14:27,920 --> 00:14:29,600
长围t的这些序列

400
00:14:31,080 --> 00:14:32,200
切好之后

401
00:14:32,240 --> 00:14:34,480
每一次随机去里面取一段

402
00:14:34,480 --> 00:14:35,000
对吧

403
00:14:35,720 --> 00:14:37,879
就随机取它

404
00:14:38,960 --> 00:14:39,840
这是最简单的做法

405
00:14:39,960 --> 00:14:40,279
就是说

406
00:14:40,279 --> 00:14:41,600
我们把它整个序列切成

407
00:14:41,600 --> 00:14:42,680
长围t的一段一段

408
00:14:42,680 --> 00:14:44,360
然后在里面拎出一个

409
00:14:44,519 --> 00:14:46,240
但是这东西最大的问题是什么

410
00:14:46,240 --> 00:14:47,279
这东西最大的问题是说

411
00:14:47,279 --> 00:14:49,360
假设你下次还是这么做的话

412
00:14:50,320 --> 00:14:52,600
那么你可能遍历不到

413
00:14:53,919 --> 00:14:54,840
这个数据了

414
00:14:56,039 --> 00:14:57,960
就是你每次切法是一定的

415
00:14:58,000 --> 00:14:59,800
就是每次总切在这个地方

416
00:15:00,519 --> 00:15:01,720
那么怎么解决这个问题

417
00:15:02,399 --> 00:15:03,560
解决问题是说

418
00:15:03,560 --> 00:15:04,759
我每次切的时候

419
00:15:05,120 --> 00:15:06,800
我在前面这个起始点

420
00:15:06,800 --> 00:15:07,960
我是随机起始

421
00:15:09,159 --> 00:15:11,960
就是说算法的idea

422
00:15:11,960 --> 00:15:12,680
就是说

423
00:15:12,680 --> 00:15:14,800
我们每一次开始切的时候

424
00:15:14,800 --> 00:15:18,680
我在0到t之间

425
00:15:19,480 --> 00:15:21,039
随机取一个值

426
00:15:21,480 --> 00:15:23,440
取一个比如k的话

427
00:15:23,440 --> 00:15:24,360
那么就是说

428
00:15:24,360 --> 00:15:26,240
从k元素开始

429
00:15:27,360 --> 00:15:28,360
切成这样子

430
00:15:28,360 --> 00:15:29,879
长围t的序列

431
00:15:32,039 --> 00:15:34,200
就前面k个我就不要了

432
00:15:34,200 --> 00:15:35,399
在这一段就不要了

433
00:15:35,639 --> 00:15:36,759
这样子的话

434
00:15:37,440 --> 00:15:38,360
然后在里面

435
00:15:38,360 --> 00:15:40,639
就每一次随机去挑选一个序列

436
00:15:40,639 --> 00:15:41,000
拿出来

437
00:15:41,000 --> 00:15:42,279
做到我的minibatch里面

438
00:15:43,600 --> 00:15:45,039
这样子的好处是说

439
00:15:45,399 --> 00:15:47,240
我们遍历一次数据

440
00:15:47,240 --> 00:15:48,920
每个数据就是用过一次

441
00:15:49,320 --> 00:15:51,000
不是我们在序列模型里面

442
00:15:51,000 --> 00:15:54,120
每个数据会用t次

443
00:15:54,600 --> 00:15:55,200
这样子的话

444
00:15:55,200 --> 00:15:57,160
我们相对来说

445
00:15:57,160 --> 00:15:58,320
没有太多别的

446
00:15:58,560 --> 00:15:59,600
主要是用来

447
00:16:01,080 --> 00:16:02,000
不要让每一次

448
00:16:02,000 --> 00:16:03,400
每个epoke变得特别长

449
00:16:05,800 --> 00:16:07,040
第二个是说

450
00:16:07,440 --> 00:16:09,760
我们因为有这个k这个东西存在

451
00:16:10,320 --> 00:16:12,800
使得我们确实能够遍历出

452
00:16:12,800 --> 00:16:14,160
所有可能的序列

453
00:16:14,160 --> 00:16:15,160
都可以遍历出来

454
00:16:15,880 --> 00:16:16,360
OK

455
00:16:16,600 --> 00:16:18,720
所以这个就是我们这个算法

456
00:16:18,720 --> 00:16:19,840
其实要干的事情

457
00:16:20,360 --> 00:16:21,399
整个算法就是说

458
00:16:21,399 --> 00:16:22,519
实现了这个东西

459
00:16:23,080 --> 00:16:24,360
你可以看到就是说

460
00:16:24,399 --> 00:16:26,759
我们首先在corpus里面

461
00:16:26,759 --> 00:16:30,399
我们在随机就是0和t-1

462
00:16:30,399 --> 00:16:30,920
那个地方

463
00:16:31,639 --> 00:16:33,800
然后随机出一个integer

464
00:16:33,840 --> 00:16:35,600
然后把前面丢掉

465
00:16:35,639 --> 00:16:37,759
就是corpus从k

466
00:16:37,800 --> 00:16:39,639
然后到最后留给corpus

467
00:16:39,639 --> 00:16:41,080
就是每一个data

468
00:16:41,120 --> 00:16:41,600
epoke的时候

469
00:16:41,639 --> 00:16:43,440
随机前面随机丢一点

470
00:16:44,840 --> 00:16:46,160
然后当然是可以看说

471
00:16:46,160 --> 00:16:47,200
我这个长度有多少

472
00:16:47,480 --> 00:16:48,800
除以number of steps

473
00:16:49,120 --> 00:16:50,680
那就是就会得到说

474
00:16:50,720 --> 00:16:51,480
我这个序列

475
00:16:51,480 --> 00:16:54,920
可以生成多少个子序列出来

476
00:16:55,520 --> 00:16:57,120
然后后面这些东西都是说

477
00:16:57,640 --> 00:16:58,400
initial

478
00:16:59,400 --> 00:16:59,960
index

479
00:16:59,960 --> 00:17:01,759
就是说每个子序列

480
00:17:01,800 --> 00:17:04,080
开始的长是多少

481
00:17:04,600 --> 00:17:06,120
就是说是

482
00:17:07,480 --> 00:17:08,799
从0到这个地方

483
00:17:09,080 --> 00:17:09,840
就是说每一次

484
00:17:09,840 --> 00:17:11,240
就是从0开始往后跳

485
00:17:11,240 --> 00:17:13,240
就是说每次跳的是t

486
00:17:13,559 --> 00:17:14,200
就是说这样

487
00:17:14,200 --> 00:17:16,920
每个子序列开始的下标

488
00:17:17,600 --> 00:17:19,039
然后我们因为随机

489
00:17:19,200 --> 00:17:20,640
我们就对随机shuffle一下

490
00:17:20,640 --> 00:17:22,039
就整个这样子

491
00:17:22,039 --> 00:17:24,200
我们取得的每个子序列是随机的

492
00:17:25,440 --> 00:17:26,960
那么最后就是说每一个batch

493
00:17:27,880 --> 00:17:31,880
for i就是到我的batch的话

494
00:17:31,880 --> 00:17:32,559
其实说白了

495
00:17:32,559 --> 00:17:36,600
就是我能在我的随机的下标里面

496
00:17:38,720 --> 00:17:40,240
在我的随机下标里面

497
00:17:40,240 --> 00:17:41,759
把我这个batch拿出来

498
00:17:42,799 --> 00:17:43,920
然后这样子拿到了

499
00:17:43,920 --> 00:17:45,440
我minibatch里面

500
00:17:45,440 --> 00:17:47,160
每个序列

501
00:17:47,799 --> 00:17:49,600
它的开始的下标

502
00:17:50,360 --> 00:17:51,840
然后我们把这个东西

503
00:17:52,600 --> 00:17:53,600
把x构造出来

504
00:17:53,640 --> 00:17:54,400
y构造出来

505
00:17:54,440 --> 00:17:57,080
那么x就是一个batch size

506
00:17:57,080 --> 00:17:57,600
乘以t

507
00:17:57,640 --> 00:17:59,920
就是number of steps的一个东西

508
00:17:59,960 --> 00:18:01,519
y就是它那个东西

509
00:18:01,519 --> 00:18:03,120
下一个就是每个序列下一个

510
00:18:03,160 --> 00:18:04,799
那就是一个batch size乘以1的

511
00:18:04,799 --> 00:18:06,680
一个tensor

512
00:18:06,840 --> 00:18:09,279
最后我们每次返回我的x

513
00:18:09,279 --> 00:18:10,200
返回我的y

514
00:18:11,039 --> 00:18:11,680
OK

515
00:18:12,600 --> 00:18:14,200
所以这个所谓的data

516
00:18:14,200 --> 00:18:14,799
这个函数

517
00:18:14,799 --> 00:18:18,240
就是把你的从一个p开始

518
00:18:18,240 --> 00:18:20,080
就是p取到下一个

519
00:18:20,080 --> 00:18:21,559
就是取一个常为t的序列

520
00:18:21,680 --> 00:18:22,840
就是一个很小的函数

521
00:18:23,319 --> 00:18:23,840
OK

522
00:18:23,840 --> 00:18:26,920
所以这个就是我们的第一个算法

523
00:18:26,960 --> 00:18:28,360
第一个算法就是说

524
00:18:31,000 --> 00:18:33,160
给我一个很长的序列

525
00:18:33,400 --> 00:18:35,720
然后我们把它切成

526
00:18:35,720 --> 00:18:37,279
每次切成很多段

527
00:18:37,279 --> 00:18:38,680
连续切成很多段

528
00:18:38,680 --> 00:18:40,039
常为t的子序列

529
00:18:40,559 --> 00:18:43,000
然后一开始我们加了一点随机

530
00:18:43,000 --> 00:18:45,039
使得你每次切的有一点不一样

531
00:18:45,400 --> 00:18:47,519
就不要总是按照固定切法

532
00:18:47,559 --> 00:18:50,079
然后再取p随机p上的时候

533
00:18:50,079 --> 00:18:51,839
我们每次随机的把它

534
00:18:52,440 --> 00:18:53,599
逼个东西取出来

535
00:18:56,599 --> 00:18:57,240
OK

536
00:18:57,720 --> 00:18:58,920
另外一个是

537
00:19:04,359 --> 00:19:05,440
然后我们可以看一下

538
00:19:05,519 --> 00:19:06,359
我们可以看一下

539
00:19:06,359 --> 00:19:06,839
我们的

540
00:19:10,400 --> 00:19:11,079
我们

541
00:19:11,599 --> 00:19:13,240
看一下我们的验证

542
00:19:13,439 --> 00:19:14,799
就是我们生成一个

543
00:19:14,799 --> 00:19:18,119
从0到34的一个序列

544
00:19:18,119 --> 00:19:18,679
常为序列

545
00:19:18,679 --> 00:19:20,799
就是做我们的corpus

546
00:19:21,000 --> 00:19:21,679
然后我们来看一下

547
00:19:21,679 --> 00:19:23,359
这个东西会返回什么东西

548
00:19:23,639 --> 00:19:25,000
就是说我们就把它丢进去

549
00:19:25,119 --> 00:19:26,039
batch size等于2

550
00:19:26,039 --> 00:19:28,000
然后我们的长度t等于5

551
00:19:30,359 --> 00:19:31,039
就t等于5

552
00:19:31,039 --> 00:19:31,599
就是说

553
00:19:31,599 --> 00:19:33,000
你可以看到我们返回什么

554
00:19:33,000 --> 00:19:33,799
返回的

555
00:19:33,799 --> 00:19:35,119
我们刚刚有点讲错了

556
00:19:35,119 --> 00:19:36,000
那个y是什么东西

557
00:19:36,119 --> 00:19:37,359
我们在这里补正

558
00:19:37,679 --> 00:19:38,439
更正一下

559
00:19:38,519 --> 00:19:39,519
就x是什么东西

560
00:19:40,319 --> 00:19:42,879
x是一个常为5的序列

561
00:19:43,000 --> 00:19:44,759
所以它有两个样本

562
00:19:45,039 --> 00:19:46,160
p样大小等于2

563
00:19:46,160 --> 00:19:47,160
所以有两个样本

564
00:19:47,759 --> 00:19:48,519
所以这个东西

565
00:19:49,400 --> 00:19:50,160
就是常为5

566
00:19:50,160 --> 00:19:52,079
就是23 24 25 26 27

567
00:19:52,119 --> 00:19:52,639
对吧

568
00:19:53,639 --> 00:19:54,400
它的y

569
00:19:55,559 --> 00:19:57,720
它的y其实是不是一个

570
00:19:57,839 --> 00:19:58,759
不是序列

571
00:19:58,759 --> 00:19:59,480
是不是一个

572
00:19:59,480 --> 00:20:02,599
它是每一次23对应的

573
00:20:02,599 --> 00:20:04,200
y就是它是一个对应的

574
00:20:04,200 --> 00:20:04,920
23对应的

575
00:20:04,920 --> 00:20:05,879
y是24

576
00:20:06,480 --> 00:20:08,519
24对应的是25

577
00:20:08,720 --> 00:20:09,879
25对应的是26

578
00:20:10,119 --> 00:20:11,400
就是说每一次那个y

579
00:20:11,559 --> 00:20:13,079
跟你的x长处是一样的

580
00:20:13,119 --> 00:20:15,680
只是说是你的东西的后一个

581
00:20:17,200 --> 00:20:17,799
Ok

582
00:20:18,400 --> 00:20:20,400
所以他要干个什么事情

583
00:20:20,480 --> 00:20:21,759
他在做预测的时候

584
00:20:21,759 --> 00:20:22,759
干的事情是说

585
00:20:22,799 --> 00:20:23,759
我给我一个23

586
00:20:23,759 --> 00:20:24,960
我去预测24

587
00:20:26,200 --> 00:20:27,359
然后给我

588
00:20:27,359 --> 00:20:29,240
然后告诉我是23和24

589
00:20:29,240 --> 00:20:30,839
我会再去预测25

590
00:20:31,240 --> 00:20:33,039
给我一个23 24 25

591
00:20:33,039 --> 00:20:33,960
去预测26

592
00:20:33,960 --> 00:20:34,720
就是这样

593
00:20:34,759 --> 00:20:36,039
它就是一个这样子的模型

594
00:20:36,079 --> 00:20:38,960
然后最多预测5个长度

595
00:20:40,159 --> 00:20:40,680
Ok

596
00:20:41,200 --> 00:20:42,319
所以然后你看到

597
00:20:43,960 --> 00:20:45,319
所以你在一个序列里面

598
00:20:45,319 --> 00:20:46,480
永远是个连续的

599
00:20:46,879 --> 00:20:50,240
但是你在一个mini batch里面

600
00:20:50,359 --> 00:20:51,079
这两个序列

601
00:20:51,079 --> 00:20:53,079
可能是不是在空间上

602
00:20:53,079 --> 00:20:56,039
是有个相互的关系

603
00:20:56,399 --> 00:20:58,119
然后你看到下一个

604
00:20:59,440 --> 00:21:00,599
batch的时候

605
00:21:00,599 --> 00:21:02,240
你的x跟前面

606
00:21:02,680 --> 00:21:03,519
不一定是接

607
00:21:03,680 --> 00:21:04,559
这个是接起来的

608
00:21:04,720 --> 00:21:06,000
这个是接在一起

609
00:21:06,039 --> 00:21:08,519
但是8到12

610
00:21:08,519 --> 00:21:10,200
它是不会是接到3和4

611
00:21:10,200 --> 00:21:10,879
就是说

612
00:21:10,920 --> 00:21:13,639
你在下一个batch的x

613
00:21:13,680 --> 00:21:14,559
里面的样本

614
00:21:14,559 --> 00:21:15,599
跟前面一个batch

615
00:21:15,599 --> 00:21:17,519
是一个独立的分布

616
00:21:17,639 --> 00:21:19,680
就是说他们是随机采样出来的

617
00:21:20,000 --> 00:21:20,839
这是第一个

618
00:21:20,839 --> 00:21:22,119
就是说每一个mini batch

619
00:21:22,119 --> 00:21:23,079
是随机采样的

620
00:21:23,839 --> 00:21:24,920
然后就是说

621
00:21:24,960 --> 00:21:26,240
所有的mini batch里面

622
00:21:26,240 --> 00:21:27,559
和mini batch之间

623
00:21:28,039 --> 00:21:30,839
它的采样都是独立的

624
00:21:30,839 --> 00:21:32,039
这是第一种做法

625
00:21:32,319 --> 00:21:33,519
也是最简单的

626
00:21:34,519 --> 00:21:36,200
我们接下来再看一下

627
00:21:37,960 --> 00:21:40,519
第二个是

628
00:21:45,279 --> 00:21:46,319
第二个是什么意思

629
00:21:46,440 --> 00:21:47,480
第二个是保证

630
00:21:47,480 --> 00:21:50,680
两个相邻的小批量

631
00:21:51,759 --> 00:21:53,160
它的就是说

632
00:21:53,160 --> 00:21:55,519
我小批量中间的样本

633
00:21:55,519 --> 00:21:57,119
就是我第一个样本

634
00:21:57,319 --> 00:21:59,359
和下一个小批量

635
00:21:59,359 --> 00:22:01,720
中间的第二个样本

636
00:22:02,200 --> 00:22:04,839
它是一个相邻的

637
00:22:04,839 --> 00:22:06,000
是一个原始序列上

638
00:22:06,000 --> 00:22:07,039
是相邻的关系

639
00:22:08,559 --> 00:22:08,839
OK

640
00:22:08,839 --> 00:22:10,680
我们就不给大家去看实现了

641
00:22:10,680 --> 00:22:12,000
实现跟前面是一样的

642
00:22:12,039 --> 00:22:13,160
主要给大家看一下效果

643
00:22:13,440 --> 00:22:14,120
就是说

644
00:22:14,600 --> 00:22:15,360
看一下这个函数

645
00:22:15,360 --> 00:22:16,240
它叫做

646
00:22:17,160 --> 00:22:19,440
Sequence data iterated sequential

647
00:22:19,440 --> 00:22:20,480
刚刚是个random

648
00:22:20,559 --> 00:22:21,600
我们用的是sequential

649
00:22:21,880 --> 00:22:23,240
来表示它是一个

650
00:22:23,279 --> 00:22:25,640
在两个相邻的小批量之间

651
00:22:25,640 --> 00:22:26,799
序列式连续的

652
00:22:28,480 --> 00:22:29,920
让我们看一下什么样子

653
00:22:30,920 --> 00:22:31,920
就看一下效果

654
00:22:32,519 --> 00:22:33,480
效果你看一下

655
00:22:33,480 --> 00:22:34,680
第一个mini batch

656
00:22:35,000 --> 00:22:37,200
是45678

657
00:22:38,360 --> 00:22:40,519
到下一个mini batch

658
00:22:40,519 --> 00:22:41,640
它的第一个样本

659
00:22:41,640 --> 00:22:43,640
是接着前面一个mini batch

660
00:22:43,640 --> 00:22:44,480
第一个样本

661
00:22:44,480 --> 00:22:46,000
就是9到13

662
00:22:47,640 --> 00:22:50,160
然后再到x

663
00:22:50,200 --> 00:22:51,279
下一个mini batch

664
00:22:51,279 --> 00:22:51,960
第一个样本

665
00:22:51,960 --> 00:22:53,400
是14和13

666
00:22:53,400 --> 00:22:54,440
是接在一起的

667
00:22:56,400 --> 00:22:57,080
这样子的话

668
00:22:57,080 --> 00:22:57,240
好

669
00:22:57,240 --> 00:22:58,600
就是说我在训练的时候

670
00:22:59,120 --> 00:23:00,360
我知道这个是连续的话

671
00:23:00,360 --> 00:23:01,680
我就会一直往下

672
00:23:01,840 --> 00:23:02,840
就是说我知道它

673
00:23:02,840 --> 00:23:03,840
就是说我这样子

674
00:23:03,840 --> 00:23:05,640
能拿到更多的空间信息

675
00:23:05,640 --> 00:23:07,920
就我知道这个是一对应的一个关系

676
00:23:08,080 --> 00:23:09,160
然后同样的道理的话

677
00:23:09,160 --> 00:23:10,360
第二个样本

678
00:23:10,560 --> 00:23:12,080
就是这个东西

679
00:23:12,080 --> 00:23:14,520
和第二个batch中的第二个样本

680
00:23:14,560 --> 00:23:16,720
和第三个batch中的样本

681
00:23:16,720 --> 00:23:18,160
都是一个连续的关系

682
00:23:20,080 --> 00:23:20,480
OK

683
00:23:20,480 --> 00:23:22,200
就是说我们这里就是由

684
00:23:22,600 --> 00:23:25,440
两种不同采样的方法

685
00:23:25,760 --> 00:23:28,200
然后这两种不同采样方法

686
00:23:28,200 --> 00:23:30,080
导致我们在之后的模型

687
00:23:30,080 --> 00:23:31,279
会有一点的区别

688
00:23:31,319 --> 00:23:32,480
就是第一种是说

689
00:23:32,480 --> 00:23:35,640
你mini batch之间是独立的

690
00:23:35,920 --> 00:23:36,559
第二种是说

691
00:23:36,559 --> 00:23:38,960
mini batch之间是接在一起的

692
00:23:39,279 --> 00:23:40,480
就是说意味着

693
00:23:40,480 --> 00:23:42,480
我可以做一个更长的序列出来

694
00:23:43,559 --> 00:23:44,120
OK

695
00:23:44,360 --> 00:23:46,440
就最后一个就是说

696
00:23:46,440 --> 00:23:47,319
我们将

697
00:23:48,039 --> 00:23:49,480
这刚刚两个采样

698
00:23:49,480 --> 00:23:50,720
还是包到一个类里面

699
00:23:50,799 --> 00:23:51,240
就是说

700
00:23:51,240 --> 00:23:51,759
seq

701
00:23:52,440 --> 00:23:54,080
sequence data loader

702
00:23:54,200 --> 00:23:56,000
然后告诉一个batch size

703
00:23:56,400 --> 00:23:56,759
t

704
00:23:56,759 --> 00:23:57,759
就number steps

705
00:23:57,759 --> 00:23:59,359
然后告诉说

706
00:23:59,359 --> 00:24:01,599
你是不是使用

707
00:24:04,240 --> 00:24:05,960
是不是使用一个随机的

708
00:24:05,960 --> 00:24:06,559
iterator

709
00:24:07,519 --> 00:24:09,279
如果是的话

710
00:24:09,440 --> 00:24:11,319
就是使用我们的随机的采样

711
00:24:11,319 --> 00:24:12,079
如果不是的话

712
00:24:12,079 --> 00:24:12,960
我们用seq

713
00:24:12,960 --> 00:24:14,599
sequential采样方法

714
00:24:15,279 --> 00:24:16,279
最后一个max token

715
00:24:16,279 --> 00:24:17,759
max token的意思是说

716
00:24:19,359 --> 00:24:21,039
如果你的token数量太大

717
00:24:21,079 --> 00:24:21,759
就是说

718
00:24:21,759 --> 00:24:23,599
如果你加载一个

719
00:24:23,599 --> 00:24:24,880
特别大的数据的话

720
00:24:24,880 --> 00:24:26,320
我们训练起来比较慢

721
00:24:26,400 --> 00:24:27,960
我们就是随便取一个

722
00:24:27,960 --> 00:24:29,240
只是说取一个小一点的

723
00:24:29,240 --> 00:24:30,280
只是让你训练快一点

724
00:24:30,400 --> 00:24:32,200
我们因为我们这是一个

725
00:24:32,280 --> 00:24:33,680
多多少少一个demo目的

726
00:24:33,880 --> 00:24:35,800
就是不想让大家训练特别慢

727
00:24:35,800 --> 00:24:38,120
这样子看效果比较难

728
00:24:39,000 --> 00:24:41,120
然后当然是把time machine

729
00:24:41,120 --> 00:24:42,040
load出来

730
00:24:42,080 --> 00:24:45,840
然后把你每一次的

731
00:24:45,840 --> 00:24:46,800
iterator出来

732
00:24:46,800 --> 00:24:49,240
就是我们的iterator function

733
00:24:49,280 --> 00:24:50,640
每次返回出一个

734
00:24:50,640 --> 00:24:52,480
你要的x和y

735
00:24:53,520 --> 00:24:54,160
OK

736
00:24:55,880 --> 00:24:57,200
所以最后的

737
00:24:57,200 --> 00:24:58,880
我们定了一个总函数

738
00:24:59,000 --> 00:25:00,640
叫load data time machine

739
00:25:01,240 --> 00:25:02,280
就是告诉你

740
00:25:04,840 --> 00:25:05,720
就说白了

741
00:25:05,720 --> 00:25:07,840
就是把你这个东西给加载里面

742
00:25:07,840 --> 00:25:09,400
然后返回给你的data iterator

743
00:25:09,400 --> 00:25:11,200
和返回给你的workapp

744
00:25:11,400 --> 00:25:12,120
这样子的话

745
00:25:12,120 --> 00:25:13,960
我们之后所有的模型

746
00:25:14,200 --> 00:25:15,200
所有的RNN模型

747
00:25:15,480 --> 00:25:17,400
我们之后的

748
00:25:17,400 --> 00:25:18,600
所有的CNN的模型

749
00:25:18,600 --> 00:25:21,040
我们都可以用这个函数

750
00:25:21,040 --> 00:25:22,120
直接调用这个函数

751
00:25:22,120 --> 00:25:24,200
会拿到我们的data iterator

752
00:25:24,200 --> 00:25:26,360
和以及我们对应的workapp

753
00:25:27,400 --> 00:25:27,759
OK

754
00:25:27,759 --> 00:25:29,080
这就是我们的

755
00:25:29,240 --> 00:25:32,480
持续做语言模型的序列

756
00:25:32,480 --> 00:25:33,880
是怎么样

757
00:25:33,920 --> 00:25:35,039
读取数据

758
00:25:35,080 --> 00:25:37,680
以及怎么样生成我的minibatch的

759
00:25:37,960 --> 00:25:39,680
我们这样子就把我们的

760
00:25:40,720 --> 00:25:42,160
数据读取讲完了

761
00:25:42,480 --> 00:25:43,400
我们在之后

762
00:25:43,400 --> 00:25:44,360
就可以专注在

763
00:25:44,360 --> 00:25:45,880
我们的模型那一块了

