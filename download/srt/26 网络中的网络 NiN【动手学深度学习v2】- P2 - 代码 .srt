1
00:00:00,000 --> 00:00:00,440
好

2
00:00:00,440 --> 00:00:05,560
我们今天第一个要讲的网络是叫做NIN

3
00:00:05,560 --> 00:00:07,360
叫做Network in Network

4
00:00:07,400 --> 00:00:09,480
或者叫做网络中的网络

5
00:00:09,919 --> 00:00:12,599
这个网络现在用的不多

6
00:00:12,599 --> 00:00:14,040
几乎很少被用到

7
00:00:14,080 --> 00:00:17,760
但是它里面有提出了比较重要的一些概念

8
00:00:17,800 --> 00:00:20,879
在之后我们很多网络都会被持续的用到

9
00:00:20,920 --> 00:00:23,920
所以我们今天来先来讲这一个网络

10
00:00:23,920 --> 00:00:29,679
首先我们来看一下

11
00:00:29,679 --> 00:00:33,520
我们之前在AlexNet和VGG的时候

12
00:00:33,560 --> 00:00:38,160
都在最后面用了比较大的卷积层

13
00:00:38,200 --> 00:00:40,400
在VGG和AlexNet都是一样的

14
00:00:40,400 --> 00:00:44,000
用了两个4096的全连阶层

15
00:00:44,000 --> 00:00:44,799
不是卷积层

16
00:00:45,480 --> 00:00:48,719
最后通过一个全连阶层作为输出

17
00:00:49,160 --> 00:00:51,000
我们之前有讲到说

18
00:00:51,359 --> 00:00:56,799
这些全连阶层其实是特别占用你的参数的空间

19
00:00:56,799 --> 00:00:58,920
就是说基本上你网络里面

20
00:00:58,920 --> 00:01:01,320
你所有的参数都在全连阶层

21
00:01:01,359 --> 00:01:04,840
我们在之前也算过每一个全连阶层

22
00:01:04,840 --> 00:01:06,599
它用的参数的比例

23
00:01:07,680 --> 00:01:11,400
它最重要的问题是说它会带来过拟核

24
00:01:11,640 --> 00:01:11,879
好

25
00:01:11,920 --> 00:01:14,079
我们仔细来看一下这个问题

26
00:01:15,439 --> 00:01:17,920
假设我是用卷积层的话

27
00:01:17,920 --> 00:01:22,680
我们知道我们的参数的个数是你的输入的通道数

28
00:01:23,320 --> 00:01:25,320
乘以你的输出的通道数

29
00:01:25,960 --> 00:01:28,879
再乘以你的窗口的高和宽

30
00:01:28,920 --> 00:01:30,040
假设你就是k的话

31
00:01:30,040 --> 00:01:31,040
就是k的平方

32
00:01:32,840 --> 00:01:35,240
但是你是全连阶层

33
00:01:35,240 --> 00:01:40,400
那就是你的输入的整个你输入的像素

34
00:01:40,520 --> 00:01:42,480
就是你的输入的通道

35
00:01:42,520 --> 00:01:44,520
乘以你的输入的高和宽

36
00:01:45,519 --> 00:01:48,200
再乘以你的输出里面所有的像素

37
00:01:48,239 --> 00:01:50,640
也就是说你的输出的通道

38
00:01:50,679 --> 00:01:51,879
乘以你的高和宽

39
00:01:53,439 --> 00:01:54,519
我们可以看一下

40
00:01:54,519 --> 00:01:57,560
就是说在卷积层的最后一层

41
00:01:58,000 --> 00:02:01,479
我们知道我们没有把它变成一个一乘以很多时候

42
00:02:01,479 --> 00:02:04,120
就是一个7乘7的一个高宽

43
00:02:04,159 --> 00:02:05,879
然后还有一个通道数

44
00:02:05,920 --> 00:02:08,199
那么它其实是比较多的

45
00:02:09,240 --> 00:02:10,000
我们来看一下

46
00:02:10,000 --> 00:02:14,479
就是说如果是卷积层后的第一个全连阶层的参数

47
00:02:14,520 --> 00:02:15,960
如果是no net的话

48
00:02:15,960 --> 00:02:17,520
那就是16乘5乘5

49
00:02:17,560 --> 00:02:18,640
再乘120

50
00:02:18,680 --> 00:02:21,080
120是你的输出的影响层的大小

51
00:02:21,280 --> 00:02:26,120
16是你的最后一个卷积层的输出的通道数

52
00:02:26,160 --> 00:02:29,080
5乘5就是你把高宽压到了5乘5

53
00:02:30,360 --> 00:02:31,160
他还好

54
00:02:31,360 --> 00:02:32,560
就是48k

55
00:02:32,600 --> 00:02:34,120
但是你去看AlexNet

56
00:02:34,160 --> 00:02:37,719
AlexNet是你的最后一个卷积层的输出

57
00:02:37,719 --> 00:02:39,240
是256的通道

58
00:02:39,360 --> 00:02:40,480
然后乘以5乘5

59
00:02:40,680 --> 00:02:42,080
然后乘以4096

60
00:02:42,960 --> 00:02:45,240
那么你可以看到它就是26兆

61
00:02:45,280 --> 00:02:46,760
但是对VGG来说

62
00:02:46,760 --> 00:02:48,040
这个就更明显了

63
00:02:48,160 --> 00:02:50,920
VGG它的通道数变成了512

64
00:02:51,560 --> 00:02:54,360
同时它只压缩到了7乘7

65
00:02:54,960 --> 00:02:56,680
然后再乘以4096

66
00:02:56,840 --> 00:03:00,040
那就是100兆的整个参数

67
00:03:01,120 --> 00:03:04,920
所以你会基本上会发现说

68
00:03:04,920 --> 00:03:08,320
VGG它确实它的占用空间是比较大的

69
00:03:08,360 --> 00:03:09,800
如果你去看内存的话

70
00:03:09,800 --> 00:03:12,960
VGG我记得应该是占用到差不多700兆的内存

71
00:03:12,960 --> 00:03:13,480
的样子

72
00:03:13,840 --> 00:03:16,200
而且几乎是你可以认为大头

73
00:03:16,200 --> 00:03:20,240
就是在卷积层之后的第一个全连接层

74
00:03:21,400 --> 00:03:25,240
这个不仅带来大量的参数

75
00:03:25,240 --> 00:03:26,840
参数会带来很多问题

76
00:03:27,040 --> 00:03:28,200
第一个是说

77
00:03:28,600 --> 00:03:30,920
你要用很多的内存

78
00:03:31,480 --> 00:03:32,800
第二个是说

79
00:03:32,840 --> 00:03:35,840
你会占用很多的计算带宽

80
00:03:35,840 --> 00:03:39,000
我们当然会在之后会给大家解释一下

81
00:03:39,000 --> 00:03:40,599
硬件到底是怎么运行的

82
00:03:40,759 --> 00:03:43,400
但是你可以认为说一个很大的一个

83
00:03:43,400 --> 00:03:44,199
矩阵惩罚

84
00:03:44,240 --> 00:03:47,840
其实是你根本就不是被你的计算给你

85
00:03:48,000 --> 00:03:50,759
被dominate的

86
00:03:50,759 --> 00:03:53,240
就是被几乎是被你的去不断的去

87
00:03:53,240 --> 00:03:54,319
访问你的内存

88
00:03:54,960 --> 00:03:56,960
然后第三个就是说

89
00:03:57,000 --> 00:03:58,199
它会很容易过你

90
00:03:58,199 --> 00:04:01,439
你想这里有一亿个参数在这个地方

91
00:04:01,920 --> 00:04:05,920
所以很容易那一层就会把你的整个

92
00:04:06,199 --> 00:04:06,759
收敛

93
00:04:06,759 --> 00:04:08,240
你会发现收敛特别快

94
00:04:08,400 --> 00:04:10,280
然后但是你反过来讲

95
00:04:10,280 --> 00:04:11,640
你要做大量的政治化

96
00:04:11,640 --> 00:04:14,320
使得你不要要这一层把你所有东西都

97
00:04:14,320 --> 00:04:15,000
给削掉了

98
00:04:16,240 --> 00:04:16,800
OK

99
00:04:17,160 --> 00:04:19,600
所以为了解决这个问题

100
00:04:20,240 --> 00:04:22,199
就NIN的思想是说

101
00:04:22,319 --> 00:04:25,920
我就完全不要全联结层

102
00:04:28,400 --> 00:04:31,079
大家知道说我们最近有一个新的工作

103
00:04:31,079 --> 00:04:33,879
叫做MLP mix

104
00:04:33,920 --> 00:04:39,279
就是说我全力我的MLP就是多层

105
00:04:39,279 --> 00:04:40,680
感知机又可以替代CN

106
00:04:40,839 --> 00:04:42,600
但实际上这两个东西是一个东西

107
00:04:42,600 --> 00:04:44,800
它跟NIN当年的思想是一样的

108
00:04:45,000 --> 00:04:47,959
只是说NIN是说你全联结层不是很好

109
00:04:48,000 --> 00:04:49,920
所以我用卷积层来替代掉

110
00:04:50,040 --> 00:04:51,199
就不要全联结层了

111
00:04:51,360 --> 00:04:54,439
现在是说我要告诉你说全联结层不行

112
00:04:54,439 --> 00:04:56,079
不是全联结层不行

113
00:04:56,079 --> 00:04:57,920
是卷积层就网络不行

114
00:04:57,920 --> 00:04:59,719
我还是可以用我的全联结层

115
00:04:59,719 --> 00:05:00,959
但实际上是一个东西

116
00:05:01,240 --> 00:05:02,439
我们来看一下这是什么回事

117
00:05:04,879 --> 00:05:09,959
所以NIN里面它最重要的一个概念

118
00:05:09,959 --> 00:05:11,519
叫做NIN块

119
00:05:11,519 --> 00:05:13,920
就是你也知道VGG也有VGG块

120
00:05:13,920 --> 00:05:15,120
到后面的那些网络

121
00:05:15,120 --> 00:05:18,240
基本上都会有自己的局部的卷积层

122
00:05:18,240 --> 00:05:19,199
就网络的架构

123
00:05:19,360 --> 00:05:21,680
就是你整个网络的块状形状

124
00:05:22,439 --> 00:05:24,519
就NIN块是长这样子的

125
00:05:25,240 --> 00:05:26,839
它首先有一个卷积层

126
00:05:26,839 --> 00:05:29,639
就说这个地方是卷积层

127
00:05:29,639 --> 00:05:37,439
然后你的接下来你跟了两个全联结层

128
00:05:37,879 --> 00:05:39,439
我们知道是说

129
00:05:40,079 --> 00:05:41,599
一层一的卷积层

130
00:05:41,599 --> 00:05:43,319
可以等价是一个全联结层

131
00:05:43,519 --> 00:05:46,199
所以它这里用的就是一层一的卷积层

132
00:05:46,959 --> 00:05:47,599
具体来说

133
00:05:47,599 --> 00:05:50,639
它用的是两个一层一的卷积层

134
00:05:51,000 --> 00:05:53,919
就是你的窗口的大小是一层一

135
00:05:54,479 --> 00:05:55,719
这里不服为异

136
00:05:55,719 --> 00:05:57,399
无填充输的形状

137
00:05:57,440 --> 00:06:00,800
就跟卷积层的输出的输入是一样的

138
00:06:01,080 --> 00:06:03,200
就是说你不会改变你的输入的形状

139
00:06:05,080 --> 00:06:09,840
然后它也不会改变你的通道数

140
00:06:10,800 --> 00:06:12,880
那么在这个地方你可以认为是说

141
00:06:12,880 --> 00:06:16,560
这两个一层一的卷积层

142
00:06:16,680 --> 00:06:19,160
其实是当做全联结层来用的

143
00:06:19,640 --> 00:06:21,560
它唯一的作用就是说

144
00:06:21,680 --> 00:06:24,640
对于每个通道数帮你做一些后合

145
00:06:24,639 --> 00:06:30,199
然后这个图是我们之前讲一层一的卷

146
00:06:30,199 --> 00:06:31,479
积层的时候讲过一个图

147
00:06:31,479 --> 00:06:33,719
所以你可以看到是说

148
00:06:33,719 --> 00:06:37,479
这个快可以认为是个非常简单的一个卷积

149
00:06:37,479 --> 00:06:38,079
层网络

150
00:06:39,039 --> 00:06:40,319
就是我一个卷积层

151
00:06:40,519 --> 00:06:42,479
再加两个全联结层

152
00:06:43,279 --> 00:06:44,560
唯一不同的是说

153
00:06:44,560 --> 00:06:48,439
我们这里全联结层是对每一个像素作为

154
00:06:48,439 --> 00:06:49,120
全联结

155
00:06:49,319 --> 00:06:53,319
所以它就不会根据你到底输入的高宽是

156
00:06:53,319 --> 00:06:53,719
什么样子

157
00:06:53,720 --> 00:06:55,680
因为对每个高宽每个像素

158
00:06:55,680 --> 00:06:58,280
我的全联结层权重就是一样的

159
00:06:59,120 --> 00:07:01,800
你可认为是一个按照输入像素

160
00:07:01,800 --> 00:07:04,080
主义去做的全联结层

161
00:07:04,640 --> 00:07:08,400
OK这是它的一个核心的思想

162
00:07:10,080 --> 00:07:11,960
然后它的整体架构就是说

163
00:07:12,000 --> 00:07:13,440
它就没有全联结层了

164
00:07:14,440 --> 00:07:19,880
它就交替使用NIN快和不复为R的最大

165
00:07:19,880 --> 00:07:20,520
齿化层

166
00:07:20,880 --> 00:07:22,880
我们知道最大齿化层就是帮你的高宽

167
00:07:22,880 --> 00:07:23,240
减半

168
00:07:23,240 --> 00:07:30,160
最后通过全局的平均齿化层得到输出

169
00:07:31,439 --> 00:07:33,920
就所谓的全局齿化层

170
00:07:33,920 --> 00:07:40,319
就是说我的齿化层的高宽是等于你输入的高宽

171
00:07:41,240 --> 00:07:43,519
就等于是说对每一个通道

172
00:07:43,519 --> 00:07:45,800
我把你最大的值给拿出来

173
00:07:47,040 --> 00:07:50,160
所以如果我们想要得到的输

174
00:07:50,160 --> 00:07:52,600
我们得到类似1000类的话

175
00:07:52,720 --> 00:07:56,320
那么我们如果你的最后的全局齿化层

176
00:07:56,320 --> 00:07:58,720
它的输入的通道数是1000的话

177
00:07:58,960 --> 00:08:01,760
那么就是说对每一个通道拿出一个值

178
00:08:01,800 --> 00:08:05,360
我们就把这个值当做类别的预测

179
00:08:05,560 --> 00:08:07,720
再加个softmax就会得到我们的概率了

180
00:08:08,600 --> 00:08:10,200
就是说最后的最后

181
00:08:10,240 --> 00:08:14,480
它也不需要使用确定阶层

182
00:08:14,520 --> 00:08:16,640
这是一个非常极端的设计

183
00:08:16,640 --> 00:08:19,480
我们来看一下

184
00:08:19,480 --> 00:08:22,560
就是说它长是长这个样子

185
00:08:23,600 --> 00:08:25,840
首先说你这个是VGG

186
00:08:26,720 --> 00:08:30,200
VGG的快就是你可以知道是一堆三乘三的卷机

187
00:08:30,200 --> 00:08:31,520
再加上一个pooling

188
00:08:31,520 --> 00:08:32,399
maxpooling

189
00:08:32,720 --> 00:08:34,639
就NIN的话就是一个卷机

190
00:08:34,639 --> 00:08:38,120
加两个当做全连接用的一乘一的卷机

191
00:08:39,120 --> 00:08:40,960
然后你的VGG就是说

192
00:08:40,960 --> 00:08:43,600
你我应该是这里有4个VGG的快

193
00:08:43,879 --> 00:08:46,560
再加上两个大的全连接层

194
00:08:47,519 --> 00:08:51,160
就是MLP最后得到我的输出的类是1000类

195
00:08:51,199 --> 00:08:54,519
就是你一个输出通道输出数为1000的

196
00:08:54,519 --> 00:08:55,399
一个全连接层

197
00:08:56,639 --> 00:09:00,439
那么NIN就是一个更简单的网络了

198
00:09:00,439 --> 00:09:03,120
就是说它就是一个NIN快

199
00:09:03,600 --> 00:09:07,839
再加上一个不复为二的最大值

200
00:09:07,879 --> 00:09:08,839
持划层

201
00:09:09,120 --> 00:09:11,039
然后不断重复的一个过程

202
00:09:11,719 --> 00:09:12,799
直到最后

203
00:09:13,120 --> 00:09:14,399
最后这个地方就是说

204
00:09:14,399 --> 00:09:17,079
如果我把它的通道数

205
00:09:17,120 --> 00:09:21,200
设成我要的分类的个数的话

206
00:09:21,200 --> 00:09:25,720
那么最后我也不需要最后这一个全连接层

207
00:09:25,720 --> 00:09:29,400
我直接用全局的平均持划层

208
00:09:29,840 --> 00:09:31,400
来得到我的输出

209
00:09:31,560 --> 00:09:32,879
对每一个类的预测

210
00:09:33,800 --> 00:09:37,480
这就是NNN架构干的事情

211
00:09:39,960 --> 00:09:42,200
OK我们可以总结一下

212
00:09:42,400 --> 00:09:42,960
总结一下

213
00:09:42,960 --> 00:09:43,680
就是说

214
00:09:44,360 --> 00:09:47,360
它的核心思想是一个叫NNN快的东西

215
00:09:47,600 --> 00:09:50,600
就是说是每个卷积层后面加上了

216
00:09:50,600 --> 00:09:52,200
两个1乘1的卷积层

217
00:09:52,200 --> 00:09:53,680
当全连接层用

218
00:09:54,240 --> 00:09:57,360
或者就是说对每个像素增加了非线性性

219
00:09:58,280 --> 00:10:01,400
对每个像素它的通道数做全连接层

220
00:10:01,400 --> 00:10:03,160
然后是一个非线性来切

221
00:10:03,160 --> 00:10:04,160
因为它有两层

222
00:10:05,480 --> 00:10:09,840
最后是说你使用一个全局的平均持划层

223
00:10:10,080 --> 00:10:12,840
来替代VGG和Alex的全连接层

224
00:10:13,680 --> 00:10:15,520
全局持划层也是

225
00:10:16,640 --> 00:10:17,320
因为它比较狠

226
00:10:17,440 --> 00:10:19,120
就是它没有参数可以学的

227
00:10:19,120 --> 00:10:20,840
所以它也不需要去学一个

228
00:10:20,840 --> 00:10:22,720
最后一个比较大的一个全连接层

229
00:10:23,280 --> 00:10:25,960
所以好处是它不那么过人容易离合

230
00:10:26,000 --> 00:10:27,960
有更少的参数的个数

231
00:10:28,200 --> 00:10:29,280
所以整体来讲

232
00:10:29,280 --> 00:10:31,840
就是说NNN它架构比较简单

233
00:10:32,000 --> 00:10:36,320
就是NNN block加上Max pooling

234
00:10:36,360 --> 00:10:39,200
一直到最后一个全局的平均持划层

235
00:10:39,760 --> 00:10:40,880
而且它的通道

236
00:10:40,880 --> 00:10:42,880
它的参数个数非常少

237
00:10:43,159 --> 00:10:45,919
因为你没有整个就没有全连接层

238
00:10:46,519 --> 00:10:48,159
不管你的类别数有多大

239
00:10:48,159 --> 00:10:50,279
我倒是都没什么太多关系

240
00:10:51,039 --> 00:10:51,639
OK

241
00:10:51,799 --> 00:10:54,200
这就是NNN这个网络

