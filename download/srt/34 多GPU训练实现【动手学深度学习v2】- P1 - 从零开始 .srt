1
00:00:00,000 --> 00:00:02,359
好我们来回答一下问题

2
00:00:03,120 --> 00:00:05,160
最近Keras从TF分离

3
00:00:05,200 --> 00:00:07,240
数据会不要重新整理

4
00:00:07,240 --> 00:00:08,199
然后要TensorFlow

5
00:00:11,560 --> 00:00:15,439
TensorFlow我们一开始有写1.0

6
00:00:15,439 --> 00:00:16,600
后来2.0出来了

7
00:00:16,600 --> 00:00:18,719
我就感觉整个都不一样了

8
00:00:18,960 --> 00:00:19,879
整个都不一样了

9
00:00:19,879 --> 00:00:21,839
感觉都不认识了

10
00:00:21,839 --> 00:00:23,480
但整体来讲我喜欢2.0

11
00:00:23,480 --> 00:00:25,560
我觉得2.0比1.0好用

12
00:00:25,600 --> 00:00:28,160
就一点几2.0比1.7好用很多

13
00:00:28,640 --> 00:00:31,600
进Priority那种一个Mode更好用

14
00:00:31,839 --> 00:00:34,640
所以我们就我就写信给Jeff Dean

15
00:00:35,000 --> 00:00:37,960
说你们改了我都不认得了

16
00:00:37,960 --> 00:00:39,960
你能不能派一个人来帮忙

17
00:00:39,960 --> 00:00:41,159
Jeff Dean说可以

18
00:00:41,159 --> 00:00:42,960
我派一个Engineer

19
00:00:42,960 --> 00:00:44,519
来帮你们看一下这个事情

20
00:00:44,920 --> 00:00:46,359
所以我们其实我们的翻译

21
00:00:46,359 --> 00:00:47,600
是有Google的员工

22
00:00:47,600 --> 00:00:49,079
帮我们做了很多事情

23
00:00:50,000 --> 00:00:51,480
但具体到这个问题来说

24
00:00:51,480 --> 00:00:53,840
我觉得Keras从TF分离

25
00:00:53,840 --> 00:00:56,480
并不说TF不再支持Keras

26
00:00:56,519 --> 00:00:57,679
我觉得也不是

27
00:00:57,679 --> 00:00:59,679
是说Keras说

28
00:01:00,000 --> 00:01:02,679
我们把Code搬出来

29
00:01:02,679 --> 00:01:04,000
还是搬回原来地方

30
00:01:04,000 --> 00:01:05,239
这样子开发方便

31
00:01:05,239 --> 00:01:06,319
你编译

32
00:01:06,319 --> 00:01:07,280
你每次改一个东西

33
00:01:07,280 --> 00:01:08,280
你不需要去编译

34
00:01:08,280 --> 00:01:09,760
触发编译整个TensorFlow

35
00:01:09,760 --> 00:01:11,280
因为Keras它就是一个Python

36
00:01:11,280 --> 00:01:11,680
Library

37
00:01:11,680 --> 00:01:12,960
它又不管下面东西

38
00:01:13,120 --> 00:01:14,640
下面东西都是基于别人的

39
00:01:14,640 --> 00:01:15,799
以前是基于Ciano

40
00:01:15,799 --> 00:01:16,879
现在基于TensorFlow

41
00:01:17,000 --> 00:01:18,319
所以对他来讲

42
00:01:18,319 --> 00:01:20,200
确实我就改一点Python的东西

43
00:01:20,239 --> 00:01:22,079
我要把整个TensorFlow编译一下

44
00:01:22,240 --> 00:01:23,799
怎么是太浪费了

45
00:01:24,480 --> 00:01:25,879
所以我觉得这个是

46
00:01:25,920 --> 00:01:28,519
很reasonable的一个东西

47
00:01:29,359 --> 00:01:29,959
另外一块

48
00:01:29,959 --> 00:01:32,200
但我觉得应该是不会影响到用户

49
00:01:32,200 --> 00:01:33,799
就是说等于是说

50
00:01:33,799 --> 00:01:35,759
Keras那边的新的东西

51
00:01:35,759 --> 00:01:37,759
会看

52
00:01:37,959 --> 00:01:38,920
就完美情况下

53
00:01:38,920 --> 00:01:40,319
是不会影响用户的

54
00:01:40,359 --> 00:01:41,719
但是这个东西不好说

55
00:01:41,759 --> 00:01:43,920
因为你搬出去之后

56
00:01:43,920 --> 00:01:45,560
很容易就是说

57
00:01:45,599 --> 00:01:46,679
Keras那边用户说

58
00:01:46,679 --> 00:01:47,759
我要做那样

59
00:01:47,759 --> 00:01:48,319
TensorFlow说

60
00:01:48,759 --> 00:01:50,400
TensorFlow这边觉得不行

61
00:01:50,439 --> 00:01:52,759
如果他们之间协调不够好的话

62
00:01:52,760 --> 00:01:55,359
那么可能TensorFlow的Keras版本

63
00:01:55,359 --> 00:01:57,120
和Keras那边自己的开发版本

64
00:01:57,120 --> 00:01:58,120
可能会有

65
00:01:59,359 --> 00:02:00,439
有diverge

66
00:02:00,439 --> 00:02:01,880
就是说不一样的情况

67
00:02:02,160 --> 00:02:04,240
这样子会给大家带来问题

68
00:02:04,280 --> 00:02:07,040
但我觉得这个事情是能解决的

69
00:02:07,480 --> 00:02:10,200
我相信他们应该是没问题的

70
00:02:11,640 --> 00:02:14,200
所以我们暂时是不会有影响的

71
00:02:16,159 --> 00:02:16,800
问题二

72
00:02:16,800 --> 00:02:20,360
能否用bias权等于0

73
00:02:20,480 --> 00:02:21,800
weight权大于0

74
00:02:21,800 --> 00:02:24,320
作为参数的约束条件

75
00:02:24,440 --> 00:02:26,360
训练权叫网络

76
00:02:27,440 --> 00:02:28,920
其实我不是特别理解

77
00:02:29,080 --> 00:02:30,240
bias权等于0

78
00:02:30,240 --> 00:02:31,200
weight权大于0

79
00:02:31,200 --> 00:02:33,200
作为约束条件

80
00:02:33,440 --> 00:02:34,400
就是说

81
00:02:34,600 --> 00:02:35,320
那么

82
00:02:35,760 --> 00:02:37,240
你的weight权大于0

83
00:02:37,280 --> 00:02:38,720
那就比较尴尬了

84
00:02:38,760 --> 00:02:39,600
你的

85
00:02:40,880 --> 00:02:42,120
假设你是神经网

86
00:02:42,160 --> 00:02:42,840
你是个

87
00:02:42,840 --> 00:02:44,320
你输入是个图片的话

88
00:02:44,320 --> 00:02:47,000
图片你输入就是个正的

89
00:02:47,280 --> 00:02:48,600
你bias等于0

90
00:02:48,640 --> 00:02:49,640
你weight权大于0

91
00:02:49,640 --> 00:02:52,240
那么你所谓的材质都是一个正的

92
00:02:53,560 --> 00:02:55,000
那么你就没有负值了

93
00:02:55,000 --> 00:02:56,280
我觉得可能会奇怪

94
00:02:57,640 --> 00:02:59,080
权链阶你也很奇怪的

95
00:02:59,240 --> 00:02:59,960
就是说

96
00:03:00,000 --> 00:03:01,120
但我就不是很理解

97
00:03:01,120 --> 00:03:02,640
我觉得可能你可以试试

98
00:03:02,640 --> 00:03:04,200
看听上去挺奇怪的

99
00:03:06,520 --> 00:03:08,080
在resnet的卷积层中

100
00:03:08,080 --> 00:03:11,200
能否替换成MLP来实现一个很深的

101
00:03:11,200 --> 00:03:12,360
神经网络层

102
00:03:12,920 --> 00:03:13,360
可以

103
00:03:14,120 --> 00:03:16,120
最近不是有paper吗

104
00:03:16,120 --> 00:03:16,880
最近paper说

105
00:03:16,879 --> 00:03:17,439
你用

106
00:03:18,079 --> 00:03:21,199
special构造的权链阶层来实现

107
00:03:21,199 --> 00:03:22,759
就是等价也是用一些

108
00:03:22,759 --> 00:03:24,079
一乘一的卷积层来实现

109
00:03:24,639 --> 00:03:25,639
就是说我们也讲过

110
00:03:25,639 --> 00:03:26,560
一乘一的卷积层

111
00:03:26,560 --> 00:03:27,960
等价一个权链阶层

112
00:03:28,319 --> 00:03:29,840
的特殊版本

113
00:03:31,079 --> 00:03:31,479
OK

114
00:03:31,479 --> 00:03:32,799
所以你是能够做的

115
00:03:32,799 --> 00:03:34,120
但如果你是想说

116
00:03:34,120 --> 00:03:36,319
我用把卷积层替换成

117
00:03:36,919 --> 00:03:38,400
最原始的

118
00:03:38,400 --> 00:03:41,199
最简单的权链阶层是不行的

119
00:03:41,199 --> 00:03:42,359
我们有讲过

120
00:03:42,359 --> 00:03:43,319
你可以回去看一下

121
00:03:43,319 --> 00:03:44,199
我们讲过

122
00:03:44,199 --> 00:03:46,840
从权链阶层到卷积层

123
00:03:47,759 --> 00:03:48,919
如果你要全权

124
00:03:48,919 --> 00:03:50,919
你的参数特别大

125
00:03:51,280 --> 00:03:53,120
计算量可能没变

126
00:03:53,120 --> 00:03:54,519
但是参数会特别大

127
00:03:54,719 --> 00:03:56,000
就基本就overfitting了

128
00:03:56,000 --> 00:03:57,120
你做不了很深的

129
00:03:58,799 --> 00:03:59,400
问题4

130
00:03:59,400 --> 00:04:01,759
既然XXLong是一种正则

131
00:04:01,759 --> 00:04:02,639
那么原则上

132
00:04:02,639 --> 00:04:03,560
它能像dropout

133
00:04:03,680 --> 00:04:05,359
加强模型的犯坏能力

134
00:04:05,359 --> 00:04:07,199
那么应该提升模型的精度

135
00:04:07,199 --> 00:04:09,800
为什么说batchlong只加速训练

136
00:04:09,800 --> 00:04:13,280
还对精度没有影响呢

137
00:04:17,480 --> 00:04:18,680
你这个问题很好

138
00:04:26,000 --> 00:04:26,439
这个问题

139
00:04:26,439 --> 00:04:27,800
我的解释是我不知道

140
00:04:27,800 --> 00:04:29,399
我可能回去看一下

141
00:04:29,959 --> 00:04:31,639
我觉得你这个问题问的挺好的

142
00:04:31,639 --> 00:04:33,639
我们之前有讲过

143
00:04:33,719 --> 00:04:36,680
我说batchlong不会给你的精度带来

144
00:04:37,360 --> 00:04:39,600
不会给你的精度带来提升

145
00:04:39,600 --> 00:04:41,159
它只会让你加速训练

146
00:04:41,159 --> 00:04:42,079
但我们又讲过

147
00:04:42,079 --> 00:04:45,040
batchlong可以几乎认为是一种正则

148
00:04:45,040 --> 00:04:46,400
正则理论上来说

149
00:04:46,400 --> 00:04:49,560
是能提升你的犯法能力的

150
00:04:50,160 --> 00:04:53,439
但为什么这个东西不能够过去

151
00:04:53,600 --> 00:04:54,439
这个我其实不知道

152
00:04:54,920 --> 00:04:55,439
不好意思

153
00:04:55,439 --> 00:04:56,960
所以我得回去看一下

154
00:04:57,200 --> 00:04:58,920
看一下论文怎么说的

155
00:05:01,560 --> 00:05:02,480
问题5

156
00:05:02,920 --> 00:05:04,600
我们的工作站是什么配置

157
00:05:04,600 --> 00:05:05,560
我们没有工作站

158
00:05:06,200 --> 00:05:07,400
我们一个云

159
00:05:07,920 --> 00:05:08,600
亚马逊

160
00:05:10,040 --> 00:05:10,640
亚马逊

161
00:05:10,680 --> 00:05:11,640
亚马逊我不知道

162
00:05:11,640 --> 00:05:13,080
我是在亚马逊云计算

163
00:05:13,079 --> 00:05:14,039
跟亚马逊

164
00:05:16,079 --> 00:05:17,000
跟亚马逊

165
00:05:17,359 --> 00:05:19,879
可能亚马逊好像也用的是云计算

166
00:05:20,039 --> 00:05:20,839
其实我还真不知道

167
00:05:20,839 --> 00:05:22,479
我没在亚马逊的部门工作过

168
00:05:22,479 --> 00:05:23,079
我在云计算

169
00:05:23,079 --> 00:05:24,199
云计算我们不需要工作站

170
00:05:24,199 --> 00:05:24,959
我们都用云

171
00:05:25,639 --> 00:05:27,479
所以我只需要我的

172
00:05:27,479 --> 00:05:28,639
我有笔记本

173
00:05:28,639 --> 00:05:30,439
笔记本唯一的干脆性是发邮件

174
00:05:30,439 --> 00:05:33,799
和连接到我的云上的机器就行了

175
00:05:37,120 --> 00:05:37,360
好

176
00:05:37,360 --> 00:05:38,000
问题6

177
00:05:38,000 --> 00:05:40,120
做建筑图纸的处理

178
00:05:40,120 --> 00:05:41,439
有没有相关的深度学习

179
00:05:41,480 --> 00:05:43,399
可以输实现输入一张图纸

180
00:05:43,399 --> 00:05:44,560
进行一定处理后

181
00:05:44,560 --> 00:05:46,079
输出一张图纸的操作

182
00:05:46,079 --> 00:05:47,920
还这个东西还真有

183
00:05:50,800 --> 00:05:51,560
就是说

184
00:05:51,759 --> 00:05:53,040
设计上还真

185
00:05:53,040 --> 00:05:54,519
我觉得这一块是有的

186
00:05:54,519 --> 00:05:55,639
但我没关注过

187
00:05:55,639 --> 00:05:57,079
我觉得你建议你去看一下

188
00:05:57,079 --> 00:05:58,639
相关的一些

189
00:05:58,639 --> 00:06:00,040
搜一搜相关的一些东西

190
00:06:00,040 --> 00:06:01,279
我记得是有的

191
00:06:01,319 --> 00:06:02,079
就是说

192
00:06:02,800 --> 00:06:03,639
建筑图纸

193
00:06:03,639 --> 00:06:04,480
你要两种做法

194
00:06:04,480 --> 00:06:05,120
一种是说

195
00:06:05,120 --> 00:06:07,439
我把它做成一个pixel

196
00:06:07,439 --> 00:06:08,800
来用干也好

197
00:06:08,800 --> 00:06:09,399
用什么也好

198
00:06:09,400 --> 00:06:10,920
来生成一个新的pixel

199
00:06:10,920 --> 00:06:11,920
based图纸

200
00:06:11,960 --> 00:06:12,960
需要我想的

201
00:06:12,960 --> 00:06:14,000
现在可以换换人脸

202
00:06:14,120 --> 00:06:14,960
换什么东西

203
00:06:15,360 --> 00:06:16,400
另外一种是说

204
00:06:16,400 --> 00:06:17,280
我建筑图纸

205
00:06:17,280 --> 00:06:18,800
我当然是一个实量图

206
00:06:19,000 --> 00:06:19,800
实量图的话

207
00:06:19,800 --> 00:06:23,400
我是可以去学习其中一些东西的

208
00:06:23,440 --> 00:06:23,960
对吧

209
00:06:23,960 --> 00:06:26,680
你把它认为做成一个文本来处理

210
00:06:27,240 --> 00:06:29,920
确实我记得是在建筑方面

211
00:06:29,920 --> 00:06:32,040
是有个AI辅助做图

212
00:06:32,320 --> 00:06:33,720
这样的东西确实有的

213
00:06:33,720 --> 00:06:34,640
但我没关注过

214
00:06:34,640 --> 00:06:36,320
建议你去搜一下

215
00:06:39,960 --> 00:06:41,080
就问你一期

216
00:06:41,080 --> 00:06:41,800
all reduce

217
00:06:41,800 --> 00:06:43,400
all gather的作用是什么

218
00:06:43,520 --> 00:06:44,880
实际用的时候发现

219
00:06:44,880 --> 00:06:46,360
PyTorch类似分布式OP

220
00:06:46,360 --> 00:06:47,400
不能传

221
00:06:48,440 --> 00:06:50,320
T柱不能会破坏计算图

222
00:06:50,320 --> 00:06:51,320
不能自动求教

223
00:06:51,320 --> 00:06:52,760
怎么办呢

224
00:06:54,600 --> 00:06:55,280
这个事情

225
00:06:56,560 --> 00:06:57,080
对的

226
00:06:57,080 --> 00:06:58,000
就是说

227
00:06:58,920 --> 00:06:59,720
取决于实现

228
00:07:00,480 --> 00:07:01,520
大家是说

229
00:07:01,560 --> 00:07:02,600
all reduce干嘛

230
00:07:02,680 --> 00:07:03,160
all reduce

231
00:07:03,160 --> 00:07:03,760
all gather

232
00:07:03,760 --> 00:07:05,080
就是说一些通讯的东西

233
00:07:05,080 --> 00:07:06,440
all reduce我们实现过了

234
00:07:07,120 --> 00:07:08,080
把东西

235
00:07:08,959 --> 00:07:10,359
把N个东西加在一起

236
00:07:10,399 --> 00:07:11,319
然后把所有的结果

237
00:07:11,319 --> 00:07:12,240
也复制回去

238
00:07:12,240 --> 00:07:13,079
all gather

239
00:07:13,199 --> 00:07:13,599
all gather

240
00:07:13,599 --> 00:07:14,719
就是不要加在一起

241
00:07:14,719 --> 00:07:15,719
就是把一些东西

242
00:07:15,719 --> 00:07:17,199
就是说一些东西

243
00:07:17,599 --> 00:07:20,159
scatter和gather是一个相反的

244
00:07:20,159 --> 00:07:21,159
scatter我们有讲过

245
00:07:21,159 --> 00:07:22,959
就把一个东西切成N份

246
00:07:22,959 --> 00:07:25,039
发给各个地方

247
00:07:25,519 --> 00:07:26,199
all gather

248
00:07:26,199 --> 00:07:29,000
就是把那些分别在不同地方

249
00:07:29,000 --> 00:07:30,399
东西全部合并起来

250
00:07:30,519 --> 00:07:31,560
然后再给告诉

251
00:07:31,560 --> 00:07:33,079
再传broadcast

252
00:07:33,079 --> 00:07:34,560
就是说传给所有人

253
00:07:35,240 --> 00:07:36,439
这都是一些

254
00:07:37,199 --> 00:07:38,040
最早企业

255
00:07:38,560 --> 00:07:40,600
就是说比如MPI这个东西

256
00:07:42,199 --> 00:07:44,120
HPC就是高性能计算里面的

257
00:07:44,120 --> 00:07:45,160
一些很常见的

258
00:07:45,160 --> 00:07:47,399
一些做分布式的一些

259
00:07:48,120 --> 00:07:49,079
一些操作

260
00:07:49,120 --> 00:07:50,519
但是分布式操作

261
00:07:50,519 --> 00:07:52,360
确实你放进去的时候

262
00:07:52,360 --> 00:07:56,079
会破坏一些你的自动求导

263
00:07:56,680 --> 00:07:58,399
因为自动求导

264
00:07:58,399 --> 00:08:01,159
跨GPU是不那么好做的

265
00:08:02,439 --> 00:08:03,319
Pytorch

266
00:08:03,959 --> 00:08:05,040
我不知道Pytorch

267
00:08:05,040 --> 00:08:05,959
能不能做这个事情

268
00:08:06,040 --> 00:08:07,240
我知道有些框架是能

269
00:08:07,240 --> 00:08:08,320
TensorFlow是能做的

270
00:08:09,480 --> 00:08:10,920
Pytorch现在不能做

271
00:08:10,920 --> 00:08:12,760
那就很遗憾了

272
00:08:12,760 --> 00:08:13,440
那就不要做

273
00:08:14,240 --> 00:08:15,000
那就手写

274
00:08:15,280 --> 00:08:16,160
就是自己

275
00:08:16,160 --> 00:08:17,360
其实你也没关系

276
00:08:17,360 --> 00:08:18,880
就是说你被破坏之后

277
00:08:18,880 --> 00:08:21,120
你就是几个小图

278
00:08:21,120 --> 00:08:21,800
自己搞一搞

279
00:08:22,160 --> 00:08:23,520
然后再手动把它弄回去

280
00:08:23,760 --> 00:08:24,800
这是你可以做的

281
00:08:25,120 --> 00:08:27,000
但未来说不定Pytorch可以做了

282
00:08:31,360 --> 00:08:31,880
问题8

283
00:08:31,880 --> 00:08:33,240
两个GPU训练时

284
00:08:33,240 --> 00:08:34,720
最后的T图是把两个GPU上的

285
00:08:34,720 --> 00:08:35,400
T图先加吗

286
00:08:35,400 --> 00:08:35,840
是的

287
00:08:36,759 --> 00:08:37,840
为什么能加T图

288
00:08:38,400 --> 00:08:39,120
就带回忆一下

289
00:08:39,120 --> 00:08:40,000
T图怎么算了

290
00:08:40,000 --> 00:08:41,960
所谓的mini-batch的T图

291
00:08:41,960 --> 00:08:44,639
就是每一个样本的T图求核

292
00:08:46,960 --> 00:08:49,040
所以你现在的两个GPU

293
00:08:49,040 --> 00:08:50,360
就是说每个GPU

294
00:08:50,360 --> 00:08:52,800
把自己的样本的那些T图求核

295
00:08:52,800 --> 00:08:54,280
然后完整的T图

296
00:08:54,280 --> 00:08:55,240
就是两个GPU的T图

297
00:08:55,240 --> 00:08:56,040
再求核就行了

298
00:08:56,040 --> 00:08:56,400
对吧

299
00:08:56,400 --> 00:08:57,600
所以T图是累加的

300
00:08:57,600 --> 00:08:59,240
所以是可以这样子相加的

301
00:09:03,639 --> 00:09:05,120
为什么参数大的模型

302
00:09:05,120 --> 00:09:06,080
不一定慢

303
00:09:07,279 --> 00:09:09,879
flop是为越多的模型性能越好

304
00:09:09,879 --> 00:09:10,879
这样是为什么

305
00:09:12,919 --> 00:09:14,399
我们有讲过一点点

306
00:09:14,399 --> 00:09:16,279
就是说你不是那么唯一的

307
00:09:16,279 --> 00:09:18,519
就是说你最后看的是什么

308
00:09:18,519 --> 00:09:20,600
最后我们在上一节有讲

309
00:09:20,600 --> 00:09:22,240
就是讲硬件有讲说

310
00:09:22,279 --> 00:09:23,919
你的性能取决于

311
00:09:24,000 --> 00:09:27,039
你每算一个乘法

312
00:09:27,840 --> 00:09:29,399
或每算一个东西

313
00:09:29,440 --> 00:09:31,279
你要访问多少个bit

314
00:09:32,200 --> 00:09:33,320
你的等于是

315
00:09:34,320 --> 00:09:35,960
你的flop是

316
00:09:35,960 --> 00:09:37,480
就你的计算量

317
00:09:37,879 --> 00:09:40,600
除以你的内存访问

318
00:09:40,840 --> 00:09:42,360
这个比例越高越好

319
00:09:43,560 --> 00:09:45,879
因为你的CPU

320
00:09:46,040 --> 00:09:47,240
不管是CPUGPU

321
00:09:47,320 --> 00:09:47,760
很容易

322
00:09:47,760 --> 00:09:49,920
你不是被卡在你的频率上面

323
00:09:50,040 --> 00:09:51,200
你大部分是被卡在

324
00:09:51,200 --> 00:09:52,440
你的访问数据上面

325
00:09:52,440 --> 00:09:54,120
访问你内存这个地方

326
00:09:54,120 --> 00:09:54,920
所以的话

327
00:09:54,920 --> 00:09:58,120
尽量模型参数比较小

328
00:09:58,120 --> 00:09:59,520
算力比较高的

329
00:09:59,560 --> 00:10:00,160
性能更好

330
00:10:00,160 --> 00:10:01,480
就是卷机性能还不错

331
00:10:01,480 --> 00:10:01,760
对吧

332
00:10:01,760 --> 00:10:03,120
矩阵乘法也还可以

333
00:10:03,320 --> 00:10:10,480
为什么分布到多GPU的测试

334
00:10:10,480 --> 00:10:12,000
进度会比单GPU抖动

335
00:10:12,000 --> 00:10:13,400
其实不是的

336
00:10:13,400 --> 00:10:14,760
其实是说抖动

337
00:10:14,760 --> 00:10:17,040
是因为我们的学习率变大了

338
00:10:17,280 --> 00:10:18,520
如果你学习率不变

339
00:10:18,520 --> 00:10:20,040
你的p单大小不变的话

340
00:10:20,080 --> 00:10:23,600
多GPU和单GPU是不会有任何区别的

341
00:10:23,680 --> 00:10:26,520
所以我们还是要分离开这两个事情

342
00:10:26,560 --> 00:10:29,800
就是说我们干了两个事情

343
00:10:29,920 --> 00:10:33,000
一个是就把一个GPU的任务

344
00:10:33,000 --> 00:10:34,399
放到了多个GPU上

345
00:10:34,440 --> 00:10:36,679
假设我别的参数不变

346
00:10:36,679 --> 00:10:37,519
Batch Size不变

347
00:10:37,519 --> 00:10:38,279
学习率不变

348
00:10:38,279 --> 00:10:39,639
理论上你的测试进度

349
00:10:39,639 --> 00:10:40,799
是不会发生变化的

350
00:10:40,840 --> 00:10:42,960
变化的唯一的变化是你的性能

351
00:10:42,960 --> 00:10:44,639
就是没处理一个

352
00:10:44,679 --> 00:10:46,399
扫一遍数据要多少秒钟

353
00:10:47,440 --> 00:10:52,639
但是为了得到更好的速度

354
00:10:52,639 --> 00:10:54,600
我们需要把Batch Size变大

355
00:10:54,720 --> 00:10:55,919
一旦Batch Size变大

356
00:10:55,919 --> 00:10:57,679
就带来你的参数

357
00:10:57,679 --> 00:10:59,200
你的收敛会发生变化

358
00:10:59,360 --> 00:11:00,679
这就是为什么Batch Size变大

359
00:11:00,679 --> 00:11:01,759
我们把能力Rate

360
00:11:01,759 --> 00:11:02,919
把学习率往上调

361
00:11:02,919 --> 00:11:04,559
翻进度更陡

362
00:11:05,079 --> 00:11:05,399
OK

363
00:11:05,399 --> 00:11:06,519
我们等会会来讲一讲

364
00:11:07,639 --> 00:11:09,199
在讲分布式的时候

365
00:11:09,199 --> 00:11:10,719
分布式一样的有这个问题

366
00:11:12,839 --> 00:11:13,919
能力Rate太大

367
00:11:13,919 --> 00:11:15,439
会导致训练不收敛吗

368
00:11:15,439 --> 00:11:16,679
会的能力Rate

369
00:11:16,679 --> 00:11:17,919
当然会不收敛了

370
00:11:18,120 --> 00:11:19,079
Batch Size太大

371
00:11:19,079 --> 00:11:20,120
会导致Loss

372
00:11:20,120 --> 00:11:22,039
Batch Size太大

373
00:11:22,039 --> 00:11:22,959
不会导致Loss

374
00:11:22,959 --> 00:11:24,199
会变成Loss Number

375
00:11:24,199 --> 00:11:26,559
通常是说你的Batch Size太大

376
00:11:26,559 --> 00:11:29,480
你的能力Rate可能你也调得不行

377
00:11:29,599 --> 00:11:31,120
就是说能力Rate太大

378
00:11:31,120 --> 00:11:32,600
会导致你的Loss Number

379
00:11:32,600 --> 00:11:33,919
是有数值稳定的问题

380
00:11:33,960 --> 00:11:34,919
在Batch Size变大

381
00:11:34,919 --> 00:11:35,759
理论上是不会的

382
00:11:35,759 --> 00:11:37,519
因为我们是对Batch Size

383
00:11:37,840 --> 00:11:39,200
就是Batch Size变大

384
00:11:39,200 --> 00:11:39,919
因为我们的T度

385
00:11:39,919 --> 00:11:40,919
是最后求均值了

386
00:11:40,919 --> 00:11:41,440
记得吗

387
00:11:42,279 --> 00:11:44,440
所以说Batch Size变大

388
00:11:44,440 --> 00:11:45,200
其实理论上

389
00:11:45,200 --> 00:11:46,759
你的数值稳定性会更好

390
00:11:49,240 --> 00:11:50,679
GPU的显存如何优化

391
00:11:50,919 --> 00:11:52,919
最近跑新老是out of memory

392
00:11:52,919 --> 00:11:54,960
我的显存是$14

393
00:11:57,919 --> 00:12:00,519
显存说句真话

394
00:12:00,679 --> 00:12:02,360
手动优化是很难的

395
00:12:02,360 --> 00:12:03,519
靠的是框架

396
00:12:04,079 --> 00:12:04,960
框架来讲

397
00:12:06,240 --> 00:12:07,159
我其实觉得

398
00:12:07,159 --> 00:12:09,159
PyTorch的显存优化还行

399
00:12:09,799 --> 00:12:10,639
它不是最好的

400
00:12:10,639 --> 00:12:11,559
但是也不错

401
00:12:11,559 --> 00:12:12,879
所以我觉得框架

402
00:12:12,879 --> 00:12:14,399
可能你能干的事情不多

403
00:12:14,399 --> 00:12:15,120
我建议

404
00:12:15,120 --> 00:12:16,480
除非你特别懂这一块

405
00:12:16,639 --> 00:12:17,079
不然的话

406
00:12:17,079 --> 00:12:18,840
你就是把Batch Size弄小一点

407
00:12:19,399 --> 00:12:20,840
或者是你的模型想一想

408
00:12:20,840 --> 00:12:22,759
怎么样把模型搞得简单一点

409
00:12:23,840 --> 00:12:26,720
如果我说如何优化显存

410
00:12:26,720 --> 00:12:29,159
可能我得讲个一两个小时

411
00:12:29,159 --> 00:12:29,879
可能还讲不完

412
00:12:29,879 --> 00:12:32,200
这一块里面还挺复杂的

413
00:12:34,480 --> 00:12:35,279
对京都来讲

414
00:12:35,279 --> 00:12:36,600
Batch Size等于1是最好的情况

415
00:12:36,720 --> 00:12:37,039
是的

416
00:12:37,360 --> 00:12:37,799
很遗憾

417
00:12:37,799 --> 00:12:39,320
Batch Size等于1的时候

418
00:12:39,320 --> 00:12:40,360
在收敛来讲

419
00:12:40,360 --> 00:12:41,200
可能是

420
00:12:44,120 --> 00:12:45,360
可能是最好的

421
00:12:49,720 --> 00:12:52,159
Primary Server和PyTorch可以结合吗

422
00:12:52,159 --> 00:12:53,159
具体如何实现

423
00:12:53,480 --> 00:12:55,399
PyTorch没有实现Primary Server

424
00:12:55,399 --> 00:12:57,679
M3的TensorFlow有

425
00:12:57,919 --> 00:12:59,039
因为PyTorch它不做

426
00:12:59,039 --> 00:13:00,399
Async那一套东西

427
00:13:00,559 --> 00:13:03,399
PyTorch做了一个最简单的Data Parallel

428
00:13:03,599 --> 00:13:04,319
所以就是说

429
00:13:04,319 --> 00:13:06,319
你不需要Primary Server是没关系的

430
00:13:10,399 --> 00:13:12,039
但是我觉得有第三方实现

431
00:13:12,199 --> 00:13:13,639
我觉得比如说

432
00:13:14,639 --> 00:13:16,399
头条他们不是实现了一个

433
00:13:16,399 --> 00:13:17,199
PS Lite

434
00:13:18,519 --> 00:13:20,319
一个BytePS

435
00:13:20,480 --> 00:13:22,159
它是应该支持PyTorch的

436
00:13:24,480 --> 00:13:25,519
问题是5

437
00:13:25,519 --> 00:13:27,199
用了NN.Data Parallel

438
00:13:27,240 --> 00:13:29,320
是不是数据也被自动分配到GPU上

439
00:13:29,320 --> 00:13:29,759
是的

440
00:13:30,280 --> 00:13:31,840
他就是说他理论上

441
00:13:31,840 --> 00:13:33,920
在算Net.4的时候

442
00:13:33,920 --> 00:13:36,040
他会把你这个东西跟我们之间切开

443
00:13:37,040 --> 00:13:37,840
问题16

444
00:13:38,120 --> 00:13:39,040
我们等会讲

445
00:13:39,240 --> 00:13:39,879
Batch Size调大

446
00:13:39,879 --> 00:13:41,120
为什么进度会变第一

447
00:13:42,560 --> 00:13:44,920
验证几项准确度震荡较大

448
00:13:44,920 --> 00:13:46,080
是哪个参数影响最大

449
00:13:46,960 --> 00:13:48,080
是能力Rate

450
00:13:50,759 --> 00:13:51,680
问题18

451
00:13:52,200 --> 00:13:54,040
让网络前级才能训练

452
00:13:54,040 --> 00:13:55,400
我们采用BN等操作

453
00:13:55,399 --> 00:13:57,000
为什么不采用不同

454
00:13:57,000 --> 00:13:58,720
为了不采用不同stage

455
00:13:58,720 --> 00:13:59,639
采用不同学习率

456
00:13:59,840 --> 00:14:00,959
比如说开始学习率

457
00:14:00,959 --> 00:14:02,480
这个问题我们之前有同学问过

458
00:14:02,480 --> 00:14:03,279
我觉得挺好的

459
00:14:04,519 --> 00:14:06,279
你可以这么做没问题

460
00:14:06,600 --> 00:14:08,079
等会我们可能

461
00:14:08,919 --> 00:14:10,959
我们会在Fine Tuning的时候讲一下

462
00:14:10,959 --> 00:14:12,480
我们也会用到类似的东西

463
00:14:12,480 --> 00:14:14,279
但是这东西用起来最大麻烦

464
00:14:14,279 --> 00:14:15,480
是比较麻烦

465
00:14:15,600 --> 00:14:16,720
你怎么调对吧

466
00:14:16,919 --> 00:14:19,039
你到底谁大谁小

467
00:14:19,039 --> 00:14:21,000
我知道到底大多少谁

468
00:14:21,120 --> 00:14:22,279
就不好调

469
00:14:22,840 --> 00:14:23,679
加一个Batch Long

470
00:14:23,679 --> 00:14:25,079
Batch Long也没什么参数可以调

471
00:14:25,080 --> 00:14:26,280
Batch Long有操参数

472
00:14:26,440 --> 00:14:27,759
但是大家不会调

473
00:14:27,800 --> 00:14:29,400
所以方便简单

474
00:14:31,440 --> 00:14:34,440
在用Torch的数据比拼中

475
00:14:34,440 --> 00:14:36,200
将input的label放在GPU0上

476
00:14:36,200 --> 00:14:37,480
会不会导致性的问题

477
00:14:37,720 --> 00:14:40,520
因为这些数据最初挪到GPU上

478
00:14:41,240 --> 00:14:41,720
对

479
00:14:41,720 --> 00:14:43,400
就是说你这个问题很好

480
00:14:43,400 --> 00:14:45,080
就是说你发现说

481
00:14:46,680 --> 00:14:47,320
你发现说

482
00:14:47,320 --> 00:14:48,680
我们做了一个额外的操作

483
00:14:48,680 --> 00:14:50,840
我们把数据挪到了GPU0上

484
00:14:51,280 --> 00:14:53,200
通常把数据挪到GPU0上

485
00:14:53,200 --> 00:14:54,080
不是太多问题

486
00:14:54,080 --> 00:14:55,360
因为是挪个数据

487
00:14:55,360 --> 00:14:56,680
相对来说比较小

488
00:14:56,960 --> 00:14:59,080
但是这个东西看上去是挺额外的

489
00:14:59,360 --> 00:15:01,040
我没有仔细看这个问题

490
00:15:01,040 --> 00:15:03,840
我觉得不应该需要去挪它

491
00:15:03,840 --> 00:15:04,720
但实际上来说

492
00:15:04,720 --> 00:15:06,040
你不挪它又会出错

493
00:15:06,040 --> 00:15:08,120
所以我其实没仔细看出错原因是什么

494
00:15:08,120 --> 00:15:09,280
你可去研究一下

495
00:15:09,320 --> 00:15:12,520
我觉得我是感觉那是一句多余的话

496
00:15:12,520 --> 00:15:14,280
就是我们在刚刚代码实现

497
00:15:14,639 --> 00:15:15,280
把

498
00:15:15,440 --> 00:15:16,040
又没必要了

499
00:15:16,040 --> 00:15:17,440
我放在GPU上CPU上

500
00:15:17,440 --> 00:15:18,720
你自己给我挪去呗

501
00:15:20,600 --> 00:15:22,160
理论上这样子更直观

