1
00:00:00,000 --> 00:00:04,799
首先我们来讲PyTorch里面怎么样构造

2
00:00:04,799 --> 00:00:06,799
程和快这个概念

3
00:00:07,400 --> 00:00:10,800
首先我们回忆一下我们的多层感知机是怎么定义的

4
00:00:10,800 --> 00:00:13,400
我们首先import了Torch

5
00:00:13,599 --> 00:00:15,599
然后Torch我们用NN这个module

6
00:00:15,800 --> 00:00:17,900
这个是我们之前一直做过的事情

7
00:00:18,199 --> 00:00:22,199
但这里我们在NN这个module里面把

8
00:00:22,199 --> 00:00:24,699
functional这个module弄进来

9
00:00:24,900 --> 00:00:26,000
把它记成F

10
00:00:26,100 --> 00:00:28,400
它其实里面定义了一些函数

11
00:00:28,400 --> 00:00:30,500
就是没有包括参数的一些函数

12
00:00:30,600 --> 00:00:32,800
我们之后会看到怎么使用

13
00:00:34,000 --> 00:00:35,800
然后我们构造了一个

14
00:00:35,800 --> 00:00:38,600
这是之前我们构造的简单的单层

15
00:00:38,800 --> 00:00:41,500
升级网络就是一个线性层

16
00:00:41,500 --> 00:00:42,600
然后加一个reloop

17
00:00:42,600 --> 00:00:43,700
再加一个线性层

18
00:00:44,600 --> 00:00:47,900
当然我们可以生成一个随机的一个input

19
00:00:47,900 --> 00:00:49,700
然后得到我们的output

20
00:00:50,300 --> 00:00:54,700
就是x是一个2乘以20的一个随机矩阵

21
00:00:54,700 --> 00:00:56,500
2是你的p量大小

22
00:00:56,500 --> 00:00:57,800
20是输入的维度

23
00:00:57,800 --> 00:00:59,900
所以这个地方也是20的样子

24
00:01:01,000 --> 00:01:02,800
然后拿到我们输出的话

25
00:01:02,800 --> 00:01:05,300
那就是一个2乘以20

26
00:01:05,300 --> 00:01:06,800
还是你的pn维度对吧

27
00:01:06,900 --> 00:01:10,600
然后你的10就是你的输出的大小

28
00:01:11,700 --> 00:01:14,900
OK这就是我们之前讲过的一个东西

29
00:01:15,100 --> 00:01:16,700
但我们这里稍微提一下

30
00:01:16,700 --> 00:01:20,600
就是说我们用的NN sequential这个类

31
00:01:20,600 --> 00:01:23,800
它是定义了一个特殊的一个module

32
00:01:24,500 --> 00:01:25,900
所以在PyTorch里面

33
00:01:26,900 --> 00:01:28,700
Module是一个很重要的概念

34
00:01:29,800 --> 00:01:31,000
我们来看一下一个Module

35
00:01:31,600 --> 00:01:33,200
Module可以认为是说

36
00:01:33,500 --> 00:01:36,100
任何一个层和一个

37
00:01:36,600 --> 00:01:37,700
升级网络

38
00:01:37,700 --> 00:01:40,400
它应该都是Module的一个子类

39
00:01:41,200 --> 00:01:42,500
比如说我们

40
00:01:43,500 --> 00:01:47,000
自定义一个MLP实现刚刚一样的函数

41
00:01:47,000 --> 00:01:48,000
那我们怎么做呢

42
00:01:48,500 --> 00:01:51,500
我们说我们的定义一个MLP

43
00:01:52,500 --> 00:01:56,200
然后它是NN module的一个子类

44
00:01:58,000 --> 00:02:00,900
这样子的话它继承了这个Module的话

45
00:02:00,900 --> 00:02:04,900
我们会继承得到了非常多的好用的函数

46
00:02:05,299 --> 00:02:07,900
然后我们首先定义两个函数

47
00:02:07,900 --> 00:02:10,400
所以所谓的Module它有两个比较重要的

48
00:02:10,400 --> 00:02:10,699
函数

49
00:02:10,699 --> 00:02:13,199
一个是init这个函数

50
00:02:13,400 --> 00:02:14,800
我们在里面会定义一下

51
00:02:14,900 --> 00:02:16,000
我们需要哪些类

52
00:02:16,000 --> 00:02:17,900
我们需要哪些参数

53
00:02:18,400 --> 00:02:20,000
所以在这个地方我们可以看一下

54
00:02:20,699 --> 00:02:23,800
所以init第一件事情是去调用它的

55
00:02:23,800 --> 00:02:26,199
副类就是Module这个init函数

56
00:02:26,199 --> 00:02:28,900
把一些所需要的一些类部的参数

57
00:02:28,900 --> 00:02:30,099
给你全部设好

58
00:02:30,300 --> 00:02:32,800
这样子我们之后在初始化

59
00:02:32,800 --> 00:02:36,000
wait和别的地方的时候

60
00:02:36,000 --> 00:02:37,800
我们就可以把它全部弄好

61
00:02:39,000 --> 00:02:43,000
接下来我们定义的是两个全连接层

62
00:02:43,500 --> 00:02:45,000
就是说这是我们的隐藏层

63
00:02:45,300 --> 00:02:47,599
是之前我们的linear

64
00:02:47,599 --> 00:02:49,099
就是我们的输入维度是20

65
00:02:49,199 --> 00:02:50,599
输出是256

66
00:02:50,799 --> 00:02:54,500
我们把它存在一个类的成员

67
00:02:54,900 --> 00:02:57,799
变量里面就是self.hidden

68
00:02:58,799 --> 00:03:01,000
存放的话我们把输出层

69
00:03:01,000 --> 00:03:03,400
就是你的输出是10

70
00:03:03,400 --> 00:03:04,900
你的输入是256

71
00:03:05,000 --> 00:03:05,900
也放在里面

72
00:03:05,900 --> 00:03:07,000
我们放在out

73
00:03:07,000 --> 00:03:09,500
但是out和hidden就是你随便怎么定义了

74
00:03:10,799 --> 00:03:14,000
然后这个就是说这个函数包括了

75
00:03:14,000 --> 00:03:18,000
所有的你的那一些所要的层

76
00:03:18,000 --> 00:03:19,500
就是你在网络里面要的层

77
00:03:19,500 --> 00:03:20,800
全部在这个里面

78
00:03:21,400 --> 00:03:23,199
接下来我们要唯一要定的是

79
00:03:23,199 --> 00:03:25,400
我的前项函数怎么运算

80
00:03:26,300 --> 00:03:27,900
就是我们定的forward函数

81
00:03:28,599 --> 00:03:30,000
forward函数也挺简单

82
00:03:30,000 --> 00:03:32,199
就是说我们的输入是我们的x

83
00:03:33,199 --> 00:03:34,900
self当然是说python变身

84
00:03:35,000 --> 00:03:38,500
就是self就是指向自己的东西

85
00:03:39,000 --> 00:03:41,300
所以我们给定我们的输入的话

86
00:03:41,300 --> 00:03:42,500
我们怎么做输出呢

87
00:03:43,099 --> 00:03:47,900
我们先把输入放到我们的hidden层里面

88
00:03:48,500 --> 00:03:48,800
对吧

89
00:03:48,800 --> 00:03:50,500
得到我们的隐藏层的输出

90
00:03:51,300 --> 00:03:53,300
然后我们要去调用redo这个函数

91
00:03:53,300 --> 00:03:55,599
我们用redo就直接用的是

92
00:03:55,599 --> 00:03:57,599
module里面实现的这个函数

93
00:03:57,599 --> 00:03:59,000
f里面实现的这个函数

94
00:03:59,199 --> 00:04:00,599
就f里面大家可以去看一下

95
00:04:00,800 --> 00:04:03,400
实现了大量的常用的一些函数

96
00:04:04,400 --> 00:04:07,000
然后做了我们的激活之后

97
00:04:07,000 --> 00:04:09,599
我们就放入我们的输出里面

98
00:04:10,099 --> 00:04:10,699
就是out

99
00:04:11,199 --> 00:04:12,400
最后返回出来

100
00:04:12,699 --> 00:04:14,099
那就是这样子的话

101
00:04:14,199 --> 00:04:15,099
给定输入

102
00:04:15,099 --> 00:04:18,199
我们就把怎么样做前项计算给定好了

103
00:04:19,399 --> 00:04:20,500
然后我们可以看一下

104
00:04:20,500 --> 00:04:21,899
看一下我们怎么用

105
00:04:22,199 --> 00:04:24,100
这个用起来跟前面是一样的

106
00:04:24,100 --> 00:04:26,399
记得我们怎么构造我们的mlp

107
00:04:26,399 --> 00:04:28,800
就是我们实例化类

108
00:04:29,199 --> 00:04:30,699
然后我们把x丢进去

109
00:04:31,500 --> 00:04:33,199
可以拿到我们的输出对吧

110
00:04:33,699 --> 00:04:35,199
这个就是我们的输出

111
00:04:35,199 --> 00:04:38,199
是一个2乘以10的一个矩阵

112
00:04:38,600 --> 00:04:39,899
当然值会变

113
00:04:40,000 --> 00:04:42,800
因为我们的输入那个层

114
00:04:42,800 --> 00:04:43,900
它的随机

115
00:04:44,000 --> 00:04:45,800
它的权重会发生变化

116
00:04:45,800 --> 00:04:47,300
所以值的大小会变

117
00:04:47,300 --> 00:04:49,800
但是值的输入形状是不会变的

118
00:04:49,900 --> 00:04:52,500
就跟之前我们定义的那个mlp

119
00:04:52,500 --> 00:04:54,300
是应该是一模一样的

120
00:04:55,100 --> 00:04:56,500
好这样子我们就

121
00:04:57,000 --> 00:04:59,800
怎么样通过自定义

122
00:05:00,100 --> 00:05:02,600
计成module这个类来实现

123
00:05:02,600 --> 00:05:05,000
我们跟之前一样的一个函数

124
00:05:06,500 --> 00:05:07,800
那接下来我们可以看一下

125
00:05:07,900 --> 00:05:10,600
我们就是用同样的方法

126
00:05:10,600 --> 00:05:12,100
来实现我们刚刚

127
00:05:12,300 --> 00:05:13,900
deln的nn的sequential

128
00:05:13,900 --> 00:05:15,300
这个类是怎么实现的

129
00:05:15,500 --> 00:05:17,100
其实它的实现非常简单

130
00:05:17,700 --> 00:05:18,600
就可以看一下

131
00:05:18,600 --> 00:05:20,500
这个地方就直接实现了跟

132
00:05:21,500 --> 00:05:23,500
跟之前nnsequential类

133
00:05:23,500 --> 00:05:25,200
是几乎是一样的功能

134
00:05:26,100 --> 00:05:27,100
那怎么实现呢

135
00:05:27,700 --> 00:05:28,400
一样的

136
00:05:28,700 --> 00:05:31,100
我们定义一个nnmodule的一个籽类

137
00:05:31,200 --> 00:05:33,300
就是叫我们叫做mysequential

138
00:05:34,400 --> 00:05:36,400
它的init函数跟之前一样

139
00:05:37,000 --> 00:05:38,800
就是我们要构造我们的模型

140
00:05:39,000 --> 00:05:41,400
唯一不一样是说我们接受了一个

141
00:05:42,000 --> 00:05:43,800
list of input arguments

142
00:05:44,199 --> 00:05:46,000
就是在python里面就是用的信号

143
00:05:47,300 --> 00:05:49,500
同样跟之前我们要用

144
00:05:49,500 --> 00:05:51,199
类的初始化函数

145
00:05:52,000 --> 00:05:55,300
然后对于这里面每一个的籽类

146
00:05:55,500 --> 00:05:57,199
记得我们之前是在sequential里面

147
00:05:57,199 --> 00:06:01,199
传入了一个按顺序传入我们的类

148
00:06:02,100 --> 00:06:05,800
我们这里就是对里面所有传进来的

149
00:06:05,800 --> 00:06:08,199
这些layer就是这些层

150
00:06:08,199 --> 00:06:09,500
我们把它叫block了

151
00:06:09,800 --> 00:06:12,699
我们放在一个特殊的一个成员别量里面

152
00:06:12,699 --> 00:06:14,199
就是下滑线modules

153
00:06:14,800 --> 00:06:17,399
就这个是一个特殊的一个容器

154
00:06:17,399 --> 00:06:20,300
就pytorch会知道放在里面的都是一些

155
00:06:20,300 --> 00:06:22,699
我需要的一些层

156
00:06:23,300 --> 00:06:25,000
所以它其实是一个

157
00:06:25,599 --> 00:06:27,399
它是一个ordered dictionary

158
00:06:27,399 --> 00:06:30,699
就是排按序的一个字典

159
00:06:31,000 --> 00:06:33,399
所以我们就是按顺序把它插入进去

160
00:06:33,399 --> 00:06:36,899
然后用把它自己本身作为它的key

161
00:06:37,099 --> 00:06:38,000
然后把它放进去

162
00:06:38,800 --> 00:06:41,599
所以这样子的话就是说在init这个

163
00:06:41,599 --> 00:06:42,399
函数里面

164
00:06:43,199 --> 00:06:45,300
就是说在下面我们调用的时候

165
00:06:45,300 --> 00:06:47,000
就这些所有的这些类

166
00:06:47,199 --> 00:06:50,800
我们就把它放进args

167
00:06:50,800 --> 00:06:54,099
然后我们按顺序放进下滑线modules

168
00:06:54,099 --> 00:06:55,500
这个成员别量里面

169
00:06:57,099 --> 00:06:58,199
同样道理的话

170
00:06:58,199 --> 00:07:00,300
我们要计算forward的话

171
00:07:00,399 --> 00:07:02,300
那就是getting我们的输入x

172
00:07:02,899 --> 00:07:04,800
就对所有module的values

173
00:07:04,899 --> 00:07:06,300
就是放进去的层

174
00:07:06,399 --> 00:07:07,899
因为它是一个ordered dict

175
00:07:07,900 --> 00:07:08,900
它是排好序的

176
00:07:08,900 --> 00:07:11,900
所以它返回的list

177
00:07:11,900 --> 00:07:13,700
就跟我们插入进去的

178
00:07:13,700 --> 00:07:15,800
应该顺序是一样的

179
00:07:15,900 --> 00:07:18,700
所以按顺序我们把input放进去

180
00:07:18,700 --> 00:07:19,700
然后得到output

181
00:07:19,800 --> 00:07:20,700
然后重写一下

182
00:07:20,700 --> 00:07:22,300
然后最后我们返回x

183
00:07:22,600 --> 00:07:24,900
就返回按一层一层调用

184
00:07:25,000 --> 00:07:26,900
返回我们所有要的类

185
00:07:27,600 --> 00:07:29,100
同样的话我们可以调用起来

186
00:07:29,300 --> 00:07:31,400
跟我们之前的nsequential几乎是一样

187
00:07:31,400 --> 00:07:31,800
对吧

188
00:07:32,100 --> 00:07:34,600
就是我们之前n.sequential

189
00:07:34,600 --> 00:07:36,600
然后把我们所谓的隐藏的层

190
00:07:36,600 --> 00:07:37,800
我们的计划函数

191
00:07:38,000 --> 00:07:40,500
我们的切心层全部放进去

192
00:07:41,000 --> 00:07:42,300
构造好之后

193
00:07:42,300 --> 00:07:43,900
我们传入我们的输入

194
00:07:43,900 --> 00:07:47,100
会得到一个也是一个2乘10的一个输出

195
00:07:47,800 --> 00:07:49,400
这个就是这样

196
00:07:49,400 --> 00:07:51,400
我们就定义了一个

197
00:07:51,800 --> 00:07:53,700
我们自己的sequential的类

198
00:07:53,900 --> 00:07:56,500
其实也就是一个module的一个子类

199
00:07:59,300 --> 00:08:02,000
当然我们还可以做更多的灵活的计算

200
00:08:02,100 --> 00:08:03,700
就是说我们为什么说

201
00:08:03,800 --> 00:08:08,699
大家不要当你sequential类

202
00:08:08,699 --> 00:08:11,199
不能完全满足你的需求的时候

203
00:08:11,199 --> 00:08:13,599
我们用这样子的方式来构的函数

204
00:08:13,599 --> 00:08:15,800
它的主要的好处是说

205
00:08:15,800 --> 00:08:17,899
我们在init和for的函数里面

206
00:08:17,899 --> 00:08:20,599
可以做大量的自定义的计算

207
00:08:21,099 --> 00:08:23,399
比如说这个地方

208
00:08:23,399 --> 00:08:25,699
我们用了两个

209
00:08:26,300 --> 00:08:27,699
我们有两个不一样的地方

210
00:08:28,300 --> 00:08:29,099
它还是一样的

211
00:08:29,099 --> 00:08:30,500
它是继承的module

212
00:08:30,699 --> 00:08:33,299
但是我们在这个地方

213
00:08:33,399 --> 00:08:36,199
就直接生成了用touch random

214
00:08:36,399 --> 00:08:40,500
然后直接生成了一个random weights

215
00:08:41,500 --> 00:08:43,699
这个weights是不参与训练的

216
00:08:44,299 --> 00:08:45,099
大家理解吗

217
00:08:45,099 --> 00:08:48,500
就是说它的required grad等于force

218
00:08:48,799 --> 00:08:51,299
就是说这个是一个随机的weights

219
00:08:51,799 --> 00:08:53,399
但是它不会参加训练

220
00:08:53,399 --> 00:08:54,899
因为它不会计算体育

221
00:08:55,799 --> 00:08:57,599
它其实就是一个

222
00:08:57,599 --> 00:09:01,099
我们就放进一个随机的一个权重在里面

223
00:09:01,300 --> 00:09:03,200
剩下当然是linear还是一样的

224
00:09:03,300 --> 00:09:04,600
我们可以构造一个linear

225
00:09:06,100 --> 00:09:07,300
然后在forward里面

226
00:09:07,500 --> 00:09:08,700
我们干了什么事情

227
00:09:08,700 --> 00:09:10,000
我们可以在forward里面

228
00:09:10,000 --> 00:09:11,200
写任何的函数

229
00:09:11,300 --> 00:09:13,200
就是说任何你想要干的函数都行

230
00:09:13,399 --> 00:09:15,600
所以比如说这里举个例子

231
00:09:15,899 --> 00:09:17,800
我们先把x放进来

232
00:09:17,899 --> 00:09:19,399
我们先做linear

233
00:09:20,500 --> 00:09:21,300
做完linear之后

234
00:09:21,300 --> 00:09:22,899
我说我来手写

235
00:09:22,899 --> 00:09:26,899
我说我就把手写把x和我的random weights

236
00:09:26,899 --> 00:09:27,800
做乘法

237
00:09:27,899 --> 00:09:29,300
做矩阵乘法mm

238
00:09:30,300 --> 00:09:31,800
然后再加上一个1

239
00:09:32,300 --> 00:09:33,500
就加上我们一个偏移

240
00:09:33,500 --> 00:09:35,900
然后再做一个激活函数

241
00:09:36,600 --> 00:09:40,000
然后我又回过去调用一下我的线性函数

242
00:09:40,300 --> 00:09:41,500
我的线性类

243
00:09:41,800 --> 00:09:43,000
又调一下linear

244
00:09:43,600 --> 00:09:44,400
得到x

245
00:09:44,700 --> 00:09:46,000
然后我还做一点事情

246
00:09:46,000 --> 00:09:49,700
如果当你的绝对值的求和大于1的话

247
00:09:49,700 --> 00:09:51,300
我就把你除2

248
00:09:51,300 --> 00:09:53,500
一直除到你很小为止

249
00:09:54,700 --> 00:09:57,300
又加入了一个python的一个while

250
00:09:57,300 --> 00:09:58,000
就在里面

251
00:09:58,799 --> 00:10:00,899
最后我说我就返回一个求和

252
00:10:01,000 --> 00:10:02,899
我也不返回一个矩阵了

253
00:10:02,899 --> 00:10:05,000
我就返回一个标量

254
00:10:07,200 --> 00:10:07,700
可以看到

255
00:10:08,200 --> 00:10:08,899
就是说

256
00:10:09,100 --> 00:10:10,399
然后我们当然跟之前一样

257
00:10:10,399 --> 00:10:12,000
我们把类构造出来

258
00:10:12,200 --> 00:10:13,299
把x放进去

259
00:10:13,500 --> 00:10:15,600
大家可以看到我们返回来是一个标量

260
00:10:16,799 --> 00:10:17,200
OK

261
00:10:17,799 --> 00:10:23,000
所以这个例子就是给大家一个样例

262
00:10:23,000 --> 00:10:27,899
就是说我们可以做比较灵活的方法

263
00:10:28,000 --> 00:10:31,299
就是我们通过继承module这个方法

264
00:10:31,299 --> 00:10:34,699
可以比之前的sequential能够更灵活的

265
00:10:34,699 --> 00:10:37,000
去定义我们的参数是什么样子

266
00:10:37,199 --> 00:10:39,500
和我们怎么做前项计算

267
00:10:41,500 --> 00:10:43,500
如果大家回一下我们的反向计算

268
00:10:43,500 --> 00:10:45,000
反向计算是不需要定义的

269
00:10:45,000 --> 00:10:46,199
都是自动求导

270
00:10:46,199 --> 00:10:50,299
就反向计算就是对于函数的自动求导

271
00:10:51,899 --> 00:10:52,199
OK

272
00:10:55,299 --> 00:10:56,000
我们来

273
00:10:58,699 --> 00:11:01,500
就是说另外一个方法就是说你可以嵌套使用

274
00:11:01,699 --> 00:11:04,699
就是说你不管是几乎是你touch里面

275
00:11:04,699 --> 00:11:06,299
所有的不管是层也好

276
00:11:06,299 --> 00:11:07,500
我们现在定的

277
00:11:07,699 --> 00:11:09,699
n linear也好

278
00:11:09,699 --> 00:11:11,699
还是我们的sequential也好

279
00:11:11,699 --> 00:11:13,100
还是我们自定义的话

280
00:11:13,100 --> 00:11:16,100
它都是n的一个module的子类

281
00:11:16,399 --> 00:11:20,299
所以我们其实可以很灵活的去嵌套使用的

282
00:11:20,699 --> 00:11:21,899
比如说这里一个

283
00:11:22,600 --> 00:11:24,799
我们定义一个嵌套的MLP

284
00:11:24,899 --> 00:11:26,399
就这个函数本身是没意义的

285
00:11:26,600 --> 00:11:30,000
我们就给大家展示一下怎么样灵活构造

286
00:11:30,699 --> 00:11:32,399
就首先我定一个net

287
00:11:32,899 --> 00:11:35,000
它net是一个sequential的一个类

288
00:11:35,199 --> 00:11:36,500
我说我有个线性层

289
00:11:36,500 --> 00:11:37,600
我有个全连接层

290
00:11:37,699 --> 00:11:38,600
我有个relu

291
00:11:38,600 --> 00:11:40,000
全连接层有个relu

292
00:11:40,500 --> 00:11:42,199
然后我还定一个线性类

293
00:11:42,899 --> 00:11:44,399
我的forward就是说

294
00:11:44,399 --> 00:11:45,899
那其实就是挺简单

295
00:11:45,899 --> 00:11:48,000
就是把x放到我的net里面

296
00:11:48,000 --> 00:11:49,299
把结果拿出来

297
00:11:49,299 --> 00:11:51,799
然后又放到我的全连接层里面

298
00:11:51,799 --> 00:11:53,299
拿到一个结果就返回它

299
00:11:53,600 --> 00:11:54,600
这就是一个

300
00:11:54,899 --> 00:11:57,700
它其实就是嵌套了sequential和一个单独的

301
00:11:57,700 --> 00:12:00,000
一个linear的一个嵌套

302
00:12:01,399 --> 00:12:03,500
那接下来我可以

303
00:12:03,500 --> 00:12:05,399
因为它都是一个n的module

304
00:12:05,500 --> 00:12:06,899
对于sequential来说

305
00:12:06,899 --> 00:12:09,899
我的输入可以是任何n的module的子类

306
00:12:10,200 --> 00:12:14,800
所以我又定义了一个sequential的一个类

307
00:12:14,800 --> 00:12:16,600
它的第一个block

308
00:12:17,000 --> 00:12:19,200
第一个module就是我们这里的定义的

309
00:12:19,200 --> 00:12:20,300
nest MLP

310
00:12:20,700 --> 00:12:23,600
接下来我又放一个线性层进去

311
00:12:23,600 --> 00:12:26,500
最后我再把我们之前放进一个

312
00:12:26,500 --> 00:12:30,100
我们有一个固定住隐藏层的一个MLP

313
00:12:30,800 --> 00:12:32,500
然后最后全部放进去之后

314
00:12:32,500 --> 00:12:35,100
可以看到是说我们的输入输出

315
00:12:35,100 --> 00:12:38,000
就是我们之前要的一个标量

316
00:12:39,100 --> 00:12:39,399
OK

317
00:12:39,399 --> 00:12:42,300
所以这个就是一个非常灵活的方法

318
00:12:42,300 --> 00:12:44,399
给大家构造各种类

319
00:12:45,100 --> 00:12:46,000
所以总结一下

320
00:12:46,000 --> 00:12:47,600
我们这里讲的就是说

321
00:12:47,600 --> 00:12:50,500
如何通过计程模具这个类

322
00:12:50,800 --> 00:12:53,399
来做比较灵活的模型构造

323
00:12:53,800 --> 00:12:55,700
所以计程这个类只要干两件事情

324
00:12:55,700 --> 00:12:58,399
第一个是说定义好我们init里面

325
00:12:58,399 --> 00:13:01,300
定义好我们要哪些层里面要哪些层

326
00:13:01,500 --> 00:13:04,700
接下来是说我们的前项运算怎么计算

327
00:13:05,899 --> 00:13:08,899
所以这个类虽然写起来代码是比

328
00:13:08,899 --> 00:13:10,800
n sequential要多一点点

329
00:13:11,100 --> 00:13:13,000
但是它给了你灵活性

330
00:13:13,300 --> 00:13:16,300
这就是我们讲的这一阶

331
00:13:17,899 --> 00:13:19,200
我们就问题的话

332
00:13:19,200 --> 00:13:22,800
我们干脆过到我们前面4节过完之后

333
00:13:22,800 --> 00:13:24,200
我们一次性回答问题

334
00:13:24,700 --> 00:13:26,900
就是过到我们GPU使用GPU之前

