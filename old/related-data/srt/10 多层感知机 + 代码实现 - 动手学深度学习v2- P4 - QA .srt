1
00:00:00,000 --> 00:00:02,640
有问题1是说

2
00:00:04,639 --> 00:00:07,240
X大于0为什么输出是1

3
00:00:07,679 --> 00:00:10,759
通过设计W和B吗

4
00:00:10,759 --> 00:00:11,960
还是通过训练

5
00:00:12,240 --> 00:00:14,160
X大于0的时候

6
00:00:14,200 --> 00:00:16,640
就是说我不是说X大于0

7
00:00:16,640 --> 00:00:18,800
是说Sigma那个函数

8
00:00:19,240 --> 00:00:21,000
然后可以回到我们的

9
00:00:22,000 --> 00:00:22,839
回到我们的

10
00:00:31,000 --> 00:00:37,079
对于说

11
00:00:37,119 --> 00:00:40,240
这个X和我们的输入X不是一个东西

12
00:00:42,799 --> 00:00:44,159
就凯恩老师说

13
00:00:45,159 --> 00:00:48,519
这个X是说这个函数的输入X和这个X

14
00:00:48,519 --> 00:00:49,719
本质上不是一个东西

15
00:00:49,920 --> 00:00:52,960
它这个X其实是说你整个的计算

16
00:00:53,400 --> 00:00:55,920
所以大家可能这里有一点点误会

17
00:00:56,560 --> 00:00:57,560
所以是说

18
00:00:58,280 --> 00:01:00,560
当然是通过学习WB

19
00:01:00,560 --> 00:01:03,240
使得你进入我们的Sigma时候

20
00:01:03,240 --> 00:01:04,439
如果是大于0输出为1

21
00:01:04,439 --> 00:01:05,480
我们分类正确

22
00:01:08,600 --> 00:01:10,040
第二个问题是说

23
00:01:10,400 --> 00:01:14,520
请问神经网络中的一层网络

24
00:01:14,520 --> 00:01:15,680
到底是指什么

25
00:01:15,920 --> 00:01:19,879
是指一层神经元经过线性变化后

26
00:01:19,920 --> 00:01:21,120
成为一层网络

27
00:01:21,120 --> 00:01:22,960
还是说一层神经元

28
00:01:22,960 --> 00:01:24,680
通过线性变化加非线性

29
00:01:24,719 --> 00:01:25,560
成为一层

30
00:01:26,560 --> 00:01:27,439
一般来讲

31
00:01:27,439 --> 00:01:31,640
我们是一层是包括了激活函数的

32
00:01:32,280 --> 00:01:35,159
我们回到我们的在

33
00:01:36,000 --> 00:01:36,840
这个

34
00:01:38,719 --> 00:01:39,719
换一个

35
00:01:40,040 --> 00:01:40,840
比如说一层

36
00:01:42,200 --> 00:01:46,599
通常的一层我们是讲带权重的一层

37
00:01:47,520 --> 00:01:49,400
所以这里是有两层

38
00:01:49,960 --> 00:01:51,920
所以我们写当时写三层

39
00:01:52,280 --> 00:01:53,280
但实际上来说

40
00:01:53,280 --> 00:01:53,879
你怎么看

41
00:01:53,920 --> 00:01:55,200
看你怎么看都行

42
00:01:55,480 --> 00:01:56,760
一个看法是说

43
00:01:56,799 --> 00:01:58,320
我假设隐藏层

44
00:01:58,320 --> 00:01:59,920
我把一层画在这个地方

45
00:02:00,799 --> 00:02:01,960
这是一个layer

46
00:02:03,600 --> 00:02:04,840
这个又是一个layer

47
00:02:06,840 --> 00:02:07,840
就是说输入层

48
00:02:07,840 --> 00:02:08,719
我就不算成了

49
00:02:08,719 --> 00:02:09,960
因为它就是一个输入

50
00:02:09,960 --> 00:02:11,240
就是没什么东西

51
00:02:11,599 --> 00:02:13,120
然后它里面包含什么

52
00:02:13,240 --> 00:02:14,920
包含你会发现每一个箭头

53
00:02:15,039 --> 00:02:15,560
这里

54
00:02:15,879 --> 00:02:16,680
每一个箭头

55
00:02:16,680 --> 00:02:18,240
它就是一个w

56
00:02:19,439 --> 00:02:20,520
其实它每个箭头

57
00:02:20,520 --> 00:02:24,120
包含了就是一个可以学习的权重

58
00:02:26,200 --> 00:02:27,040
理解吗

59
00:02:27,040 --> 00:02:28,720
就是说我们隐藏层

60
00:02:28,720 --> 00:02:30,200
因为你的输入是4

61
00:02:30,240 --> 00:02:31,520
输出是5的话

62
00:02:31,520 --> 00:02:32,520
那么它的权重

63
00:02:32,520 --> 00:02:34,160
它就是一个4乘以5

64
00:02:34,160 --> 00:02:34,840
或者5乘以4

65
00:02:34,880 --> 00:02:35,640
反正你正着写

66
00:02:35,640 --> 00:02:36,760
反着写都没关系

67
00:02:37,120 --> 00:02:38,720
我的头把我都挡住了

68
00:02:39,720 --> 00:02:41,320
然后就是说

69
00:02:41,320 --> 00:02:42,720
所以你每一个元素

70
00:02:42,720 --> 00:02:44,960
它对是这里面的一根箭头

71
00:02:45,760 --> 00:02:47,600
然后当然你这里还有一个sigma

72
00:02:47,600 --> 00:02:48,080
对吧

73
00:02:48,240 --> 00:02:51,520
你的h是在激活函数之后的

74
00:02:51,640 --> 00:02:53,240
所以我所谓的一层

75
00:02:53,240 --> 00:02:55,120
通常是说你权重

76
00:02:55,159 --> 00:02:56,640
加上你的激活函数

77
00:02:56,680 --> 00:02:58,480
和它的计算是怎么做的

78
00:02:58,879 --> 00:03:01,120
所以这个里面我们说我们有两层

79
00:03:01,879 --> 00:03:03,760
意思是说我们是有两层

80
00:03:03,760 --> 00:03:06,920
可以学习的层里面带了权重的

81
00:03:07,120 --> 00:03:08,680
但以后我们讲卷积成绩网络

82
00:03:08,680 --> 00:03:09,920
也是差不多是一个概念

83
00:03:09,920 --> 00:03:10,760
就是一个层

84
00:03:11,400 --> 00:03:12,200
输入层

85
00:03:12,200 --> 00:03:13,640
在这个定义里面输入层

86
00:03:13,640 --> 00:03:14,680
我们就不算层了

87
00:03:15,320 --> 00:03:15,840
反过来讲

88
00:03:15,840 --> 00:03:17,280
你也可以说我可以把

89
00:03:19,280 --> 00:03:21,360
我可以把这个东西归入输入层

90
00:03:21,360 --> 00:03:21,840
对吧

91
00:03:22,400 --> 00:03:24,120
我就是输出

92
00:03:24,240 --> 00:03:25,879
我就是不算层也可以

93
00:03:25,879 --> 00:03:26,960
反正就是你这

94
00:03:26,960 --> 00:03:28,319
因为只有两个w在这里

95
00:03:28,319 --> 00:03:29,400
所以你只有两层

96
00:03:30,360 --> 00:03:30,879
OK

97
00:03:36,280 --> 00:03:37,400
就是说另外一个问题

98
00:03:37,400 --> 00:03:39,240
是说你的数据的区域

99
00:03:39,240 --> 00:03:41,599
r是怎么测量或者统计

100
00:03:41,599 --> 00:03:42,759
rho怎么定

101
00:03:42,920 --> 00:03:47,599
实际中我们确实要找到数据分布的区域

102
00:03:47,599 --> 00:03:48,560
可以找到吗

103
00:03:48,879 --> 00:03:49,360
所以

104
00:03:52,479 --> 00:03:55,159
所以这个就是

105
00:03:55,640 --> 00:03:58,400
统计和继续学习的区别

106
00:03:59,080 --> 00:04:00,480
统计我们是不管的

107
00:04:00,520 --> 00:04:02,640
从统计的角度来讲

108
00:04:02,640 --> 00:04:04,280
我的rho是定义出来的东西

109
00:04:04,280 --> 00:04:05,000
你像数学

110
00:04:05,120 --> 00:04:06,560
数学我会记

111
00:04:06,680 --> 00:04:08,000
关注你怎么计算吗

112
00:04:08,000 --> 00:04:08,960
我不关注的

113
00:04:09,159 --> 00:04:11,080
我都是假设数据怎么样

114
00:04:11,080 --> 00:04:12,120
假设这个怎么样

115
00:04:12,159 --> 00:04:14,800
所以我的收敛定理

116
00:04:14,800 --> 00:04:17,000
从来都是一个统计上的一个东西

117
00:04:17,720 --> 00:04:18,800
收敛那一块东西

118
00:04:18,800 --> 00:04:19,800
整个是统计上的

119
00:04:20,319 --> 00:04:24,079
统计可以说统计学习

120
00:04:25,720 --> 00:04:28,920
但是积极学习或者说深度学习

121
00:04:28,960 --> 00:04:31,360
它可以认为是统计的一个计算面

122
00:04:31,480 --> 00:04:33,720
如果统计你可以认为是数学的一块的话

123
00:04:33,720 --> 00:04:37,199
那么就是积极学习对是统计的计算机的

124
00:04:37,199 --> 00:04:38,120
那一个分支

125
00:04:39,120 --> 00:04:40,319
所以积极学习里面

126
00:04:40,319 --> 00:04:42,199
我们当然就如果是计算

127
00:04:42,199 --> 00:04:44,960
你如果是学cs的话

128
00:04:44,960 --> 00:04:46,600
你当然不知道rho怎么算

129
00:04:46,600 --> 00:04:47,439
这个算起来很难

130
00:04:48,040 --> 00:04:50,079
你当然会去想rho怎么算

131
00:04:50,079 --> 00:04:50,639
这个怎么算

132
00:04:50,639 --> 00:04:51,719
实际上你算不出来

133
00:04:51,719 --> 00:04:53,800
当然我们可以做一点假设

134
00:04:53,920 --> 00:04:55,560
我们可以做人工生成的数据

135
00:04:55,560 --> 00:04:56,600
我是能生成的

136
00:04:56,719 --> 00:04:58,399
但实际上是做不了的

137
00:04:58,439 --> 00:04:59,879
这个只是一个收敛定理

138
00:04:59,920 --> 00:05:02,199
这是一个统计上的一个数学

139
00:05:02,879 --> 00:05:05,759
不能太知道你的实际的生产

140
00:05:06,159 --> 00:05:06,879
但反过来讲

141
00:05:07,399 --> 00:05:09,120
最早的人工智能

142
00:05:09,120 --> 00:05:13,600
就是也是CMU的奠基人之一

143
00:05:13,600 --> 00:05:16,360
就和Simon他们做

144
00:05:16,360 --> 00:05:19,480
他们其实像最早奠基人那一块

145
00:05:19,480 --> 00:05:20,680
就像Simon这个人

146
00:05:20,720 --> 00:05:23,680
他其实是拿过诺贝尔经济学奖

147
00:05:24,040 --> 00:05:26,439
所以他本质上是做数学的

148
00:05:26,759 --> 00:05:28,759
然后那时候计算机也刚出来

149
00:05:28,759 --> 00:05:30,439
所以是一帮统计学家

150
00:05:30,480 --> 00:05:31,560
一帮数学家

151
00:05:31,600 --> 00:05:33,680
然后慢慢的转到计算机

152
00:05:33,680 --> 00:05:34,439
所以你可以看见

153
00:05:34,439 --> 00:05:37,520
我们其实是有很多数学在里面的

154
00:05:37,600 --> 00:05:38,439
但现在来讲

155
00:05:38,439 --> 00:05:41,040
我们这个课不就尽量不去讲那个东西

156
00:05:41,040 --> 00:05:42,240
我们之前有讲过

157
00:05:42,360 --> 00:05:44,879
因为那一块来不及实践

158
00:05:44,959 --> 00:05:47,360
所以我们基本上就点到为止

159
00:05:47,360 --> 00:05:48,719
大家知道有这个东西

160
00:05:48,920 --> 00:05:50,800
如果有兴趣的同学可以去站一下

161
00:05:50,800 --> 00:05:53,920
但是如果大家觉得不好理解的话

162
00:05:53,920 --> 00:05:55,079
就这样了

163
00:05:57,159 --> 00:05:58,680
问题4是说

164
00:05:59,040 --> 00:06:00,959
请问是说

165
00:06:01,000 --> 00:06:04,800
正是因为感知机只能产生XOR函数

166
00:06:04,839 --> 00:06:07,439
所以人们才会使用SVM吗

167
00:06:08,920 --> 00:06:11,600
其实感知机这个问题还不是

168
00:06:11,720 --> 00:06:15,000
SVM是上个世纪90年代出来的

169
00:06:15,040 --> 00:06:16,720
感知机其实那一块

170
00:06:16,720 --> 00:06:19,520
当年60年代70年代

171
00:06:19,560 --> 00:06:22,040
基本上被冬天了

172
00:06:22,040 --> 00:06:24,760
就SVM中间其实还有几十年

173
00:06:24,800 --> 00:06:27,280
就大家其实后面是慢慢多层感知机

174
00:06:27,280 --> 00:06:29,280
应该是在SVM之前的

175
00:06:29,360 --> 00:06:30,560
所以是说

176
00:06:31,520 --> 00:06:34,320
但是我们没有我们这个课不讲SVM

177
00:06:34,600 --> 00:06:38,000
虽然我的老板是SVM的奠基人之一

178
00:06:38,560 --> 00:06:38,879
但是

179
00:06:38,879 --> 00:06:42,199
应该是你个认为是说

180
00:06:42,240 --> 00:06:44,360
SVM替代了感知机

181
00:06:45,240 --> 00:06:46,800
不能说是SV是因为

182
00:06:46,800 --> 00:06:49,719
就感知机多层感知机解决了

183
00:06:49,719 --> 00:06:51,319
感知机XOR的问题

184
00:06:51,759 --> 00:06:53,319
但是之后没有流行

185
00:06:53,360 --> 00:06:55,240
是因为两个问题

186
00:06:55,360 --> 00:06:57,879
第一你得选超参数

187
00:06:58,279 --> 00:06:59,000
你得选

188
00:07:01,519 --> 00:07:03,920
你得选多个隐藏层

189
00:07:04,120 --> 00:07:05,719
每个隐藏层长多大

190
00:07:05,920 --> 00:07:07,519
那个是老中医了

191
00:07:07,519 --> 00:07:08,919
怎么调这个东西

192
00:07:08,959 --> 00:07:10,319
而且收敛也不好收敛

193
00:07:10,319 --> 00:07:11,799
就是说我们也知道

194
00:07:11,799 --> 00:07:13,799
要调各种学习率才能收敛

195
00:07:14,120 --> 00:07:16,399
然后SVM的好处是说

196
00:07:16,439 --> 00:07:18,240
他没有那么多超参数可以选

197
00:07:18,240 --> 00:07:19,519
他对超参数不敏感

198
00:07:19,519 --> 00:07:21,719
比如说基于科农核的SVM

199
00:07:21,799 --> 00:07:24,199
他对他你调宽一点

200
00:07:24,199 --> 00:07:25,479
调窄一点都没关系

201
00:07:25,719 --> 00:07:29,159
第二个是说SVM他优化会

202
00:07:30,279 --> 00:07:31,599
接起来比较容易一点点

203
00:07:31,599 --> 00:07:34,359
相对来说不要不需要用SGD

204
00:07:34,359 --> 00:07:35,399
或怎么样

205
00:07:35,520 --> 00:07:38,520
第三个可能更重要的是说

206
00:07:38,520 --> 00:07:39,680
对于学术界来讲

207
00:07:39,720 --> 00:07:42,760
假设你两个模型的效果差不多

208
00:07:42,800 --> 00:07:45,040
SVM和多层感知机

209
00:07:45,040 --> 00:07:46,280
其实就是在现在

210
00:07:46,280 --> 00:07:47,320
其实也是差不多的

211
00:07:47,320 --> 00:07:48,160
就是说我没有说

212
00:07:48,160 --> 00:07:50,200
S多层感知机会比SVM好

213
00:07:50,200 --> 00:07:50,720
没有了

214
00:07:51,400 --> 00:07:53,120
就是说如果两个模型

215
00:07:53,160 --> 00:07:54,920
在实际效果上都差不多

216
00:07:54,920 --> 00:07:55,720
精度都差不多

217
00:07:55,720 --> 00:07:57,000
可能申请完了还好一点点

218
00:07:57,000 --> 00:07:58,760
就仔细调能调出来

219
00:07:59,000 --> 00:08:00,520
但是呢

220
00:08:00,800 --> 00:08:03,080
SVM用起来更简单

221
00:08:03,080 --> 00:08:04,360
就是说不用调那么参数

222
00:08:04,360 --> 00:08:05,920
第二个最重要的是说

223
00:08:05,920 --> 00:08:07,400
SVM他的数学很好

224
00:08:08,280 --> 00:08:09,759
我老板是学物理的

225
00:08:10,600 --> 00:08:11,879
他本来学物理

226
00:08:11,920 --> 00:08:13,480
他拿过物理竞赛的金牌

227
00:08:13,480 --> 00:08:14,400
当年在德国

228
00:08:14,600 --> 00:08:16,840
所以他们那边人是泛含的

229
00:08:16,840 --> 00:08:18,680
那一套数学去做SVM

230
00:08:18,720 --> 00:08:21,360
所以他SVM就很漂亮的数学证明

231
00:08:21,400 --> 00:08:23,560
他从叫瓦普里克

232
00:08:23,560 --> 00:08:26,120
他们那边从统计statistical

233
00:08:26,120 --> 00:08:26,560
learning theory

234
00:08:26,600 --> 00:08:27,480
我们会讲一点点

235
00:08:27,680 --> 00:08:30,080
过来来解释SVM的

236
00:08:30,080 --> 00:08:31,480
比如说他的VC dimension

237
00:08:31,520 --> 00:08:32,840
那一套东西很漂亮

238
00:08:33,000 --> 00:08:34,879
所以对学术界来讲

239
00:08:34,879 --> 00:08:37,320
我在实用性都差不多情况下

240
00:08:37,320 --> 00:08:38,240
可能SVM还好

241
00:08:38,240 --> 00:08:39,600
容易用的情况下

242
00:08:39,600 --> 00:08:41,440
我当然会追求有数学性的东西

243
00:08:42,320 --> 00:08:44,120
所以这就是说

244
00:08:44,120 --> 00:08:47,000
SVM在90年代到2000年

245
00:08:47,000 --> 00:08:49,320
是一直是我们机器学习的一个主流

246
00:08:49,480 --> 00:08:50,320
因为数学很好

247
00:08:50,320 --> 00:08:51,360
而且效果也不差

248
00:08:51,639 --> 00:08:54,680
现在之所以说我不建议大家用SVM

249
00:08:54,680 --> 00:08:55,920
也没有说SVM不好

250
00:08:55,920 --> 00:08:57,840
就是说你用MLP的话

251
00:08:57,840 --> 00:08:58,560
你试一下

252
00:08:58,560 --> 00:09:00,800
反正如果你想改成一个别的神经网络

253
00:09:00,799 --> 00:09:02,799
你就是就改一下模型就过去了

254
00:09:02,919 --> 00:09:04,479
优化算法什么东西都不用变

255
00:09:04,479 --> 00:09:05,279
什么东西都不用变

256
00:09:05,279 --> 00:09:06,679
就是改几行就过去了

257
00:09:06,839 --> 00:09:08,000
但是用SVM的话

258
00:09:08,000 --> 00:09:09,159
整个优化都得换

259
00:09:09,159 --> 00:09:10,519
什么东西都得重新学

260
00:09:10,559 --> 00:09:12,759
所以就是说相对来说没那么容易

261
00:09:14,759 --> 00:09:16,879
所以就是说我们这里只讲了MLP

262
00:09:16,879 --> 00:09:18,439
没有就是多层感知机

263
00:09:18,439 --> 00:09:20,159
没有讲SVM的原因

264
00:09:22,120 --> 00:09:22,559
OK

265
00:09:23,919 --> 00:09:25,479
另外一个是说

266
00:09:25,599 --> 00:09:27,599
XOR函数有什么应用吗

267
00:09:27,599 --> 00:09:28,279
还没有什么应用

268
00:09:28,679 --> 00:09:31,279
我给你就是我给你举个反例

269
00:09:31,559 --> 00:09:33,399
就是说是什么意思

270
00:09:33,399 --> 00:09:34,039
就是

271
00:09:36,799 --> 00:09:39,799
就当年做感知机的一帮人说

272
00:09:39,879 --> 00:09:41,240
我这个东西多厉害

273
00:09:41,240 --> 00:09:41,519
对吧

274
00:09:41,519 --> 00:09:42,679
我硬件给你打出来

275
00:09:42,839 --> 00:09:43,759
就是说你可以看到说

276
00:09:43,759 --> 00:09:44,519
当年感知机

277
00:09:44,519 --> 00:09:45,519
我给你做了个硬件

278
00:09:45,519 --> 00:09:48,360
和现在深度学习用GPU也好

279
00:09:48,360 --> 00:09:49,480
做AI芯片也好

280
00:09:49,480 --> 00:09:50,559
没本质区别

281
00:09:50,919 --> 00:09:52,439
你的计算跟不上

282
00:09:52,439 --> 00:09:53,480
我给你打个硬件

283
00:09:54,000 --> 00:09:56,399
所以就是说我们吹吹吹

284
00:09:56,399 --> 00:09:57,879
就跟现在深度学习一样的

285
00:09:57,879 --> 00:09:59,879
就是说我可以给你吹吹吹

286
00:09:59,919 --> 00:10:01,439
说这个东西多好

287
00:10:01,439 --> 00:10:02,759
但是突然有个人跑过来说

288
00:10:02,759 --> 00:10:04,519
其实你这东西有局限性

289
00:10:04,519 --> 00:10:05,480
我给你举个反例

290
00:10:05,519 --> 00:10:06,600
XOR函数简单

291
00:10:07,120 --> 00:10:08,519
你不能离合

292
00:10:09,519 --> 00:10:10,039
对吧

293
00:10:10,200 --> 00:10:11,439
就是说给你举个反例

294
00:10:11,439 --> 00:10:13,879
让大家一下就没人没兴趣了

295
00:10:13,919 --> 00:10:16,600
搞半天简单函数都不能离合

296
00:10:16,960 --> 00:10:19,039
所以但是之后你多层感知机

297
00:10:19,039 --> 00:10:20,159
你可以证明是说

298
00:10:20,159 --> 00:10:22,240
只要是只要有一个隐藏层

299
00:10:22,279 --> 00:10:24,279
你是可以离合一个任意函数的

300
00:10:24,440 --> 00:10:26,320
就理论上你可以离合任意函数

301
00:10:26,920 --> 00:10:28,640
你可以一层感知机

302
00:10:28,640 --> 00:10:29,920
能离合整个世界

303
00:10:30,240 --> 00:10:33,120
这是理论上实际上做不到

304
00:10:33,120 --> 00:10:34,760
因为优化算法解不了

305
00:10:34,760 --> 00:10:35,760
这是实际的问题

306
00:10:43,480 --> 00:10:44,680
就是说第6个问题

307
00:10:44,680 --> 00:10:47,560
就是说假设你的X轴是特征一

308
00:10:47,560 --> 00:10:48,640
Y轴是特征二

309
00:10:48,640 --> 00:10:50,120
那么红蓝是他的label

310
00:10:50,120 --> 00:10:51,080
对的是这样子的

311
00:10:51,080 --> 00:10:53,320
就是说这是一个XOR函数

312
00:10:53,320 --> 00:10:54,040
就是你有个

313
00:10:54,040 --> 00:10:55,240
你可认为是有两个输入

314
00:10:55,799 --> 00:10:57,200
两个特征

315
00:10:57,240 --> 00:10:59,680
所以它的红和蓝是它的label

316
00:10:59,680 --> 00:11:01,200
就是XOR它的输出

317
00:11:01,360 --> 00:11:03,440
所以它的每一个对应的是一条数据

318
00:11:03,440 --> 00:11:03,680
对吧

319
00:11:03,680 --> 00:11:04,560
因为4个点

320
00:11:04,680 --> 00:11:06,159
XOR就4个点就定义好了

321
00:11:06,159 --> 00:11:06,560
对吧

322
00:11:06,680 --> 00:11:07,279
所以4个点

323
00:11:07,279 --> 00:11:08,440
每个点是一个数据

324
00:11:10,240 --> 00:11:12,759
所以说就是说我的XOR函数

325
00:11:12,759 --> 00:11:14,440
可以通过4个样本来

326
00:11:14,519 --> 00:11:16,159
一个4个样本

327
00:11:16,159 --> 00:11:18,080
每个样本是二维输出

328
00:11:18,080 --> 00:11:20,200
是一个正义负义来给定

329
00:11:20,279 --> 00:11:22,639
所以就是说我对一个很简单的

330
00:11:22,639 --> 00:11:24,680
4个样本两维特征的数

331
00:11:24,800 --> 00:11:26,360
你都不能拿你喝

332
00:11:26,640 --> 00:11:27,440
就是说

333
00:11:27,480 --> 00:11:29,920
所以就是说感知机的局限性

334
00:11:29,920 --> 00:11:31,560
就是你理解是没错的

335
00:11:35,680 --> 00:11:35,960
好

336
00:11:35,960 --> 00:11:37,200
第7个问题挺好的

337
00:11:37,200 --> 00:11:39,560
就是说你为什么神经网络

338
00:11:39,560 --> 00:11:42,280
要增加隐藏层的层数

339
00:11:42,280 --> 00:11:44,640
而不是神级元的个数

340
00:11:45,280 --> 00:11:49,360
是不是有些神级元万有近似性质吗

341
00:11:49,400 --> 00:11:51,800
就是说这里是一个很好玩的一个东西

342
00:11:51,800 --> 00:11:53,440
就是说我回到

343
00:11:54,160 --> 00:11:56,160
然后我来回到我们的slides

344
00:12:01,800 --> 00:12:02,080
好

345
00:12:02,080 --> 00:12:03,240
我们回到这个地方

346
00:12:04,080 --> 00:12:04,840
就是说

347
00:12:05,400 --> 00:12:08,760
我们刚刚讲过是说你要两种可能

348
00:12:08,760 --> 00:12:10,440
一个是说你变得很胖

349
00:12:10,960 --> 00:12:11,840
就你变得很

350
00:12:11,840 --> 00:12:13,200
就是说我换两个图

351
00:12:15,720 --> 00:12:18,280
就是说一个选择是说你的输入进来

352
00:12:18,720 --> 00:12:20,840
我可以用一个很胖的一个东西来学

353
00:12:20,840 --> 00:12:22,120
然后输出

354
00:12:22,240 --> 00:12:26,679
out edge和in的话

355
00:12:27,360 --> 00:12:29,840
这就是我做一个窄一点的

356
00:12:29,840 --> 00:12:31,159
再胖一点的东西

357
00:12:32,039 --> 00:12:33,200
我还有一个选择是说

358
00:12:33,200 --> 00:12:33,919
我同样的模型

359
00:12:33,919 --> 00:12:35,560
我要达到同样差不多的

360
00:12:35,560 --> 00:12:36,519
就是模型复杂度

361
00:12:36,519 --> 00:12:37,879
我们没有讲模型复杂度

362
00:12:38,000 --> 00:12:39,159
你可以认为简单认为

363
00:12:39,159 --> 00:12:40,919
就是我的模型的能力

364
00:12:42,320 --> 00:12:44,080
就是说我可以说

365
00:12:44,080 --> 00:12:45,919
我可以做的这样子做深一点

366
00:12:45,919 --> 00:12:46,320
对吧

367
00:12:46,320 --> 00:12:47,639
每一层搞小一点

368
00:12:49,440 --> 00:12:50,639
就是说这个是out

369
00:12:50,919 --> 00:12:51,759
这个是in

370
00:12:52,919 --> 00:12:54,480
就是说你要两种做法

371
00:12:54,600 --> 00:12:55,560
一种是

372
00:12:55,600 --> 00:12:57,360
但是这两个的模型复杂度

373
00:12:57,360 --> 00:12:58,360
其实可以认为是可以

374
00:12:58,360 --> 00:12:59,279
几乎是等价的

375
00:12:59,279 --> 00:13:00,960
就是说它的capacity

376
00:13:00,960 --> 00:13:03,200
它几乎认为是相等的

377
00:13:03,200 --> 00:13:04,399
从理论上来讲

378
00:13:05,440 --> 00:13:06,399
我们可以证明

379
00:13:06,720 --> 00:13:08,240
它也可以通过合适的

380
00:13:08,960 --> 00:13:09,919
理论上相等

381
00:13:10,000 --> 00:13:13,480
但是问题是这个模型不好训练

382
00:13:14,120 --> 00:13:15,320
就是哪个模型不好训练

383
00:13:15,320 --> 00:13:17,320
这个模型不好训练

384
00:13:18,399 --> 00:13:19,720
这个模型好训练一些

385
00:13:20,720 --> 00:13:23,240
这个模型叫深度学习

386
00:13:23,840 --> 00:13:25,440
这个东西叫深度学习

387
00:13:26,320 --> 00:13:28,000
这个东西叫浅度学习

388
00:13:28,000 --> 00:13:29,680
就那么点东西

389
00:13:29,879 --> 00:13:30,480
就是说

390
00:13:31,480 --> 00:13:33,080
为什么它好训练一些

391
00:13:33,279 --> 00:13:34,320
是因为说

392
00:13:34,840 --> 00:13:37,240
这个东西特别容易overfilling

393
00:13:37,399 --> 00:13:38,840
就特别容易过拟合

394
00:13:38,840 --> 00:13:40,560
因为它就是说

395
00:13:42,360 --> 00:13:43,519
就是说你可以认为

396
00:13:43,519 --> 00:13:44,200
就是说

397
00:13:44,200 --> 00:13:46,000
我要一次性吃个胖子

398
00:13:46,000 --> 00:13:46,720
在这个地方

399
00:13:46,720 --> 00:13:47,759
因为它每个神经元

400
00:13:47,759 --> 00:13:48,960
是一个并行的东西

401
00:13:49,240 --> 00:13:50,840
就是说你说我所有的并行的东西

402
00:13:50,840 --> 00:13:52,320
每个神经元要写的好

403
00:13:52,560 --> 00:13:54,560
大家一起合作学一个东西

404
00:13:54,600 --> 00:13:55,320
很难

405
00:13:55,600 --> 00:13:56,720
就是说理论上可以

406
00:13:56,720 --> 00:13:58,720
但是我们做不了理论解

407
00:13:58,720 --> 00:13:59,160
对吧

408
00:13:59,360 --> 00:14:00,800
实际操作特别难

409
00:14:00,800 --> 00:14:02,480
所以它学习起来不好学

410
00:14:02,720 --> 00:14:04,160
但这个东西就好一点点

411
00:14:04,160 --> 00:14:04,759
就怎么意思

412
00:14:04,879 --> 00:14:05,440
就是说

413
00:14:06,120 --> 00:14:07,519
你要学一个东西

414
00:14:07,519 --> 00:14:08,680
你要学一个很复杂的东西

415
00:14:08,680 --> 00:14:09,280
你怎么学

416
00:14:09,840 --> 00:14:11,920
你不能一开始就把框框框就跳进去

417
00:14:11,920 --> 00:14:12,280
对吧

418
00:14:12,280 --> 00:14:14,040
所以你先从简单开始学

419
00:14:14,920 --> 00:14:17,240
比如说这举一个实贯上例子

420
00:14:17,360 --> 00:14:19,399
这个东西没有理论太多依据

421
00:14:19,600 --> 00:14:20,440
我要学一个

422
00:14:20,680 --> 00:14:22,600
比如说我把一只猫的图片

423
00:14:22,799 --> 00:14:23,879
和一只狗的图片

424
00:14:23,919 --> 00:14:24,440
猫

425
00:14:24,440 --> 00:14:25,120
猫

426
00:14:25,120 --> 00:14:25,440
猫

427
00:14:25,440 --> 00:14:26,560
比如说这是猫

428
00:14:27,720 --> 00:14:28,720
还是狗

429
00:14:30,159 --> 00:14:31,440
我的灵魂画手

430
00:14:31,440 --> 00:14:33,159
我还没有我家娃画的好

431
00:14:33,840 --> 00:14:36,000
我要写一个猫和狗的一个图片

432
00:14:36,120 --> 00:14:37,000
我最后学学

433
00:14:37,320 --> 00:14:38,039
这是个猫

434
00:14:38,039 --> 00:14:38,759
一个cat

435
00:14:38,759 --> 00:14:39,639
一个dog

436
00:14:41,360 --> 00:14:42,399
我要把这个东西

437
00:14:42,399 --> 00:14:43,080
把一个

438
00:14:43,080 --> 00:14:44,279
就是我要学一个函数

439
00:14:44,279 --> 00:14:45,919
把它从转过去

440
00:14:45,919 --> 00:14:46,840
我怎么转呢

441
00:14:47,000 --> 00:14:48,519
我不能一次性转

442
00:14:48,720 --> 00:14:50,240
我先说我先学一点点

443
00:14:50,600 --> 00:14:50,960
学一点

444
00:14:50,960 --> 00:14:52,080
把耳朵学出来

445
00:14:52,360 --> 00:14:53,320
学个嘴巴

446
00:14:54,000 --> 00:14:55,440
耳朵可能学耳朵太难

447
00:14:55,440 --> 00:14:56,519
就学一点简单东西

448
00:14:56,519 --> 00:14:57,399
然后再学个头

449
00:14:57,399 --> 00:14:57,720
对吧

450
00:14:57,720 --> 00:14:58,399
学个头

451
00:14:58,480 --> 00:14:59,240
然后就是说

452
00:14:59,240 --> 00:15:00,440
每一次每一层

453
00:15:00,440 --> 00:15:03,360
你可以把它做一个简单点的任务

454
00:15:03,480 --> 00:15:04,440
学一点点东西

455
00:15:04,440 --> 00:15:06,600
然后慢慢的学着

456
00:15:06,600 --> 00:15:07,240
学得越来越好

457
00:15:07,240 --> 00:15:08,399
最后学到我们的东西去

458
00:15:09,000 --> 00:15:10,480
就是说这是我们的一个

459
00:15:11,240 --> 00:15:12,400
我们的一个

460
00:15:13,639 --> 00:15:14,120
怎么说

461
00:15:14,240 --> 00:15:16,759
就是我们觉得神机网络里应该怎么做

462
00:15:17,120 --> 00:15:18,000
但实际上来说

463
00:15:18,000 --> 00:15:19,440
确实深一点的话

464
00:15:19,440 --> 00:15:21,720
它训练起来方便一点

465
00:15:21,720 --> 00:15:22,240
容易

466
00:15:22,240 --> 00:15:24,080
更容易找到一个比较好的解

467
00:15:24,360 --> 00:15:27,040
这就是为什么叫深度学习

468
00:15:27,480 --> 00:15:29,040
所谓整个深度学习

469
00:15:29,320 --> 00:15:31,200
你可认为在2004年之前

470
00:15:31,560 --> 00:15:32,399
跟之前没区别

471
00:15:32,399 --> 00:15:33,399
我们之后会讲到

472
00:15:33,600 --> 00:15:36,680
它跟60年代70年代没本质区别

473
00:15:36,720 --> 00:15:38,360
就是说只是做的更深了

474
00:15:39,280 --> 00:15:40,440
但是因为它更深

475
00:15:40,440 --> 00:15:42,280
所以导致它训练起来更容易

476
00:15:43,480 --> 00:15:45,680
所以就是效果更好

477
00:15:46,280 --> 00:15:46,440
好

478
00:15:46,440 --> 00:15:47,240
这个就是

479
00:15:48,960 --> 00:15:49,560
我来看一下

480
00:15:51,440 --> 00:15:53,920
神经元和卷积核有什么关系

481
00:15:54,080 --> 00:15:55,240
神经元和卷积核

482
00:15:55,240 --> 00:15:56,320
我们下次讲

483
00:15:56,560 --> 00:16:00,720
我们会讲卷积是怎么从我们现在的

484
00:16:02,160 --> 00:16:04,320
多层感知器一直过去的

485
00:16:04,320 --> 00:16:05,960
它其实很容易过去的一个东西

486
00:16:05,960 --> 00:16:07,440
就没什么特别fancy的

487
00:16:11,080 --> 00:16:12,760
就relu为什么管用

488
00:16:13,759 --> 00:16:15,519
它在大跃龄的部分

489
00:16:15,519 --> 00:16:16,559
也是一个线性变化

490
00:16:16,559 --> 00:16:18,080
为什么能促进学习呢

491
00:16:18,240 --> 00:16:19,639
激活的本质是什么

492
00:16:20,480 --> 00:16:22,200
不是引入非线性性吗

493
00:16:22,240 --> 00:16:23,720
relu是一个非线性模型

494
00:16:23,960 --> 00:16:24,720
非线性函数

495
00:16:24,840 --> 00:16:25,840
不是一个线性函数

496
00:16:27,120 --> 00:16:27,679
理解吗

497
00:16:27,720 --> 00:16:30,600
就是说所谓的线性函数是一根线

498
00:16:31,039 --> 00:16:32,399
relu它虽然是一个直的

499
00:16:32,399 --> 00:16:33,399
但它不是一根线

500
00:16:33,399 --> 00:16:34,360
它是一个折线

501
00:16:34,360 --> 00:16:35,799
折线不是线性函数

502
00:16:36,240 --> 00:16:37,639
线性函数是

503
00:16:38,840 --> 00:16:40,919
我现在用了笔之后写上瘾了

504
00:16:41,920 --> 00:16:47,280
线性函数一定是fx等于ax加上b

505
00:16:48,040 --> 00:16:49,080
这是线性函数

506
00:16:49,080 --> 00:16:50,120
线性函数不管怎么样

507
00:16:50,120 --> 00:16:51,040
它就是一根线

508
00:16:51,760 --> 00:16:54,040
relu虽然它是一根直的

509
00:16:54,040 --> 00:16:55,120
但它不是一根线

510
00:16:55,400 --> 00:16:57,640
它是一个这样子的东西

511
00:16:57,640 --> 00:16:59,680
所以它不是一个线性函数

512
00:16:59,920 --> 00:17:02,400
它是一个piecewise linear

513
00:17:02,400 --> 00:17:03,880
它是一个分段线性函数

514
00:17:03,880 --> 00:17:04,759
但它不是线性

515
00:17:04,920 --> 00:17:06,640
所以加入relu之后

516
00:17:06,960 --> 00:17:08,080
你就有多层了

517
00:17:09,240 --> 00:17:09,640
对吧

518
00:17:09,800 --> 00:17:13,240
所以确实激活函数的本质是引入非线性性

519
00:17:13,240 --> 00:17:14,280
它不要干别的事情

520
00:17:16,160 --> 00:17:17,600
我们之后会稍微解释一下

521
00:17:17,600 --> 00:17:20,360
激活函数你可以认为它本质就是把非线性打断

522
00:17:20,360 --> 00:17:21,960
你可以加一个别的模型都没关系

523
00:17:22,200 --> 00:17:23,280
就是说你可以随便加一个

524
00:17:23,480 --> 00:17:25,200
我可以设计任何东西都行

525
00:17:25,200 --> 00:17:26,600
我可以写一点可以

526
00:17:26,600 --> 00:17:26,840
对吧

527
00:17:26,840 --> 00:17:28,440
我这个写性没画好

528
00:17:29,440 --> 00:17:30,400
我可以这么写一点点

529
00:17:30,400 --> 00:17:30,840
对吧

530
00:17:32,320 --> 00:17:34,080
我往上翘也可以

531
00:17:34,080 --> 00:17:35,280
其实没本质区别

532
00:17:35,280 --> 00:17:39,120
我可以这么来这么去都没关系

533
00:17:39,520 --> 00:17:40,760
其实都没关系

534
00:17:41,080 --> 00:17:43,080
不要觉得它很玄乎

535
00:17:43,080 --> 00:17:46,000
它唯一的有关系的是在我们之后会讲

536
00:17:46,000 --> 00:17:47,480
我们预告太多了

537
00:17:47,480 --> 00:17:50,480
就在这个点的就梯度会有一点点影响

538
00:17:50,760 --> 00:17:52,160
但是没本质关系

539
00:17:52,160 --> 00:17:53,560
所以激活函数你可以随便选

540
00:18:00,040 --> 00:18:02,440
不同任务下的激活函数是不是都不一样

541
00:18:02,440 --> 00:18:04,080
是通过实验来确定的吗

542
00:18:04,400 --> 00:18:07,240
其实都差不多

543
00:18:08,200 --> 00:18:08,920
你不知道

544
00:18:08,920 --> 00:18:11,000
激活函数我觉得就是说

545
00:18:12,000 --> 00:18:15,200
激活函数它远远没有选择隐藏层大小

546
00:18:15,200 --> 00:18:16,440
那些操作参数来的重要

547
00:18:16,440 --> 00:18:18,279
所以大家就用RELU

548
00:18:18,480 --> 00:18:20,319
就尽量不要用别的

549
00:18:20,519 --> 00:18:21,799
就是说你可以选

550
00:18:21,799 --> 00:18:23,480
但本质上没有太多区别

551
00:18:25,200 --> 00:18:28,480
模型的深度和宽度哪一个更影响性能

552
00:18:28,480 --> 00:18:29,599
有理论指导吗

553
00:18:29,720 --> 00:18:32,000
是不是加深哪个更有效

554
00:18:32,039 --> 00:18:35,359
怎么根据输入空间选择最优的深度和宽度

555
00:18:35,520 --> 00:18:39,800
确实是说理论上来讲没有区别

556
00:18:39,840 --> 00:18:41,120
但实际上来说

557
00:18:41,120 --> 00:18:43,440
深一点的会好一点点

558
00:18:46,720 --> 00:18:49,280
最优就比较难了

559
00:18:49,400 --> 00:18:50,000
没有最优

560
00:18:50,000 --> 00:18:52,000
这个东西哪有最优

561
00:18:52,080 --> 00:18:54,800
你可以我们可能会

562
00:18:55,000 --> 00:18:58,320
你可以我们会有比赛

563
00:18:58,320 --> 00:18:59,720
大家来试一下

564
00:18:59,720 --> 00:19:01,040
就是真的要实际来

565
00:19:01,200 --> 00:19:03,880
我的个人的经验

566
00:19:03,880 --> 00:19:06,800
假设我有一个数据

567
00:19:06,800 --> 00:19:10,000
假设我有个数据

568
00:19:10,000 --> 00:19:13,280
我想要去做一个多层感知机

569
00:19:13,280 --> 00:19:14,320
就也叫MLP

570
00:19:14,320 --> 00:19:16,440
Multi Layer Perceptron

571
00:19:17,000 --> 00:19:18,560
我做一个MLP的话

572
00:19:18,560 --> 00:19:20,360
那么我一开始肯定不会做很深

573
00:19:20,360 --> 00:19:21,480
我也不会做很宽

574
00:19:21,600 --> 00:19:22,920
假设我还是举个例子

575
00:19:22,920 --> 00:19:25,440
我的数我的输入假设是128

576
00:19:25,560 --> 00:19:26,680
我的输入假设是2

577
00:19:26,680 --> 00:19:27,880
简单一点

578
00:19:28,240 --> 00:19:30,120
那么我第一步要干的事情是说

579
00:19:30,120 --> 00:19:33,160
我先我先试一下线性的行不行

580
00:19:33,160 --> 00:19:37,040
你就跑一下线性就没有隐藏层

581
00:19:37,320 --> 00:19:40,120
接下来我来做一个有隐藏层的

582
00:19:40,800 --> 00:19:43,240
就是把一个隐藏加一个隐藏层

583
00:19:43,279 --> 00:19:44,080
那么加个隐藏层

584
00:19:44,080 --> 00:19:45,200
一开始我不会做很大

585
00:19:45,200 --> 00:19:46,360
我做一个16

586
00:19:46,400 --> 00:19:47,880
因为它是128到2

587
00:19:48,560 --> 00:19:50,040
那么16也不错

588
00:19:50,080 --> 00:19:50,720
我换一下

589
00:19:50,720 --> 00:19:52,920
我这个用上瘾了

590
00:19:52,920 --> 00:19:56,240
我觉得大家建议很好

591
00:19:56,240 --> 00:19:57,519
用个笔特别好用

592
00:19:57,519 --> 00:20:00,800
就是说我假设我要把128

593
00:20:00,840 --> 00:20:02,880
变到一个2

594
00:20:03,880 --> 00:20:05,040
我把头遮住了

595
00:20:07,200 --> 00:20:08,519
188到2

596
00:20:09,200 --> 00:20:10,720
我第一个第一步

597
00:20:10,759 --> 00:20:12,160
我会就直接

598
00:20:12,440 --> 00:20:13,560
不要隐藏层

599
00:20:13,880 --> 00:20:16,680
直接128到2直接过去

600
00:20:17,519 --> 00:20:19,440
第二次如果觉得我先试一下

601
00:20:19,440 --> 00:20:20,680
第二次我会加一个隐藏的

602
00:20:20,680 --> 00:20:21,320
188

603
00:20:21,320 --> 00:20:22,480
比如说到2

604
00:20:22,480 --> 00:20:25,920
我加一个16

605
00:20:27,200 --> 00:20:29,360
然后16我会再试32

606
00:20:29,360 --> 00:20:31,160
再试比如说再试64

607
00:20:31,160 --> 00:20:33,040
再试128

608
00:20:33,440 --> 00:20:34,200
都可以

609
00:20:35,200 --> 00:20:36,200
那么这个是第二步

610
00:20:36,200 --> 00:20:36,920
就单隐藏层

611
00:20:36,920 --> 00:20:38,720
我会再去看一下第三步

612
00:20:38,720 --> 00:20:40,400
第三步再换一个颜色

613
00:20:40,759 --> 00:20:41,519
第三步

614
00:20:42,360 --> 00:20:43,680
那么就128

615
00:20:43,680 --> 00:20:45,600
假设我这假设我是说

616
00:20:46,279 --> 00:20:49,080
128效果不行

617
00:20:49,360 --> 00:20:50,640
16也效果不行

618
00:20:50,640 --> 00:20:51,400
这个太简单

619
00:20:51,400 --> 00:20:52,240
这个太复杂了

620
00:20:52,240 --> 00:20:54,080
32到64还可以的话

621
00:20:54,120 --> 00:20:55,880
那么第三次

622
00:20:55,880 --> 00:20:57,400
我会加一个两个隐藏层

623
00:20:57,400 --> 00:20:58,600
比如说我还是

624
00:20:58,600 --> 00:21:00,120
我是用一个32

625
00:21:00,160 --> 00:21:01,640
再加一个8

626
00:21:01,920 --> 00:21:03,440
就新加了一个8在这个地方

627
00:21:03,440 --> 00:21:03,720
对吧

628
00:21:03,720 --> 00:21:05,480
比32稍微复杂一点

629
00:21:05,840 --> 00:21:06,920
我当然可以

630
00:21:06,920 --> 00:21:07,880
这个可以改成16

631
00:21:07,880 --> 00:21:08,360
对吧

632
00:21:08,360 --> 00:21:10,000
我这个也可以改成64

633
00:21:10,000 --> 00:21:11,080
我这个改成

634
00:21:11,120 --> 00:21:12,240
我还是用回816

635
00:21:12,240 --> 00:21:13,720
就是说你可以去

636
00:21:14,440 --> 00:21:16,200
多试几次

637
00:21:16,759 --> 00:21:17,560
就是说你没有

638
00:21:17,560 --> 00:21:18,759
就是说你从简单开始

639
00:21:18,759 --> 00:21:20,160
慢慢的把它变复杂

640
00:21:20,200 --> 00:21:23,480
你可以通过加宽和加深都可以

641
00:21:23,480 --> 00:21:25,280
就是说你最后去试一下

642
00:21:25,280 --> 00:21:26,840
最后你就是写一个for loop

643
00:21:26,840 --> 00:21:28,480
来变一下所有的东西

644
00:21:28,480 --> 00:21:29,240
都训练一遍

645
00:21:29,240 --> 00:21:30,520
就是就完事了

646
00:21:30,720 --> 00:21:32,720
但是这个是你最早

647
00:21:32,720 --> 00:21:34,319
你当你没有什么想法怎么做

648
00:21:34,319 --> 00:21:35,200
时候你就这么做

649
00:21:35,240 --> 00:21:36,879
如果你接下来

650
00:21:36,920 --> 00:21:39,279
慢慢的有了一些直观上的理解

651
00:21:39,440 --> 00:21:40,279
这个理解

652
00:21:40,319 --> 00:21:42,039
我当然有一些我的理解

653
00:21:42,039 --> 00:21:43,119
但是我不好

654
00:21:43,279 --> 00:21:44,319
直接说出来

655
00:21:44,319 --> 00:21:45,000
因为

656
00:21:45,440 --> 00:21:46,519
一不好说出来

657
00:21:46,519 --> 00:21:47,759
就不好怎么总结

658
00:21:47,759 --> 00:21:48,720
第二个是说过去

659
00:21:48,720 --> 00:21:50,200
我的直觉不一定是对的

660
00:21:50,240 --> 00:21:51,879
所以大家就是老中医

661
00:21:51,879 --> 00:21:54,039
大家试一遍就有感觉了

662
00:21:55,240 --> 00:21:55,879
OK

663
00:21:56,279 --> 00:21:57,359
我们还有挺多问题

664
00:21:58,359 --> 00:21:59,960
为什么inception的block设计

665
00:21:59,960 --> 00:22:00,480
更管用

666
00:22:00,600 --> 00:22:02,440
我们先不管这个问题

667
00:22:02,559 --> 00:22:04,440
这个是我们之后会解释的

668
00:22:05,160 --> 00:22:06,799
为什么多层感知机

669
00:22:06,799 --> 00:22:08,960
后面的W2 W3没有转制

670
00:22:09,079 --> 00:22:10,640
这个东西看你怎么定义了

671
00:22:10,840 --> 00:22:13,000
就看你定义是m乘n还是n乘m

672
00:22:13,000 --> 00:22:14,559
就有我还没仔细看

673
00:22:14,559 --> 00:22:15,920
也可能我数学是错的

674
00:22:15,960 --> 00:22:16,640
写错了

675
00:22:16,640 --> 00:22:18,680
就是说转制没转制都没关系

676
00:22:18,680 --> 00:22:20,519
就是说最后看你是W2

677
00:22:20,519 --> 00:22:22,680
是定义是从顺着来

678
00:22:22,680 --> 00:22:23,440
还是竖着来

679
00:22:26,960 --> 00:22:27,480
第15

680
00:22:27,960 --> 00:22:30,160
怎么让感知机里和所有函数

681
00:22:30,160 --> 00:22:31,319
又保持动态性的

682
00:22:31,319 --> 00:22:32,720
就像泛化性的

683
00:22:34,680 --> 00:22:37,120
要打造动态神级网络吗

684
00:22:37,160 --> 00:22:40,319
要不训练完参数是永远是死的

685
00:22:40,599 --> 00:22:41,120
是的

686
00:22:41,120 --> 00:22:43,160
训练完之后参数是要固定的

687
00:22:43,440 --> 00:22:44,839
就你不要做动态

688
00:22:44,880 --> 00:22:46,599
为什么不要做动态

689
00:22:46,599 --> 00:22:48,640
我理解就是说我给一个新

690
00:22:48,640 --> 00:22:51,680
我给一个同样的样本

691
00:22:51,720 --> 00:22:54,519
我每次做预测的结果会不一样

692
00:22:55,120 --> 00:22:56,360
那就是动态

693
00:22:57,759 --> 00:22:58,640
但是这个不行

694
00:22:58,960 --> 00:23:00,200
这个东西会有问题

695
00:23:02,680 --> 00:23:03,599
比如说

696
00:23:04,120 --> 00:23:05,799
Google出过一个很大的事情

697
00:23:05,799 --> 00:23:08,240
在很早以前

698
00:23:08,319 --> 00:23:10,279
他的图片的分类

699
00:23:10,279 --> 00:23:12,440
把一个黑人分成一个星星

700
00:23:13,960 --> 00:23:15,079
这是一个非常大的问题

701
00:23:15,079 --> 00:23:15,799
在美国

702
00:23:16,160 --> 00:23:17,400
所以说你在种族歧视

703
00:23:17,480 --> 00:23:20,400
等于是你把我上传一个图片

704
00:23:20,400 --> 00:23:22,400
Google你把我分成一个星星

705
00:23:22,400 --> 00:23:23,799
我把分类成金丝猴

706
00:23:23,799 --> 00:23:24,480
对不对

707
00:23:24,480 --> 00:23:25,880
我就不高兴

708
00:23:25,880 --> 00:23:26,599
我会告你

709
00:23:26,920 --> 00:23:28,160
所以就是说

710
00:23:28,160 --> 00:23:29,880
你千万不能让神经网络

711
00:23:29,880 --> 00:23:30,760
在分类的时候

712
00:23:30,760 --> 00:23:31,800
有random性在里面

713
00:23:31,800 --> 00:23:32,800
就会出问题的

714
00:23:32,800 --> 00:23:33,600
就是说你

715
00:23:33,600 --> 00:23:35,880
我就是说我假设有动态性

716
00:23:35,920 --> 00:23:37,360
那我实际测下来

717
00:23:37,400 --> 00:23:39,600
我自己在自己测没问题

718
00:23:39,600 --> 00:23:40,840
感觉每次都分类正确

719
00:23:40,840 --> 00:23:42,520
但实际上deploy的时候

720
00:23:42,560 --> 00:23:43,480
部署的时候

721
00:23:43,480 --> 00:23:44,840
发现有一定随机量

722
00:23:44,880 --> 00:23:46,200
因为我可能长得跟金丝猴

723
00:23:46,200 --> 00:23:47,080
就是有点像

724
00:23:47,600 --> 00:23:48,840
他本来两个纸很像

725
00:23:48,840 --> 00:23:50,520
但是一定的抖动性

726
00:23:50,520 --> 00:23:51,720
使得我变成金丝猴了

727
00:23:51,720 --> 00:23:52,280
怎么办

728
00:23:52,560 --> 00:23:53,040
对吧

729
00:23:53,520 --> 00:23:55,760
所以你最好是不要有动态性

730
00:23:55,760 --> 00:23:56,240
在里面

731
00:23:56,240 --> 00:23:57,200
但是反过来讲

732
00:23:57,200 --> 00:23:58,480
你所谓的泛化性

733
00:23:58,480 --> 00:23:59,519
就是另外一个东西

734
00:23:59,519 --> 00:24:01,319
就是说所谓的鲁邦性

735
00:24:01,599 --> 00:24:03,160
鲁邦性是说我的

736
00:24:03,200 --> 00:24:05,599
假设我在这里

737
00:24:05,599 --> 00:24:06,640
我给我分类

738
00:24:06,680 --> 00:24:08,079
分类正确是人

739
00:24:08,160 --> 00:24:09,559
假设我换一下头

740
00:24:09,559 --> 00:24:10,599
或者抬一下头

741
00:24:10,599 --> 00:24:11,519
就变成了金丝猴

742
00:24:11,519 --> 00:24:12,480
那是不行的

743
00:24:12,519 --> 00:24:14,599
就数据有变有干扰

744
00:24:14,599 --> 00:24:15,839
有东西变化的时候

745
00:24:15,839 --> 00:24:18,200
我的输入应该是要保持比较稳定的

746
00:24:18,839 --> 00:24:19,759
这个是

747
00:24:20,720 --> 00:24:22,519
我觉得这个就是要比较

748
00:24:22,680 --> 00:24:23,759
重要的一个事情

749
00:24:24,000 --> 00:24:24,720
但是这个东西

750
00:24:24,720 --> 00:24:28,039
我们这个课不会讲稳定性

751
00:24:28,039 --> 00:24:28,480
讲太多

752
00:24:28,480 --> 00:24:29,880
这是一个robustness

753
00:24:29,920 --> 00:24:32,680
是一个比较重要的话题

754
00:24:32,720 --> 00:24:34,279
我们会讲一些

755
00:24:34,480 --> 00:24:36,039
就是我们整个神级网络设计

756
00:24:36,039 --> 00:24:38,120
都是有要使得它更稳定

757
00:24:38,160 --> 00:24:41,279
但是实际上稳定性是有专门的

758
00:24:41,839 --> 00:24:43,400
现在有一个领域在研究

759
00:24:43,440 --> 00:24:44,799
就还是挺新的一个领域

760
00:24:44,799 --> 00:24:47,559
如果你是想做相关研究的话

761
00:24:47,559 --> 00:24:49,079
我觉得是OK的

762
00:24:50,240 --> 00:24:53,519
而且相对很多来医疗

763
00:24:53,519 --> 00:24:54,680
无人车

764
00:24:54,720 --> 00:24:55,920
稳定性都非常重要

765
00:24:55,960 --> 00:24:56,720
就说医疗

766
00:24:56,720 --> 00:24:57,799
你稳定性没搞好

767
00:24:57,799 --> 00:24:58,879
就出人命了

768
00:24:58,920 --> 00:25:00,319
无人车你没搞好

769
00:25:00,319 --> 00:25:01,240
出人命了

770
00:25:03,359 --> 00:25:04,279
第16

771
00:25:05,400 --> 00:25:07,799
今天直播比前两次都卡

772
00:25:07,799 --> 00:25:09,960
再调一点码率试试看

773
00:25:10,160 --> 00:25:10,960
这是个好问题

774
00:25:10,960 --> 00:25:11,480
就是说

775
00:25:11,480 --> 00:25:14,839
其实我上一次比上一次码率要低

776
00:25:15,639 --> 00:25:18,160
我这确实看到是有一定丢包率

777
00:25:18,400 --> 00:25:20,200
我可能再测一测

778
00:25:20,200 --> 00:25:22,279
可能有点玄学

779
00:25:23,680 --> 00:25:27,200
我如果明天如果还是这样的话

780
00:25:27,200 --> 00:25:29,000
我可以再调低一点点看一下

781
00:25:29,039 --> 00:25:29,839
调低一点

782
00:25:29,839 --> 00:25:32,359
我不喜欢调太低的问题是说

783
00:25:33,759 --> 00:25:35,839
因为我录屏也是用这个码率录的

784
00:25:35,839 --> 00:25:36,799
那我觉得录的时候

785
00:25:36,799 --> 00:25:41,759
那就是会很的又很糊

786
00:25:42,839 --> 00:25:43,440
OK

787
00:25:43,559 --> 00:25:44,839
我实际看一下

788
00:25:44,839 --> 00:25:46,359
如果今天我觉得录屏的

789
00:25:46,480 --> 00:25:48,119
我的录的还行的话

790
00:25:48,119 --> 00:25:49,879
那我明天还可以再调低一点点

791
00:25:50,880 --> 00:25:55,160
在网络训练中

792
00:25:55,360 --> 00:25:57,880
前几次迭代的训练准确率高于验证级

793
00:25:57,880 --> 00:25:59,280
有什么可以解释的办法吗

794
00:25:59,400 --> 00:25:59,760
有的

795
00:25:59,760 --> 00:26:02,720
我们会明天会讲

796
00:26:05,200 --> 00:26:07,120
在设置隐藏层的时候

797
00:26:07,120 --> 00:26:08,640
会人为固定

798
00:26:08,800 --> 00:26:10,200
评估特征的数量

799
00:26:10,200 --> 00:26:12,240
然后再设置层数和单元数吗

800
00:26:12,400 --> 00:26:13,960
其实这个是

801
00:26:14,200 --> 00:26:17,080
你会用一个验证数据集来做这个事情

802
00:26:17,079 --> 00:26:19,879
你就是说你可以猜

803
00:26:19,879 --> 00:26:21,399
猜完之后你得去试

804
00:26:21,399 --> 00:26:23,279
你真的拿数据去溜一溜

805
00:26:23,399 --> 00:26:23,799
对吧

806
00:26:23,799 --> 00:26:25,240
所以我们也会讲一下

807
00:26:25,240 --> 00:26:27,720
大概你会怎么样做调参

808
00:26:27,720 --> 00:26:28,919
这个就是调参

809
00:26:28,919 --> 00:26:33,079
那调参就是整个是整个神经网络的

810
00:26:33,079 --> 00:26:34,919
机器学习的整个的一个

811
00:26:36,399 --> 00:26:37,839
就是你数据科学家

812
00:26:38,079 --> 00:26:39,559
80%时间在搞数据

813
00:26:39,559 --> 00:26:41,359
20%时间在调参

814
00:26:43,359 --> 00:26:44,559
所以这个确实

815
00:26:44,599 --> 00:26:45,240
OK

816
00:26:45,519 --> 00:26:46,920
我们应该

817
00:26:48,359 --> 00:26:48,880
OK

