1
00:00:00,000 --> 00:00:02,160
分辨率和size是什么关系

2
00:00:02,160 --> 00:00:08,400
分辨率就是说W和H是讲的是你当前那个feature map的高宽

3
00:00:08,400 --> 00:00:10,080
你越大当然分辨率越高

4
00:00:10,080 --> 00:00:15,040
就是每一个等于是每一个block你的输入的大小

5
00:00:15,040 --> 00:00:17,359
你的size就是说你这个anchor

6
00:00:17,359 --> 00:00:21,679
就是说你生成的模框在里面要占你整个里面都多大

7
00:00:21,679 --> 00:00:22,760
就是它没有

8
00:00:22,760 --> 00:00:23,879
当然没有什么关系

9
00:00:23,879 --> 00:00:29,280
只是说我们在SSD里面因为做的是多层的多尺度的

10
00:00:29,280 --> 00:00:32,000
所以就是说当你W和H比较大的时候

11
00:00:32,000 --> 00:00:34,399
我会选择比较小的S

12
00:00:34,399 --> 00:00:35,840
当你这个比较小的时候

13
00:00:35,840 --> 00:00:37,320
我的S会取得比较大

14
00:00:37,320 --> 00:00:38,240
OK

15
00:00:40,200 --> 00:00:40,679
问题二

16
00:00:40,679 --> 00:00:43,079
number of inputs和number of anchors是什么关系

17
00:00:43,079 --> 00:00:45,280
每个channel上的anchor各属固定吗

18
00:00:45,960 --> 00:00:47,640
number of inputs实际是说

19
00:00:47,640 --> 00:00:49,960
这个也是PyTorch比较恶心的地方

20
00:00:49,960 --> 00:00:53,320
就是说number of inputs是你的输入的通道数

21
00:00:54,240 --> 00:00:55,840
所以这个东西是你之前是多少

22
00:00:55,840 --> 00:00:56,560
你就是多少

23
00:00:56,560 --> 00:00:57,359
反正你也管不了

24
00:00:57,359 --> 00:00:59,719
大概就是64 128这个数字

25
00:01:00,399 --> 00:01:03,039
number of anchors跟这个东西是没什么关系的

26
00:01:03,039 --> 00:01:05,439
number of anchors是说我们number of anchors的意思

27
00:01:05,439 --> 00:01:09,079
就是说每一个像素要生成多少个模框

28
00:01:09,079 --> 00:01:09,799
是等于4的

29
00:01:09,799 --> 00:01:11,239
这个东西是固定等于4的

30
00:01:11,480 --> 00:01:12,120
OK

31
00:01:12,280 --> 00:01:14,799
所以每一个channel上的anchor数的固定

32
00:01:14,799 --> 00:01:16,840
就是说我们在做预测的时候

33
00:01:17,359 --> 00:01:21,960
channel数是把它用来做存我们每一个像素的预测值

34
00:01:22,840 --> 00:01:24,439
记不记得在我们的NNN

35
00:01:24,439 --> 00:01:26,719
那个Networking Network那一张讲过说

36
00:01:26,760 --> 00:01:30,920
他用最后一个卷积把channel数设成你的

37
00:01:30,920 --> 00:01:33,480
等于你的number class来做预测

38
00:01:34,239 --> 00:01:34,799
OK

39
00:01:34,959 --> 00:01:36,640
这就是channel数是可以做

40
00:01:36,640 --> 00:01:38,039
你认为是一个特征维度

41
00:01:38,039 --> 00:01:40,359
你可以用来做放你的预测值的

42
00:01:40,599 --> 00:01:42,799
所以在做预测的时候

43
00:01:42,799 --> 00:01:46,000
卷积的channel输出channel数

44
00:01:46,000 --> 00:01:48,200
是不再是用一个

45
00:01:48,719 --> 00:01:51,680
我们可以认为就是一个等价于你的一个

46
00:01:52,039 --> 00:01:55,439
我们之前的全连阶层的输出了

47
00:01:56,920 --> 00:01:59,319
OK

48
00:01:59,319 --> 00:02:02,560
另外一个问题就是说你的输出的类别预测

49
00:02:02,560 --> 00:02:04,519
个数就是你的

50
00:02:04,679 --> 00:02:08,199
W×H就是每一个像素

51
00:02:09,400 --> 00:02:11,120
但是没有input的channel数是没有的

52
00:02:11,439 --> 00:02:12,319
这个是没有的

53
00:02:12,319 --> 00:02:14,400
因为input的channel数已经变成了

54
00:02:14,439 --> 00:02:16,680
已经被卷积给干掉了

55
00:02:17,039 --> 00:02:18,199
所以就是说

56
00:02:18,879 --> 00:02:20,120
这个数是不用的

57
00:02:20,319 --> 00:02:21,199
但是这个是有的

58
00:02:21,199 --> 00:02:21,719
就是说

59
00:02:21,719 --> 00:02:25,240
然后对每一个像素为中心生成的多少个

60
00:02:25,800 --> 00:02:27,240
我们在这个地方是4

61
00:02:27,280 --> 00:02:28,879
number class是等于1

62
00:02:29,080 --> 00:02:31,000
我们在这个样例里面就乘2

63
00:02:31,200 --> 00:02:31,719
OK

64
00:02:35,080 --> 00:02:37,600
问题4可以把变换的过程画一下图吗

65
00:02:37,600 --> 00:02:39,240
你是说permute

66
00:02:39,840 --> 00:02:41,240
permute就是

67
00:02:41,600 --> 00:02:43,560
你得去permute

68
00:02:43,560 --> 00:02:45,760
就是说把一个通道

69
00:02:45,760 --> 00:02:48,080
就你这直观上比较难讲

70
00:02:48,520 --> 00:02:49,640
就直观上说

71
00:02:49,640 --> 00:02:50,320
你就卷

72
00:02:50,320 --> 00:02:51,240
你就矩阵的时候

73
00:02:51,240 --> 00:02:52,120
你permute就是说

74
00:02:52,120 --> 00:02:53,360
你可以是转制

75
00:02:53,360 --> 00:02:53,600
对吧

76
00:02:53,600 --> 00:02:54,360
就转一下

77
00:02:54,360 --> 00:02:56,200
但是你4D的话不好画

78
00:02:56,200 --> 00:02:57,120
就是说你可以认为

79
00:02:57,120 --> 00:02:57,840
就是说

80
00:02:58,120 --> 00:02:59,400
就换一个顺序

81
00:03:01,720 --> 00:03:04,440
就这个东西直观上确实不那么好理解

82
00:03:04,440 --> 00:03:05,320
就是说东西

83
00:03:05,360 --> 00:03:06,680
我觉得你可能还是得

84
00:03:07,920 --> 00:03:08,600
去

85
00:03:09,200 --> 00:03:11,080
自己写点代码去验证一下

86
00:03:11,200 --> 00:03:12,520
就直观上确实不好解释

87
00:03:12,520 --> 00:03:13,280
这个东西怎么

88
00:03:13,280 --> 00:03:15,200
就是说你可能形成逻辑的时候

89
00:03:15,240 --> 00:03:17,000
我就把通道

90
00:03:17,000 --> 00:03:18,960
就那个维度可以换来换去

91
00:03:18,960 --> 00:03:20,200
这个是很容易换的

92
00:03:20,320 --> 00:03:22,520
就你习惯这个想法也是可以的

93
00:03:22,520 --> 00:03:25,439
目标检测的时候

94
00:03:25,439 --> 00:03:26,840
如果图片尺寸很大

95
00:03:26,840 --> 00:03:27,920
比如说1000×1000

96
00:03:27,920 --> 00:03:30,240
那么毛框再乘以是不是很大

97
00:03:30,879 --> 00:03:31,200
对了

98
00:03:31,200 --> 00:03:31,960
就是说

99
00:03:32,800 --> 00:03:34,080
目标检测时候

100
00:03:34,080 --> 00:03:36,240
如果图片的尺寸特别大

101
00:03:36,240 --> 00:03:36,920
1000×1000

102
00:03:36,920 --> 00:03:38,200
那么你的毛框就要炸掉了

103
00:03:38,200 --> 00:03:38,760
对吧

104
00:03:40,480 --> 00:03:41,640
1000×1000还好吧

105
00:03:41,640 --> 00:03:42,360
我们是512

106
00:03:42,360 --> 00:03:43,080
我们是256

107
00:03:43,080 --> 00:03:44,120
我们是256×256

108
00:03:44,240 --> 00:03:44,879
1000×1000

109
00:03:45,040 --> 00:03:45,760
大概

110
00:03:46,280 --> 00:03:46,600
对

111
00:03:46,600 --> 00:03:47,600
你是炸掉了

112
00:03:47,920 --> 00:03:48,360
所以

113
00:03:49,840 --> 00:03:50,960
所以这个地方

114
00:03:51,120 --> 00:03:52,120
特别大的图片

115
00:03:52,560 --> 00:03:54,120
SSD不那么适合

116
00:03:54,120 --> 00:03:55,400
所以蒸发不适合

117
00:03:55,719 --> 00:03:57,920
所以大家1000×1000的时候

118
00:03:57,920 --> 00:03:59,120
会用比如说MASTA

119
00:03:59,120 --> 00:04:01,040
FASTA这种2 stage的

120
00:04:01,439 --> 00:04:02,879
就是说我先有一个

121
00:04:02,920 --> 00:04:03,840
RPM

122
00:04:03,840 --> 00:04:05,879
就是说能够把毛框数量

123
00:04:06,319 --> 00:04:07,640
尽量降下来

124
00:04:07,680 --> 00:04:10,040
然后再就是说做两次的下降

125
00:04:10,040 --> 00:04:10,640
就是说

126
00:04:10,800 --> 00:04:12,480
我先搞一堆毛框出来

127
00:04:12,480 --> 00:04:14,000
然后把它一个

128
00:04:14,960 --> 00:04:16,439
Regional Proposal Network

129
00:04:16,439 --> 00:04:17,879
就是RPM

130
00:04:17,879 --> 00:04:19,240
把它降一次

131
00:04:19,240 --> 00:04:20,560
然后再降一次

132
00:04:21,039 --> 00:04:21,599
OK

133
00:04:21,599 --> 00:04:22,199
另外一个时候

134
00:04:22,199 --> 00:04:23,120
你可以考虑EURO

135
00:04:23,120 --> 00:04:23,719
EURO这种

136
00:04:23,719 --> 00:04:25,000
就是我们有讲过

137
00:04:25,079 --> 00:04:25,919
不管你多大

138
00:04:25,919 --> 00:04:26,839
我的

139
00:04:27,120 --> 00:04:28,759
我的毛框数都不会很多

140
00:04:30,239 --> 00:04:30,879
OK

141
00:04:34,799 --> 00:04:35,399
问题8

142
00:04:35,399 --> 00:04:38,039
如何处理尺度特殊的物体

143
00:04:38,039 --> 00:04:40,639
特别细长的电线杆

144
00:04:40,839 --> 00:04:41,279
电线杆

145
00:04:41,279 --> 00:04:41,719
你这个

146
00:04:43,000 --> 00:04:44,039
你这个问题问的挺好的

147
00:04:44,120 --> 00:04:45,279
电线杆怎么处理

148
00:04:45,799 --> 00:04:46,399
你怎么处理

149
00:04:46,560 --> 00:04:47,399
其实不难

150
00:04:47,399 --> 00:04:47,799
为什么

151
00:04:48,240 --> 00:04:49,039
就是说

152
00:04:49,599 --> 00:04:50,719
电线杆这个东西

153
00:04:50,720 --> 00:04:52,880
你就是在设计毛框的时候

154
00:04:53,080 --> 00:04:54,280
你可以设计成

155
00:04:54,280 --> 00:04:57,240
你高宽比比较长的那一些东西

156
00:04:58,720 --> 00:04:59,800
就是说最简单

157
00:04:59,800 --> 00:05:00,560
你去怎么看

158
00:05:00,720 --> 00:05:01,440
你就是说

159
00:05:01,480 --> 00:05:02,280
最简单的

160
00:05:02,400 --> 00:05:03,320
你去看一下

161
00:05:03,320 --> 00:05:04,160
你的

162
00:05:04,720 --> 00:05:06,120
你把你的目标检测机里面

163
00:05:06,120 --> 00:05:07,000
所谓的毛框

164
00:05:07,000 --> 00:05:07,880
不是毛框

165
00:05:07,880 --> 00:05:09,240
是真实的标号的

166
00:05:09,240 --> 00:05:10,760
那些Boundary Box拿出来

167
00:05:10,800 --> 00:05:11,720
去统计一下

168
00:05:11,720 --> 00:05:13,280
他们的size

169
00:05:13,280 --> 00:05:14,600
就他们这个框

170
00:05:14,600 --> 00:05:16,440
在你的图片里面的占比

171
00:05:16,440 --> 00:05:17,480
到底是多大

172
00:05:17,920 --> 00:05:18,520
你可统计

173
00:05:18,640 --> 00:05:19,960
反正你有信息

174
00:05:20,680 --> 00:05:22,680
然后你可以看他的高宽比

175
00:05:22,680 --> 00:05:23,439
是什么样子

176
00:05:23,439 --> 00:05:25,759
就是你可以通过去来设计

177
00:05:25,759 --> 00:05:28,120
你的毛框的超参数

178
00:05:28,359 --> 00:05:29,959
当然当你没有更多想法的时候

179
00:05:29,959 --> 00:05:31,279
我们就是随便拍脑袋想

180
00:05:31,839 --> 00:05:32,879
如果你有的话

181
00:05:32,879 --> 00:05:33,599
你可以做

182
00:05:33,599 --> 00:05:34,319
你可以去看一下

183
00:05:34,319 --> 00:05:35,519
ULO的实现

184
00:05:35,639 --> 00:05:36,919
ULO的一系列实现里面

185
00:05:36,919 --> 00:05:38,560
他会去做这个事情

186
00:05:38,599 --> 00:05:40,120
他会去真的去看里面的

187
00:05:40,679 --> 00:05:42,439
看你是不是有很小的物体

188
00:05:42,839 --> 00:05:44,279
通常在什么地方出现

189
00:05:44,599 --> 00:05:45,879
他都会去看这些东西

190
00:05:45,879 --> 00:05:47,240
然后可以帮你做的事情

191
00:05:50,959 --> 00:05:52,680
FOR这个东西怎么用

192
00:05:52,919 --> 00:05:54,039
FOR效果线就是说

193
00:05:54,039 --> 00:05:54,560
我就不要

194
00:05:54,560 --> 00:05:54,959
我就说

195
00:05:54,959 --> 00:05:55,959
我不要那个I

196
00:05:55,959 --> 00:05:56,919
就是你本来可以写上

197
00:05:56,919 --> 00:05:58,039
FOR I IN这个东西

198
00:05:58,399 --> 00:05:59,799
那I反正我必须要

199
00:05:59,839 --> 00:06:01,919
就是说等于是FOR IN RANGE R

200
00:06:01,919 --> 00:06:03,519
就是我把这下面这块代码

201
00:06:03,519 --> 00:06:04,479
重复两遍

202
00:06:04,519 --> 00:06:06,199
但是我也跟效标无关

203
00:06:06,199 --> 00:06:07,799
我就是想copy两次而已

204
00:06:07,959 --> 00:06:09,120
就是说我就是效果线

205
00:06:09,120 --> 00:06:10,759
就表示我这个变量不需要

206
00:06:10,759 --> 00:06:11,639
就丢掉了

207
00:06:15,159 --> 00:06:16,240
问题8

208
00:06:16,240 --> 00:06:18,560
看论文一直很模糊

209
00:06:19,079 --> 00:06:20,560
每个毛框预测的

210
00:06:20,560 --> 00:06:21,639
C加1个类别数

211
00:06:21,639 --> 00:06:22,839
之间共享local吗

212
00:06:23,040 --> 00:06:24,040
这个ULO之间差别什么

213
00:06:25,480 --> 00:06:27,199
每个毛框预测的

214
00:06:27,199 --> 00:06:29,199
C加1的类别数之间

215
00:06:29,199 --> 00:06:31,280
是不share

216
00:06:31,280 --> 00:06:33,120
在那一层是不share

217
00:06:33,319 --> 00:06:35,120
不共享信息的

218
00:06:35,160 --> 00:06:37,079
但是他们是共享特征的

219
00:06:38,079 --> 00:06:39,480
就他特征是共享的

220
00:06:41,120 --> 00:06:42,920
就是说等于是说

221
00:06:42,959 --> 00:06:43,800
每一个毛框

222
00:06:43,800 --> 00:06:45,439
他的预测就是去

223
00:06:46,040 --> 00:06:47,000
数都是一样的

224
00:06:47,000 --> 00:06:48,480
就是卷积层数都是一样

225
00:06:48,480 --> 00:06:49,960
就是说在整个完整的上面

226
00:06:49,960 --> 00:06:50,920
然后你去

227
00:06:50,960 --> 00:06:52,240
你这个毛框的预测的时候

228
00:06:52,240 --> 00:06:54,040
是说我都每个人看到信息

229
00:06:54,040 --> 00:06:54,640
都是一样的

230
00:06:54,640 --> 00:06:57,560
但是根据你不同的毛框的

231
00:06:57,600 --> 00:06:58,160
不一样

232
00:06:58,360 --> 00:07:00,000
所以他就要去里面

233
00:07:00,000 --> 00:07:02,080
去重新重构里面

234
00:07:02,080 --> 00:07:03,800
那些卷积最后一层的

235
00:07:04,000 --> 00:07:05,000
卷积层的输出

236
00:07:05,000 --> 00:07:07,000
使得你尽量看你想要的东西

237
00:07:09,640 --> 00:07:11,319
为什么要用L1 L2

238
00:07:11,319 --> 00:07:12,680
对比较远的panelty

239
00:07:13,120 --> 00:07:14,120
比较不好吗

240
00:07:14,160 --> 00:07:14,920
就是说

241
00:07:15,160 --> 00:07:16,200
为什么用L1

242
00:07:16,360 --> 00:07:17,200
是因为

243
00:07:17,240 --> 00:07:18,759
就是说我怕

244
00:07:19,039 --> 00:07:20,719
通常大家是用L1

245
00:07:20,920 --> 00:07:22,039
因为为什么

246
00:07:22,039 --> 00:07:24,000
大家说如果有一个预测

247
00:07:24,639 --> 00:07:25,439
特别远

248
00:07:25,639 --> 00:07:26,560
特别不靠谱

249
00:07:27,599 --> 00:07:28,759
我其实不关心

250
00:07:28,759 --> 00:07:29,319
因为为什么

251
00:07:29,399 --> 00:07:32,240
因为我做了特别多的毛框的预测

252
00:07:32,759 --> 00:07:35,039
我里面有几百个毛框预测

253
00:07:35,039 --> 00:07:36,039
可能最后我要的

254
00:07:36,039 --> 00:07:36,599
就是说

255
00:07:36,879 --> 00:07:38,680
在里面预测的比较好的毛框

256
00:07:38,680 --> 00:07:39,639
可能有几百个

257
00:07:39,639 --> 00:07:41,319
剩下的可能还有几千个毛框

258
00:07:41,319 --> 00:07:42,039
我都没管

259
00:07:42,079 --> 00:07:43,439
但我最后要的是谁

260
00:07:43,480 --> 00:07:44,519
我最后要的

261
00:07:44,519 --> 00:07:45,719
就这么几个

262
00:07:45,920 --> 00:07:47,520
的LMS去掉之后

263
00:07:47,520 --> 00:07:48,360
所以的话

264
00:07:48,400 --> 00:07:49,800
剩下那些几百个里面

265
00:07:49,800 --> 00:07:51,200
有一些预测不好的东西

266
00:07:51,200 --> 00:07:52,360
我真的不关心

267
00:07:52,520 --> 00:07:54,760
就是说我只要关心我的

268
00:07:55,760 --> 00:07:58,160
那几个比较好的毛框预测就行了

269
00:07:58,160 --> 00:07:59,560
所以在这个地方

270
00:07:59,720 --> 00:08:02,440
我不需要对预测特别不好的毛框

271
00:08:02,440 --> 00:08:03,600
加一个特别大的罚

272
00:08:03,600 --> 00:08:06,040
因为就是说等于是我就弃疗了

273
00:08:07,160 --> 00:08:07,720
OK

274
00:08:09,080 --> 00:08:11,400
问题10新block新型二个

275
00:08:11,400 --> 00:08:11,800
什么意思

276
00:08:11,800 --> 00:08:12,840
就是你看看一下Python

277
00:08:13,280 --> 00:08:14,680
新block就把它展开

278
00:08:14,680 --> 00:08:16,040
展成一个

279
00:08:16,600 --> 00:08:17,680
你本来是一个list

280
00:08:17,680 --> 00:08:18,759
或者一个tube的话

281
00:08:18,759 --> 00:08:19,959
就是说一个类表的话

282
00:08:19,959 --> 00:08:21,879
把它展成一个元素放进去

283
00:08:22,160 --> 00:08:23,519
新型就是一个

284
00:08:24,240 --> 00:08:25,560
就是一个key value的

285
00:08:26,720 --> 00:08:28,040
就是一个key和一个value

286
00:08:28,400 --> 00:08:29,319
展示一个dictionary

287
00:08:29,319 --> 00:08:30,879
然后把它展开丢进去

288
00:08:31,319 --> 00:08:32,799
大家可以查一下Python

289
00:08:32,960 --> 00:08:34,399
这是Python的用法

290
00:08:34,440 --> 00:08:35,680
就跟之前笑话线一样

291
00:08:35,680 --> 00:08:36,720
都是Python的用法

292
00:08:39,919 --> 00:08:40,200
对

293
00:08:40,200 --> 00:08:41,000
这问题11

294
00:08:41,480 --> 00:08:42,600
Cross-entropy loss

295
00:08:42,600 --> 00:08:43,480
还要one loss的

296
00:08:43,480 --> 00:08:44,480
取值范围一样吗

297
00:08:44,480 --> 00:08:45,320
更广泛一个问题

298
00:08:45,320 --> 00:08:46,680
说多个loss直接添加

299
00:08:46,680 --> 00:08:47,680
会不会导致一个loss

300
00:08:47,680 --> 00:08:49,000
dominate整个损失函数

301
00:08:49,000 --> 00:08:50,480
其他loss没有什么用

302
00:08:50,639 --> 00:08:50,960
对的

303
00:08:50,960 --> 00:08:51,879
你这个问题挺好的

304
00:08:53,120 --> 00:08:53,800
就是说

305
00:08:55,159 --> 00:08:56,840
我们之后会看到是说

306
00:08:56,840 --> 00:08:57,920
在style

307
00:08:57,920 --> 00:08:59,000
就是style triffle

308
00:08:59,000 --> 00:08:59,840
是一样是潜移的

309
00:08:59,840 --> 00:09:01,159
会看到也是两个损失

310
00:09:01,159 --> 00:09:02,200
还是怎么样加

311
00:09:02,519 --> 00:09:03,360
通常来说

312
00:09:03,360 --> 00:09:03,960
就是说

313
00:09:03,960 --> 00:09:05,480
假设你一个模型

314
00:09:05,480 --> 00:09:07,000
你要两个损失的话

315
00:09:07,039 --> 00:09:08,159
那么你得去说

316
00:09:08,159 --> 00:09:09,440
他们加起来怎么样子

317
00:09:10,399 --> 00:09:11,240
因为你出问题

318
00:09:11,240 --> 00:09:12,000
会出来什么地方

319
00:09:12,000 --> 00:09:13,480
如果一个loss特别大

320
00:09:14,080 --> 00:09:15,440
一个loss特别小

321
00:09:15,440 --> 00:09:16,039
那么就是说

322
00:09:16,039 --> 00:09:17,240
你的大的loss

323
00:09:17,240 --> 00:09:18,759
拿到了所有的T多

324
00:09:18,800 --> 00:09:19,879
那么就是他的训练

325
00:09:19,879 --> 00:09:21,399
总是去优化这个loss

326
00:09:21,600 --> 00:09:22,960
忽略掉了这一个东西

327
00:09:24,039 --> 00:09:24,560
对吧

328
00:09:24,800 --> 00:09:26,480
所以你需要说

329
00:09:26,480 --> 00:09:27,720
加一个权重

330
00:09:27,720 --> 00:09:30,039
使得这两个loss

331
00:09:30,759 --> 00:09:31,600
差不多

332
00:09:32,200 --> 00:09:33,560
我们这里没有加权重

333
00:09:33,560 --> 00:09:34,000
为什么

334
00:09:34,000 --> 00:09:34,680
就是说

335
00:09:35,080 --> 00:09:36,759
因为你可以看一下图

336
00:09:38,759 --> 00:09:39,639
不是这个图

337
00:09:41,279 --> 00:09:42,240
就可以看一下这个图

338
00:09:43,800 --> 00:09:46,279
就你看到

339
00:09:46,279 --> 00:09:46,680
就是说

340
00:09:46,680 --> 00:09:48,519
这个是我的class prediction

341
00:09:48,519 --> 00:09:48,960
的error

342
00:09:48,960 --> 00:09:50,240
和我的bounded box的

343
00:09:50,240 --> 00:09:50,560
那个

344
00:09:50,560 --> 00:09:52,080
它也不是损失

345
00:09:52,360 --> 00:09:53,639
你可以把损失画出来

346
00:09:53,639 --> 00:09:54,519
我们没有画

347
00:09:54,560 --> 00:09:55,600
就你看到这两个东西

348
00:09:55,600 --> 00:09:57,240
是差不多的一个尺寸

349
00:09:57,680 --> 00:09:59,279
所以我们就没有加权重了

350
00:09:59,440 --> 00:10:00,440
在真实情况下

351
00:10:00,440 --> 00:10:00,960
你怎么办

352
00:10:01,320 --> 00:10:01,960
就是说

353
00:10:01,960 --> 00:10:02,879
在真实情况下

354
00:10:02,879 --> 00:10:04,159
你一开始你不知道

355
00:10:04,200 --> 00:10:05,039
你不知道的话

356
00:10:05,039 --> 00:10:07,320
你就是反正就把这两个loss

357
00:10:07,320 --> 00:10:08,320
画去画一画

358
00:10:08,320 --> 00:10:08,720
看一看

359
00:10:08,840 --> 00:10:10,680
看看他们是不是在一个scale上面

360
00:10:10,759 --> 00:10:11,920
是不是差不多

361
00:10:11,960 --> 00:10:12,759
如果不是的话

362
00:10:12,799 --> 00:10:14,159
那你就停下来

363
00:10:14,159 --> 00:10:15,200
重新调整它

364
00:10:15,240 --> 00:10:15,559
对吧

365
00:10:15,559 --> 00:10:16,759
这个是直观上

366
00:10:16,759 --> 00:10:17,559
你可以这么做

367
00:10:21,000 --> 00:10:21,879
好问题12

368
00:10:21,879 --> 00:10:23,240
安可的回归是哪个

369
00:10:23,240 --> 00:10:23,960
安可的回归

370
00:10:23,960 --> 00:10:24,600
就是说

371
00:10:24,600 --> 00:10:26,159
对于这个毛框

372
00:10:26,159 --> 00:10:27,960
它和真实边界框

373
00:10:27,960 --> 00:10:29,080
那个偏移

374
00:10:30,000 --> 00:10:32,080
就是说我会把这个偏移

375
00:10:32,080 --> 00:10:33,159
我去预测偏移

376
00:10:33,159 --> 00:10:34,679
这个偏移是个回归问题

377
00:10:34,679 --> 00:10:36,080
是用的L1 loss

378
00:10:38,080 --> 00:10:39,960
NMS在GPU上

379
00:10:39,960 --> 00:10:41,639
怎么修改才能跑得快

380
00:10:41,960 --> 00:10:42,840
这是一个

381
00:10:43,399 --> 00:10:44,759
这个我们展开不了

382
00:10:44,759 --> 00:10:45,639
因为你可以去看一下

383
00:10:45,639 --> 00:10:46,840
Media它的实现

384
00:10:48,559 --> 00:10:49,039
举个例子

385
00:10:49,679 --> 00:10:50,759
我觉得三年前

386
00:10:51,080 --> 00:10:53,600
Media工程师帮我们改进

387
00:10:53,639 --> 00:10:55,000
就M3的改进

388
00:10:55,039 --> 00:10:56,679
Mask RCN的性能

389
00:10:57,120 --> 00:10:58,639
通过手动

390
00:10:58,879 --> 00:11:00,279
改operator

391
00:11:00,279 --> 00:11:01,439
基本上可以把整个

392
00:11:01,439 --> 00:11:02,679
Mask RCN的性能

393
00:11:02,679 --> 00:11:04,120
提升个三倍以上

394
00:11:04,720 --> 00:11:05,799
所以就是说

395
00:11:06,240 --> 00:11:07,480
NMS就是说

396
00:11:07,480 --> 00:11:08,679
现在在GPU上

397
00:11:08,679 --> 00:11:10,360
怎么样跑得快

398
00:11:10,360 --> 00:11:11,919
有很多代码

399
00:11:11,919 --> 00:11:12,919
大家可以去搜一下

400
00:11:12,919 --> 00:11:14,159
我们当然肯定就是

401
00:11:14,159 --> 00:11:15,600
这个课就不会给大家去讲到

402
00:11:15,600 --> 00:11:16,919
到底是怎么样实现的

403
00:11:16,960 --> 00:11:19,320
但是确实NMS和整个那一套

404
00:11:19,720 --> 00:11:21,560
怎么样把operator

405
00:11:21,560 --> 00:11:22,840
尽量搬到CUDA上

406
00:11:22,840 --> 00:11:23,919
当然极端情况下

407
00:11:23,919 --> 00:11:24,919
就是你把整个网络

408
00:11:24,919 --> 00:11:26,440
写成一个CUDA的一个

409
00:11:26,879 --> 00:11:27,320
颗脑

410
00:11:27,320 --> 00:11:28,639
那当然就更快了

411
00:11:28,639 --> 00:11:29,560
就python都不要碰

412
00:11:29,560 --> 00:11:30,000
对吧

413
00:11:30,039 --> 00:11:31,039
那是最快的

414
00:11:31,840 --> 00:11:32,720
但实际上来说

415
00:11:32,720 --> 00:11:33,360
你做不了

416
00:11:33,759 --> 00:11:36,279
但是确实在目标检测里面

417
00:11:36,279 --> 00:11:37,840
大家还是在这一块

418
00:11:37,840 --> 00:11:39,240
code特别是NMS

419
00:11:39,240 --> 00:11:40,399
code还是比较细的

420
00:11:40,399 --> 00:11:41,399
大家可以看一下

421
00:11:44,039 --> 00:11:45,480
整个流程的梯度

422
00:11:45,480 --> 00:11:46,279
怎么传的

423
00:11:46,279 --> 00:11:47,519
中间的流程的梯度

424
00:11:47,519 --> 00:11:49,080
保证梯度不会断掉的

425
00:11:49,560 --> 00:11:49,919
不会

426
00:11:50,159 --> 00:11:51,039
这个梯度不会断

427
00:11:51,360 --> 00:11:52,440
就梯度是说

428
00:11:52,560 --> 00:11:53,159
你可认为说

429
00:11:53,159 --> 00:11:54,639
我这个网络有5层

430
00:11:54,639 --> 00:11:55,799
有5个stage

431
00:11:55,799 --> 00:11:56,879
每个stage出来

432
00:11:56,879 --> 00:11:57,960
有两个predict

433
00:11:57,960 --> 00:11:59,440
有两个loss

434
00:12:00,200 --> 00:12:01,000
就是说

435
00:12:01,399 --> 00:12:03,560
我如果我给你大家画一下的话

436
00:12:03,560 --> 00:12:03,919
就是说

437
00:12:03,919 --> 00:12:05,919
我当然我今天不讲这个东西

438
00:12:06,440 --> 00:12:07,560
就是说如果画一下的话

439
00:12:07,560 --> 00:12:08,440
就是说

440
00:12:09,960 --> 00:12:10,560
有5块

441
00:12:10,840 --> 00:12:11,399
对吧

442
00:12:12,519 --> 00:12:13,720
我一共有5块

443
00:12:14,399 --> 00:12:16,159
每一块会出来

444
00:12:16,759 --> 00:12:17,480
一块对吧

445
00:12:17,480 --> 00:12:18,360
一个东西对吧

446
00:12:18,360 --> 00:12:19,639
一个类别

447
00:12:19,639 --> 00:12:22,240
一个是一个offset的预测

448
00:12:22,600 --> 00:12:23,600
然后最后的loss

449
00:12:23,600 --> 00:12:24,680
就把这一块东西

450
00:12:24,919 --> 00:12:26,799
全部放回来

451
00:12:26,840 --> 00:12:28,320
所以你回传的时候

452
00:12:28,320 --> 00:12:28,879
不会有问题

453
00:12:29,000 --> 00:12:29,600
回传的时候

454
00:12:29,600 --> 00:12:30,320
就是说

455
00:12:30,360 --> 00:12:31,879
你就是这么传回来

456
00:12:31,879 --> 00:12:32,120
对吧

457
00:12:32,120 --> 00:12:32,960
这个传回来

458
00:12:32,960 --> 00:12:33,639
这么传回来

459
00:12:33,639 --> 00:12:34,159
这么传回来

460
00:12:34,159 --> 00:12:34,639
这么传回来

461
00:12:34,639 --> 00:12:35,200
这么传回来

462
00:12:35,200 --> 00:12:35,680
对吧

463
00:12:36,000 --> 00:12:36,639
就不会断的

464
00:12:39,240 --> 00:12:45,879
问题15

465
00:12:45,879 --> 00:12:47,600
把深度学习解决问题的步骤

466
00:12:47,600 --> 00:12:48,200
总结一下

467
00:12:48,399 --> 00:12:49,519
结合目标

468
00:12:50,320 --> 00:12:50,919
结合目标

469
00:12:50,919 --> 00:12:51,639
检测抽象

470
00:12:51,680 --> 00:12:53,000
解决问题的步骤

471
00:12:55,399 --> 00:12:55,720
好

472
00:12:55,720 --> 00:12:56,720
你这个问题很长

473
00:12:56,840 --> 00:12:58,759
比如说我整理的

474
00:12:58,799 --> 00:13:00,159
需要解决的问题

475
00:13:00,159 --> 00:13:02,120
确定输出

476
00:13:02,120 --> 00:13:03,639
定义输出的计算表达

477
00:13:03,639 --> 00:13:05,000
添加需要的图

478
00:13:09,600 --> 00:13:13,720
我觉得你总结的挺好的

479
00:13:13,720 --> 00:13:17,440
我觉得我总结可能

480
00:13:21,200 --> 00:13:25,039
我觉得这个问题我就跳过了

481
00:13:25,240 --> 00:13:26,360
因为这个问题比较大

482
00:13:26,360 --> 00:13:27,399
我得想一想

483
00:13:27,399 --> 00:13:29,399
就是说总结这个事情

484
00:13:29,440 --> 00:13:30,360
通常是很难的

485
00:13:30,360 --> 00:13:32,720
你这个问题太general了

486
00:13:32,720 --> 00:13:34,080
我们就先跳过一下

487
00:13:34,440 --> 00:13:35,720
就是我回答的

488
00:13:35,759 --> 00:13:38,120
可能更细更具体一点

489
00:13:38,120 --> 00:13:39,320
我可能更好回答一点

490
00:13:40,560 --> 00:13:41,560
问题16

491
00:13:42,639 --> 00:13:44,879
class和bouncybox predict的卷集

492
00:13:44,879 --> 00:13:46,240
是固定的

493
00:13:46,240 --> 00:13:47,039
就是

494
00:13:52,879 --> 00:13:53,879
就是说你这个问题

495
00:13:54,080 --> 00:13:55,080
这个问题是问说

496
00:13:55,080 --> 00:13:56,320
你的predict

497
00:13:56,320 --> 00:13:57,279
总是用的kernel

498
00:13:57,279 --> 00:13:58,320
size等于3

499
00:13:58,360 --> 00:13:59,799
是一个固定的

500
00:13:59,840 --> 00:14:01,679
但是假设你的毛框

501
00:14:01,679 --> 00:14:03,480
确实是要看的比较大的时候

502
00:14:03,480 --> 00:14:04,039
会怎么样

503
00:14:04,039 --> 00:14:04,519
对吧

504
00:14:04,639 --> 00:14:05,399
所以

505
00:14:05,879 --> 00:14:06,560
所以这个东西

506
00:14:06,560 --> 00:14:08,160
我觉得你要去看的是说

507
00:14:08,160 --> 00:14:10,520
虽然我这个class的predict里面

508
00:14:10,520 --> 00:14:11,720
kernelsize等于3

509
00:14:11,720 --> 00:14:13,240
每次只能看三个像素

510
00:14:13,320 --> 00:14:14,440
3乘3的像素

511
00:14:14,480 --> 00:14:16,680
但是你下面的卷积层

512
00:14:16,840 --> 00:14:17,680
一层一层过来

513
00:14:17,680 --> 00:14:18,680
已经把你的视野

514
00:14:18,680 --> 00:14:20,000
已经放得很大了

515
00:14:20,960 --> 00:14:21,920
就已经就是说

516
00:14:21,920 --> 00:14:22,840
你看到33

517
00:14:22,840 --> 00:14:24,200
已经其实在原始图片中

518
00:14:24,200 --> 00:14:25,680
对应的已经是比较大的了

519
00:14:25,720 --> 00:14:28,560
所以是没有太多关系

520
00:14:28,600 --> 00:14:31,080
如果你说一个像素下不同毛框

521
00:14:31,120 --> 00:14:32,440
就是说3乘3

522
00:14:32,440 --> 00:14:34,600
就是说我给你看到的是3乘3

523
00:14:34,600 --> 00:14:36,240
就是说其实让你33也很大

524
00:14:36,320 --> 00:14:37,639
可以一直往回走

525
00:14:37,639 --> 00:14:38,639
是能看到整个

526
00:14:38,639 --> 00:14:40,879
甚至能看到整个图片的一大块

527
00:14:41,200 --> 00:14:42,120
都是可以的

528
00:14:42,799 --> 00:14:44,440
就是说到底你要

529
00:14:44,440 --> 00:14:45,519
如果你这个

530
00:14:45,519 --> 00:14:47,240
到底你的图片方的是别的

531
00:14:47,240 --> 00:14:48,720
就是说它真的是取决于

532
00:14:48,720 --> 00:14:50,440
你那个连接是怎么连的了

533
00:14:52,399 --> 00:14:54,080
就你反过去想说

534
00:14:54,080 --> 00:14:55,919
我做图片分类的时候

535
00:14:55,960 --> 00:14:56,840
我

536
00:14:57,240 --> 00:14:58,039
我一个图片

537
00:14:58,039 --> 00:14:59,080
如果在一个物体

538
00:14:59,080 --> 00:15:00,080
在一个边角出现

539
00:15:00,080 --> 00:15:02,000
或者一个物体长得很长很宽

540
00:15:02,000 --> 00:15:02,680
那也没关系

541
00:15:02,799 --> 00:15:03,279
对吧

542
00:15:03,519 --> 00:15:05,360
就是说它的权重会去

543
00:15:06,080 --> 00:15:07,399
去adapt这件事情

544
00:15:09,240 --> 00:15:10,200
问题17

545
00:15:10,519 --> 00:15:12,519
目标检测怎么样做fine tune

546
00:15:14,080 --> 00:15:15,279
对目标检测fine tune

547
00:15:15,279 --> 00:15:16,279
一般的是说

548
00:15:16,279 --> 00:15:18,759
把整个CN的东西

549
00:15:18,759 --> 00:15:20,800
是一般是做fine tune的

550
00:15:20,800 --> 00:15:22,639
首先当然也有不做fine tune的

551
00:15:22,639 --> 00:15:24,639
但是做的比较做的还是比较多

552
00:15:24,680 --> 00:15:26,639
就fine tune的时候是说

553
00:15:26,680 --> 00:15:28,639
你的那些class predict

554
00:15:28,639 --> 00:15:29,759
bounded box predict

555
00:15:29,759 --> 00:15:31,360
是从新训练的

556
00:15:31,399 --> 00:15:32,759
但是别的那些东西

557
00:15:32,840 --> 00:15:34,000
backbone的那些东西

558
00:15:34,120 --> 00:15:37,399
一般是pre-trained模型出来的

559
00:15:38,639 --> 00:15:39,200
对吧

560
00:15:40,320 --> 00:15:41,080
所以是说

561
00:15:41,080 --> 00:15:43,279
我觉得应该不是你后面那一个做法

562
00:15:43,279 --> 00:15:45,799
应该就是class predict

563
00:15:46,279 --> 00:15:48,120
就目标检测的fine tune

564
00:15:48,120 --> 00:15:48,600
ok

565
00:15:48,600 --> 00:15:49,600
我大概理解意思了

566
00:15:49,600 --> 00:15:50,639
目标检测fine tune

567
00:15:50,639 --> 00:15:52,240
一般是从图片分类的

568
00:15:52,240 --> 00:15:53,600
pre-trained模型拿过来

569
00:15:53,960 --> 00:15:54,720
做fine tune

570
00:15:55,360 --> 00:15:56,360
它不是

571
00:15:57,440 --> 00:15:59,559
一不怎么是说

572
00:15:59,559 --> 00:16:02,440
把一个之前一个目标检测模型

573
00:16:02,440 --> 00:16:02,759
拿过来

574
00:16:02,759 --> 00:16:03,919
之前拿过来也可以

575
00:16:04,080 --> 00:16:04,560
你就是说

576
00:16:04,560 --> 00:16:06,120
你把它拿过来

577
00:16:06,120 --> 00:16:07,480
你的class predict

578
00:16:07,480 --> 00:16:08,400
bounded box predict

579
00:16:08,400 --> 00:16:08,880
里面东西

580
00:16:08,880 --> 00:16:10,520
你要是对东西拎出来

581
00:16:10,520 --> 00:16:13,320
不要东西得拿掉

582
00:16:13,360 --> 00:16:15,280
就说你是可以的

583
00:16:15,320 --> 00:16:16,920
如果你是从图片分类过来的话

584
00:16:17,080 --> 00:16:18,200
整个那一块都是

585
00:16:18,200 --> 00:16:19,520
在predict那些东西

586
00:16:19,520 --> 00:16:21,120
都是要随机初始化的

587
00:16:24,520 --> 00:16:25,960
目标检测派透气能做

588
00:16:25,960 --> 00:16:26,400
派透气

589
00:16:26,400 --> 00:16:29,000
它里面其实是CIA加核派

590
00:16:29,000 --> 00:16:29,880
是CUDA写的

591
00:16:29,880 --> 00:16:31,280
所以是没关系的

592
00:16:31,640 --> 00:16:33,400
就大家会做大量的

593
00:16:33,679 --> 00:16:34,639
比如说你去看

594
00:16:34,679 --> 00:16:36,600
比如说detection2

595
00:16:36,720 --> 00:16:38,159
就是说派透气实现的

596
00:16:38,159 --> 00:16:39,319
就Facebook实现的

597
00:16:39,360 --> 00:16:40,600
里面有大量的

598
00:16:41,240 --> 00:16:42,319
自己手动定义的

599
00:16:42,319 --> 00:16:44,120
CIA的operator注册进去的

600
00:16:46,759 --> 00:16:47,919
问题19

601
00:16:48,159 --> 00:16:49,319
嵌入式设备上

602
00:16:49,319 --> 00:16:50,639
计算资源有限

603
00:16:50,959 --> 00:16:52,240
怎么样修改SSD的策略

604
00:16:52,240 --> 00:16:53,199
减少计算量

605
00:16:55,199 --> 00:16:56,039
就嵌入式的话

606
00:16:56,039 --> 00:16:57,919
我建议你去用ULO系列

607
00:16:58,039 --> 00:16:59,159
ULO系列更快一点

608
00:16:59,159 --> 00:17:01,159
ULO怎么样在

609
00:17:02,159 --> 00:17:04,079
Raspberry Pi上面跑ULO

610
00:17:04,079 --> 00:17:05,639
我觉得你可以搜出很多东西来

611
00:17:06,759 --> 00:17:07,599
你可以搜一搜

612
00:17:07,599 --> 00:17:09,799
怎么在Raspberry Pi上面跑ULO

613
00:17:09,799 --> 00:17:11,440
我记得反正在YouTube上

614
00:17:11,440 --> 00:17:13,000
至少是我看到过几个

615
00:17:13,000 --> 00:17:13,799
这样的视频

616
00:17:14,000 --> 00:17:15,000
跑起来也不慢

617
00:17:19,000 --> 00:17:22,159
尺寸的特征图是如何组合到一起的

618
00:17:22,159 --> 00:17:24,480
是原先通道数作为多尺寸围绕

619
00:17:24,519 --> 00:17:25,960
每个毛框作为一个通道

620
00:17:25,960 --> 00:17:27,759
替代原先的RGB通道

621
00:17:28,920 --> 00:17:29,879
我再理解一下

622
00:17:30,400 --> 00:17:34,880
尺寸的特征图是如何组织到一起的

623
00:17:34,880 --> 00:17:36,640
是原来的通道数的维度

624
00:17:36,640 --> 00:17:38,200
作为多通道的维度吗

625
00:17:38,640 --> 00:17:40,640
我其实不是很理解这个问题

626
00:17:41,560 --> 00:17:44,080
我们说的尺度是说你在一个

627
00:17:45,480 --> 00:17:46,520
你在一个网络上面

628
00:17:47,360 --> 00:17:50,920
一个图片进来的话

629
00:17:51,080 --> 00:17:52,680
然后在每一个层

630
00:17:52,680 --> 00:17:55,880
你的每一个层的输出的大小会不一样

631
00:17:55,880 --> 00:17:56,960
所以他看到的

632
00:17:56,960 --> 00:18:01,480
他去看到的尺度的是不一样的

633
00:18:01,480 --> 00:18:02,799
就是分辨率是不一样的

634
00:18:02,799 --> 00:18:04,120
你越到上面分辨率

635
00:18:04,120 --> 00:18:05,640
就空间分辨率就越低

636
00:18:05,640 --> 00:18:06,840
但是他更高伪

637
00:18:06,840 --> 00:18:07,279
对吧

638
00:18:07,279 --> 00:18:08,720
越下面看到的是说

639
00:18:08,720 --> 00:18:10,720
你的空间分辨率就是越高

640
00:18:10,720 --> 00:18:11,880
然后你看到是局部

641
00:18:11,880 --> 00:18:14,360
所以这就是我们在不同的stage

642
00:18:14,360 --> 00:18:17,799
都去做目标的检测

643
00:18:17,799 --> 00:18:19,480
这个就是多尺度的意思

644
00:18:22,759 --> 00:18:24,240
目标检测的话

645
00:18:24,240 --> 00:18:26,400
调用比较好的包

646
00:18:27,960 --> 00:18:28,559
有

647
00:18:28,559 --> 00:18:30,600
比如说detection2

648
00:18:30,600 --> 00:18:32,000
就是Facebook出的

649
00:18:32,000 --> 00:18:35,279
和我觉得国内是商汤做了MM

650
00:18:35,279 --> 00:18:37,240
应该也不是商汤做

651
00:18:37,240 --> 00:18:44,440
是港中文MMlab做的MMdetection

652
00:18:44,440 --> 00:18:46,920
就这两个是比较用的比较多的包

653
00:18:46,920 --> 00:18:49,039
古龙CV当然也是有做detection

654
00:18:49,039 --> 00:18:50,680
AutoGroon也可以做detection

655
00:18:50,680 --> 00:18:52,600
你可以去用一下

656
00:18:52,600 --> 00:18:54,680
如果detection2很难用

657
00:18:55,519 --> 00:18:58,840
我是不会用

658
00:18:58,840 --> 00:19:01,200
就是说你得对他那一套

659
00:19:01,200 --> 00:19:03,240
就是他主要是现在的RCN系列

660
00:19:03,240 --> 00:19:05,400
就是说他主要是你是

661
00:19:05,400 --> 00:19:06,799
对小白用户不友好

662
00:19:06,799 --> 00:19:08,000
我觉得detection2

663
00:19:08,000 --> 00:19:09,519
就是说里面参数太多了

664
00:19:09,519 --> 00:19:10,880
但是对于research来讲

665
00:19:10,880 --> 00:19:11,360
research

666
00:19:11,360 --> 00:19:12,640
你对这一块比较好用的话

667
00:19:12,640 --> 00:19:14,400
是用的比较多的detection2

668
00:19:14,400 --> 00:19:15,600
MMdetection

669
00:19:15,600 --> 00:19:17,160
我其实没怎么太用过

670
00:19:17,160 --> 00:19:19,400
但就是说你去看一看搜一搜

671
00:19:19,400 --> 00:19:21,680
就是说还是有detection里面

672
00:19:21,680 --> 00:19:23,039
有很成熟的库了

673
00:19:23,079 --> 00:19:24,720
古龙CV当然也是有detection

674
00:19:24,720 --> 00:19:26,240
AutoGroon也可以做detection

675
00:19:26,240 --> 00:19:27,639
就是说大家都可以

676
00:19:27,639 --> 00:19:29,399
就是说你可以就具体喜欢用谁

677
00:19:29,399 --> 00:19:30,960
还是你可能自己去

678
00:19:30,960 --> 00:19:34,039
看看自己喜欢谁

679
00:19:35,920 --> 00:19:36,720
除了速度上

680
00:19:36,720 --> 00:19:39,119
SSD和RCN系列在精度上的差异

681
00:19:39,119 --> 00:19:41,119
除了Trix和算法主流

682
00:19:43,599 --> 00:19:44,319
就是说

683
00:19:45,440 --> 00:19:48,000
SSD和RCN和EULA

684
00:19:48,000 --> 00:19:49,879
这就SSD和EULA比较像

685
00:19:49,879 --> 00:19:51,839
但是SSD和RCN系列

686
00:19:51,839 --> 00:19:53,679
它在算法上是不一样的

687
00:19:53,679 --> 00:19:55,039
一个是一个two stage的

688
00:19:55,039 --> 00:19:55,919
一个比较

689
00:19:57,000 --> 00:19:59,799
操作一个是一个single stage的

690
00:19:59,799 --> 00:20:01,839
就是说你还有anchor free的

691
00:20:01,839 --> 00:20:02,679
就是不用anchor的

692
00:20:02,679 --> 00:20:03,359
就是另外一个系列

693
00:20:03,359 --> 00:20:04,959
比如centenet一块系列

694
00:20:04,959 --> 00:20:06,759
因为它的模型上是不一样的

695
00:20:06,759 --> 00:20:08,279
你可认为是说

696
00:20:09,279 --> 00:20:10,319
卷积成绩网络

697
00:20:10,319 --> 00:20:10,959
就我们知道

698
00:20:10,959 --> 00:20:11,679
比如VGG

699
00:20:11,679 --> 00:20:12,359
REST

700
00:20:12,359 --> 00:20:13,919
Inceptron

701
00:20:13,919 --> 00:20:15,439
他们就是长得挺不一样的

702
00:20:15,439 --> 00:20:15,879
对吧

703
00:20:15,879 --> 00:20:17,079
所以你可认为他们之间

704
00:20:17,079 --> 00:20:19,319
还是有架构上是有差异的

705
00:20:19,439 --> 00:20:19,799
但是

706
00:20:19,799 --> 00:20:24,480
他们说你最后的精度看到差异

707
00:20:25,159 --> 00:20:27,519
不仅仅代表是架构上差异

708
00:20:27,519 --> 00:20:29,159
有可能是用的Trix上的差异

709
00:20:29,159 --> 00:20:30,279
就当你所有的人

710
00:20:30,279 --> 00:20:32,039
用上所有的Trix的时候

711
00:20:32,399 --> 00:20:33,079
就说

712
00:20:33,759 --> 00:20:35,240
因为你最后的取

713
00:20:35,240 --> 00:20:37,200
最后你说你数据增强怎么样

714
00:20:37,200 --> 00:20:39,399
就是我们竞赛有看到

715
00:20:39,559 --> 00:20:40,319
竞赛里

716
00:20:40,319 --> 00:20:42,000
就是用一个特定的模型

717
00:20:42,399 --> 00:20:43,559
很有可能是

718
00:20:44,079 --> 00:20:45,319
大家同样一个模型

719
00:20:45,319 --> 00:20:46,240
你的精度比我高

720
00:20:46,240 --> 00:20:46,559
为什么

721
00:20:46,559 --> 00:20:48,839
是因为你的可能data accommodation做更好

722
00:20:49,039 --> 00:20:51,000
你的training用的更好

723
00:20:51,000 --> 00:20:52,639
或者你什么东西用的更好

724
00:20:52,639 --> 00:20:53,559
所以是说

725
00:20:53,559 --> 00:20:55,119
你最后看到的精度

726
00:20:55,119 --> 00:20:57,599
是一个整体的系统的精度

727
00:20:57,759 --> 00:20:59,039
数据怎么处理的

728
00:20:59,079 --> 00:21:01,199
模型训练

729
00:21:01,199 --> 00:21:04,959
以及各种很多这样的细节组合

730
00:21:05,000 --> 00:21:07,559
所以你看到的精度是整个的系统

731
00:21:07,639 --> 00:21:08,799
所以你很难比较

732
00:21:08,879 --> 00:21:11,679
现在的现在大家都做的比较烦

733
00:21:11,679 --> 00:21:12,480
就是说

734
00:21:12,480 --> 00:21:14,000
就里面东西都挺多的

735
00:21:14,000 --> 00:21:14,599
就是说

736
00:21:14,599 --> 00:21:15,879
你很难去

737
00:21:16,400 --> 00:21:17,840
真的很公平的

738
00:21:17,840 --> 00:21:19,440
比较两个

739
00:21:20,160 --> 00:21:21,640
很不一样的架构

740
00:21:21,640 --> 00:21:23,120
他到底谁好谁坏

741
00:21:23,600 --> 00:21:25,840
这真的就是你看你真的实现了

742
00:21:25,840 --> 00:21:26,800
除非有人说

743
00:21:26,800 --> 00:21:27,320
OK

744
00:21:27,320 --> 00:21:28,960
我把所有的细节都实现好了

745
00:21:28,960 --> 00:21:30,000
你就是把两个模型

746
00:21:30,000 --> 00:21:31,680
只换两个模型看精度

747
00:21:31,880 --> 00:21:34,240
但你可能有些trick对这个模型有用

748
00:21:34,240 --> 00:21:37,240
有些trick对另外一个模型有用

749
00:21:37,240 --> 00:21:38,280
所以就很难说

750
00:21:38,280 --> 00:21:40,360
所以最后的最后就变成了一个

751
00:21:40,360 --> 00:21:41,240
信念的问题

752
00:21:41,240 --> 00:21:42,920
就是说你最后喜欢谁

753
00:21:43,680 --> 00:21:44,640
你喜欢这种

754
00:21:44,640 --> 00:21:45,320
你是喜欢Euro

755
00:21:45,480 --> 00:21:46,960
你喜欢FastCN

756
00:21:46,960 --> 00:21:47,559
比如说

757
00:21:47,839 --> 00:21:49,159
SSD我们就不算他

758
00:21:49,159 --> 00:21:50,399
因为他最近

759
00:21:50,599 --> 00:21:53,960
因为他没有太多人在上面做维护

760
00:21:54,000 --> 00:21:55,359
就是说没有一直在改进

761
00:21:55,359 --> 00:21:57,439
所以最后是说你喜欢谁

762
00:21:57,480 --> 00:21:58,200
然后

763
00:21:58,439 --> 00:21:59,079
是喜欢

764
00:21:59,079 --> 00:22:00,919
特别是说你喜欢哪一个包

765
00:22:01,319 --> 00:22:02,960
看看包里面的实现

766
00:22:03,799 --> 00:22:06,279
就是说最后是卖的是信念

767
00:22:08,439 --> 00:22:09,399
我模型

768
00:22:09,399 --> 00:22:10,079
问题23

769
00:22:10,079 --> 00:22:11,119
我的模型保存

770
00:22:11,119 --> 00:22:12,000
再重新加载之后

771
00:22:12,000 --> 00:22:12,839
accuracy不一样

772
00:22:12,839 --> 00:22:14,679
这情况是排头的原因吗

773
00:22:16,319 --> 00:22:17,599
有可能是拍

774
00:22:22,960 --> 00:22:24,399
模型精度不一样

775
00:22:24,599 --> 00:22:25,399
这个东西就很

776
00:22:25,399 --> 00:22:27,159
这个东西是一个很经典的问题

777
00:22:27,319 --> 00:22:28,480
就debug怎么debug

778
00:22:28,599 --> 00:22:29,319
就是说

779
00:22:30,159 --> 00:22:31,000
首先你去看一下

780
00:22:31,000 --> 00:22:32,039
你每次加载之后

781
00:22:32,039 --> 00:22:33,319
你的预测同样一个值

782
00:22:33,319 --> 00:22:34,799
预测值有没有变化

783
00:22:34,799 --> 00:22:35,759
如果有变化的话

784
00:22:35,759 --> 00:22:37,679
那里面有随机的种子在里面

785
00:22:37,720 --> 00:22:38,839
所以有随机性的话

786
00:22:38,839 --> 00:22:40,079
当然是会不一样了

787
00:22:40,799 --> 00:22:43,559
第二个是说你在训练的时候

788
00:22:43,559 --> 00:22:44,759
你的精度就是这样

789
00:22:45,279 --> 00:22:47,240
还是说取决于你说

790
00:22:47,240 --> 00:22:48,200
你比如说

791
00:22:48,200 --> 00:22:50,519
我是不是开了evaluation mode

792
00:22:50,640 --> 00:22:52,000
开了evaluation mode的话

793
00:22:52,119 --> 00:22:53,400
就是说你的job part

794
00:22:53,400 --> 00:22:54,279
什么东西都不一样

795
00:22:54,279 --> 00:22:54,559
对吧

796
00:22:54,559 --> 00:22:55,680
我们之前有讲过

797
00:22:55,960 --> 00:22:57,160
所以你要去仔细看

798
00:22:57,160 --> 00:22:57,799
是说

799
00:22:57,879 --> 00:23:00,079
真的是说在训练的时候

800
00:23:00,079 --> 00:23:01,319
你做evaluation accuracy

801
00:23:01,319 --> 00:23:03,200
可能跟你的load再进去

802
00:23:03,200 --> 00:23:04,359
可能会有不一样

803
00:23:04,400 --> 00:23:06,519
因为你的load再进去的时候

804
00:23:06,519 --> 00:23:07,119
训练的时候

805
00:23:07,119 --> 00:23:09,319
他可能是用的evaluation

806
00:23:09,480 --> 00:23:10,599
模式都不一样

807
00:23:10,599 --> 00:23:11,559
这是有可能的

808
00:23:11,559 --> 00:23:12,559
另外是说

809
00:23:12,559 --> 00:23:14,480
你在不同的硬件上

810
00:23:14,480 --> 00:23:15,120
可能不一样的

811
00:23:15,120 --> 00:23:15,839
GPU和CPU

812
00:23:15,839 --> 00:23:16,759
精度当然会不一样

813
00:23:18,120 --> 00:23:18,680
这个东西

814
00:23:18,680 --> 00:23:20,240
你得一个一个去debug

815
00:23:21,440 --> 00:23:23,240
我们怎么还有那么多问题

816
00:23:24,000 --> 00:23:24,200
好

817
00:23:24,200 --> 00:23:25,000
我们再快一点

818
00:23:26,360 --> 00:23:27,599
每个像素做某框

819
00:23:27,599 --> 00:23:29,319
这里像素是feature map上像素

820
00:23:29,519 --> 00:23:29,960
是的

821
00:23:30,240 --> 00:23:31,559
每个像素是feature map上

822
00:23:31,559 --> 00:23:32,720
像素做的是某框

823
00:23:32,720 --> 00:23:34,839
不是输入图变成像素

824
00:23:35,680 --> 00:23:37,039
25

825
00:23:37,079 --> 00:23:38,160
类别的损失变数

826
00:23:38,160 --> 00:23:38,920
比较好理解

827
00:23:38,920 --> 00:23:41,160
偏移在梯度优化中的是什么

828
00:23:42,240 --> 00:23:43,400
偏移就是说

829
00:23:43,800 --> 00:23:45,640
对我的模框到真实

830
00:23:45,640 --> 00:23:46,840
Bounty Box偏移

831
00:23:47,280 --> 00:23:49,880
我要去预测是一个时数

832
00:23:49,920 --> 00:23:51,160
我要去预测那个值

833
00:23:51,759 --> 00:23:53,600
就是说它就是一个回归问题

834
00:23:55,519 --> 00:23:57,640
在最算分类制性度的时候

835
00:23:57,640 --> 00:24:00,960
不再不用在模框经过Bounty Box

836
00:24:03,960 --> 00:24:06,480
就做分类的精度的时候

837
00:24:06,480 --> 00:24:08,360
会用到NMS

838
00:24:08,360 --> 00:24:09,480
NMS我们回忆一下

839
00:24:09,480 --> 00:24:10,240
上期有讲到

840
00:24:10,240 --> 00:24:12,600
它会用到你的制性度

841
00:24:14,400 --> 00:24:16,560
对于每个模框

842
00:24:16,560 --> 00:24:17,240
Fall的时候

843
00:24:17,240 --> 00:24:18,360
其实没用到模框信息

844
00:24:18,360 --> 00:24:19,480
还是看整个feature map

845
00:24:19,480 --> 00:24:20,480
那当然会

846
00:24:21,720 --> 00:24:24,200
当然不会会导致所有的

847
00:24:24,200 --> 00:24:24,840
模框预测值

848
00:24:24,840 --> 00:24:26,680
是因为你为什么不一样

849
00:24:26,680 --> 00:24:27,680
是因为你的

850
00:24:28,320 --> 00:24:29,320
你的loss不一样

851
00:24:29,440 --> 00:24:31,560
首先你的标号不一样

852
00:24:31,800 --> 00:24:32,759
所以标号不一样

853
00:24:32,759 --> 00:24:35,360
导致说你回传的时候会不一样

854
00:24:35,360 --> 00:24:36,920
另外一个是说你是随机优化的

855
00:24:37,040 --> 00:24:38,000
随机值一开始

856
00:24:38,000 --> 00:24:39,160
你就每个人是不一样的

857
00:24:39,160 --> 00:24:41,280
所以不会让每个模框的预测值

858
00:24:41,280 --> 00:24:42,280
都一样

859
00:24:42,839 --> 00:24:43,759
是不会的

860
00:24:43,920 --> 00:24:45,399
因为你的标号不一样

861
00:24:45,399 --> 00:24:47,200
你的初始weight也不一样

862
00:24:49,359 --> 00:24:51,079
能不能把损失函数拆开

863
00:24:51,079 --> 00:24:52,480
单独对计算

864
00:24:52,480 --> 00:24:53,000
可以的

865
00:24:53,000 --> 00:24:53,639
这个没关系

866
00:24:53,639 --> 00:24:54,599
最后的最后

867
00:24:54,599 --> 00:24:56,240
你是要把它合在一起

868
00:24:56,399 --> 00:24:59,359
最后算backup

869
00:24:59,480 --> 00:25:01,079
算回传的时候

870
00:25:01,079 --> 00:25:02,519
你只能有一个值回传

871
00:25:02,639 --> 00:25:04,480
所以你需要把它重新弄在一起

872
00:25:04,480 --> 00:25:08,839
SS

873
00:25:10,960 --> 00:25:12,759
SSD的缺点是

874
00:25:12,880 --> 00:25:14,160
我觉得不能算缺点

875
00:25:14,279 --> 00:25:16,640
因为你可以在下面的时候

876
00:25:16,640 --> 00:25:19,200
通过好的安科尔和比较大的

877
00:25:19,200 --> 00:25:20,079
那些feature map上

878
00:25:20,079 --> 00:25:22,039
对小物体做检测是没关系的

879
00:25:22,240 --> 00:25:23,400
FPM

880
00:25:23,400 --> 00:25:26,039
我觉得FPM你不能说是说对SSD的

881
00:25:26,039 --> 00:25:26,480
改进

882
00:25:26,599 --> 00:25:28,480
我觉得这是两个不一样的工作

883
00:25:28,480 --> 00:25:30,039
我觉得挺难说改进的

884
00:25:31,000 --> 00:25:33,960
多标签识别的准确率怎么评估

885
00:25:34,079 --> 00:25:35,519
还有严重的数据不平衡

886
00:25:35,519 --> 00:25:36,640
有什么处理办法

887
00:25:36,880 --> 00:25:38,360
多标签

888
00:25:39,440 --> 00:25:40,039
多标签

889
00:25:40,039 --> 00:25:41,360
你是说我一个bounty box

890
00:25:41,360 --> 00:25:42,840
有多个标签吗

891
00:25:43,759 --> 00:25:44,600
就我就不是

892
00:25:44,600 --> 00:25:46,799
因为多标签就multi label

893
00:25:47,000 --> 00:25:48,000
multi label

894
00:25:48,600 --> 00:25:49,680
我们就是说

895
00:25:50,519 --> 00:25:52,279
multi label这个东西

896
00:25:52,519 --> 00:25:53,440
最简单你怎么做

897
00:25:53,840 --> 00:25:55,039
就是你的softmax

898
00:25:55,039 --> 00:25:57,440
不是本来是一个变成00001

899
00:25:57,440 --> 00:25:57,880
0000

900
00:25:58,240 --> 00:25:58,720
对吧

901
00:25:58,720 --> 00:26:00,079
然后你当然可以说

902
00:26:00,120 --> 00:26:00,840
多标签的话

903
00:26:00,840 --> 00:26:01,880
你就多放几个1进去

904
00:26:01,880 --> 00:26:02,200
对吧

905
00:26:02,200 --> 00:26:04,720
然后可能你再储一个标签数

906
00:26:04,720 --> 00:26:05,759
就变成一个这样子

907
00:26:05,759 --> 00:26:06,200
对吧

908
00:26:06,440 --> 00:26:07,799
严重的数据不平衡

909
00:26:07,799 --> 00:26:08,880
有什么好处理办法吗

910
00:26:08,880 --> 00:26:10,120
数据不平衡是

911
00:26:11,400 --> 00:26:12,200
经典问题

912
00:26:12,200 --> 00:26:13,000
就是说你可以通过

913
00:26:13,039 --> 00:26:15,039
负类叫小类物体采样

914
00:26:15,039 --> 00:26:15,640
就是说

915
00:26:17,360 --> 00:26:19,039
如果就是说

916
00:26:19,799 --> 00:26:21,000
你可以说

917
00:26:21,000 --> 00:26:23,160
我们把数据特别多的类

918
00:26:23,519 --> 00:26:24,720
权重价低一点

919
00:26:24,720 --> 00:26:25,519
这个可以这么做

920
00:26:26,080 --> 00:26:26,799
另外一个

921
00:26:26,840 --> 00:26:27,759
另外一块就是说

922
00:26:27,759 --> 00:26:28,799
你多标一点数据

923
00:26:28,799 --> 00:26:30,000
就是类别不行的事情

924
00:26:30,000 --> 00:26:30,680
你多标一点

925
00:26:30,680 --> 00:26:32,840
就是说在实际的工业应用中

926
00:26:32,840 --> 00:26:33,920
你总是可以标数据的

927
00:26:35,680 --> 00:26:37,560
图片中有没有固定形状的物体

928
00:26:37,560 --> 00:26:39,120
比如说土壤怎么检测

929
00:26:39,320 --> 00:26:40,120
土壤怎么检测

930
00:26:40,120 --> 00:26:40,400
OK

931
00:26:40,400 --> 00:26:42,039
我们下下一周会讲

932
00:26:42,080 --> 00:26:42,400
OK

933
00:26:42,400 --> 00:26:43,640
我们做语义分割

934
00:26:43,640 --> 00:26:45,720
会讲怎么做土壤

935
00:26:45,759 --> 00:26:46,160
OK

936
00:26:46,160 --> 00:26:47,360
今天我们的

937
00:26:47,640 --> 00:26:50,120
我们也差不多没拖没拖长

938
00:26:50,320 --> 00:26:51,640
我们拖了两分钟

939
00:26:51,640 --> 00:26:53,560
我们面试问题

940
00:26:53,560 --> 00:26:54,560
我们今天比较多

941
00:26:55,080 --> 00:26:55,680
挺好的

942
00:26:55,680 --> 00:26:58,200
问题多也是表示大家听进去了

943
00:26:58,200 --> 00:26:58,680
对吧

944
00:26:59,120 --> 00:27:00,640
也是对我工作的肯定

