1
00:00:00,000 --> 00:00:04,919
我们今天的第一个就是讲一下

2
00:00:04,960 --> 00:00:08,800
如何去做一个SIFA10的

3
00:00:08,800 --> 00:00:10,359
一个比还不错的分数

4
00:00:10,880 --> 00:00:12,560
就我们之前的在Fashion

5
00:00:12,560 --> 00:00:14,519
Administrator都是比较简单数据集

6
00:00:14,560 --> 00:00:16,280
SIFA10是一个

7
00:00:16,399 --> 00:00:18,640
稍微复杂一点点的数据集

8
00:00:18,640 --> 00:00:20,280
而且是在学术界

9
00:00:20,320 --> 00:00:22,800
大量的被大家使用

10
00:00:23,240 --> 00:00:24,280
在现实生活中

11
00:00:24,280 --> 00:00:25,879
SIFA10相对来说比较简单

12
00:00:25,920 --> 00:00:26,960
但是我觉得SIFA10

13
00:00:26,960 --> 00:00:28,719
确实是一个很经典的数据集

14
00:00:28,719 --> 00:00:31,119
大家经常用它来跑一点数据

15
00:00:31,160 --> 00:00:32,399
而且我们这个计时本

16
00:00:32,399 --> 00:00:34,359
其实是能达到SIFA10的

17
00:00:34,359 --> 00:00:35,200
最好的效果

18
00:00:35,200 --> 00:00:37,320
大概是98%还是99%的样子

19
00:00:38,679 --> 00:00:39,280
OK

20
00:00:40,280 --> 00:00:40,799
另外一个

21
00:00:40,799 --> 00:00:42,239
当然这个数据集是

22
00:00:42,960 --> 00:00:44,159
它应该是在

23
00:00:45,399 --> 00:00:47,120
它可能是在另外一个

24
00:00:47,120 --> 00:00:48,200
Tiny image

25
00:00:48,200 --> 00:00:50,079
一个更大的数据集里面

26
00:00:50,079 --> 00:00:50,879
给C出来的

27
00:00:51,560 --> 00:00:55,000
给了里面的10个类别

28
00:00:55,399 --> 00:00:56,759
这里面有个Kaggle的link

29
00:00:56,759 --> 00:00:57,719
当然Kaggle上面

30
00:00:57,719 --> 00:00:58,960
它其实是一个

31
00:00:59,560 --> 00:01:01,039
比较奇怪的数据集

32
00:01:01,079 --> 00:01:02,079
Kaggle上面这一个

33
00:01:02,200 --> 00:01:04,480
就是你的测试就特别大

34
00:01:05,039 --> 00:01:06,239
所以你下下来的时候

35
00:01:06,239 --> 00:01:07,879
它有将近700兆

36
00:01:08,400 --> 00:01:09,400
你下下来的时候

37
00:01:09,400 --> 00:01:11,599
测试级的解压特别难解

38
00:01:11,640 --> 00:01:13,759
它是一个TZ的压的比较狠

39
00:01:13,799 --> 00:01:15,000
你看它测试级

40
00:01:15,000 --> 00:01:17,480
它是其实是应该是在测试级里面

41
00:01:17,480 --> 00:01:18,920
放了很多

42
00:01:19,200 --> 00:01:21,200
没有那么无关的图片

43
00:01:21,239 --> 00:01:23,079
然后使得它做特别大

44
00:01:23,079 --> 00:01:25,680
这样是给大家反作弊用的

45
00:01:26,360 --> 00:01:28,160
然后它训练机相对来说

46
00:01:28,160 --> 00:01:28,720
比较小一点

47
00:01:28,720 --> 00:01:29,880
就100兆的样子

48
00:01:29,920 --> 00:01:31,920
所以大家说我们今天给的

49
00:01:31,920 --> 00:01:35,840
不会在整个完整的C发尺上去做训练

50
00:01:35,840 --> 00:01:36,960
因为这个东西

51
00:01:37,000 --> 00:01:39,680
确认次大概要十来分钟的样子

52
00:01:39,720 --> 00:01:41,520
我们用的它一个子集

53
00:01:41,760 --> 00:01:43,960
但是我建议大家可以去试一试

54
00:01:44,480 --> 00:01:45,360
特别是下

55
00:01:45,400 --> 00:01:46,920
那个东西可能最难的是

56
00:01:46,920 --> 00:01:48,160
你把它下下来解压

57
00:01:48,320 --> 00:01:51,080
解压要解个一个小时

58
00:01:52,160 --> 00:01:54,480
就是它用的比较狠的TZ的压缩

59
00:01:54,719 --> 00:01:55,200
OK

60
00:01:56,120 --> 00:01:56,960
我们

61
00:01:59,360 --> 00:01:59,720
好

62
00:01:59,720 --> 00:02:01,680
我们就是说

63
00:02:02,840 --> 00:02:05,160
我们先回去我们的机制版

64
00:02:06,400 --> 00:02:09,000
首先我们会导入一下我们要的东西

65
00:02:09,760 --> 00:02:13,280
我们这个东西是share的ut

66
00:02:13,280 --> 00:02:14,640
就是Python的一个

67
00:02:14,640 --> 00:02:16,879
让它导弹文件挺方便的东西

68
00:02:17,040 --> 00:02:19,000
我们pandas也会进来

69
00:02:19,200 --> 00:02:20,879
然后我们用的还是touch

70
00:02:20,879 --> 00:02:21,400
vision

71
00:02:21,600 --> 00:02:23,159
这次比赛大家用的

72
00:02:23,719 --> 00:02:25,879
其实不是那么的

73
00:02:25,879 --> 00:02:28,319
我去看大家写的

74
00:02:28,319 --> 00:02:29,240
其实没有怎么样

75
00:02:29,240 --> 00:02:29,800
touch vision

76
00:02:29,800 --> 00:02:30,719
touch vision相对来说

77
00:02:30,719 --> 00:02:34,000
我觉得是一个经典的

78
00:02:34,039 --> 00:02:36,000
但是更新不那么频繁的一个库

79
00:02:36,199 --> 00:02:38,479
现在外面其实有很多更好的

80
00:02:38,680 --> 00:02:40,079
更新个人维护的

81
00:02:40,079 --> 00:02:41,680
而且更新更快的一些库

82
00:02:41,719 --> 00:02:42,680
大家可以使用

83
00:02:42,719 --> 00:02:45,479
但是可能在对于很大部分程度来讲

84
00:02:45,599 --> 00:02:46,680
touch vision也不错

85
00:02:47,800 --> 00:02:48,280
OK

86
00:02:49,960 --> 00:02:50,199
好

87
00:02:50,199 --> 00:02:52,120
首先我们有提到

88
00:02:52,120 --> 00:02:53,240
我们没有

89
00:02:53,240 --> 00:02:54,200
这个是一个demo

90
00:02:54,439 --> 00:02:57,400
就是没有用真正的CIFS

91
00:02:57,400 --> 00:02:58,719
因为它下下来解压

92
00:02:58,719 --> 00:02:59,840
确实挺慢的

93
00:03:00,159 --> 00:03:02,920
所以我们是做了一个很小的一个数据集

94
00:03:02,960 --> 00:03:04,480
它其实就是说

95
00:03:05,240 --> 00:03:09,640
每一个类把前面1000个虚拟图片拿过来

96
00:03:09,680 --> 00:03:10,960
然后测试的话

97
00:03:10,960 --> 00:03:13,159
就是每一类里面挑了5个

98
00:03:13,200 --> 00:03:14,360
然后做了一个很小的数据集

99
00:03:14,360 --> 00:03:15,920
我们上传在一个地方

100
00:03:16,599 --> 00:03:18,439
这样子我们就直接notebook下载

101
00:03:18,439 --> 00:03:18,960
也很快

102
00:03:18,960 --> 00:03:20,480
大概就是几分钟的样子

103
00:03:20,920 --> 00:03:24,440
如果大家是下真的数据的话

104
00:03:24,440 --> 00:03:24,920
你怎么办

105
00:03:25,320 --> 00:03:27,760
你就是把demo这里改成一个force

106
00:03:27,760 --> 00:03:28,800
就是改成一个

107
00:03:29,040 --> 00:03:30,360
不要跑demo模式

108
00:03:30,400 --> 00:03:32,800
然后把你的数据放在

109
00:03:32,920 --> 00:03:36,640
你的上一层的data文件夹下

110
00:03:36,920 --> 00:03:38,840
放在cifa-10的下面

111
00:03:38,840 --> 00:03:42,320
就是把你的开国数据下下来

112
00:03:42,320 --> 00:03:44,640
然后解压到这个文件集里就行了

113
00:03:44,840 --> 00:03:45,960
就是说这个是

114
00:03:45,960 --> 00:03:47,320
如果你要跑完整数据集

115
00:03:47,320 --> 00:03:48,120
可以这么用

116
00:03:48,280 --> 00:03:50,520
好

117
00:03:50,520 --> 00:03:51,520
我们先来看一下

118
00:03:52,520 --> 00:03:53,280
我们这个里面

119
00:03:54,280 --> 00:03:55,719
它的label

120
00:03:55,879 --> 00:03:57,560
它label放在一个

121
00:03:57,599 --> 00:03:58,280
放在这个地方

122
00:03:58,400 --> 00:04:01,960
放在一个trendlabels.csv里面

123
00:04:03,080 --> 00:04:04,520
然后你把它

124
00:04:04,560 --> 00:04:05,800
就这个函数据很简单

125
00:04:05,800 --> 00:04:07,000
就是把它读进来

126
00:04:07,439 --> 00:04:09,400
我们就直接把它

127
00:04:09,560 --> 00:04:10,879
最简单readlines

128
00:04:11,000 --> 00:04:12,159
就都没有panas

129
00:04:12,159 --> 00:04:13,400
我们直接一行一行读进来

130
00:04:13,400 --> 00:04:14,560
叫split就行了

131
00:04:14,920 --> 00:04:16,040
就基本上你可以看到说

132
00:04:16,040 --> 00:04:17,439
label它就是一些

133
00:04:17,719 --> 00:04:19,040
你这个图片

134
00:04:19,040 --> 00:04:21,040
它就是一点png

135
00:04:21,040 --> 00:04:22,839
二点png这么下去了

136
00:04:22,879 --> 00:04:24,480
然后它就是一个

137
00:04:25,000 --> 00:04:26,040
你那个文件名

138
00:04:26,040 --> 00:04:27,719
它没有带下的嘴

139
00:04:27,920 --> 00:04:30,279
然后就是说你这个类是什么样子

140
00:04:30,319 --> 00:04:31,839
基本看到什么

141
00:04:31,879 --> 00:04:33,120
青蛙卡车

142
00:04:33,240 --> 00:04:33,519
鹿

143
00:04:35,040 --> 00:04:36,040
什么船

144
00:04:36,839 --> 00:04:40,560
就是人造物体和猫猫狗狗放在一起

145
00:04:40,560 --> 00:04:42,159
也是一个挺奇怪的数据集

146
00:04:45,439 --> 00:04:45,720
好

147
00:04:45,720 --> 00:04:47,560
接下来就是说干嘛

148
00:04:48,200 --> 00:04:49,480
就接下来就是说

149
00:04:49,760 --> 00:04:52,520
因为我们这里是用的一个

150
00:04:53,720 --> 00:04:56,120
PyTorch一个比较简单的

151
00:04:56,160 --> 00:04:57,240
读文件的方法

152
00:04:57,240 --> 00:04:58,000
就是说

153
00:04:58,360 --> 00:04:59,200
在PyTorch里面

154
00:04:59,200 --> 00:05:01,240
有一个非常简单的loader

155
00:05:01,240 --> 00:05:01,800
就是说

156
00:05:01,800 --> 00:05:03,440
假设你的图片

157
00:05:03,720 --> 00:05:05,080
存在一个文件夹里面

158
00:05:05,520 --> 00:05:07,600
然后你的每一类

159
00:05:08,240 --> 00:05:09,920
的类别号

160
00:05:10,280 --> 00:05:12,320
创造出一个文件夹

161
00:05:12,320 --> 00:05:14,400
假设就是说你有10个类的话

162
00:05:14,440 --> 00:05:16,320
那么你就在某一个文件夹下面

163
00:05:16,320 --> 00:05:18,840
创建10个子文件夹

164
00:05:19,280 --> 00:05:21,240
然后每一个类对应的那些图片

165
00:05:21,280 --> 00:05:23,320
你就把它塞到子文件夹下面

166
00:05:24,200 --> 00:05:27,120
这是一个最简单的一个图片

167
00:05:27,120 --> 00:05:28,840
分类的一个数据集的格式

168
00:05:29,320 --> 00:05:32,440
所以我们这里用的是它最简单的

169
00:05:32,440 --> 00:05:32,960
这个格式

170
00:05:32,960 --> 00:05:34,200
可以直接loader来跑

171
00:05:34,520 --> 00:05:35,680
当然实际情况下

172
00:05:35,680 --> 00:05:36,640
你可以不用

173
00:05:36,640 --> 00:05:37,400
不一定要这么做

174
00:05:37,760 --> 00:05:38,560
实际情况下

175
00:05:38,560 --> 00:05:41,080
你可以写个customized data iterator

176
00:05:41,080 --> 00:05:42,680
或者你去找一找

177
00:05:42,879 --> 00:05:44,759
别的一些支持的iterator也行

178
00:05:44,759 --> 00:05:45,759
就是说直接读

179
00:05:45,879 --> 00:05:49,120
但我们这里就是给大家还是说

180
00:05:49,600 --> 00:05:51,720
尝试用的最简单的

181
00:05:51,720 --> 00:05:53,680
最常用的读文件的方式

182
00:05:54,400 --> 00:05:56,040
所以这些函数在干嘛呢

183
00:05:56,240 --> 00:05:58,280
这些函数就是把你的图片搬过去

184
00:05:58,280 --> 00:06:01,439
然后把它的子文件夹创建好

185
00:06:01,439 --> 00:06:03,079
然后把图片copy到下面

186
00:06:03,480 --> 00:06:06,720
因为我们刚刚cfash的格式

187
00:06:06,720 --> 00:06:08,280
就是说给你一个trend data

188
00:06:08,280 --> 00:06:09,199
trend的文件夹下面

189
00:06:09,199 --> 00:06:10,319
所有的trend的图片

190
00:06:10,560 --> 00:06:12,399
test的下面有所有的test的图片

191
00:06:12,520 --> 00:06:13,280
然后你的label

192
00:06:13,280 --> 00:06:14,840
就是在那个csv文件里面

193
00:06:15,280 --> 00:06:16,800
所以这里就是说白了

194
00:06:17,520 --> 00:06:19,080
帮你重新整理一下

195
00:06:19,480 --> 00:06:20,720
所以这看上去很长

196
00:06:20,720 --> 00:06:22,040
其实挺简单的

197
00:06:23,120 --> 00:06:27,160
就是说我们会创建几个文件夹

198
00:06:27,160 --> 00:06:30,320
我们就不那么去给大家去过一遍

199
00:06:30,600 --> 00:06:31,520
具体怎么实现的

200
00:06:31,520 --> 00:06:33,720
其实就是copy来copy去了

201
00:06:34,200 --> 00:06:36,080
它的结果可以跟大家讲一下

202
00:06:36,400 --> 00:06:39,200
就是说我会把所有的文件

203
00:06:39,480 --> 00:06:41,880
会它的根目录叫做trend

204
00:06:42,040 --> 00:06:43,920
下滑线valid下滑线test

205
00:06:44,600 --> 00:06:46,839
然后会里面下面会有几个文件夹

206
00:06:47,360 --> 00:06:49,279
一个是一个叫trend的文件夹

207
00:06:49,279 --> 00:06:51,839
就包含了训练的数据

208
00:06:53,000 --> 00:06:55,639
然后valid就是一个验证级的数据

209
00:06:56,159 --> 00:06:58,159
这两个其实都是从测

210
00:06:58,159 --> 00:06:59,279
从训练数据来的

211
00:06:59,279 --> 00:07:00,639
就是我都是有标号的

212
00:07:00,879 --> 00:07:02,719
然后trend加valid

213
00:07:02,920 --> 00:07:06,920
说白了就是你cfash的原始的trend文件

214
00:07:06,920 --> 00:07:08,319
因为我们要调参

215
00:07:08,719 --> 00:07:11,439
调参我们这里没有做物质交叉验证

216
00:07:11,439 --> 00:07:12,600
因为做起来比较贵

217
00:07:12,600 --> 00:07:14,719
所以我们就直接做了一折

218
00:07:14,719 --> 00:07:16,879
一折就是说我就把整个trend

219
00:07:16,879 --> 00:07:18,360
data其实就是trend

220
00:07:18,360 --> 00:07:18,959
valid

221
00:07:19,160 --> 00:07:21,040
分成一个trend的文件夹

222
00:07:21,040 --> 00:07:22,120
一个valid的文件夹

223
00:07:23,079 --> 00:07:23,680
OK

224
00:07:24,240 --> 00:07:26,680
剩下的就是一个测试的文件夹了

225
00:07:27,839 --> 00:07:31,680
基本上这样子的话

226
00:07:31,680 --> 00:07:33,879
我们会在trend上面trend一个模型

227
00:07:34,040 --> 00:07:36,720
在valid看一下结果来调参数

228
00:07:37,079 --> 00:07:38,920
选定参数之后

229
00:07:38,920 --> 00:07:42,720
最后在完整的训练的数据机里面

230
00:07:43,280 --> 00:07:44,319
训练一遍

231
00:07:45,680 --> 00:07:47,240
所以这个就是要干的方法

232
00:07:47,680 --> 00:07:48,960
测试机是一样的

233
00:07:48,960 --> 00:07:50,680
我们就把我们的测试图片

234
00:07:50,680 --> 00:07:53,160
搬到trendvalid的test的

235
00:07:53,160 --> 00:07:55,160
下面一个test的文字文件夹下面

236
00:07:56,439 --> 00:07:57,360
因为他不需要label

237
00:07:57,560 --> 00:07:59,560
就label我们就放在一个文件夹

238
00:07:59,560 --> 00:08:02,240
叫做unknown下面

239
00:08:02,439 --> 00:08:03,680
就是我label不知道

240
00:08:03,680 --> 00:08:05,520
所以就不在意

241
00:08:06,720 --> 00:08:07,319
OK

242
00:08:07,879 --> 00:08:09,199
所以当然是说

243
00:08:09,519 --> 00:08:12,240
我们就是把你的

244
00:08:14,319 --> 00:08:15,480
文件夹整理一下

245
00:08:16,000 --> 00:08:17,719
Batch size如果是demo的话

246
00:08:17,719 --> 00:08:19,439
因为我们demo就比较小了

247
00:08:19,480 --> 00:08:20,639
就用的是32

248
00:08:20,680 --> 00:08:21,879
而且32的话

249
00:08:21,879 --> 00:08:22,879
好处是说

250
00:08:22,879 --> 00:08:24,319
你的GPU memory不够的话

251
00:08:24,319 --> 00:08:26,000
应该也是跑起来没问题的

252
00:08:26,199 --> 00:08:26,800
一般来说

253
00:08:26,800 --> 00:08:29,039
你可能SIFA使用个128

254
00:08:29,240 --> 00:08:31,079
或者256还是可以的

255
00:08:31,120 --> 00:08:34,279
因为它比MNIST稍微复杂一点

256
00:08:34,319 --> 00:08:36,000
所以用大一点是没关系的

257
00:08:36,320 --> 00:08:38,519
然后valid ratio

258
00:08:38,519 --> 00:08:39,279
就是说

259
00:08:39,279 --> 00:08:41,039
你在trend data里面

260
00:08:41,200 --> 00:08:44,360
我90%用来训练我的模型

261
00:08:44,360 --> 00:08:46,320
剩下的10%用来做验证

262
00:08:46,320 --> 00:08:47,639
来选操参数

263
00:08:48,360 --> 00:08:48,679
OK

264
00:08:48,679 --> 00:08:49,879
所以这个函数运行完之后

265
00:08:49,879 --> 00:08:52,440
就是帮你创建一堆文件夹

266
00:08:52,879 --> 00:08:53,399
数据

267
00:08:53,399 --> 00:08:54,759
把image copy过去

268
00:08:55,399 --> 00:08:57,200
当然这是一个比较傻的一个办法

269
00:08:57,720 --> 00:09:00,600
因为假设你真的有100G

270
00:09:00,639 --> 00:09:02,559
或者一个T的图片的话

271
00:09:02,840 --> 00:09:05,000
我们在这里就是帮你copy两次

272
00:09:05,960 --> 00:09:08,120
那你就可能就200G空间没了

273
00:09:08,120 --> 00:09:09,240
或者两个T空间没了

274
00:09:09,240 --> 00:09:10,320
而且做起来很慢

275
00:09:10,360 --> 00:09:12,440
所以在实际中

276
00:09:12,679 --> 00:09:15,919
通常我们是在图片放在哪里

277
00:09:15,919 --> 00:09:16,840
就是放在哪里

278
00:09:16,879 --> 00:09:17,919
然后只是说

279
00:09:17,919 --> 00:09:20,080
我写一个customized data iterator

280
00:09:20,120 --> 00:09:21,120
来做这个事情

281
00:09:21,960 --> 00:09:23,519
OK

282
00:09:23,519 --> 00:09:24,320
但是反过来讲

283
00:09:24,440 --> 00:09:26,039
如果你的图片不大的话

284
00:09:26,080 --> 00:09:28,559
确实你用这种形式做出来

285
00:09:28,559 --> 00:09:29,600
不管是哪个框架

286
00:09:29,600 --> 00:09:31,679
通常都会支持这样子的文件格式

287
00:09:31,720 --> 00:09:33,759
所以也是挺推荐的一个做法

288
00:09:33,799 --> 00:09:34,840
而且它简单

289
00:09:35,360 --> 00:09:36,120
你很方便

290
00:09:36,120 --> 00:09:37,159
手边点进去看一下

291
00:09:37,159 --> 00:09:38,639
每个图片里面有哪些东西

292
00:09:41,240 --> 00:09:45,000
另外一个是做augmentation

293
00:09:45,600 --> 00:09:47,360
我们之前有讲过augmentation

294
00:09:47,360 --> 00:09:48,120
就是说

295
00:09:49,600 --> 00:09:51,639
我们在fashion list上做了一些

296
00:09:51,639 --> 00:09:53,080
然后我们讲过那些东西

297
00:09:53,120 --> 00:09:53,960
所以我们来看一下

298
00:09:53,960 --> 00:09:55,440
它这里用的是什么

299
00:09:56,919 --> 00:09:58,639
首先我们看到是说

300
00:09:58,639 --> 00:10:00,639
我们把图片

301
00:10:00,679 --> 00:10:03,200
它本来是32x32

302
00:10:03,759 --> 00:10:05,200
要把它放大一点

303
00:10:06,520 --> 00:10:07,960
就放大到40

304
00:10:08,320 --> 00:10:09,360
为什么呢

305
00:10:09,600 --> 00:10:11,080
因为图片太小了

306
00:10:11,080 --> 00:10:13,000
而且它已经采采的比较好了

307
00:10:13,040 --> 00:10:16,240
然后你再我放大到40的话

308
00:10:16,360 --> 00:10:19,040
我就给我一点倒腾的空间

309
00:10:19,040 --> 00:10:21,080
可以去里面再采到

310
00:10:21,080 --> 00:10:23,759
采出一个32x32的一个小图片出来

311
00:10:25,160 --> 00:10:25,759
OK

312
00:10:25,800 --> 00:10:28,640
当然你可以说我不resize

313
00:10:28,680 --> 00:10:29,280
我不放大

314
00:10:29,280 --> 00:10:30,440
就是采出更小的

315
00:10:30,440 --> 00:10:31,879
比如28x28的出来

316
00:10:31,919 --> 00:10:33,639
然后这样做也是可以的

317
00:10:35,120 --> 00:10:37,840
所以我们这里用的是先放大一点

318
00:10:37,840 --> 00:10:40,639
然后再随机采小

319
00:10:40,840 --> 00:10:42,439
然后scale就是说

320
00:10:42,840 --> 00:10:44,919
你可以看到是64%

321
00:10:44,919 --> 00:10:48,159
就是说最小的采的区域

322
00:10:48,759 --> 00:10:50,919
需要cover到我的图片的

323
00:10:50,919 --> 00:10:52,600
至少是64%的

324
00:10:52,639 --> 00:10:53,840
就是说你可以认为是

325
00:10:54,559 --> 00:10:56,200
长高宽

326
00:10:56,200 --> 00:10:58,320
至少都是有80%留在里面

327
00:10:58,840 --> 00:11:00,600
但最大是整个图

328
00:11:01,480 --> 00:11:03,200
Ratio这里就是说

329
00:11:03,200 --> 00:11:06,720
我们因为Sifast这个图片

330
00:11:06,720 --> 00:11:07,279
相对来说

331
00:11:07,279 --> 00:11:09,120
它放黄真正的已经比较好了

332
00:11:09,120 --> 00:11:11,480
所以我们就Ratio就没有调了

333
00:11:13,159 --> 00:11:15,960
但是水平的flip

334
00:11:17,159 --> 00:11:18,440
我们没有

335
00:11:18,960 --> 00:11:19,360
sorry

336
00:11:22,639 --> 00:11:24,279
我们没有变颜色

337
00:11:24,519 --> 00:11:25,920
因为相对来说

338
00:11:27,240 --> 00:11:29,800
它颜色都还挺正常的

339
00:11:30,320 --> 00:11:31,760
然后都to tensor之后

340
00:11:31,760 --> 00:11:32,680
然后normalize

341
00:11:32,680 --> 00:11:34,800
对RGB的三个channel normalize

342
00:11:36,360 --> 00:11:37,880
当然你这里不能normalize

343
00:11:37,880 --> 00:11:39,000
可能也问题不大

344
00:11:39,840 --> 00:11:40,200
OK

345
00:11:40,200 --> 00:11:41,440
所以测试的话

346
00:11:41,560 --> 00:11:43,840
就是什么都不用做了

347
00:11:43,880 --> 00:11:44,920
因为就是说

348
00:11:44,960 --> 00:11:46,800
反正是3小成32进去

349
00:11:47,200 --> 00:11:48,760
所以我们就直接to tensor

350
00:11:48,760 --> 00:11:49,600
然后normalize

351
00:11:49,600 --> 00:11:51,240
一样的normalize出来就行了

352
00:11:51,360 --> 00:11:51,560
好

353
00:11:51,560 --> 00:11:53,920
这就是我们的图片增广员做的事情

354
00:11:56,200 --> 00:11:56,480
好

355
00:11:56,480 --> 00:11:57,880
接下来就是读图片了

356
00:11:58,240 --> 00:11:58,720
图片的话

357
00:11:59,160 --> 00:12:02,360
我们有个train的data set

358
00:12:02,519 --> 00:12:04,080
train valid data set

359
00:12:04,320 --> 00:12:06,360
然后我们有一个valid data set

360
00:12:06,360 --> 00:12:07,480
一个test data set

361
00:12:08,120 --> 00:12:09,160
之所以要分开写

362
00:12:09,160 --> 00:12:11,120
是因为train data set

363
00:12:11,120 --> 00:12:12,800
和train valid data set

364
00:12:12,800 --> 00:12:14,840
它的transform augmentation

365
00:12:14,840 --> 00:12:16,440
是用的我们train augmentation

366
00:12:16,519 --> 00:12:19,560
就真的要做随机的那些变换的

367
00:12:19,600 --> 00:12:21,440
然后做valid和做test

368
00:12:21,480 --> 00:12:22,480
它都不需要

369
00:12:22,480 --> 00:12:26,120
就是用的transform的不一样

370
00:12:26,200 --> 00:12:27,960
所以这是分开写的原因

371
00:12:29,200 --> 00:12:30,200
所以可以看到是说

372
00:12:30,200 --> 00:12:31,000
我们用的是什么

373
00:12:31,000 --> 00:12:33,399
用的是touch vision里面的

374
00:12:33,440 --> 00:12:34,600
image folder

375
00:12:34,600 --> 00:12:35,639
这一个格式

376
00:12:35,800 --> 00:12:37,080
就是说给一个

377
00:12:38,840 --> 00:12:40,000
主的文件夹

378
00:12:40,000 --> 00:12:41,600
然后丢进去就行了

379
00:12:41,600 --> 00:12:42,800
然后你只要你的label

380
00:12:42,800 --> 00:12:44,480
是按照文件夹的形式排好

381
00:12:44,480 --> 00:12:45,879
它就可以自动读出来

382
00:12:46,600 --> 00:12:46,960
OK

383
00:12:46,960 --> 00:12:50,279
就属于尽量在用鼠标操作

384
00:12:50,519 --> 00:12:52,680
就尽量是操作系统操作

385
00:12:53,000 --> 00:12:55,360
这样子我就盯住一个API用就行了

386
00:12:55,360 --> 00:12:56,000
就别的

387
00:12:56,000 --> 00:12:56,800
我也不用管

388
00:12:56,800 --> 00:12:57,560
你data set里面

389
00:12:57,560 --> 00:12:58,519
还有什么别的

390
00:12:58,800 --> 00:12:59,759
读的方式

391
00:13:02,160 --> 00:13:02,440
好

392
00:13:02,440 --> 00:13:03,279
然后当然是说

393
00:13:03,279 --> 00:13:06,279
我们定我们的data loader

394
00:13:06,279 --> 00:13:09,160
data loader就是给定我的batch size

395
00:13:09,320 --> 00:13:11,519
如果是train和train valid

396
00:13:11,519 --> 00:13:14,240
就是这两个都是train模型的用的

397
00:13:14,279 --> 00:13:15,399
所以我们的shuffler

398
00:13:15,399 --> 00:13:17,560
是要开随机的shuffler

399
00:13:17,560 --> 00:13:18,720
因为我们是要随机的

400
00:13:18,720 --> 00:13:19,759
scd下降

401
00:13:20,800 --> 00:13:21,840
drop last

402
00:13:21,840 --> 00:13:22,399
就是说

403
00:13:22,399 --> 00:13:24,399
如果你最后一个batch

404
00:13:24,680 --> 00:13:27,240
它的p2那些不满足我要的需求

405
00:13:27,279 --> 00:13:27,840
就是说

406
00:13:27,840 --> 00:13:29,759
假设最后我扫一遍之后

407
00:13:29,759 --> 00:13:31,279
扫到后面还余了一截

408
00:13:31,600 --> 00:13:33,639
比如说我现在是batch size32

409
00:13:33,639 --> 00:13:35,440
然后之后我发现只剩16了

410
00:13:35,440 --> 00:13:37,440
或者只剩7了8了

411
00:13:37,560 --> 00:13:40,159
然后我这里操作就把它丢掉

412
00:13:40,799 --> 00:13:42,200
当然一般来说

413
00:13:42,200 --> 00:13:45,519
我觉得你就给了丢掉就丢掉也没事

414
00:13:46,240 --> 00:13:47,399
一般来说

415
00:13:48,240 --> 00:13:51,360
反正你是随机的

416
00:13:51,560 --> 00:13:54,560
所以其实问题不大

417
00:13:54,560 --> 00:13:56,480
反正你这次丢了那几个

418
00:13:56,480 --> 00:13:57,639
那一次丢了别的

419
00:13:58,800 --> 00:14:00,120
但是你的

420
00:14:00,480 --> 00:14:01,840
如果你是test的

421
00:14:01,920 --> 00:14:02,759
你不能丢

422
00:14:03,720 --> 00:14:06,080
test的drop last一定是false

423
00:14:06,080 --> 00:14:07,080
因为你test

424
00:14:07,080 --> 00:14:09,120
你需要对每一个图片给你预测一下

425
00:14:09,159 --> 00:14:10,680
所以就算不买也没关系

426
00:14:10,920 --> 00:14:13,000
我们这里valid和train都丢了

427
00:14:13,000 --> 00:14:13,960
其实反过来讲

428
00:14:13,960 --> 00:14:15,440
我觉得你丢不丢都没关系

429
00:14:15,800 --> 00:14:21,159
我们在很早以前

430
00:14:21,159 --> 00:14:24,200
就是在框架做的不那么好的时候

431
00:14:24,200 --> 00:14:25,039
在最后一个batch

432
00:14:25,039 --> 00:14:26,440
会有点performance的问题

433
00:14:26,480 --> 00:14:29,440
但现在其实大家在做都不错了

434
00:14:29,440 --> 00:14:31,519
因为为什么会有问题

435
00:14:31,519 --> 00:14:33,560
是说最后一次丢进去的时候

436
00:14:33,560 --> 00:14:35,720
你批量大小跟前面不一样

437
00:14:35,920 --> 00:14:38,120
可能它的性能可能会

438
00:14:38,120 --> 00:14:39,120
就是说是一个7

439
00:14:39,200 --> 00:14:40,039
batch size等于7

440
00:14:40,360 --> 00:14:41,840
它性能会出一点点问题

441
00:14:42,560 --> 00:14:43,879
但实际上现在来说

442
00:14:43,879 --> 00:14:45,159
其实我觉得你丢不丢

443
00:14:45,159 --> 00:14:46,519
都没什么太多关系

444
00:14:46,840 --> 00:14:49,639
当然我们另外一个丢的好处是说

445
00:14:49,639 --> 00:14:52,080
我们至少在train上面的时候

446
00:14:52,080 --> 00:14:54,039
永远知道我批量的batch size

447
00:14:54,039 --> 00:14:55,320
永远是等于一个固定数

448
00:14:55,400 --> 00:14:57,840
我们在很多计算精度什么东西

449
00:14:57,840 --> 00:14:59,879
都说手写的时候会方便一点

450
00:14:59,879 --> 00:15:01,480
这也是它的另外一个好处了

451
00:15:02,560 --> 00:15:03,080
OK

452
00:15:04,560 --> 00:15:05,560
好 模型

453
00:15:05,800 --> 00:15:08,560
模型我们直接在用的是

454
00:15:08,600 --> 00:15:11,560
我们之前有定过的resnet18

455
00:15:12,280 --> 00:15:13,960
然后它那个函数大家记不记得

456
00:15:13,960 --> 00:15:16,520
就是它有它两个输入

457
00:15:16,640 --> 00:15:19,400
一个是说你最后一层的有多少类

458
00:15:19,480 --> 00:15:20,280
我们这里是10

459
00:15:20,440 --> 00:15:23,080
因为Sifa10的名字

460
00:15:23,200 --> 00:15:26,720
Sifa是一个加拿大的研究机构

461
00:15:27,440 --> 00:15:29,600
然后10就是表示它是10类

462
00:15:29,600 --> 00:15:30,800
所以叫Sifa10

463
00:15:30,960 --> 00:15:32,440
所以可能Sifa可能

464
00:15:33,120 --> 00:15:35,200
大家可能没听说过研究机构

465
00:15:35,360 --> 00:15:36,560
虽然也是挺不错

466
00:15:36,600 --> 00:15:37,120
挺厉害的

467
00:15:37,120 --> 00:15:37,800
然后挺有名的

468
00:15:37,800 --> 00:15:39,320
做了出了很多大事

469
00:15:42,520 --> 00:15:44,879
但是可能Sifa10这个数据集

470
00:15:44,879 --> 00:15:46,000
可能更出名一点

471
00:15:46,120 --> 00:15:47,400
这就是一个数据集

472
00:15:47,400 --> 00:15:50,600
把一个b

473
00:15:50,639 --> 00:15:52,879
就是让你整个研究机构

474
00:15:52,879 --> 00:15:54,240
都是靠这个数据集出名

475
00:15:54,519 --> 00:15:55,560
反过来讲M-LIST

476
00:15:55,759 --> 00:15:57,560
M-LIST在神经网络

477
00:15:58,040 --> 00:16:01,000
罗昆大神在很长一段时间之内

478
00:16:01,000 --> 00:16:03,080
他大家都是

479
00:16:03,080 --> 00:16:04,800
大家都忘记这个大神了

480
00:16:04,800 --> 00:16:07,560
但是M-LIST在不管是神经网络

481
00:16:07,560 --> 00:16:08,360
在冬季

482
00:16:08,360 --> 00:16:11,240
还是在现在火热的情况

483
00:16:11,240 --> 00:16:12,879
M-LIST简单数据集

484
00:16:12,879 --> 00:16:14,159
一直都是很流行的

485
00:16:15,160 --> 00:16:17,160
这也是一个挺好玩的东西

486
00:16:18,000 --> 00:16:18,279
OK

487
00:16:18,279 --> 00:16:19,040
所以我们这里

488
00:16:19,559 --> 00:16:22,599
3就是说表示我的输入是三通道

489
00:16:22,599 --> 00:16:23,719
就是RGB三通道

490
00:16:23,719 --> 00:16:24,919
因为它是彩色图片

491
00:16:25,039 --> 00:16:26,439
因为之前为什么要这么写

492
00:16:26,439 --> 00:16:28,120
是因为我们的Fashion M-LIST

493
00:16:28,120 --> 00:16:28,879
或M-LIST

494
00:16:28,879 --> 00:16:30,319
它都是一个单通道的

495
00:16:30,319 --> 00:16:31,319
因为是黑白图片

496
00:16:32,079 --> 00:16:33,240
然后当然是说Loss

497
00:16:33,799 --> 00:16:34,919
就是Cross Entropy

498
00:16:34,919 --> 00:16:36,199
Reduction and Null

499
00:16:36,199 --> 00:16:37,480
就是说你不要把我加起来

500
00:16:37,480 --> 00:16:38,279
不要把我上

501
00:16:38,799 --> 00:16:39,799
加不加都无所谓

502
00:16:40,159 --> 00:16:41,439
就是说加了也挺好的

503
00:16:43,439 --> 00:16:44,079
OK

504
00:16:44,079 --> 00:16:45,199
我们的Trend

505
00:16:45,199 --> 00:16:47,799
Trend跟之前没本质区别

506
00:16:47,800 --> 00:16:50,720
我就不给大家特别的去过一遍了

507
00:16:51,280 --> 00:16:52,520
可以看到是说

508
00:16:52,520 --> 00:16:54,280
跟之前唯一的区别在什么地方

509
00:16:54,720 --> 00:16:56,720
就是在我的优化的时候

510
00:16:56,720 --> 00:16:57,960
我加了这两个参数

511
00:16:58,480 --> 00:16:59,480
看一下别的都一样

512
00:16:59,640 --> 00:17:01,240
你看给一个Net

513
00:17:01,600 --> 00:17:04,080
给一个训练的Iterator

514
00:17:04,519 --> 00:17:06,560
给一个我们做验证的Iterator

515
00:17:06,880 --> 00:17:08,840
然后说我要扫多少遍数据

516
00:17:09,480 --> 00:17:11,840
然后我的学习率

517
00:17:12,039 --> 00:17:13,560
我的Wait Decay

518
00:17:13,840 --> 00:17:15,640
我要拿GPU来Trend

519
00:17:16,440 --> 00:17:17,519
这两个是多的

520
00:17:18,799 --> 00:17:22,559
这LR Period和LR Decay

521
00:17:22,839 --> 00:17:24,799
它就是两个在

522
00:17:25,639 --> 00:17:28,799
一定时间内很常用的一个

523
00:17:28,799 --> 00:17:31,480
随就学习率下降的一个方法

524
00:17:31,799 --> 00:17:33,159
但现在也还常用

525
00:17:33,200 --> 00:17:34,799
但现在可能你用cosine

526
00:17:34,839 --> 00:17:36,839
这种更smooth的方法更好

527
00:17:37,079 --> 00:17:39,119
但是这一个东西是比较简单的

528
00:17:39,159 --> 00:17:39,879
它是什么意思

529
00:17:40,319 --> 00:17:41,240
就是说

530
00:17:41,480 --> 00:17:42,559
首先

531
00:17:42,960 --> 00:17:44,440
我们并没有讲

532
00:17:44,559 --> 00:17:45,720
随机梯度下降

533
00:17:45,720 --> 00:17:46,399
它的理论

534
00:17:46,600 --> 00:17:48,480
在突优化上面

535
00:17:48,480 --> 00:17:50,160
随机梯度下降

536
00:17:51,200 --> 00:17:52,519
收敛的前提

537
00:17:52,519 --> 00:17:53,440
是你的学习率

538
00:17:53,440 --> 00:17:55,000
要不断的减少

539
00:17:55,320 --> 00:17:57,400
我们在之前都是学习率是一个平的

540
00:17:58,320 --> 00:17:59,320
为什么要减少

541
00:17:59,480 --> 00:18:01,640
是因为在迭代

542
00:18:01,640 --> 00:18:04,560
越靠近你的最优解的时候

543
00:18:05,759 --> 00:18:06,720
你相对来说

544
00:18:06,720 --> 00:18:07,800
你应该做的

545
00:18:07,800 --> 00:18:08,920
应该就是说

546
00:18:08,920 --> 00:18:09,960
你不要乱走了

547
00:18:09,960 --> 00:18:11,280
已经靠近最优解

548
00:18:11,400 --> 00:18:12,680
整个平面比较平了

549
00:18:12,680 --> 00:18:13,960
你就不要乱走了

550
00:18:14,160 --> 00:18:16,960
但如果你不把学习率降低的情况下

551
00:18:17,400 --> 00:18:19,360
因为我是随机梯度下降

552
00:18:19,400 --> 00:18:21,840
所以每次我随机采用的一些样本

553
00:18:21,840 --> 00:18:23,880
里面样本有方叉

554
00:18:23,880 --> 00:18:24,519
因为随机的

555
00:18:25,400 --> 00:18:26,160
因为它是

556
00:18:26,799 --> 00:18:28,799
你批量越大

557
00:18:28,799 --> 00:18:29,720
当然是方叉越小

558
00:18:29,720 --> 00:18:31,200
如果批量很小的情况下

559
00:18:31,400 --> 00:18:32,680
随机取它有方叉

560
00:18:32,680 --> 00:18:34,079
方叉你可认为是一个噪音

561
00:18:34,600 --> 00:18:36,440
方叉可以好处是

562
00:18:36,440 --> 00:18:39,759
让你不容易陷到一个很local的局部

563
00:18:39,759 --> 00:18:40,559
缺小点

564
00:18:40,680 --> 00:18:42,720
但它坏处是它可以帮你带偏

565
00:18:43,720 --> 00:18:45,079
所以解决这个办法

566
00:18:45,079 --> 00:18:46,920
是说你把学习率变低一点

567
00:18:46,920 --> 00:18:49,120
就算方叉固定的时候

568
00:18:49,120 --> 00:18:52,600
我的学习率变得越来越低

569
00:18:52,600 --> 00:18:55,880
每一次我乱走的趋势就会变低

570
00:18:55,880 --> 00:18:58,039
所以就是说在最后收敛的时候

571
00:18:58,039 --> 00:19:00,039
我不希望它真的在乱走

572
00:19:00,279 --> 00:19:01,480
所以这就是这个意思

573
00:19:02,559 --> 00:19:04,839
所以这个意思就是说

574
00:19:05,120 --> 00:19:07,759
每隔多少个epoch

575
00:19:07,799 --> 00:19:08,960
也就是这个东西设的

576
00:19:08,960 --> 00:19:10,960
比如说每隔5个data epoch

577
00:19:11,440 --> 00:19:14,400
我把学习率降低一点

578
00:19:15,000 --> 00:19:16,000
这个可以是一个decay

579
00:19:16,000 --> 00:19:17,680
就是说是0.5的话

580
00:19:17,759 --> 00:19:19,039
假设是0.5的话

581
00:19:19,039 --> 00:19:21,480
那么每隔几个data epoch

582
00:19:21,519 --> 00:19:24,960
我把学习率减半

583
00:19:26,079 --> 00:19:26,600
OK

584
00:19:31,000 --> 00:19:34,079
所以这是一个常见的

585
00:19:36,160 --> 00:19:37,960
基本上一个decay的方法

586
00:19:38,279 --> 00:19:40,400
然后我们这就是

587
00:19:40,440 --> 00:19:41,600
用什么东西

588
00:19:42,240 --> 00:19:43,240
就前面都一样

589
00:19:43,640 --> 00:19:45,559
我们用的是一个SGD出来

590
00:19:46,039 --> 00:19:47,480
然后这里用的是说

591
00:19:47,480 --> 00:19:51,480
在optimizer的意思

592
00:19:51,519 --> 00:19:53,960
里面有个LR scheduler

593
00:19:54,000 --> 00:19:55,480
就是learning rate scheduler

594
00:19:55,480 --> 00:19:57,240
就是说怎么去调LR

595
00:19:57,640 --> 00:20:00,519
它里面有一个叫step LR

596
00:20:01,279 --> 00:20:02,400
然后可以告诉你说

597
00:20:02,680 --> 00:20:04,319
你把圈还是丢给它

598
00:20:04,360 --> 00:20:05,160
告诉你说

599
00:20:05,160 --> 00:20:07,320
每隔多少个epoch

600
00:20:07,840 --> 00:20:09,320
我把这个LR

601
00:20:10,519 --> 00:20:13,360
把decay值乘以当前的LR

602
00:20:14,279 --> 00:20:14,560
OK

603
00:20:14,600 --> 00:20:15,960
这就是唯一的不一样

604
00:20:16,519 --> 00:20:17,960
然后你就要SGD了

605
00:20:18,000 --> 00:20:18,720
当然是

606
00:20:19,560 --> 00:20:21,360
别的我们都其实差不多

607
00:20:22,519 --> 00:20:23,960
然后你可以看到是

608
00:20:25,040 --> 00:20:27,320
我们用的是所有的device

609
00:20:27,360 --> 00:20:28,320
我们之前有讲过

610
00:20:28,360 --> 00:20:29,080
用multi GPU

611
00:20:29,240 --> 00:20:31,640
因为在SIFAR-10这个数据机上

612
00:20:31,680 --> 00:20:32,920
能用多个GPU

613
00:20:32,960 --> 00:20:33,840
就是多个GPU了

614
00:20:33,840 --> 00:20:34,920
所以用的data pile了

615
00:20:34,920 --> 00:20:36,920
我们有讲过是大概是怎么回事

616
00:20:38,080 --> 00:20:40,680
所以这些东西都差不多

617
00:20:41,560 --> 00:20:42,920
这些东西都是我们

618
00:20:43,400 --> 00:20:45,279
为了给大家演示

619
00:20:45,400 --> 00:20:46,320
为了给大家演示

620
00:20:46,360 --> 00:20:48,200
整个具体是怎么回事

621
00:20:48,240 --> 00:20:48,840
给大家写的

622
00:20:48,840 --> 00:20:50,600
而且是这些东西都是多余的

623
00:20:50,640 --> 00:20:51,000
这个东西

624
00:20:51,039 --> 00:20:53,000
其实就是给大家画画图看的

625
00:20:53,560 --> 00:20:56,120
所以看上去很长这个代码

626
00:20:56,480 --> 00:20:58,400
但实际上实际现实生活中

627
00:20:58,400 --> 00:21:00,440
大家不通常不需要写那么长的代码

628
00:21:00,480 --> 00:21:01,160
一般是说

629
00:21:01,160 --> 00:21:02,519
你把这个东西都封装好

630
00:21:02,560 --> 00:21:04,880
或者你就一个fit一个API

631
00:21:05,000 --> 00:21:06,080
就掉就行了

632
00:21:06,120 --> 00:21:07,640
但我们这里主要是教学目的

633
00:21:07,640 --> 00:21:09,080
所以给大家一一的展开

634
00:21:09,080 --> 00:21:11,000
里面每一句话是在干嘛

635
00:21:12,279 --> 00:21:14,720
当然跟之前不一样的是说

636
00:21:15,039 --> 00:21:16,120
我们如果我们有

637
00:21:16,120 --> 00:21:17,039
这有个scheduler

638
00:21:17,039 --> 00:21:18,120
scheduler就是说

639
00:21:18,120 --> 00:21:19,120
每个一poke之后

640
00:21:19,120 --> 00:21:20,200
他就step一下

641
00:21:20,400 --> 00:21:21,480
就要更新一下

642
00:21:21,640 --> 00:21:22,200
OK

643
00:21:22,800 --> 00:21:25,279
然后因为我们已经把trainer

644
00:21:25,279 --> 00:21:26,000
丢给scheduler

645
00:21:26,000 --> 00:21:26,880
在创建它时候

646
00:21:26,880 --> 00:21:28,880
所以它其实能够访问trainer

647
00:21:28,880 --> 00:21:29,800
那个line rate

648
00:21:29,840 --> 00:21:31,560
所以它就会对学习率

649
00:21:31,560 --> 00:21:32,640
做相应的变化

650
00:21:32,840 --> 00:21:33,320
OK

651
00:21:33,320 --> 00:21:34,640
所以别的东西都是print

652
00:21:34,840 --> 00:21:35,280
什么东西

653
00:21:35,280 --> 00:21:37,800
我们就不那么就给大家一一介绍

654
00:21:37,800 --> 00:21:39,280
主要是说跟之前不一样

655
00:21:39,280 --> 00:21:40,000
是什么样子

656
00:21:40,280 --> 00:21:41,800
当然如果大家如果

657
00:21:45,560 --> 00:21:46,680
错过了前面几堂课

658
00:21:46,680 --> 00:21:48,080
大家可以回去看一下我们的录像

659
00:21:48,080 --> 00:21:48,680
就是说

660
00:21:48,920 --> 00:21:50,440
讲一下每个函数到底在干嘛

661
00:21:50,600 --> 00:21:51,759
什么chernbatch

662
00:21:51,800 --> 00:21:53,200
ch13在干嘛

663
00:21:53,200 --> 00:21:54,440
就是chapter13的

664
00:21:54,640 --> 00:21:55,520
chernbatch函数

665
00:21:55,520 --> 00:21:56,640
我们之前有讲过

666
00:21:56,720 --> 00:21:57,280
上周

667
00:21:58,480 --> 00:21:59,000
OK

668
00:22:00,040 --> 00:22:00,759
所以可以看到

669
00:22:00,759 --> 00:22:02,600
就是说我们就直接可以churn了

670
00:22:03,120 --> 00:22:03,920
我们device

671
00:22:04,039 --> 00:22:05,640
当然是把所有的GPU拿出来

672
00:22:05,960 --> 00:22:07,000
number of epoch

673
00:22:07,000 --> 00:22:07,920
我们用的是2

674
00:22:08,360 --> 00:22:10,480
learning rate用的其实不大

675
00:22:11,000 --> 00:22:12,000
2e-4

676
00:22:12,039 --> 00:22:12,960
然后wait decay

677
00:22:13,120 --> 00:22:13,920
5e-4

678
00:22:14,200 --> 00:22:15,480
也是挺小的

679
00:22:15,759 --> 00:22:17,840
然后这两个是新参数

680
00:22:18,480 --> 00:22:19,400
意思是说

681
00:22:19,400 --> 00:22:23,400
每隔4个data epoch之后

682
00:22:23,400 --> 00:22:25,360
我把当前的学习率

683
00:22:25,360 --> 00:22:26,519
乘以0.9

684
00:22:28,000 --> 00:22:29,759
就是说如果学习率是1的话

685
00:22:29,759 --> 00:22:31,120
那么过了4个data

686
00:22:31,120 --> 00:22:32,440
epoch就变成0.9

687
00:22:32,799 --> 00:22:34,200
再过了4个就0.8亿

688
00:22:34,200 --> 00:22:35,840
就指数级的往下降

689
00:22:36,720 --> 00:22:37,200
OK

690
00:22:37,200 --> 00:22:38,680
所以基本上就可以开始churn了

691
00:22:39,840 --> 00:22:42,080
所以我们就不给大家直接重新churn了

692
00:22:42,200 --> 00:22:43,720
因为这个东西churn起来也

693
00:22:44,920 --> 00:22:45,559
不快

694
00:22:45,559 --> 00:22:47,000
就是看到

695
00:22:47,000 --> 00:22:49,240
就是每秒就能够churn个450个

696
00:22:49,240 --> 00:22:49,840
一个赞母

697
00:22:50,080 --> 00:22:50,840
然后

698
00:22:52,160 --> 00:22:54,519
所以就不适合现场演示

699
00:22:55,120 --> 00:22:55,799
但反过来讲

700
00:22:55,799 --> 00:22:56,519
可以看到是说

701
00:22:56,519 --> 00:22:57,480
SIFAR学这个数据

702
00:22:57,480 --> 00:22:58,600
也不那么好churn

703
00:22:58,880 --> 00:22:59,440
就可以看到

704
00:22:59,440 --> 00:23:01,000
是churn了20个epoch之后

705
00:23:01,240 --> 00:23:03,720
你的loss还在一直往下降

706
00:23:04,079 --> 00:23:05,319
你的churn accuracy

707
00:23:05,680 --> 00:23:07,720
基本上可以还在往上涨

708
00:23:08,640 --> 00:23:09,720
所以通常来讲

709
00:23:09,839 --> 00:23:12,759
你的一般在SIFAR这个数据上

710
00:23:13,200 --> 00:23:14,880
一般你要churn到个

711
00:23:15,279 --> 00:23:16,839
100个epoch的样子

712
00:23:16,839 --> 00:23:19,119
可能比我们这个数页分类的数据

713
00:23:19,119 --> 00:23:20,200
可能churn起来更难一点

714
00:23:20,200 --> 00:23:21,920
因为它的背景更复杂一点

715
00:23:22,079 --> 00:23:23,079
图片更小

716
00:23:23,079 --> 00:23:24,359
更难分辨一些

717
00:23:24,680 --> 00:23:25,720
所以这个东西

718
00:23:25,720 --> 00:23:26,640
我们有个练习题

719
00:23:26,640 --> 00:23:27,440
大家可以去看一下

720
00:23:27,440 --> 00:23:28,240
就是说告诉你

721
00:23:28,240 --> 00:23:29,480
给了你一组参数

722
00:23:29,480 --> 00:23:31,319
基本你用了那组参数的话

723
00:23:31,319 --> 00:23:32,720
还会得到一个

724
00:23:32,759 --> 00:23:33,480
accuracy

725
00:23:33,480 --> 00:23:35,160
会有一个很不错的结果了

726
00:23:35,680 --> 00:23:36,759
当然是说

727
00:23:38,120 --> 00:23:39,039
最简单的

728
00:23:39,039 --> 00:23:40,799
我知道比较SIFAR

729
00:23:40,799 --> 00:23:42,680
实际上做出很好的效果

730
00:23:42,680 --> 00:23:43,839
最简单是什么样子

731
00:23:44,559 --> 00:23:46,480
你就来一个rest net的50

732
00:23:46,559 --> 00:23:48,279
比如说直接churn

733
00:23:48,880 --> 00:23:50,480
选一个比较大的学习率

734
00:23:51,079 --> 00:23:52,920
前面150轮不decay

735
00:23:52,960 --> 00:23:54,599
就是用一个很大的学习率

736
00:23:54,599 --> 00:23:55,680
churn150轮

737
00:23:56,039 --> 00:23:57,519
然后再在后面几轮

738
00:23:57,519 --> 00:23:58,960
就decay几次就行了

739
00:24:00,480 --> 00:24:02,360
这个是我知道的

740
00:24:02,360 --> 00:24:04,000
一个比较简单的做法

741
00:24:04,360 --> 00:24:06,079
当然现在也许有更新的方法

742
00:24:06,120 --> 00:24:06,839
当然是

743
00:24:07,960 --> 00:24:08,519
这个数据

744
00:24:08,559 --> 00:24:10,400
反正也是被大家玩的比较多的

745
00:24:13,519 --> 00:24:13,880
OK

746
00:24:13,880 --> 00:24:15,599
所以最后的话就是说

747
00:24:16,079 --> 00:24:17,319
我们刚刚知道是说

748
00:24:17,319 --> 00:24:19,160
我们每一次其实是用了

749
00:24:20,640 --> 00:24:22,920
我们把SIFAR-10给的

750
00:24:22,920 --> 00:24:25,160
我们训练的数据分成两块

751
00:24:25,160 --> 00:24:26,039
分成一个churn

752
00:24:26,039 --> 00:24:27,000
分成一个valid

753
00:24:27,039 --> 00:24:28,160
90%做churn

754
00:24:28,160 --> 00:24:29,000
10%做valid

755
00:24:29,000 --> 00:24:30,599
所以我们能够去调参数

756
00:24:30,759 --> 00:24:32,440
所以刚刚这个东西是干嘛的

757
00:24:33,119 --> 00:24:34,880
就是churn iterator

758
00:24:34,880 --> 00:24:35,839
valid iterator

759
00:24:35,839 --> 00:24:38,720
就是说在给我的训练数据上

760
00:24:38,720 --> 00:24:40,559
做调参用的

761
00:24:40,960 --> 00:24:42,160
这个东西是可以知道

762
00:24:42,160 --> 00:24:43,079
valid accuracy

763
00:24:44,559 --> 00:24:45,559
这个地方挺低的

764
00:24:45,559 --> 00:24:47,079
因为你根本就还没收敛

765
00:24:48,279 --> 00:24:49,160
所以是说

766
00:24:49,160 --> 00:24:51,480
假设你参数调好之后

767
00:24:51,680 --> 00:24:52,799
那么接下来就是说

768
00:24:52,799 --> 00:24:54,680
我要在完整的

769
00:24:54,720 --> 00:24:55,960
他给我的数据上

770
00:24:56,240 --> 00:24:57,400
我们之前的churn

771
00:24:57,400 --> 00:24:59,000
加上valid上面

772
00:24:59,040 --> 00:25:00,560
重新训练一个模型

773
00:25:01,200 --> 00:25:02,320
就是说这样子的话

774
00:25:02,320 --> 00:25:03,759
而且我就没有valid了

775
00:25:04,000 --> 00:25:06,759
因为他给我的测试机没有label了

776
00:25:07,080 --> 00:25:09,600
所以就在完整的训练机上

777
00:25:09,600 --> 00:25:10,480
训练一次

778
00:25:10,519 --> 00:25:12,200
用我们刚刚调好的参数

779
00:25:12,519 --> 00:25:14,600
这就是调参所谓的

780
00:25:15,080 --> 00:25:16,000
然后调好之后

781
00:25:16,000 --> 00:25:17,880
最后是我们去预测

782
00:25:17,880 --> 00:25:19,880
就是对每一个testator的

783
00:25:19,880 --> 00:25:21,240
就你Y就不用管了

784
00:25:21,240 --> 00:25:23,040
因为你的Y都是一个假的label

785
00:25:23,080 --> 00:25:24,200
把X拉进来

786
00:25:24,240 --> 00:25:26,200
然后直接在GPU0上

787
00:25:26,200 --> 00:25:28,400
然后预测Y的hat

788
00:25:28,440 --> 00:25:30,360
然后把每一次

789
00:25:30,360 --> 00:25:32,000
就把你值最高的

790
00:25:32,600 --> 00:25:33,319
Y hat里面

791
00:25:33,319 --> 00:25:34,360
max

792
00:25:34,400 --> 00:25:36,480
值最高的index拿出来

793
00:25:36,600 --> 00:25:38,440
然后就配拖型

794
00:25:38,440 --> 00:25:41,840
你就比较傻一点点

795
00:25:41,840 --> 00:25:45,000
就是说你得先把他转成一个int32

796
00:25:45,000 --> 00:25:46,960
然后变成多到CPU上

797
00:25:46,960 --> 00:25:48,160
再转到numpy上面

798
00:25:49,880 --> 00:25:50,240
然后

799
00:25:51,039 --> 00:25:52,000
就是

800
00:25:54,200 --> 00:25:58,440
然后把这东西打印出一个dataframe

801
00:25:58,480 --> 00:26:00,680
然后就存成一个submission.csv

802
00:26:00,680 --> 00:26:01,920
就完事了

803
00:26:02,559 --> 00:26:03,839
这就是这样子的话

804
00:26:03,839 --> 00:26:05,920
我们就可以去提交了

805
00:26:06,680 --> 00:26:07,640
所以这就是

806
00:26:07,680 --> 00:26:08,559
而且可以看到说

807
00:26:08,559 --> 00:26:09,200
这个地方

808
00:26:09,200 --> 00:26:12,319
我们是只有一个训练的accuracy

809
00:26:12,319 --> 00:26:13,279
也没有validation

810
00:26:13,319 --> 00:26:13,920
就是说

811
00:26:14,079 --> 00:26:15,200
你就只能基本上

812
00:26:15,200 --> 00:26:17,519
根据你的训练的accuracy

813
00:26:17,519 --> 00:26:19,720
和你之前调的参数的

814
00:26:19,720 --> 00:26:20,880
最好的情况看一下

815
00:26:20,880 --> 00:26:22,120
如果差不多没什么问题

816
00:26:22,120 --> 00:26:23,079
就可以提交了

817
00:26:23,400 --> 00:26:26,440
这就是一个比较简单的

818
00:26:27,559 --> 00:26:29,319
做一个cfas的一个样例

819
00:26:30,240 --> 00:26:32,120
可以看到我们跟之前的什么区别

820
00:26:32,839 --> 00:26:34,240
其实主要是两个区别

821
00:26:34,240 --> 00:26:34,799
对吧

822
00:26:36,200 --> 00:26:37,440
一个区别

823
00:26:37,759 --> 00:26:40,120
主要是在data augmentation上面

824
00:26:40,519 --> 00:26:41,680
我们把它弄大一点点

825
00:26:41,720 --> 00:26:43,519
然后再随机random crop

826
00:26:43,559 --> 00:26:47,120
我们之前就是左右flip就行了

827
00:26:48,120 --> 00:26:50,080
而第二个是说

828
00:26:50,080 --> 00:26:52,280
我们在优化算法上面

829
00:26:52,280 --> 00:26:54,280
加了一个decay在里面

830
00:26:54,280 --> 00:26:55,440
learning rate decay

831
00:26:55,720 --> 00:26:58,160
这样子我们就可以真的在

832
00:26:58,160 --> 00:26:58,800
相对来说

833
00:26:58,800 --> 00:27:00,880
train比较长的epoch

834
00:27:00,880 --> 00:27:02,640
然后慢慢的下降

835
00:27:02,640 --> 00:27:05,040
然后让它慢慢的收敛

836
00:27:05,320 --> 00:27:08,400
这个是两个主要的办法

837
00:27:08,640 --> 00:27:10,160
另外一个我们没有做

838
00:27:10,160 --> 00:27:11,600
我们没有做fine tuning

839
00:27:11,600 --> 00:27:13,840
因为这个数据相对来说比较小

840
00:27:14,320 --> 00:27:15,960
三小成三小数据

841
00:27:16,440 --> 00:27:19,279
你imagine that是24x24出来了

842
00:27:19,519 --> 00:27:24,160
然后你做fine tuning的话

843
00:27:24,160 --> 00:27:25,079
你可以试一下

844
00:27:25,079 --> 00:27:26,160
fine tuning是有效果的

845
00:27:26,160 --> 00:27:29,319
首先我们在上一次大家有问

846
00:27:29,480 --> 00:27:30,799
医疗图片fine tuning

847
00:27:31,559 --> 00:27:34,360
就是说你试一下

848
00:27:34,360 --> 00:27:35,279
总是没错的

849
00:27:35,279 --> 00:27:36,680
很有可能不会给你

850
00:27:37,039 --> 00:27:38,160
很有可能不会给你变好

851
00:27:38,240 --> 00:27:40,000
但也通常也不会给你变差

852
00:27:40,120 --> 00:27:40,519
对吧

853
00:27:40,519 --> 00:27:43,360
因为反正就是说你变差没关系

854
00:27:43,360 --> 00:27:45,279
反正你就是因为它只是个初始点

855
00:27:45,279 --> 00:27:46,519
你就往下走就行了

856
00:27:46,519 --> 00:27:48,680
所以假设它对你没帮助

857
00:27:48,759 --> 00:27:49,839
你就多劝几次

858
00:27:49,839 --> 00:27:52,039
然后收敛到你自己要的东西

859
00:27:52,039 --> 00:27:53,079
假设对你有帮助

860
00:27:53,079 --> 00:27:54,799
你就是少劝几次

861
00:27:54,799 --> 00:27:56,200
学习率弄小一点

