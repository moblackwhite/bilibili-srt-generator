1
00:00:00,000 --> 00:00:02,000
问题九

2
00:00:02,000 --> 00:00:06,600
实话和卷迹是不是更适合图片分类的数据

3
00:00:06,600 --> 00:00:09,599
对时序数据是不是不类

4
00:00:09,599 --> 00:00:12,400
不用这类数据做分类

5
00:00:12,400 --> 00:00:15,400
就是说我理解是说你要想做时序的东西

6
00:00:15,400 --> 00:00:17,800
时序是可以用卷迹的

7
00:00:17,800 --> 00:00:19,000
这个是没问题

8
00:00:19,000 --> 00:00:20,800
就一维卷迹

9
00:00:20,800 --> 00:00:24,400
就说时序我们之后会讲一个文本分类的应用

10
00:00:24,400 --> 00:00:26,800
就是时序和文本分类有点类似

11
00:00:26,800 --> 00:00:28,199
文本就是一个一条线

12
00:00:28,199 --> 00:00:29,199
时序也是一条线

13
00:00:30,199 --> 00:00:33,399
然后至于用不用

14
00:00:33,399 --> 00:00:36,399
持画这个东西呢

15
00:00:37,399 --> 00:00:38,200
其实

16
00:00:39,600 --> 00:00:41,000
可以用可以不用

17
00:00:41,000 --> 00:00:42,600
我觉得一般大家还是用一下

18
00:00:42,600 --> 00:00:44,399
持画本身用了也没什么坏处

19
00:00:44,399 --> 00:00:45,799
就是这样子

20
00:00:45,799 --> 00:00:47,399
你可能不用也没关系

21
00:00:47,399 --> 00:00:49,200
就是说我觉得持画

22
00:00:49,200 --> 00:00:52,600
但我很难说的要用还是不用

23
00:00:53,000 --> 00:00:54,799
我知道有人用有知道有人不用

24
00:00:54,799 --> 00:00:57,200
所以可能自己试一下看看效果

25
00:01:00,200 --> 00:01:04,400
问题是调个卷迹层通道数增加到16

26
00:01:04,400 --> 00:01:06,400
意味着信息被放大了吗

27
00:01:06,400 --> 00:01:09,600
或者说这些信息在通道中是什么流通的

28
00:01:09,600 --> 00:01:11,799
就是说

29
00:01:12,200 --> 00:01:16,400
你可以认为一般来说的话

30
00:01:16,400 --> 00:01:17,400
我的

31
00:01:17,800 --> 00:01:25,400
通常的做法是当我的高宽减半的时候

32
00:01:25,400 --> 00:01:27,000
我会把通道数

33
00:01:27,600 --> 00:01:28,800
加两倍

34
00:01:29,400 --> 00:01:30,799
就整体是会变小了

35
00:01:30,799 --> 00:01:34,000
因为整体信息你得压到最后的

36
00:01:34,000 --> 00:01:36,000
最后10类或者100类或者1000类对吧

37
00:01:36,000 --> 00:01:37,799
整体你得把信息的压缩

38
00:01:37,799 --> 00:01:40,200
一般来压法是说

39
00:01:40,200 --> 00:01:42,600
高宽减半的同时把通道数翻倍

40
00:01:43,400 --> 00:01:45,599
就你直观上可以理解是说

41
00:01:46,800 --> 00:01:49,400
我把一些就是说

42
00:01:49,400 --> 00:01:51,600
我为什么能够把高宽减半呢

43
00:01:51,600 --> 00:01:52,800
因为你的高宽

44
00:01:52,800 --> 00:01:54,800
你的输入里面每一个像素还是说

45
00:01:54,800 --> 00:01:56,800
我一个模式的和某一个模式

46
00:01:57,000 --> 00:01:58,400
就是说每一个输出

47
00:01:58,400 --> 00:01:59,600
是某一个特定的

48
00:01:59,600 --> 00:02:01,400
对某个特定的模式的识别

49
00:02:01,600 --> 00:02:03,600
但是我当我减少的时候

50
00:02:03,600 --> 00:02:05,800
我是说我能够通过

51
00:02:05,800 --> 00:02:07,000
为什么能减少

52
00:02:07,000 --> 00:02:08,599
减少是因为你肯定损失东西了

53
00:02:08,599 --> 00:02:09,000
对吧

54
00:02:09,000 --> 00:02:11,200
以前我用4个像素表示一个东西

55
00:02:11,200 --> 00:02:12,800
现在我变成一个像素了

56
00:02:12,800 --> 00:02:14,200
那么为什么是减

57
00:02:14,200 --> 00:02:17,200
我要减少是因为我增加了通道数

58
00:02:17,200 --> 00:02:19,400
使得我能够匹配的模型

59
00:02:19,400 --> 00:02:21,000
那些模式变多了

60
00:02:21,000 --> 00:02:22,599
而且可能变得更大了

61
00:02:22,599 --> 00:02:25,599
所以就是说同样一个像素

62
00:02:25,799 --> 00:02:29,400
表示的信息变增加了

63
00:02:29,400 --> 00:02:30,799
那最后的最后一个像素

64
00:02:30,799 --> 00:02:32,199
你最后那一层一个像素

65
00:02:32,199 --> 00:02:33,120
表示一个类别

66
00:02:33,120 --> 00:02:33,400
对吧

67
00:02:33,400 --> 00:02:35,479
那个像那也是一个一只猫一只狗

68
00:02:35,479 --> 00:02:36,199
一个数字

69
00:02:36,199 --> 00:02:38,400
它当然表示的信息量是非常大的

70
00:02:39,000 --> 00:02:41,799
所以说你的

71
00:02:41,799 --> 00:02:43,680
你不是说信息量被放大了

72
00:02:43,680 --> 00:02:45,400
就是说你的输入输出

73
00:02:45,599 --> 00:02:47,120
就是你的高宽减少了

74
00:02:47,120 --> 00:02:48,319
你的通道数增加了

75
00:02:48,319 --> 00:02:51,599
实际上整体你应该还是在往下减的一个过程

76
00:02:51,799 --> 00:02:52,599
OK

77
00:02:53,599 --> 00:02:55,599
问题11为什么要view

78
00:02:55,599 --> 00:02:57,000
而不是用reshape

79
00:02:57,000 --> 00:03:01,400
view和reshape在很多时候没有什么区别

80
00:03:01,400 --> 00:03:04,400
而view和reshape唯一不一样的是说

81
00:03:04,400 --> 00:03:06,000
view是说当你

82
00:03:06,000 --> 00:03:08,000
view就是对你的数据

83
00:03:08,000 --> 00:03:10,599
它的构造不会发生任何变化

84
00:03:10,599 --> 00:03:12,199
就是说我还是那一坨东西

85
00:03:12,199 --> 00:03:13,799
view就是说我看它

86
00:03:13,799 --> 00:03:15,400
就是说在类似是一块

87
00:03:15,400 --> 00:03:18,400
然后我view是说我可以把它放成一个

88
00:03:18,400 --> 00:03:20,599
把它表示什么形状

89
00:03:20,599 --> 00:03:21,599
我是可以表示的

90
00:03:21,799 --> 00:03:25,199
所以但是如果你是reshape的话

91
00:03:25,199 --> 00:03:27,680
它可以是如果是按行存的话

92
00:03:27,680 --> 00:03:29,000
reshape它可以把你

93
00:03:29,000 --> 00:03:31,400
做一些数据的copy出来

94
00:03:31,400 --> 00:03:32,599
但view是不行的

95
00:03:32,599 --> 00:03:37,199
所以reshape它理论上是比view的功能更高一点

96
00:03:37,199 --> 00:03:37,799
可以处理

97
00:03:37,799 --> 00:03:39,400
它会帮你挪一点点数据

98
00:03:39,400 --> 00:03:42,599
有时候但是反过来讲

99
00:03:42,599 --> 00:03:46,199
它可能会比你慢

100
00:03:46,199 --> 00:03:47,599
在绝大部分情况来讲

101
00:03:47,599 --> 00:03:49,000
我们很少用那种

102
00:03:49,599 --> 00:03:52,000
就说你数据可能是用stripe的存的时候

103
00:03:52,000 --> 00:03:54,199
就是说先把一个数组

104
00:03:54,199 --> 00:03:56,599
然后一列三列五列拿出来

105
00:03:56,599 --> 00:03:57,800
然后再用view的时候

106
00:03:57,800 --> 00:03:58,400
你可能用不了了

107
00:03:58,400 --> 00:03:59,199
只能用reshape

108
00:03:59,199 --> 00:04:00,800
它会把你数据重新copy一次

109
00:04:00,800 --> 00:04:04,199
所以我觉得实际使用中没有什么太多区别

110
00:04:04,199 --> 00:04:05,599
reshape和view都一样

111
00:04:09,599 --> 00:04:12,199
选择深度学习模型适用

112
00:04:12,199 --> 00:04:15,800
MLP还是CNN是一种式的形态来选择

113
00:04:15,800 --> 00:04:17,399
还是说理论上推导

114
00:04:17,399 --> 00:04:20,000
然后在新德通再选一个模型

115
00:04:20,000 --> 00:04:21,399
理论推导就不要来了

116
00:04:21,399 --> 00:04:22,599
我们没有理论

117
00:04:25,399 --> 00:04:27,199
我的个人看法

118
00:04:27,199 --> 00:04:30,599
当你的数据不大的时候

119
00:04:30,599 --> 00:04:32,399
就说几百位几千位的时候

120
00:04:32,399 --> 00:04:33,799
你用MLP还是可以的

121
00:04:33,799 --> 00:04:35,599
你先用MLP没问题

122
00:04:35,599 --> 00:04:39,199
当你是输入数据是很大的图片的话

123
00:04:39,199 --> 00:04:40,599
200x200的图片的话

124
00:04:40,599 --> 00:04:42,599
MLP就没戏了

125
00:04:42,599 --> 00:04:45,000
我们有讲过为什么要做卷迹

126
00:04:45,000 --> 00:04:46,199
我们在上个星期

127
00:04:46,199 --> 00:04:48,000
是因为当你的输入变大的时候

128
00:04:48,000 --> 00:04:50,000
你的MLP基本上就over fitting了

129
00:04:50,000 --> 00:04:51,599
而且太大了做不了

130
00:04:51,599 --> 00:04:54,800
所以如果你能用MLP

131
00:04:54,800 --> 00:04:56,800
又能用CNN的话

132
00:04:56,800 --> 00:04:58,399
你可以试一下MLP没问题

133
00:04:58,399 --> 00:04:59,399
MLP快呀

134
00:05:01,000 --> 00:05:03,800
如果说有一些东西你用不了MLP

135
00:05:03,800 --> 00:05:05,599
大图片的输入你没办法

136
00:05:05,599 --> 00:05:07,199
ImageNet用不了MLP

137
00:05:07,199 --> 00:05:08,399
所以你只能用CNN

138
00:05:08,399 --> 00:05:10,399
这就是你没有别的选择

139
00:05:10,399 --> 00:05:11,399
就是这个意思

140
00:05:11,399 --> 00:05:13,199
没有什么理论不一样

141
00:05:13,399 --> 00:05:14,000
就是说

142
00:05:15,199 --> 00:05:16,399
就看实用

143
00:05:16,399 --> 00:05:19,399
一你得跑得动对吧

144
00:05:19,399 --> 00:05:20,800
渗透学习第一核心任务

145
00:05:20,800 --> 00:05:22,399
你得跑得动

146
00:05:22,399 --> 00:05:23,000
你跑不动了

147
00:05:23,000 --> 00:05:24,599
你再厉害也没用

148
00:05:24,599 --> 00:05:26,199
所以跑得动的情况下

149
00:05:27,800 --> 00:05:28,599
挑个简单的

150
00:05:30,000 --> 00:05:31,800
然后简单不行再想复杂的

151
00:05:31,800 --> 00:05:32,800
都可以从简单开始

152
00:05:37,599 --> 00:05:39,199
问题136到116

153
00:05:39,199 --> 00:05:43,000
16个通道是怎么去前面六个通道的信息的

154
00:05:43,000 --> 00:05:45,600
对我们上个我们昨天上周也讲过

155
00:05:45,600 --> 00:05:46,800
但是复习一下

156
00:05:46,800 --> 00:05:50,199
多通道卷积层是怎么作用的

157
00:05:50,199 --> 00:05:53,399
实际上是说每一个16通道的每一个通道

158
00:05:53,399 --> 00:05:56,199
他都会对前面六个通道的卷积作为是

159
00:05:56,199 --> 00:05:58,800
通道做信息做融合

160
00:05:58,800 --> 00:06:02,399
所以就是按组合取了加全了

161
00:06:02,399 --> 00:06:06,800
所以他不是他是一个全连接的意思是关系

162
00:06:07,800 --> 00:06:13,800
问题14输出通道的增加增加的是什么

163
00:06:13,800 --> 00:06:16,800
你可以认为我们也上次也讲过输出通道

164
00:06:16,800 --> 00:06:18,600
就是说多输多输出的

165
00:06:18,600 --> 00:06:21,199
那也大家可以去看一下回避震上视频

166
00:06:21,199 --> 00:06:22,400
大家可以回顾一下

167
00:06:22,400 --> 00:06:24,000
就是说输出通道

168
00:06:24,000 --> 00:06:29,199
你很很大程度可以认为是匹配的某一种特定的模式

169
00:06:29,199 --> 00:06:31,800
包括了比如说如果你是手写数字

170
00:06:31,800 --> 00:06:33,000
包括了比如笔触

171
00:06:33,000 --> 00:06:35,199
这个弯这个弯那个勾

172
00:06:35,199 --> 00:06:38,600
那种横线竖线或者纹理这种东西

173
00:06:38,600 --> 00:06:40,800
就每个通道

174
00:06:40,800 --> 00:06:43,600
可能是匹配某一个局部的一个特征

175
00:06:43,600 --> 00:06:46,399
一些纹理颜色什么样的信息

176
00:06:46,399 --> 00:06:51,399
越下层你的你这些匹配的模式越低层越高层

177
00:06:51,399 --> 00:06:53,800
当然就越越越大

178
00:06:53,800 --> 00:06:56,199
那些可能是一个一个圈

179
00:06:56,199 --> 00:06:58,600
一个一个整个一大块什么东西

180
00:06:59,600 --> 00:07:01,800
所以啊

181
00:07:01,800 --> 00:07:05,200
你可认为是就我们希望你给你更多通道

182
00:07:05,200 --> 00:07:06,800
数时的你能识别信息

183
00:07:06,800 --> 00:07:10,000
但具体你识别什么样的pattern是根据你

184
00:07:10,000 --> 00:07:11,800
那是你的网络的事情

185
00:07:11,800 --> 00:07:13,600
我们有讲过说啊

186
00:07:13,600 --> 00:07:16,200
你看我们设计了一个最简单的神经网络

187
00:07:16,200 --> 00:07:18,000
去识别一个啊

188
00:07:18,000 --> 00:07:21,800
数的那个那个那个一条一条边对吧

189
00:07:21,800 --> 00:07:24,600
我们上星期有讲过这个事情

190
00:07:24,600 --> 00:07:29,400
问题15

191
00:07:29,400 --> 00:07:33,400
输出的通道数的选择有什么理由吗

192
00:07:33,400 --> 00:07:37,200
如果讲输出的6和16改成5和15可以吗

193
00:07:37,200 --> 00:07:38,400
哎可以的

194
00:07:38,400 --> 00:07:39,800
你可试一下啊

195
00:07:39,800 --> 00:07:43,000
大家去试一下

196
00:07:43,000 --> 00:07:44,300
我不知道多少人去试过了

197
00:07:44,300 --> 00:07:46,300
就每次问的每次碰到这个问题

198
00:07:46,300 --> 00:07:47,400
我都是说大家试一下

199
00:07:47,400 --> 00:07:48,800
但我可以给大家演示一下

200
00:07:48,800 --> 00:07:49,600
你可以试啊

201
00:07:49,600 --> 00:07:50,200
很简单啊

202
00:07:50,200 --> 00:07:51,600
这东西啊

203
00:07:51,600 --> 00:07:52,600
你怎么试呢

204
00:07:52,600 --> 00:07:54,400
就是跑到最前面啊

205
00:07:55,000 --> 00:07:56,200
就此划成

206
00:07:57,000 --> 00:07:58,000
跑这里啊

207
00:08:00,000 --> 00:08:01,000
你不改吗

208
00:08:01,000 --> 00:08:02,200
这不是很简单吗

209
00:08:05,000 --> 00:08:07,200
啊要改改5对吧

210
00:08:08,200 --> 00:08:10,600
然后你这要改15对吧

211
00:08:11,400 --> 00:08:12,600
这拍照是不好的地方

212
00:08:12,600 --> 00:08:14,400
你这里还得改15

213
00:08:14,400 --> 00:08:16,000
你得改两个地方

214
00:08:16,000 --> 00:08:17,400
啊这里你还得改5

215
00:08:17,400 --> 00:08:18,600
啊得改两个地方

216
00:08:19,100 --> 00:08:20,800
所以我其实不是很习惯这个事情

217
00:08:21,700 --> 00:08:23,000
啊我们再重新跑一下

218
00:08:25,400 --> 00:08:26,700
然后我们看到最后啊

219
00:08:31,200 --> 00:08:35,000
就记得我们刚刚是0.828对吧

220
00:08:35,000 --> 00:08:37,600
我改成5和15看一下大家有什么区别啊

221
00:08:43,400 --> 00:08:44,700
没太多区别对吧

222
00:08:44,700 --> 00:08:45,900
就是感觉上啊

223
00:08:55,399 --> 00:08:57,600
也还是没有

224
00:08:57,600 --> 00:08:59,100
因为你模型变小了嘛

225
00:08:59,100 --> 00:09:00,799
那over fitting的概率更低了

226
00:09:00,799 --> 00:09:03,100
所以你没有看到over fitting在这个地方

227
00:09:03,100 --> 00:09:05,299
然后但可能会under fitting

228
00:09:05,299 --> 00:09:07,000
就说你通道数变少了对吧

229
00:09:08,299 --> 00:09:08,699
哎

230
00:09:09,399 --> 00:09:11,100
所以最后的最后

231
00:09:12,299 --> 00:09:16,299
最后的最后测试进度突然出现了一点下降

232
00:09:16,299 --> 00:09:18,299
但我觉得是随机数造成的

233
00:09:18,299 --> 00:09:21,199
因为他training accuracy没变

234
00:09:21,199 --> 00:09:23,199
就是说你测试进度变成0.80

235
00:09:23,300 --> 00:09:26,200
但我觉得就是那么最后一个地方出现了问题

236
00:09:26,200 --> 00:09:27,300
我可以重新再跑一下

237
00:09:31,100 --> 00:09:32,200
等会我们再回来看啊

238
00:09:32,200 --> 00:09:36,200
我感觉其实可能啊问题不大

239
00:09:36,600 --> 00:09:38,200
你调调餐调调none rate

240
00:09:38,200 --> 00:09:40,400
说不定大家都长差不多啊

241
00:09:40,800 --> 00:09:41,600
但是啊

242
00:09:41,900 --> 00:09:42,800
所以这个东西啊

243
00:09:42,800 --> 00:09:45,100
这个东西你不能说我说是什么是什么

244
00:09:45,100 --> 00:09:47,300
这个东西我说都没用

245
00:09:47,300 --> 00:09:49,900
最后的最后就是说你得去看这个数据啊

246
00:09:49,900 --> 00:09:51,600
所以这就是调餐了啊

247
00:09:51,900 --> 00:09:52,300
所以

248
00:09:53,800 --> 00:09:54,700
改成5和15

249
00:09:54,700 --> 00:09:57,200
只要说你在理论上能跑啊

250
00:09:57,200 --> 00:09:59,400
不会给你造成啊数据

251
00:09:59,400 --> 00:10:01,300
比如说形状不对的话

252
00:10:01,300 --> 00:10:02,400
你是可以改的

253
00:10:02,400 --> 00:10:05,300
而且通常我改一下可能是问题不大了

254
00:10:05,300 --> 00:10:06,900
而且你大家都去试一试

255
00:10:06,900 --> 00:10:08,200
你当时说你改5乘15

256
00:10:08,200 --> 00:10:10,800
你改成3和8怎么样对吧

257
00:10:10,800 --> 00:10:12,800
你把6和16翻倍怎么样

258
00:10:12,800 --> 00:10:14,300
就改成12和32怎么样

259
00:10:14,300 --> 00:10:15,700
大家一定去试一下

260
00:10:15,700 --> 00:10:16,600
就改5乘5

261
00:10:16,600 --> 00:10:18,900
5乘15这个嗯改没改

262
00:10:18,900 --> 00:10:20,100
我觉得差不多

263
00:10:20,100 --> 00:10:22,200
所以你调餐一定是先多调一点

264
00:10:22,200 --> 00:10:23,400
然后再慢慢的细调

265
00:10:24,500 --> 00:10:25,000
OK

266
00:10:27,100 --> 00:10:27,600
啊

267
00:10:28,500 --> 00:10:29,500
应该跑完了吧

268
00:10:30,200 --> 00:10:31,000
回去看一下

269
00:10:37,300 --> 00:10:39,500
所以确实5乘15还是会低一点啊

270
00:10:39,500 --> 00:10:41,100
0.8

271
00:10:41,100 --> 00:10:43,300
所以刚刚我们是0.82对吧

272
00:10:43,500 --> 00:10:44,600
所以刚刚0.82

273
00:10:44,600 --> 00:10:45,800
我不知道是不是运气啊

274
00:10:45,800 --> 00:10:47,300
这个东西啊

275
00:10:48,300 --> 00:10:52,400
所以这个最大的问题是

276
00:10:52,400 --> 00:10:54,900
你可能要重复跑个几次啊

277
00:10:54,900 --> 00:10:56,400
当然你跑一次也差不多

278
00:10:56,400 --> 00:10:57,700
那我就现在我们先假设

279
00:10:57,700 --> 00:10:59,700
因为这个地方我觉得不那么合理

280
00:10:59,700 --> 00:11:04,500
是因为为什么他0.82和0.80之间有一点点的overfeeling

281
00:11:04,500 --> 00:11:06,500
但这个overfeeling其实挺小的

282
00:11:06,500 --> 00:11:08,200
你可以看到这个图这个地方啊

283
00:11:08,200 --> 00:11:08,900
但他确实有

284
00:11:08,900 --> 00:11:10,600
我觉得挺好奇的

285
00:11:10,600 --> 00:11:11,500
因为你模型变小了

286
00:11:11,500 --> 00:11:13,000
为什么还有啊

287
00:11:13,000 --> 00:11:14,300
刚刚啊

288
00:11:14,300 --> 00:11:15,860
模型更复杂还没有了

289
00:11:15,860 --> 00:11:16,560
但反过来讲

290
00:11:16,560 --> 00:11:18,360
就是说training accuracy大概都差不多

291
00:11:18,360 --> 00:11:21,260
就那个6乘16稍微高那么一点点啊

292
00:11:21,260 --> 00:11:22,660
测试进度确实我们现在低一些

293
00:11:23,660 --> 00:11:24,060
OK

294
00:11:24,060 --> 00:11:26,260
就这样或者说你把前面那个参数再重新跑一下

295
00:11:26,260 --> 00:11:27,660
看看是不是还这样

296
00:11:27,660 --> 00:11:29,360
你可能重复个5次开

297
00:11:29,360 --> 00:11:31,360
一般来说我们去啊

298
00:11:31,360 --> 00:11:32,760
如果你有时间的话

299
00:11:32,760 --> 00:11:34,860
我们会去像这种小数据机啊

300
00:11:34,860 --> 00:11:35,960
我们会重复跑5次

301
00:11:35,960 --> 00:11:36,660
然后去平均

302
00:11:38,160 --> 00:11:38,360
对吧

303
00:11:38,360 --> 00:11:38,960
就啊

304
00:11:38,960 --> 00:11:41,460
就我们加auto-grown那个视频有讲过

305
00:11:41,460 --> 00:11:42,460
这是backing

306
00:11:42,460 --> 00:11:45,360
因为你每次你的初始值那个种子不一样

307
00:11:45,460 --> 00:11:46,360
导致你会不一样

308
00:11:47,659 --> 00:11:47,860
OK

309
00:11:49,659 --> 00:11:51,159
所以啊

310
00:11:51,159 --> 00:11:53,259
就5乘15我觉得没什么特别好调

311
00:11:53,259 --> 00:11:57,360
大家去调调3乘8或者是12乘32或者怎么样

312
00:11:57,360 --> 00:11:59,860
所以这个大家可以去试一下

313
00:11:59,860 --> 00:12:00,960
所以啊

314
00:12:00,960 --> 00:12:03,159
我们这一讲课肯定不可能把你所有都调了

315
00:12:03,159 --> 00:12:05,960
你说你自己有时间不调调啊

316
00:12:05,960 --> 00:12:07,159
这东西跑起来挺快的

317
00:12:09,560 --> 00:12:10,460
问题16啊

318
00:12:10,460 --> 00:12:11,159
面试题吗

319
00:12:11,159 --> 00:12:12,659
这个是面试题吗

320
00:12:12,659 --> 00:12:13,659
我我倒是

321
00:12:16,259 --> 00:12:17,659
我觉得

322
00:12:18,659 --> 00:12:19,659
面试可能会

323
00:12:19,659 --> 00:12:23,860
我觉得面试不会让你背下来某一个Nernet的参数吧

324
00:12:26,560 --> 00:12:27,759
就好处是什么呀

325
00:12:27,759 --> 00:12:32,659
好处是说Nernet是一个比较奇怪的一个网络

326
00:12:32,659 --> 00:12:35,060
就说你6和16到底为什么6和16对吧

327
00:12:35,060 --> 00:12:36,759
这个数字好奇怪

328
00:12:36,759 --> 00:12:40,860
就说没什么感觉没什么太多关系对吧

329
00:12:40,860 --> 00:12:42,259
所以好处是说

330
00:12:42,259 --> 00:12:44,860
你到越后面你的网络

331
00:12:46,360 --> 00:12:49,559
层数越多几百层其实背起来更简单

332
00:12:49,559 --> 00:12:51,960
所以你到后面的经典网络

333
00:12:51,960 --> 00:12:53,860
你还真是可以背下来的

334
00:12:53,860 --> 00:12:56,460
虽然我觉得背下来可能意义不大

335
00:12:56,460 --> 00:12:58,259
就你要了解是说啊

336
00:12:58,259 --> 00:13:03,460
整个图片进来确实它的形状是在怎么样去压缩它

337
00:13:03,460 --> 00:13:05,159
就是我们整个基本上

338
00:13:05,159 --> 00:13:08,259
你可以认为每一层就是把一个啊

339
00:13:08,259 --> 00:13:09,259
你先不说信息吧

340
00:13:09,259 --> 00:13:11,960
就每一层把一个特定输入的形状的东西

341
00:13:11,960 --> 00:13:15,259
压成一个特定输出形状的东西

342
00:13:15,259 --> 00:13:18,159
你要去理解他是怎么样做这个压缩操作的

343
00:13:18,159 --> 00:13:18,960
就说特别

344
00:13:18,960 --> 00:13:23,559
所以为什么我们要打出来每一个层之间的输入的变化

345
00:13:23,559 --> 00:13:25,559
这个其实说你大概知道

346
00:13:25,559 --> 00:13:27,860
大概理解它的pattern是怎么work的就行了

347
00:13:27,860 --> 00:13:29,460
因为你在实际中的话

348
00:13:29,460 --> 00:13:31,860
可能你用的图片的输入的大小跟你

349
00:13:31,860 --> 00:13:32,659
跟我们测试

350
00:13:32,659 --> 00:13:34,360
跟我们数据体是不一样的

351
00:13:34,360 --> 00:13:35,159
在情况下

352
00:13:35,159 --> 00:13:37,259
那么你把一个网络套过去

353
00:13:37,259 --> 00:13:39,159
可能你要做适当的调整

354
00:13:43,860 --> 00:13:45,659
问题是7啊

355
00:13:45,659 --> 00:13:46,960
Max还是Average

356
00:13:46,960 --> 00:13:48,559
Max会不会损失很多信息

357
00:13:48,559 --> 00:13:49,960
Average也损失信息

358
00:13:49,960 --> 00:13:50,659
对吧

359
00:13:50,659 --> 00:13:53,159
就是说用Max的意思是说

360
00:13:53,159 --> 00:13:56,059
我只关心你最大的那个信号量

361
00:13:57,559 --> 00:14:01,259
就我关心里面有没有有没有狗

362
00:14:01,259 --> 00:14:04,860
如果你个如果你就是说有没有狗

363
00:14:04,860 --> 00:14:06,460
这个信息怎么来了

364
00:14:06,460 --> 00:14:08,860
就是说我就最大的那个就行了

365
00:14:08,860 --> 00:14:11,960
我只关心里面某一个pattern是不是出现了

366
00:14:11,960 --> 00:14:13,759
如果出现他的指数很大

367
00:14:13,759 --> 00:14:14,360
对不对

368
00:14:14,360 --> 00:14:17,060
那我就把这个最大指数拎出来就行了

369
00:14:17,060 --> 00:14:20,560
所以Max不见得比Average损失信息

370
00:14:20,560 --> 00:14:23,759
Max pooling其实比Average pooling训练起来更好一点

371
00:14:23,759 --> 00:14:25,060
因为它的数值更大一点

372
00:14:28,759 --> 00:14:29,460
问题是8

373
00:14:29,460 --> 00:14:32,560
Nernet当年是用什么语言实现的呢

374
00:14:32,560 --> 00:14:35,160
Nernet是用Nuwa实现的

375
00:14:35,159 --> 00:14:36,059
就是Torch

376
00:14:36,059 --> 00:14:37,860
就是说大家你像我们讲PyTorch

377
00:14:37,860 --> 00:14:39,259
PyTorch为什么叫PyTorch

378
00:14:39,259 --> 00:14:41,559
它的前身叫Torch

379
00:14:41,559 --> 00:14:43,860
当年80年代啊

380
00:14:43,860 --> 00:14:46,259
但最早最早是用C++写的啊

381
00:14:46,259 --> 00:14:48,860
用C写的那个东西

382
00:14:48,860 --> 00:14:52,559
后来他们是Nercon他们用了Nuwa这个小语言

383
00:14:52,559 --> 00:14:53,559
Nuwa这个语言呢

384
00:14:53,559 --> 00:14:55,459
就Lua这个语言

385
00:14:55,459 --> 00:14:56,759
大家可能都没听说过

386
00:14:58,259 --> 00:15:01,559
一个很小众的一个脚本语言

387
00:15:01,559 --> 00:15:03,059
做游戏的人可能听说过

388
00:15:03,060 --> 00:15:05,660
这个是一个脚本语言

389
00:15:05,660 --> 00:15:10,660
就是现在几乎用的在游戏行业可能还在用

390
00:15:10,660 --> 00:15:14,360
但我们现在在数据科学就基本上不用了

391
00:15:14,360 --> 00:15:17,460
这个语言就是说当年他们那帮人啊

392
00:15:17,460 --> 00:15:20,760
他们有自己的品位

393
00:15:20,760 --> 00:15:23,160
所以觉得Nuwa高大上

394
00:15:23,160 --> 00:15:24,960
这个现在是说我说我会Python

395
00:15:24,960 --> 00:15:26,760
大家觉得Python啊

396
00:15:26,760 --> 00:15:27,960
就这样吧

397
00:15:27,960 --> 00:15:29,960
大家都会或者说我会PHP

398
00:15:29,960 --> 00:15:32,460
而我其实不会PHP

399
00:15:32,560 --> 00:15:33,759
但是你说你会PHP

400
00:15:33,759 --> 00:15:35,460
我就说So what

401
00:15:35,460 --> 00:15:35,960
但是呢

402
00:15:35,960 --> 00:15:39,160
你说我会用一个奇怪一点的语言

403
00:15:39,160 --> 00:15:39,860
比如说

404
00:15:42,460 --> 00:15:43,360
哎

405
00:15:43,360 --> 00:15:45,860
我都举不出那个一点点奇怪的语言了

406
00:15:48,660 --> 00:15:53,560
比如说当年Chris做了做Swift的那个人

407
00:15:53,560 --> 00:15:56,560
Swift也不算一个小众语言

408
00:15:56,560 --> 00:15:57,860
在苹果上用的挺多的

409
00:15:57,860 --> 00:15:59,259
他们在里面加了很多东西

410
00:15:59,259 --> 00:16:00,960
说我要做Machine Learning

411
00:16:01,960 --> 00:16:04,460
说我会用Swift做Machine Learning

412
00:16:04,460 --> 00:16:06,160
是不是很小众

413
00:16:06,160 --> 00:16:07,759
听上去很厉害

414
00:16:07,759 --> 00:16:09,660
当然还有还有那个什么

415
00:16:09,660 --> 00:16:11,460
MIT搞的那个语言叫

416
00:16:12,660 --> 00:16:13,160
哎呦

417
00:16:13,160 --> 00:16:15,060
你现在就不记得去了

418
00:16:15,060 --> 00:16:16,460
你可以认为是MATLAB的

419
00:16:16,460 --> 00:16:18,560
有点像MATLAB的那个语言

420
00:16:18,560 --> 00:16:23,660
我们我们之前就当年确实还是有很多种语言的啊

421
00:16:23,660 --> 00:16:27,060
但是现在大家都用Python居多了

422
00:16:27,160 --> 00:16:31,460
所以Nernet最早主要是Torch上的实现

423
00:16:31,460 --> 00:16:32,760
主要是用Lua

424
00:16:32,760 --> 00:16:36,260
当年还有一个框架叫Ciano

425
00:16:36,260 --> 00:16:39,460
也是Shabanger他们做的

426
00:16:39,460 --> 00:16:40,360
是用Python做的

427
00:16:41,260 --> 00:16:44,160
当年就剩这两个神经网络框架

428
00:16:44,160 --> 00:16:46,160
后来他们两个都拉了图名下了

429
00:16:49,760 --> 00:16:51,360
这一点小历史八卦

430
00:16:52,260 --> 00:16:52,860
问题19

431
00:16:52,860 --> 00:16:55,660
神经网络的准确率到多少算好的

432
00:16:56,360 --> 00:16:57,459
这个你不好说

433
00:16:57,459 --> 00:16:58,959
这个取决于数据

434
00:16:58,959 --> 00:17:03,759
在FastMList 0.82是一个很很很低的一个进度啊

435
00:17:03,759 --> 00:17:04,759
就是啊

436
00:17:04,759 --> 00:17:06,960
其实你可以当0.95

437
00:17:06,960 --> 00:17:08,759
我觉得FastMList可以做到0.95

438
00:17:08,759 --> 00:17:10,059
可能是可以做的

439
00:17:10,059 --> 00:17:12,759
0.95或者95以上都有可能

440
00:17:12,759 --> 00:17:13,960
但我也没仔细调过

441
00:17:13,960 --> 00:17:16,059
我觉得可能0.95

442
00:17:16,059 --> 00:17:20,359
所以这个depends说你的数据级的好坏

443
00:17:20,359 --> 00:17:21,259
你的数据级好的话

444
00:17:21,259 --> 00:17:22,359
你可以上很高

445
00:17:22,359 --> 00:17:23,159
数据级很差的话

446
00:17:23,159 --> 00:17:25,359
当然就就比较难了

447
00:17:26,260 --> 00:17:27,460
但这是一个很好的问题

448
00:17:27,460 --> 00:17:29,560
就是说你其实你根本就不知道

449
00:17:29,560 --> 00:17:30,660
你自己做的时候

450
00:17:30,660 --> 00:17:32,660
你如果你不用的标准数据机的话

451
00:17:32,660 --> 00:17:35,060
你根本就不知道准确率多少合适

452
00:17:35,060 --> 00:17:35,560
对吧

453
00:17:35,560 --> 00:17:36,460
你自己有数据机

454
00:17:36,460 --> 00:17:37,060
你怎么办

455
00:17:38,560 --> 00:17:40,360
所以这个就是说

456
00:17:40,360 --> 00:17:45,960
你很多时候你觉得个人感觉够用了就行了

457
00:17:45,960 --> 00:17:47,760
就是说你看一看效果

458
00:17:47,760 --> 00:17:48,860
就是说通常来说

459
00:17:48,860 --> 00:17:52,660
0.8几0.9几这个东西没什么直观的意义

460
00:17:52,660 --> 00:17:53,960
有意义的是说

461
00:17:53,960 --> 00:17:57,860
你去看一下真的预测的结果满不满足

462
00:17:57,860 --> 00:17:59,160
说你上线啊

463
00:17:59,160 --> 00:18:00,759
或者说在产品的需求

464
00:18:00,759 --> 00:18:03,360
就是说用户用这个东西会不会觉得够用了

465
00:18:03,360 --> 00:18:04,960
这个其实比较关键的

466
00:18:04,960 --> 00:18:06,160
因为你有一个预值

467
00:18:07,060 --> 00:18:08,460
就用户有个预值对吧

468
00:18:08,460 --> 00:18:09,759
就是说你语音识别好

469
00:18:09,759 --> 00:18:10,360
什么东西也好

470
00:18:10,360 --> 00:18:12,259
有个预值你低于这个预值

471
00:18:12,259 --> 00:18:13,759
大家觉得这个东西太难用了

472
00:18:13,759 --> 00:18:16,960
就是这个程序力太就用户体验就很差

473
00:18:16,960 --> 00:18:18,160
就我说句话啊

474
00:18:18,160 --> 00:18:19,560
你老识别不好啊

475
00:18:19,560 --> 00:18:21,360
但我说话我我不用语音识别

476
00:18:21,360 --> 00:18:23,160
因为我有口音

477
00:18:23,160 --> 00:18:24,460
然后你们谁都识别不准

478
00:18:24,460 --> 00:18:25,560
我英语你也识别不了

479
00:18:25,560 --> 00:18:26,560
我中文你也有问题

480
00:18:26,560 --> 00:18:28,860
所以我不怎么喜欢用语音识别

481
00:18:28,860 --> 00:18:30,960
是因为你没有达到我的预值

482
00:18:30,960 --> 00:18:33,160
我用语音识别你都是错了

483
00:18:33,160 --> 00:18:34,660
因为很尴尬

484
00:18:34,660 --> 00:18:35,260
所以呢

485
00:18:35,260 --> 00:18:36,960
但是你过了这个预值啊

486
00:18:36,960 --> 00:18:38,960
我觉得还行挺好的

487
00:18:38,960 --> 00:18:40,560
再高一点低一点也没关系了

488
00:18:40,560 --> 00:18:42,560
所以很多时候人心里有个预值

489
00:18:42,560 --> 00:18:44,560
所以你很多时候你对你来讲

490
00:18:44,560 --> 00:18:47,560
你在实际中你可能过了这个预值就行了

491
00:18:47,560 --> 00:18:48,560
你不需要做很高

492
00:18:48,960 --> 00:18:52,759
但你低于这个预值可能就没用

493
00:18:57,759 --> 00:18:59,359
问题二是为什么用了一层卷迹

494
00:18:59,359 --> 00:19:00,559
从一通道变成六通道

495
00:19:00,559 --> 00:19:02,559
是因为第一个卷迹层

496
00:19:02,559 --> 00:19:05,460
一是因为输入通道是一是个灰度图

497
00:19:05,460 --> 00:19:07,059
但第一层卷迹的输出是六

498
00:19:07,059 --> 00:19:08,659
所以大家回顾一下

499
00:19:08,659 --> 00:19:09,659
去B站啊

500
00:19:09,659 --> 00:19:11,759
去我们的课程主页啊

501
00:19:11,759 --> 00:19:15,159
如果大家是上上节课没来的话

502
00:19:15,159 --> 00:19:17,259
这是乔克的同学

503
00:19:17,359 --> 00:19:21,160
乔克的同学去一下我们的这个网页啊

504
00:19:21,160 --> 00:19:26,359
这个网页就是应该在我们的直播的窗口里面有点进去

505
00:19:26,359 --> 00:19:27,559
你看一下啊

506
00:19:27,559 --> 00:19:32,160
看一下多输入多输出通道的输

507
00:19:32,160 --> 00:19:34,059
教材也好

508
00:19:34,059 --> 00:19:35,160
slice也好

509
00:19:35,160 --> 00:19:35,960
notebook也好

510
00:19:35,960 --> 00:19:37,660
还是我们的B站的视频也好

511
00:19:37,660 --> 00:19:39,660
大家去复习一下这个东西

512
00:19:39,660 --> 00:19:41,160
六是怎么来的

513
00:19:41,160 --> 00:19:44,160
这个是上节要么没来

514
00:19:44,160 --> 00:19:46,059
要么不认真的同学的问题啊

515
00:19:48,259 --> 00:19:51,160
问题21

516
00:19:51,160 --> 00:19:53,759
autogrow是不是能做时序任务啊

517
00:19:55,759 --> 00:19:57,660
他能做是能做了

518
00:19:57,660 --> 00:20:00,359
就是就是他是用的

519
00:20:00,359 --> 00:20:01,559
他啊

520
00:20:01,960 --> 00:20:03,059
他能做是能做

521
00:20:03,059 --> 00:20:05,460
但是效果就是说你需要调一点点东西

522
00:20:05,460 --> 00:20:06,259
你得对模型

523
00:20:06,259 --> 00:20:09,559
我们没有做到很好的时序的就时序这一块呢

524
00:20:09,559 --> 00:20:12,160
属于数据太少了

525
00:20:12,160 --> 00:20:13,259
就不是很好做

526
00:20:13,259 --> 00:20:16,960
因为没有就大家都就时序预测预测股票

527
00:20:16,960 --> 00:20:17,960
当时时序测来

528
00:20:17,960 --> 00:20:21,960
时序测是一个大家呼声很旺的一个东西

529
00:20:21,960 --> 00:20:24,360
但是他没有特别好的数据集

530
00:20:24,360 --> 00:20:27,059
所以就不好就outermail就不是那么的好

531
00:20:27,059 --> 00:20:27,460
调餐

532
00:20:27,460 --> 00:20:31,059
所以我们没有只仔细去做时序任务

533
00:20:31,059 --> 00:20:35,360
未来也许我们会release一个做时序任务的一个一个一个

534
00:20:35,360 --> 00:20:37,059
或者支持的更好一点

535
00:20:37,059 --> 00:20:39,460
但现在确实做的一般啊

536
00:20:40,559 --> 00:20:41,960
你如果啊

537
00:20:41,960 --> 00:20:43,059
有特别想用的话

538
00:20:43,059 --> 00:20:45,460
你可以去autogrow的Github上留言

539
00:20:45,460 --> 00:20:46,960
就给issues留言说啊

540
00:20:46,960 --> 00:20:49,059
你是什么样的数据用怎么样

541
00:20:49,059 --> 00:20:52,960
确实可以稍微调几个东西可以让提升效果

542
00:20:52,960 --> 00:20:57,160
但整体我们确实不见得是完全针对时序做好的

543
00:21:00,259 --> 00:21:03,559
问题22能介绍一下com2d和com3d

544
00:21:03,559 --> 00:21:06,759
就3d和2d Maxpool 2d和3d的区别

545
00:21:06,759 --> 00:21:09,059
以及各使用的场景吗

546
00:21:09,059 --> 00:21:11,259
2d和3d我们有讲过

547
00:21:11,259 --> 00:21:13,559
就是嗯

548
00:21:13,559 --> 00:21:16,859
就大家去看一下我们前面的视频

549
00:21:16,859 --> 00:21:19,159
Maxpool pooling的2d 3d

550
00:21:19,159 --> 00:21:20,859
其实如果你用了com3d的话

551
00:21:20,859 --> 00:21:23,059
你的pooling当然也得用3d

552
00:21:23,059 --> 00:21:24,859
3d的是包括什么呀

553
00:21:24,859 --> 00:21:26,059
视频是3d对吧

554
00:21:26,960 --> 00:21:28,759
高宽就像是图片

555
00:21:28,759 --> 00:21:31,059
帧然后那个时时间轴啊

556
00:21:31,059 --> 00:21:34,359
你有你有很多帧就是就是一个3d的东西

557
00:21:34,359 --> 00:21:35,019
然后呢

558
00:21:35,039 --> 00:21:37,740
还有什么医学图片可能是3d的啊

559
00:21:37,740 --> 00:21:40,559
因为你和功能核实公证的时候

560
00:21:40,559 --> 00:21:42,819
你扫下去是说你每一个平面要扫一下

561
00:21:42,919 --> 00:21:43,919
你往下扫的时候

562
00:21:43,919 --> 00:21:45,000
你很多啊

563
00:21:45,000 --> 00:21:46,439
你有往下扫

564
00:21:46,439 --> 00:21:48,359
然后还有什么啊

565
00:21:48,359 --> 00:21:49,839
地球那一块啊

566
00:21:49,839 --> 00:21:51,200
就就卫星地图

567
00:21:51,220 --> 00:21:52,919
他可能会拍个时序的地图

568
00:21:53,919 --> 00:21:54,519
气象地图

569
00:21:54,519 --> 00:21:57,279
比如说气象地图是一个3d的啊

570
00:21:57,279 --> 00:22:00,559
所以就3d和2d唯一的区别是2d就是一张图片

571
00:22:00,579 --> 00:22:02,039
3d就是多一个通道

572
00:22:02,039 --> 00:22:03,039
多一个时序

573
00:22:03,039 --> 00:22:04,480
通常是一个时间轴在里面

574
00:22:05,679 --> 00:22:07,919
所以而卷积的2d3d

575
00:22:07,919 --> 00:22:08,720
大家可以去看一下

576
00:22:08,720 --> 00:22:10,960
我们之前有介绍啊

577
00:22:11,380 --> 00:22:14,299
普林凡的卷积样就是一个简化的卷积上

578
00:22:19,019 --> 00:22:20,940
这是一个预示过的同学

579
00:22:20,940 --> 00:22:23,019
是我们课程网站上练习的Sigmoid

580
00:22:23,019 --> 00:22:27,259
4个Sigmoid全部替换成几乎不收敛

581
00:22:27,279 --> 00:22:28,059
什么原因吗

582
00:22:29,700 --> 00:22:32,340
Sigmoid为什么替换不收敛啊

583
00:22:32,360 --> 00:22:32,940
真的吗

584
00:22:34,500 --> 00:22:37,500
你可能那你可能调了null rate吧

585
00:22:38,119 --> 00:22:43,000
我我我我以前调过这东西应该收敛的呀

586
00:22:43,019 --> 00:22:43,920
这东西为什么不收敛

587
00:22:46,400 --> 00:22:47,759
就是就是

588
00:22:51,599 --> 00:22:52,319
应该是这么

589
00:22:54,160 --> 00:22:54,559
是吧

590
00:22:56,599 --> 00:22:58,119
就是你就你就替换掉吧

591
00:23:00,119 --> 00:23:05,079
还有没有一个两个三个没有Sigmoid了吧

592
00:23:08,119 --> 00:23:09,920
就这么改完之后就跑吧

593
00:23:11,599 --> 00:23:12,880
我们等会再回来再看啊

594
00:23:14,839 --> 00:23:15,640
等会回来再看

595
00:23:19,680 --> 00:23:20,599
应该收敛吧

596
00:23:20,619 --> 00:23:22,759
你不收敛你调一下学习率没事的

597
00:23:24,839 --> 00:23:28,680
2024图像识别出来特征颜色形状可以打印出来吗

598
00:23:28,680 --> 00:23:31,799
或者怎么去找图网络到底学到什么

599
00:23:31,799 --> 00:23:32,160
哎

600
00:23:32,160 --> 00:23:34,240
这个还真有就Visualization

601
00:23:34,259 --> 00:23:34,880
就是说

602
00:23:35,020 --> 00:23:35,540
嗯

603
00:23:37,420 --> 00:23:38,980
比如说

604
00:23:41,180 --> 00:23:43,220
就是说这一块有研究做啊

605
00:23:43,220 --> 00:23:46,380
就是说你去看神经网络到底学到什么东西啊

606
00:23:46,400 --> 00:23:48,540
比如说我记得有个CIExplorer

607
00:23:49,900 --> 00:23:51,060
就是Georgia Tech

608
00:23:55,860 --> 00:23:56,940
Visualization

609
00:23:59,660 --> 00:24:02,420
Explainer

610
00:24:02,420 --> 00:24:04,060
就是说大家去看这个东西

611
00:24:04,059 --> 00:24:09,240
我觉得这个就是说他是一个Georgia Tech一个小哥做的做的挺好的

612
00:24:09,240 --> 00:24:10,240
我觉得挺好玩的

613
00:24:10,240 --> 00:24:12,279
就是说他这个Javascript的东西

614
00:24:12,279 --> 00:24:15,359
就是你可以看到是说他可以去看到每一个

615
00:24:18,119 --> 00:24:19,159
我们还有五分钟

616
00:24:19,179 --> 00:24:20,879
干脆给大家讲一下这个

617
00:24:20,879 --> 00:24:24,079
其实我们是根本就可以在课程里面讲

618
00:24:24,099 --> 00:24:25,000
不用放到QA里面

619
00:24:25,000 --> 00:24:28,839
就是说他可以看到这个里面是一个一张图片

620
00:24:28,839 --> 00:24:30,559
一个杯子进去啊

621
00:24:31,599 --> 00:24:32,279
放大一点

622
00:24:33,220 --> 00:24:37,019
就红绿蓝这三个通道进去啊

623
00:24:37,019 --> 00:24:42,339
就每一个你看到每一个就每一个通道里面学习的是什么东西

624
00:24:42,359 --> 00:24:46,299
就一个通道可以这个通道学习的是一个红色的啊

625
00:24:46,299 --> 00:24:47,099
杯子

626
00:24:47,099 --> 00:24:51,339
然后蓝色杯子这个东西可学习到一些这个边边角角需要多一点

627
00:24:51,339 --> 00:24:51,980
对吧

628
00:24:52,000 --> 00:24:53,859
然后当然还有一些很多

629
00:24:55,700 --> 00:24:57,220
这个就是说每个通道

630
00:24:57,240 --> 00:24:59,940
你看这个通道上面学的是一个红色的框框

631
00:24:59,940 --> 00:25:01,379
下面学的是个蓝色的啊

632
00:25:01,400 --> 00:25:04,440
这个东西可能是大家可以去看一下这个网址啊

633
00:25:04,440 --> 00:25:06,520
CNN explainer

634
00:25:06,520 --> 00:25:09,600
然后就说你可能看直播可能会啊

635
00:25:09,600 --> 00:25:11,320
分辨率低一点点

636
00:25:11,340 --> 00:25:14,480
但你基本上看到这个是卷积残低层输出

637
00:25:14,480 --> 00:25:16,000
然后是人路的输出

638
00:25:16,000 --> 00:25:16,400
然后呢

639
00:25:16,400 --> 00:25:18,400
接下来到第二个啊

640
00:25:18,420 --> 00:25:20,960
第二个卷积残不同的输出通道啊

641
00:25:20,960 --> 00:25:25,640
你看到基本上就是说他会识别一些不一样的pattern

642
00:25:25,640 --> 00:25:27,720
然后最后的最后当然是会不一样啊

643
00:25:27,720 --> 00:25:29,480
你也可以去换一个别的东西啊

644
00:25:29,480 --> 00:25:31,320
点个别的图片

645
00:25:31,319 --> 00:25:36,139
也看到卷积层之间没学到到底学到什么东西啊

646
00:25:37,220 --> 00:25:37,740
对吧

647
00:25:37,740 --> 00:25:38,579
大家可以去玩一下

648
00:25:38,579 --> 00:25:40,779
就是说增加一下直观的理解

649
00:25:42,819 --> 00:25:43,179
OK

650
00:25:46,779 --> 00:25:47,019
啊

651
00:25:47,039 --> 00:25:47,500
你说的对

652
00:25:47,519 --> 00:25:48,220
确实不收敛

653
00:25:48,220 --> 00:25:49,019
就不收敛的话

654
00:25:49,019 --> 00:25:50,659
我觉得那些rate太高了

655
00:25:50,659 --> 00:25:51,859
调成1.5吧

656
00:25:53,579 --> 00:25:54,980
就调成0.1看看呗

657
00:25:56,539 --> 00:25:57,859
基本上就是试试看嘛

658
00:26:01,819 --> 00:26:04,019
在跑得动的情况下

659
00:26:04,019 --> 00:26:06,539
中阶层尽量通道大吗

660
00:26:06,559 --> 00:26:07,419
那不一定啊

661
00:26:07,439 --> 00:26:08,460
大了会overfitting

662
00:26:10,259 --> 00:26:14,019
就像你你的多层感知机一样的

663
00:26:14,039 --> 00:26:15,859
你不能把隐藏层调的特别特别大

664
00:26:15,859 --> 00:26:18,460
那个就是啊

665
00:26:18,480 --> 00:26:22,500
你就中阶层的大小应该是跟你的数据的复杂度相关的

666
00:26:22,500 --> 00:26:25,259
大家可以回顾一下那个模型选择那一张

667
00:26:25,259 --> 00:26:26,299
我们是怎么讲的

668
00:26:26,299 --> 00:26:33,159
26目前卷积审计网络或者深度审计网络

669
00:26:33,159 --> 00:26:35,359
是不是都需要交到训练数据集

670
00:26:35,359 --> 00:26:37,200
是如果训练数据集体量很小

671
00:26:37,220 --> 00:26:38,359
是不是不适合深度学习

672
00:26:38,359 --> 00:26:40,519
是否有无间路或小数据

673
00:26:40,519 --> 00:26:42,639
深度学习

674
00:26:42,639 --> 00:26:46,240
不需要其实现在深度学习不需要很多数据集了

675
00:26:47,519 --> 00:26:49,359
我们会讲一个fine tuning

676
00:26:49,359 --> 00:26:51,879
就是一个微调的一个在之后我们会讲

677
00:26:51,879 --> 00:26:53,879
可能下下周

678
00:26:54,260 --> 00:26:56,220
现在的深度学习

679
00:26:56,220 --> 00:26:58,020
其实因为我们有大量的

680
00:26:58,020 --> 00:26:59,380
因为我们有大量训练好

681
00:26:59,400 --> 00:27:02,300
在很大数据上训练好的模型

682
00:27:02,320 --> 00:27:04,740
我们可以做到在真实的应用上

683
00:27:04,740 --> 00:27:06,580
我们可能不需要真的训练样本

684
00:27:06,600 --> 00:27:09,100
或者很少的训练样本就做的很好了

685
00:27:09,120 --> 00:27:13,940
这一块就我们这里讲的基本上是说从零开始学习

686
00:27:13,940 --> 00:27:17,180
但是在真实的产业应用的时候

687
00:27:17,180 --> 00:27:20,500
你不会真的就是从random开始学啊

688
00:27:20,520 --> 00:27:22,060
你你是有先验知识的

689
00:27:22,059 --> 00:27:25,879
你会在一个很大的数据上训练好一些模型

690
00:27:25,879 --> 00:27:27,039
把它作为起始点

691
00:27:27,039 --> 00:27:30,079
再去调到你的小数据上

692
00:27:30,099 --> 00:27:31,559
这样子你就不需要很多数据了

693
00:27:33,559 --> 00:27:34,279
27啊

694
00:27:34,299 --> 00:27:35,240
就是visualization

695
00:27:35,240 --> 00:27:36,919
visualization其实有挺多工作了

696
00:27:36,940 --> 00:27:39,399
这一块大家也是啊

697
00:27:39,399 --> 00:27:40,319
挺感兴趣的

698
00:27:40,319 --> 00:27:43,399
你可以去搜一搜啊

699
00:27:43,399 --> 00:27:45,480
我觉得这一块工作还挺多的

700
00:27:46,960 --> 00:27:47,440
但是呢

701
00:27:47,440 --> 00:27:47,960
反过来讲

702
00:27:47,980 --> 00:27:49,639
我觉得就是好看

703
00:27:49,659 --> 00:27:50,279
他没什么用

704
00:27:52,759 --> 00:27:54,039
制造怎么样

705
00:27:54,039 --> 00:27:56,359
就是说他就是尽量的就是说

706
00:27:56,379 --> 00:28:00,000
现在visualization就是尽量的去把你中间找一点东西出来

707
00:28:00,000 --> 00:28:00,799
让你看到哦

708
00:28:00,819 --> 00:28:01,720
原来是这样子

709
00:28:01,720 --> 00:28:04,599
但是不能指导你的生产

710
00:28:04,599 --> 00:28:07,039
就跟你的看好看

711
00:28:07,059 --> 00:28:07,799
但是没用

712
00:28:07,799 --> 00:28:09,159
就跟你的理论一样的

713
00:28:10,799 --> 00:28:15,359
问题28卷集网络把非结构的数据转化的结构

714
00:28:15,359 --> 00:28:16,399
后来数据解决问题

715
00:28:16,399 --> 00:28:17,759
这个想法可以在深度

716
00:28:17,819 --> 00:28:21,819
可以通用在深度

717
00:28:21,839 --> 00:28:24,779
我这不是理解非结构化的数据

718
00:28:24,779 --> 00:28:26,259
转换成结构化的数据

719
00:28:26,279 --> 00:28:28,980
他其实就你的图片是一个非结构化数据

720
00:28:28,980 --> 00:28:29,259
对吧

721
00:28:29,279 --> 00:28:31,460
这是一就是一堆二级的东西

722
00:28:31,480 --> 00:28:36,339
但是因为我是说我的卷机是一个本身是一个结构化的东西

723
00:28:36,359 --> 00:28:38,900
他就是一个结构化的权力阶层

724
00:28:38,900 --> 00:28:39,740
他是个卷机嘛

725
00:28:41,980 --> 00:28:45,339
但是说通常现在是说啊

726
00:28:45,340 --> 00:28:46,400
非结构化数据啊

727
00:28:46,420 --> 00:28:47,200
图片啊

728
00:28:47,200 --> 00:28:48,160
语音啊

729
00:28:48,180 --> 00:28:49,120
视频啊

730
00:28:49,120 --> 00:28:49,680
文本啊

731
00:28:49,700 --> 00:28:51,720
这东西你可以认为是非结构化的

732
00:28:51,720 --> 00:28:55,920
通过一个结构化的模型来抽取语音信息

733
00:28:55,920 --> 00:28:58,880
这个是深度神经网络做事情

734
00:28:58,900 --> 00:29:01,000
但新的前一点的东西就是说

735
00:29:01,000 --> 00:29:02,240
你可以做结构化的信息

736
00:29:02,260 --> 00:29:04,480
比如说一张图图我也能做

737
00:29:04,480 --> 00:29:09,600
现在你或者一棵树进来也能够也有针对做树的

738
00:29:09,600 --> 00:29:10,720
就是说

739
00:29:10,720 --> 00:29:12,200
如果你的数据是结构化的

740
00:29:12,220 --> 00:29:14,100
我也一样的可以用这个结构化的模型

741
00:29:14,120 --> 00:29:15,640
去拟合这个数据对吧

742
00:29:15,660 --> 00:29:16,840
这个没问题吧

743
00:29:16,840 --> 00:29:20,040
所以你可以认为是啊

744
00:29:20,040 --> 00:29:21,080
我的理解

745
00:29:21,080 --> 00:29:24,680
我的感觉就是说神经网络是一种语言

746
00:29:24,680 --> 00:29:27,600
你可以用这种语言来构造结构化的模型

747
00:29:27,600 --> 00:29:29,520
用他去拟合我们的数据啊

748
00:29:29,520 --> 00:29:32,280
不管数据是结构化还是非结构化的

749
00:29:32,280 --> 00:29:34,360
我可以把里面的语音信息给抽取出来

750
00:29:34,380 --> 00:29:35,320
这就是啊

751
00:29:35,340 --> 00:29:36,200
神经网络干的事情

752
00:29:37,760 --> 00:29:40,120
最后我们看一下我们收敛没有啊

753
00:29:40,140 --> 00:29:41,080
你看收敛了吧

754
00:29:42,059 --> 00:29:43,460
好我们的好

755
00:29:43,460 --> 00:29:45,460
我们就到0.825了啊

756
00:29:45,460 --> 00:29:46,419
就看一看啊

757
00:29:46,419 --> 00:29:50,379
就是用的还是之前那个通道数等于5和15

758
00:29:50,379 --> 00:29:53,419
现在我们从0.80变成0.825了

759
00:29:53,439 --> 00:29:55,059
当然我们overfeeding比较严重了

760
00:29:55,059 --> 00:29:58,419
就是我们现在有0.867到0.825啊

761
00:29:58,419 --> 00:30:00,319
当然我可以也许我可以通过调

762
00:30:00,319 --> 00:30:01,500
把那些rate再调一下

763
00:30:01,500 --> 00:30:02,579
可能会有好处

764
00:30:02,579 --> 00:30:03,779
但是啊

765
00:30:03,779 --> 00:30:04,980
大家自己去试啊

766
00:30:04,980 --> 00:30:07,259
我们今天就讲到这里

