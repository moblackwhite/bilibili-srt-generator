1
00:00:00,000 --> 00:00:02,240
问题14

2
00:00:03,080 --> 00:00:08,320
单层的输出是Y1 Y2和HT

3
00:00:08,320 --> 00:00:11,759
那么多层就是把它Y1作为输入吗

4
00:00:11,800 --> 00:00:13,880
那么每一层是不是有个H输出

5
00:00:13,880 --> 00:00:16,000
直接就不用了吗

6
00:00:16,399 --> 00:00:17,280
不是这样子的

7
00:00:17,280 --> 00:00:18,000
就是说

8
00:00:21,800 --> 00:00:24,519
我们给大家再重新可能讲太快了

9
00:00:24,519 --> 00:00:25,199
是不是

10
00:00:25,640 --> 00:00:27,000
给大家重新回忆一下这个事情

11
00:00:30,199 --> 00:00:30,920
Ok就这个图

12
00:00:36,920 --> 00:00:37,719
就是说

13
00:00:38,280 --> 00:00:39,519
单层的时候是怎么样

14
00:00:39,519 --> 00:00:42,519
单层是输入一个隐藏一个输出

15
00:00:42,840 --> 00:00:43,799
就是没有Y

16
00:00:45,079 --> 00:00:46,799
然后你的H的东西

17
00:00:47,120 --> 00:00:48,719
它是你记得我的

18
00:00:48,920 --> 00:00:51,439
我在输出层它的H那个东西

19
00:00:51,439 --> 00:00:53,760
它的输出层的它的输入

20
00:00:53,760 --> 00:00:55,120
就是你的隐藏状态

21
00:00:55,120 --> 00:00:56,359
就是H那个东西

22
00:00:57,600 --> 00:00:57,960
Ok

23
00:00:57,960 --> 00:00:59,880
所以我们可能也没

24
00:00:59,880 --> 00:01:00,960
可能大家没注意

25
00:01:01,079 --> 00:01:02,240
我们就讲得特别快

26
00:01:02,240 --> 00:01:02,960
就忽忽

27
00:01:02,960 --> 00:01:04,239
忽略掉这个东西了

28
00:01:04,239 --> 00:01:05,760
就是H这个东西

29
00:01:06,760 --> 00:01:08,920
又作为上一个输出层的输入

30
00:01:09,319 --> 00:01:11,719
又作为下一次的

31
00:01:11,719 --> 00:01:13,799
下一个时间的隐藏层的输入

32
00:01:13,840 --> 00:01:16,439
就是说你这个就这两个镜头是一个东西

33
00:01:19,480 --> 00:01:20,159
Ok

34
00:01:20,840 --> 00:01:23,359
所以然后你如果是做多层的话

35
00:01:23,799 --> 00:01:26,520
你是只有一个输出层

36
00:01:26,759 --> 00:01:28,079
所以你这个Y在这个地方

37
00:01:28,079 --> 00:01:29,239
其实这里我们写的是O

38
00:01:29,319 --> 00:01:30,280
其实Y是一样的

39
00:01:30,280 --> 00:01:31,439
在代码里面写的Y

40
00:01:31,560 --> 00:01:33,479
所以你这个东西

41
00:01:33,679 --> 00:01:34,759
所以中间都是隐藏的

42
00:01:34,759 --> 00:01:35,920
所以没有那个

43
00:01:35,920 --> 00:01:37,879
是其实是没有输出层的

44
00:01:38,359 --> 00:01:38,840
Ok

45
00:01:38,840 --> 00:01:40,039
所以所谓的多层

46
00:01:41,280 --> 00:01:42,719
因为我们是用的

47
00:01:43,120 --> 00:01:45,159
用的PyTorch的视线

48
00:01:47,079 --> 00:01:49,000
我们PyTorch大家记不记得

49
00:01:49,000 --> 00:01:50,560
我们PyTorch有讲过这个东西

50
00:01:50,759 --> 00:01:55,479
PyTorch的这个东西是不带输出层的

51
00:01:56,280 --> 00:01:57,439
然后我们的输出层

52
00:01:57,439 --> 00:01:58,920
是写在RNN model里面

53
00:01:58,920 --> 00:02:02,159
就跟我们的从零开始实现不一样

54
00:02:02,159 --> 00:02:04,920
零开始实现LS10是带了一个输出层

55
00:02:06,000 --> 00:02:06,640
Ok

56
00:02:07,320 --> 00:02:08,759
所以这个地方

57
00:02:08,759 --> 00:02:10,080
就是说你写的时候

58
00:02:10,080 --> 00:02:13,439
其实它只是给你的两个隐藏层

59
00:02:13,439 --> 00:02:14,759
然后你在RNN模型里面

60
00:02:14,759 --> 00:02:17,480
再自己再往上面加一个输出层过去了

61
00:02:18,080 --> 00:02:18,599
Ok

62
00:02:18,920 --> 00:02:22,120
所以这就是解释

63
00:02:26,479 --> 00:02:27,479
问题是我

64
00:02:27,479 --> 00:02:30,479
NLP哪个方向更容易找工作

65
00:02:30,679 --> 00:02:32,280
文本翻译除了学术研究

66
00:02:32,280 --> 00:02:34,719
是不是在NLP都不会去自己实现

67
00:02:34,719 --> 00:02:35,560
你说的对

68
00:02:35,560 --> 00:02:37,000
文本翻译这个事情

69
00:02:37,000 --> 00:02:38,359
大家已经很成熟了

70
00:02:38,399 --> 00:02:40,159
一般是不会做了

71
00:02:40,959 --> 00:02:41,799
但是反过来讲

72
00:02:41,799 --> 00:02:43,679
NLP我觉得挺好找工作的

73
00:02:43,839 --> 00:02:44,519
就是说

74
00:02:45,280 --> 00:02:46,839
跟文本打交道

75
00:02:47,199 --> 00:02:50,239
文本远远的多于图片

76
00:02:51,239 --> 00:02:53,079
就是说你的文章网上的东西

77
00:02:53,079 --> 00:02:54,560
文本是多于图片的

78
00:02:54,599 --> 00:02:55,840
所以文本像

79
00:02:56,360 --> 00:02:57,080
就是说你怎么说

80
00:02:57,599 --> 00:02:59,400
就一个人的

81
00:02:59,400 --> 00:03:01,240
就你聊微信也好

82
00:03:01,319 --> 00:03:02,159
聊天也好

83
00:03:02,280 --> 00:03:03,240
你写文章也好

84
00:03:03,240 --> 00:03:04,240
都是那个

85
00:03:04,360 --> 00:03:06,599
然后图片当然是你拍照是图片

86
00:03:06,680 --> 00:03:08,120
你可以拍视频是视频

87
00:03:08,159 --> 00:03:09,840
但是文本

88
00:03:09,840 --> 00:03:11,840
就每个人产生的文本

89
00:03:11,840 --> 00:03:12,719
确实是挺多的

90
00:03:12,719 --> 00:03:15,640
而且用文本交互

91
00:03:15,640 --> 00:03:17,199
还是目前的主流交互方式

92
00:03:17,199 --> 00:03:18,960
不管是公司写文档也好

93
00:03:18,960 --> 00:03:19,599
代码也文档

94
00:03:19,599 --> 00:03:21,439
代码现在也

95
00:03:21,599 --> 00:03:23,640
大家也不会是录个视频给你

96
00:03:23,640 --> 00:03:26,400
所以NLP我觉得还是整体来讲

97
00:03:26,400 --> 00:03:27,600
找工作挺好找的

98
00:03:28,320 --> 00:03:30,080
但是确实文本翻译这个东西

99
00:03:30,080 --> 00:03:31,400
你就没戏了

100
00:03:31,560 --> 00:03:33,440
所以NLP一般做的东西

101
00:03:33,560 --> 00:03:35,440
就是说我们会再讲到NLP

102
00:03:35,440 --> 00:03:36,080
给大家去看

103
00:03:36,080 --> 00:03:37,600
NLP主要的一些东西

104
00:03:37,600 --> 00:03:38,440
叫文本分类

105
00:03:39,720 --> 00:03:40,680
文本生成

106
00:03:40,840 --> 00:03:41,960
就文本打打Tag

107
00:03:42,280 --> 00:03:43,800
就文本summarization

108
00:03:43,840 --> 00:03:44,400
这些东西

109
00:03:44,400 --> 00:03:45,760
比较实际化一些东西

110
00:03:46,760 --> 00:03:48,000
文本翻译这个东西

111
00:03:48,000 --> 00:03:50,600
我觉得是基本就大厂做的事情

112
00:03:51,280 --> 00:03:53,120
你因为文本翻译

113
00:03:53,120 --> 00:03:54,200
我再做好一件事情

114
00:03:54,200 --> 00:03:55,120
所有人都能用

115
00:03:55,400 --> 00:03:56,800
但是你要去做一些

116
00:03:57,120 --> 00:03:58,360
别人不可能做好

117
00:03:58,360 --> 00:03:59,439
所有人都能用的

118
00:04:00,240 --> 00:04:00,759
举个例子

119
00:04:00,759 --> 00:04:02,000
就是说我财经

120
00:04:02,960 --> 00:04:05,000
我金融公司

121
00:04:05,039 --> 00:04:06,560
有很多文本报表

122
00:04:06,840 --> 00:04:08,879
你说我能不能用一个

123
00:04:10,159 --> 00:04:11,920
NLP的技术

124
00:04:11,920 --> 00:04:13,240
去分析里面文本

125
00:04:13,360 --> 00:04:14,719
比如说把里面的那些表格

126
00:04:14,719 --> 00:04:15,520
全部给我找出来

127
00:04:15,520 --> 00:04:16,199
行不行

128
00:04:16,519 --> 00:04:19,319
那些比如说我的每一个上市公司

129
00:04:19,480 --> 00:04:22,759
他的每一个Q的财务报表

130
00:04:23,000 --> 00:04:24,800
你可以我把里面那些关键的

131
00:04:24,800 --> 00:04:25,319
那些

132
00:04:25,680 --> 00:04:28,000
比如说一些句子

133
00:04:28,000 --> 00:04:29,079
给我拎出来

134
00:04:29,319 --> 00:04:30,439
做一下summarization

135
00:04:30,439 --> 00:04:31,240
是不是能做

136
00:04:31,240 --> 00:04:33,279
这就是一个很垂直领域的做法了

137
00:04:33,599 --> 00:04:35,000
另外一些比如说一些

138
00:04:36,240 --> 00:04:37,120
比如说一些

139
00:04:37,120 --> 00:04:38,800
比如说垂直行业的一些文档

140
00:04:39,000 --> 00:04:40,199
你怎么样去理解它

141
00:04:40,360 --> 00:04:41,360
怎么样做这东西

142
00:04:41,399 --> 00:04:43,159
其实都是挺不容易的

143
00:04:44,399 --> 00:04:45,279
但是文本翻译

144
00:04:45,279 --> 00:04:46,759
就是这种做好之后

145
00:04:46,759 --> 00:04:48,120
你做了两个语言翻译

146
00:04:48,120 --> 00:04:49,159
就是基本上都能用的

147
00:04:49,159 --> 00:04:50,079
所以就没戏了

148
00:04:50,120 --> 00:04:50,639
OK

149
00:04:51,639 --> 00:04:52,639
讲一下

150
00:04:53,759 --> 00:04:55,959
BPTT的原理和实现

151
00:05:00,319 --> 00:05:00,879
就这个东西

152
00:05:01,039 --> 00:05:02,800
本来我是打算讲的

153
00:05:02,959 --> 00:05:05,240
但我想我也觉得这个东西

154
00:05:05,279 --> 00:05:06,479
太技术了一点

155
00:05:06,479 --> 00:05:08,519
就是说太复杂了一点

156
00:05:08,519 --> 00:05:10,159
所以我就不讲了

157
00:05:10,639 --> 00:05:12,360
你可以去看一下我们的书

158
00:05:12,360 --> 00:05:14,120
书里面是有专门有一章

159
00:05:14,120 --> 00:05:16,000
讲BPTT的原理

160
00:05:16,079 --> 00:05:17,240
但我们没有讲实现

161
00:05:17,240 --> 00:05:18,120
但是讲的原理

162
00:05:18,120 --> 00:05:18,959
你可以去看一下

163
00:05:21,639 --> 00:05:23,319
生成的RN是不是每一层

164
00:05:23,319 --> 00:05:24,439
都需要一个初始状态

165
00:05:24,439 --> 00:05:24,919
OK

166
00:05:24,919 --> 00:05:25,599
是的

167
00:05:26,319 --> 00:05:27,159
是每一层

168
00:05:27,159 --> 00:05:29,399
就每一层都有一个state

169
00:05:29,399 --> 00:05:30,719
都需要初始化

170
00:05:31,279 --> 00:05:32,000
OK

171
00:05:32,959 --> 00:05:34,399
所以就是说假设你有

172
00:05:34,639 --> 00:05:36,479
基本上你可以认为每一层

173
00:05:36,639 --> 00:05:38,279
它都带来一个初始的状态

174
00:05:38,279 --> 00:05:39,479
都带来自己的weight

175
00:05:41,319 --> 00:05:42,599
可不可以手动实现

176
00:05:42,599 --> 00:05:44,519
Hinder size不一样的多层RN

177
00:05:44,519 --> 00:05:44,879
可以的

178
00:05:44,879 --> 00:05:45,439
这个没问题

179
00:05:45,439 --> 00:05:46,479
我觉得框架应该支持

180
00:05:47,279 --> 00:05:48,159
应该框架支持

181
00:05:48,759 --> 00:05:49,319
一般的

182
00:05:49,319 --> 00:05:50,159
不过一般来说

183
00:05:50,160 --> 00:05:51,400
大家不怎么去调那个东西

184
00:05:51,400 --> 00:05:53,800
一般Hinder size都是一样的

185
00:05:53,920 --> 00:05:55,160
因为你不会做很深

186
00:05:55,600 --> 00:05:56,200
因为为什么

187
00:05:56,200 --> 00:05:58,320
是因为你之后还有一个全连接层

188
00:05:58,400 --> 00:05:59,320
所以它跟

189
00:06:02,240 --> 00:06:04,640
它跟会有一点点不一样

190
00:06:06,800 --> 00:06:09,200
一般不怎么调Hinder size

191
00:06:09,200 --> 00:06:10,240
我看到的一般

192
00:06:10,240 --> 00:06:11,400
大家都用一个正常的

193
00:06:11,400 --> 00:06:12,360
也挺多的

194
00:06:12,720 --> 00:06:14,240
就用两次的话

195
00:06:14,240 --> 00:06:15,920
你用两个Hinder size一样都可以

196
00:06:15,920 --> 00:06:16,879
但是当然你可以调

197
00:06:16,879 --> 00:06:17,480
没问题

198
00:06:20,880 --> 00:06:23,160
N.LSTN这些图里

199
00:06:23,160 --> 00:06:24,320
O好像是有的

200
00:06:24,320 --> 00:06:25,840
你说的不带classifier

201
00:06:25,840 --> 00:06:27,320
是不是在O的计算数据里的

202
00:06:27,320 --> 00:06:28,120
一对一对

203
00:06:29,560 --> 00:06:29,800
对

204
00:06:29,800 --> 00:06:31,000
就是说我的

205
00:06:31,240 --> 00:06:32,880
我没有最后那一层

206
00:06:32,880 --> 00:06:33,600
就是说

207
00:06:33,880 --> 00:06:36,879
每一层到上一层的东西输入

208
00:06:37,240 --> 00:06:38,680
就是说怎么输

209
00:06:39,000 --> 00:06:40,200
说在中间那些层

210
00:06:40,320 --> 00:06:41,560
你的输入

211
00:06:42,160 --> 00:06:44,360
你的输入是上一层的H

212
00:06:44,879 --> 00:06:47,800
然后你每一层你输出一个H

213
00:06:47,800 --> 00:06:50,520
H又作为上下一层的输入

214
00:06:50,520 --> 00:06:51,759
又作为下一个时刻

215
00:06:51,759 --> 00:06:52,520
我自己的输入

216
00:06:52,520 --> 00:06:53,240
是这个意思

217
00:06:53,960 --> 00:06:55,439
然后当然是你作为分类

218
00:06:55,639 --> 00:06:56,920
进入softmax的时候

219
00:06:56,920 --> 00:06:57,439
是最后

220
00:06:58,360 --> 00:06:59,720
Dense layer做的事情

221
00:07:00,439 --> 00:07:01,600
OK是这个意思

222
00:07:04,439 --> 00:07:04,720
OK

223
00:07:04,720 --> 00:07:06,600
所以我们O和Y其实是一个东西

224
00:07:06,720 --> 00:07:09,840
我们代码和有一点点不一致

225
00:07:09,840 --> 00:07:10,319
OK

