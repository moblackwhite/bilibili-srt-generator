1
00:00:00,000 --> 00:00:03,000
输入是n枚卷机

2
00:00:03,000 --> 00:00:07,000
输入不是n枚是那个序列长度是n

3
00:00:07,000 --> 00:00:10,000
就是说你可以认为卷机如果是对应的卷机的话

4
00:00:10,000 --> 00:00:12,000
就是说一枚卷机的话那就是个宽

5
00:00:12,000 --> 00:00:16,000
就我给你一个高为1宽为n的一个图片

6
00:00:16,000 --> 00:00:19,000
就是一个1D的一根线

7
00:00:19,000 --> 00:00:22,000
所以你的卷机窗口是K的话

8
00:00:22,000 --> 00:00:26,000
那么你的最长路径就是说你最早的那个像素和最晚的那个像素

9
00:00:26,000 --> 00:00:28,000
你要谁才能看到它呢

10
00:00:28,000 --> 00:00:31,000
你要到n除以K之后才能看到它

11
00:00:31,000 --> 00:00:35,000
所以他要就是说你要走到最上层再走下来

12
00:00:35,000 --> 00:00:36,000
OK

13
00:00:36,000 --> 00:00:39,000
所以这个最长路径是跟层数有关

14
00:00:39,000 --> 00:00:42,000
就是假设你先要加很多层在里面

15
00:00:42,000 --> 00:00:45,000
如果做阅读理解

16
00:00:45,000 --> 00:00:49,000
比如说高考语文

17
00:00:49,000 --> 00:00:51,000
如果阅读理解

18
00:00:51,000 --> 00:00:54,000
首先你知道国家已经不让你干这个事情了

19
00:00:54,000 --> 00:00:58,000
就是你不要去用机器学习来搞做考试

20
00:00:58,000 --> 00:01:01,000
这会被取缔的

21
00:01:01,000 --> 00:01:05,000
比如说把几个问题做query

22
00:01:05,000 --> 00:01:07,000
把整篇文章作为key value

23
00:01:07,000 --> 00:01:09,000
让注意力机制提出问题可以指

24
00:01:09,000 --> 00:01:11,000
理论上可以实现吗

25
00:01:11,000 --> 00:01:12,000
可以实现

26
00:01:12,000 --> 00:01:13,000
这个东西可以实现

27
00:01:13,000 --> 00:01:15,000
就是说就问

28
00:01:15,000 --> 00:01:17,000
这个东西

29
00:01:17,000 --> 00:01:18,000
这个东西

30
00:01:18,000 --> 00:01:22,000
我们会在BERT里面给大家稍微讲一下

31
00:01:22,000 --> 00:01:23,000
这个叫做QA

32
00:01:23,000 --> 00:01:25,000
就是回答

33
00:01:25,000 --> 00:01:30,000
假设你给你一段文章

34
00:01:30,000 --> 00:01:31,000
然后让你读

35
00:01:31,000 --> 00:01:33,000
然后我问你问题

36
00:01:33,000 --> 00:01:38,000
假设你的答案是可以在句子中的一个片段出现的话

37
00:01:38,000 --> 00:01:42,000
这就是一个很典型的自然语处理的QA

38
00:01:42,000 --> 00:01:45,000
Question answering的一个东西

39
00:01:45,000 --> 00:01:50,000
就是说你就是去预测你到底那个片段的位置在哪里就行了

40
00:01:50,000 --> 00:01:53,000
所以这个东西做的大家还做的挺好的

41
00:01:53,000 --> 00:01:54,000
这个东西做的挺好的

42
00:01:54,000 --> 00:01:56,000
所以你就是绝对是可以做的

43
00:01:56,000 --> 00:01:57,000
但是反过来讲

44
00:01:57,000 --> 00:02:00,000
如果你这个答案

45
00:02:00,000 --> 00:02:02,000
不在文本里面

46
00:02:02,000 --> 00:02:04,000
就是要推理的话

47
00:02:04,000 --> 00:02:05,000
那就比较麻烦一点

48
00:02:05,000 --> 00:02:06,000
这个东西

49
00:02:06,000 --> 00:02:09,000
也现在多多少少也能做一点

50
00:02:09,000 --> 00:02:10,000
就是说你要真的很大

51
00:02:10,000 --> 00:02:13,000
那GPT-3那些模型也能做到还行

52
00:02:15,000 --> 00:02:16,000
问题三

53
00:02:16,000 --> 00:02:18,000
渔选图是怎么得到热力图的

54
00:02:18,000 --> 00:02:19,000
就那个P

55
00:02:19,000 --> 00:02:21,000
就是那个P刚刚代码看到

56
00:02:21,000 --> 00:02:22,000
那个P就是说

57
00:02:22,000 --> 00:02:23,000
热力图就是说

58
00:02:23,000 --> 00:02:24,000
它那个P就是

59
00:02:24,000 --> 00:02:27,000
它是每一行是一个样本

60
00:02:27,000 --> 00:02:29,000
然后你的一列就是你的DI位

61
00:02:29,000 --> 00:02:31,000
所以每一行就是你那个样本的

62
00:02:31,000 --> 00:02:34,000
那个D dimension的那个位置的encoding

63
00:02:37,000 --> 00:02:40,000
位置编码的位置在实际中的指的是什么

64
00:02:40,000 --> 00:02:42,000
实际中指的是一个序列

65
00:02:42,000 --> 00:02:44,000
它那个样本XI

66
00:02:44,000 --> 00:02:45,000
那个I

67
00:02:46,000 --> 00:02:47,000
就说为什么

68
00:02:47,000 --> 00:02:49,000
是因为你把一个序列输到

69
00:02:49,000 --> 00:02:52,000
输到自注意力的话

70
00:02:52,000 --> 00:02:53,000
它不管你的序列信息

71
00:02:53,000 --> 00:02:56,000
你可以把序列任何打乱都可以输进去

72
00:02:56,000 --> 00:02:58,000
现在是说我告诉你

73
00:02:58,000 --> 00:03:01,000
对于序列中的第二个样本

74
00:03:01,000 --> 00:03:05,000
我加了一个常为那个位置编码里面

75
00:03:05,000 --> 00:03:07,000
那个I的那个行

76
00:03:07,000 --> 00:03:09,000
就是说对每一个样本

77
00:03:09,000 --> 00:03:11,000
不同位置的样本

78
00:03:11,000 --> 00:03:14,000
加了一个独特的一个编码进来

79
00:03:15,000 --> 00:03:17,000
但是你换到下一个句子

80
00:03:17,000 --> 00:03:18,000
所以每个样本

81
00:03:18,000 --> 00:03:21,000
每个不同的位置加的还是同样东西

82
00:03:23,000 --> 00:03:26,000
相对位置的基准位置怎么定的

83
00:03:26,000 --> 00:03:28,000
每个序列基准位置是一样吗

84
00:03:28,000 --> 00:03:30,000
基准位置是一样

85
00:03:30,000 --> 00:03:32,000
基准位置其实是从零开始

86
00:03:32,000 --> 00:03:34,000
就刚刚你看到forward的那个函数

87
00:03:34,000 --> 00:03:36,000
forward就是说你给个X进来

88
00:03:36,000 --> 00:03:38,000
我就是把你那个P

89
00:03:38,000 --> 00:03:41,000
就是前面那个第N行

90
00:03:41,000 --> 00:03:43,000
给你加进去就行了

91
00:03:43,000 --> 00:03:46,000
所以你的序列的第零个位置

92
00:03:46,000 --> 00:03:48,000
永远是零

93
00:03:50,000 --> 00:03:53,000
位置编码需要模型自动学习吗

94
00:03:53,000 --> 00:03:54,000
如何解码呢

95
00:03:56,000 --> 00:03:58,000
目前是不需要学习的

96
00:03:58,000 --> 00:04:00,000
它的P是生成的

97
00:04:00,000 --> 00:04:01,000
所以它的编码

98
00:04:02,000 --> 00:04:03,000
所以这个编码

99
00:04:03,000 --> 00:04:06,000
我觉得你大大多多少少理解成

100
00:04:06,000 --> 00:04:09,000
我的理解成我的in code

101
00:04:09,000 --> 00:04:10,000
d code

102
00:04:10,000 --> 00:04:11,000
它实际上是说

103
00:04:13,000 --> 00:04:15,000
我只是对输入增加

104
00:04:15,000 --> 00:04:16,000
对输入进行的变换

105
00:04:16,000 --> 00:04:18,000
但是我不需要解码了

106
00:04:18,000 --> 00:04:20,000
就是说输出跟你无关

107
00:04:20,000 --> 00:04:21,000
输出反正说输出看到的

108
00:04:21,000 --> 00:04:24,000
就是你的X加的一些东西在里面

109
00:04:24,000 --> 00:04:27,000
所以就是说你不需要真的解码

110
00:04:27,000 --> 00:04:30,000
就是你可认为整个模型在里面去解码

111
00:04:30,000 --> 00:04:32,000
模型自动去弄了

112
00:04:32,000 --> 00:04:36,000
文理7介绍一下可学习的位置相对位置编码

113
00:04:36,000 --> 00:04:39,000
可学习我们在BERT里面会跟大家讲

114
00:04:39,000 --> 00:04:41,000
BERT用的是可学习的方案

115
00:04:43,000 --> 00:04:46,000
Transformer输入的数据应该怎么组织的

116
00:04:46,000 --> 00:04:49,000
Transformer输入的数据

117
00:04:49,000 --> 00:04:51,000
我们等会给大家讲代码就可以看到了

118
00:04:52,000 --> 00:04:54,000
位置编码是不是能写得

119
00:04:54,000 --> 00:04:55,000
NN Parameter这种可训练形式

120
00:04:55,000 --> 00:04:56,000
可以的

121
00:04:56,000 --> 00:04:58,000
就是说我们在BERT会讲

122
00:04:58,000 --> 00:04:59,000
说白了就是说

123
00:04:59,000 --> 00:05:00,000
你把P随机初始化一下

124
00:05:00,000 --> 00:05:02,000
然后包成一个可学习的parameter就行了

125
00:05:02,000 --> 00:05:04,000
这就是BERT的做法

126
00:05:06,000 --> 00:05:07,000
自注意力机制

127
00:05:07,000 --> 00:05:08,000
是不是可以理解成一个网络层

128
00:05:08,000 --> 00:05:10,000
像CN和RN的一层理解

129
00:05:10,000 --> 00:05:11,000
可以的

130
00:05:11,000 --> 00:05:12,000
它就是一个网络层

131
00:05:13,000 --> 00:05:15,000
Self-Attention就是一个layer

132
00:05:16,000 --> 00:05:19,000
然后你可以认为它跟RN的layer差不多

133
00:05:19,000 --> 00:05:20,000
就是给一个序列出去

134
00:05:20,000 --> 00:05:21,000
然后每个序列都会输出

135
00:05:22,000 --> 00:05:23,000
所以它确实是一个layer

136
00:05:23,000 --> 00:05:24,000
等会我们可以看到这个东西

137
00:05:26,000 --> 00:05:28,000
位置编码在经过Attention之后

138
00:05:28,000 --> 00:05:30,000
会丢失相对位置信息吗

139
00:05:33,000 --> 00:05:34,000
有可能会丢

140
00:05:34,000 --> 00:05:36,000
就是说你进到下一层的时候

141
00:05:36,000 --> 00:05:39,000
但是你希望就是说反正你最好

142
00:05:39,000 --> 00:05:40,000
虽然你对我的X做了变化

143
00:05:41,000 --> 00:05:43,000
你的位置信息还是在里面有一点点

144
00:05:43,000 --> 00:05:44,000
希望你是说

145
00:05:45,000 --> 00:05:46,000
就是说你的位置

146
00:05:46,000 --> 00:05:49,000
我希望你的位置编码在我的书里面有

147
00:05:49,000 --> 00:05:51,000
我希望你给我的书处里面

148
00:05:51,000 --> 00:05:53,000
多多少少也带了它

149
00:05:53,000 --> 00:05:55,000
所以下一层我还是能用到

150
00:05:56,000 --> 00:05:57,000
就是说这希望

151
00:06:01,000 --> 00:06:03,000
通过NLP提取文字中的内容

152
00:06:03,000 --> 00:06:04,000
这块会讲吗

153
00:06:04,000 --> 00:06:06,000
我们书里面好像没有

154
00:06:06,000 --> 00:06:07,000
书里面没有

155
00:06:07,000 --> 00:06:09,000
但是我们会在BERT里面

156
00:06:09,000 --> 00:06:10,000
给大家大概讲一下

157
00:06:10,000 --> 00:06:11,000
这个模型长什么样子

158
00:06:13,000 --> 00:06:16,000
就VIT的位置编码还没有做的比较简单

159
00:06:16,000 --> 00:06:17,000
没有预选就是一个意味向量

160
00:06:17,000 --> 00:06:20,000
就是说现在当然是说你可以做到

161
00:06:20,000 --> 00:06:22,000
如果你是图片的话

162
00:06:24,000 --> 00:06:26,000
图片怎么说图片位置信息

163
00:06:27,000 --> 00:06:29,000
它是个二维的位置信息

164
00:06:29,000 --> 00:06:32,000
所以多多少少你把图片做成一个

165
00:06:32,000 --> 00:06:34,000
意味东西是有一点点问题的

166
00:06:34,000 --> 00:06:36,000
就是说位置编码之后

167
00:06:36,000 --> 00:06:37,000
我们可以做的比较简单

168
00:06:37,000 --> 00:06:38,000
就是说你看到BERT

169
00:06:38,000 --> 00:06:39,000
它就根本就不用Sign和Sign

170
00:06:39,000 --> 00:06:41,000
就是一个可以学习的东西就行了

171
00:06:41,000 --> 00:06:44,000
但是我们至少我们是先在Transformer

172
00:06:44,000 --> 00:06:47,000
我们就还原一下Transformer的东西

173
00:06:47,000 --> 00:06:49,000
而且这个位置编码也还是比较流行的

174
00:06:49,000 --> 00:06:50,000
因为它不用学

175
00:06:51,000 --> 00:06:54,000
OK我们的QA先到这里

