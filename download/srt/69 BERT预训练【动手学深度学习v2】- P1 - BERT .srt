1
00:00:00,000 --> 00:00:02,560
解释一下什么是embedding

2
00:00:02,560 --> 00:00:05,679
embedding就建议回到前面几章去看一下

3
00:00:05,679 --> 00:00:08,880
embedding就是对每一个词每一个token

4
00:00:09,000 --> 00:00:11,679
学习一个固定长度的向量

5
00:00:11,679 --> 00:00:14,839
来这样子神经网络来处理它

6
00:00:16,480 --> 00:00:19,519
embedding层是转化单词的向量表示吗

7
00:00:19,519 --> 00:00:23,839
embedding层就是对任何一个token

8
00:00:24,280 --> 00:00:25,679
token就是要么是一个字符

9
00:00:25,679 --> 00:00:26,600
要么是一个word

10
00:00:26,600 --> 00:00:29,000
或者是通常BERT其实用的是一个

11
00:00:29,000 --> 00:00:31,160
一个词段就是一个词根

12
00:00:31,600 --> 00:00:32,479
用的一个词根

13
00:00:32,479 --> 00:00:34,359
对这个词根做一个长为

14
00:00:34,359 --> 00:00:35,480
比如说128

15
00:00:35,480 --> 00:00:37,439
或者就是市场的一个向量的表示

16
00:00:37,640 --> 00:00:38,600
这就是embedding

17
00:00:41,799 --> 00:00:42,920
做研码的时候

18
00:00:42,920 --> 00:00:45,920
10%的概率保持原有的词源

19
00:00:45,920 --> 00:00:47,159
那不是什么都没做吗

20
00:00:47,159 --> 00:00:49,960
与15%的做mask不是矛盾吗

21
00:00:50,280 --> 00:00:52,079
其实他不是什么都没做

22
00:00:52,439 --> 00:00:56,280
他就说你在做输入的时候

23
00:00:56,600 --> 00:00:59,160
我就说我挖掉一个词

24
00:00:59,439 --> 00:01:00,719
让你预测它

25
00:01:00,880 --> 00:01:02,560
但是你在看的时候

26
00:01:02,679 --> 00:01:04,200
你是看到整个句子的

27
00:01:04,799 --> 00:01:06,960
如果我是真的把你mask住了

28
00:01:07,519 --> 00:01:08,320
我就把标

29
00:01:08,320 --> 00:01:10,240
就是说我给你做完星天控

30
00:01:10,680 --> 00:01:14,079
然后我把中间一些词画一个线

31
00:01:14,079 --> 00:01:15,760
但是这个线里面东西

32
00:01:15,760 --> 00:01:16,760
我是填了东西的

33
00:01:16,760 --> 00:01:18,079
我要么是填了一个mask

34
00:01:18,439 --> 00:01:20,879
要么是填了一个随机的词

35
00:01:20,879 --> 00:01:22,879
要么是填的是正确答案给你

36
00:01:22,960 --> 00:01:25,319
所以是说你有10%的概率

37
00:01:25,320 --> 00:01:26,880
我是告诉你正确答案

38
00:01:26,880 --> 00:01:28,320
让你去干这个事情

39
00:01:28,360 --> 00:01:29,800
所以也不是说什么都没做

40
00:01:29,800 --> 00:01:31,440
就是说10%的概率去作弊了

41
00:01:32,520 --> 00:01:33,880
之所以说你要这么干

42
00:01:33,880 --> 00:01:35,160
是因为确实说

43
00:01:35,480 --> 00:01:38,040
在真实的在做fine tuning的时候

44
00:01:38,080 --> 00:01:39,600
我不会给你做任何mask

45
00:01:40,160 --> 00:01:40,680
对吧

46
00:01:40,800 --> 00:01:41,520
在真实的时候

47
00:01:41,520 --> 00:01:42,440
我因为我不要训练

48
00:01:42,720 --> 00:01:44,280
mask的language model

49
00:01:44,280 --> 00:01:45,960
所以我不会让你去做反星点控

50
00:01:45,960 --> 00:01:50,160
所以我给你的就是没有被抠掉的句子

51
00:01:50,520 --> 00:01:51,320
这样的话

52
00:01:51,320 --> 00:01:52,800
就是说有10%的概率

53
00:01:53,119 --> 00:01:54,119
你看到的是

54
00:01:54,119 --> 00:01:55,439
你在fine tuning里面

55
00:01:55,439 --> 00:01:56,799
真实的看到的东西

56
00:01:57,679 --> 00:01:58,879
当然还有10%的时候

57
00:01:58,879 --> 00:02:00,399
是看到看上去很像

58
00:02:00,399 --> 00:02:02,239
但中间有些东西错误的东西

59
00:02:03,959 --> 00:02:04,519
OK

60
00:02:06,799 --> 00:02:07,479
问题4

61
00:02:07,479 --> 00:02:09,719
BERT需要大量的训练数据

62
00:02:09,719 --> 00:02:11,280
是不是很少在CV

63
00:02:11,280 --> 00:02:13,719
或者不用在CV上

64
00:02:17,120 --> 00:02:18,639
BERT就是说

65
00:02:19,840 --> 00:02:22,400
就是说你其实你不要说BERT这东西

66
00:02:22,520 --> 00:02:23,960
就是说整个transformer这一块

67
00:02:23,960 --> 00:02:24,520
就是说

68
00:02:24,760 --> 00:02:26,280
它用在CV上

69
00:02:26,280 --> 00:02:26,879
其实

70
00:02:27,280 --> 00:02:31,159
它现在大家在慢慢的把

71
00:02:31,159 --> 00:02:32,480
不是说BERT本身

72
00:02:32,680 --> 00:02:34,080
就是说是整个是

73
00:02:34,120 --> 00:02:35,520
就BERT应该是transformer

74
00:02:35,520 --> 00:02:37,599
就是的一个变种

75
00:02:38,000 --> 00:02:41,040
就是说就纯的tension架构的模型

76
00:02:41,319 --> 00:02:43,240
现在有在过去几年

77
00:02:43,240 --> 00:02:45,120
有在大量的被用在CV上面

78
00:02:45,280 --> 00:02:47,120
是因为为什么可以用

79
00:02:47,120 --> 00:02:47,960
一方面就是说

80
00:02:47,960 --> 00:02:50,439
我确实是可以CV的label也不小

81
00:02:50,560 --> 00:02:51,879
图片分类有了几百万

82
00:02:51,879 --> 00:02:52,879
几千万也有

83
00:02:53,159 --> 00:02:54,519
另外一块就是说

84
00:02:54,639 --> 00:02:55,960
在做图片的时候

85
00:02:56,079 --> 00:02:58,359
我可以抠很多patch出来

86
00:02:58,560 --> 00:03:00,079
就是说在图一个大图里面

87
00:03:00,079 --> 00:03:01,199
抠很多小块出来

88
00:03:01,199 --> 00:03:04,400
然后它之间是有个空间关系的

89
00:03:04,439 --> 00:03:06,120
我可以利用空间关系

90
00:03:06,359 --> 00:03:07,840
就是可以多多少少模拟

91
00:03:07,840 --> 00:03:09,439
我这个语言模型的东西是吧

92
00:03:09,479 --> 00:03:11,800
所以就是说它其实是有

93
00:03:12,079 --> 00:03:14,039
这样子我也能生出很多数据出来

94
00:03:16,199 --> 00:03:17,240
然后

95
00:03:17,359 --> 00:03:18,159
问题5

96
00:03:18,159 --> 00:03:21,079
能展示一下100万batch的训练结果吗

97
00:03:21,360 --> 00:03:25,640
我们在等会在预训练模型的

98
00:03:25,640 --> 00:03:27,240
叫fine tuning的时候

99
00:03:27,240 --> 00:03:28,480
我们加载的模型

100
00:03:28,480 --> 00:03:31,280
就是在大概我忘了是100万

101
00:03:31,280 --> 00:03:32,800
还是10万个batch训练结果

102
00:03:32,880 --> 00:03:33,800
就训练个

103
00:03:35,160 --> 00:03:36,240
四五个小时

104
00:03:36,760 --> 00:03:38,080
就训练个四五个小时

105
00:03:38,080 --> 00:03:39,000
是那个模型

106
00:03:39,040 --> 00:03:40,480
我们是存下来了

107
00:03:40,480 --> 00:03:42,360
但是我们没有在这里给大家演示

108
00:03:42,439 --> 00:03:44,680
因为不然每次都要训练个几个小时

109
00:03:44,680 --> 00:03:45,320
很麻烦

110
00:03:48,360 --> 00:03:49,640
然后问题6

111
00:03:49,960 --> 00:03:51,840
如果想用BERT large

112
00:03:51,840 --> 00:03:52,760
作为预训的模型

113
00:03:52,760 --> 00:03:53,960
但BERT large太大了

114
00:03:53,960 --> 00:03:54,920
显存不足

115
00:03:54,920 --> 00:03:55,440
训练不好

116
00:03:55,440 --> 00:03:57,480
有没有模型压缩方法

117
00:03:57,480 --> 00:03:59,920
来适应训练

118
00:04:01,280 --> 00:04:02,320
一般的做法

119
00:04:02,520 --> 00:04:03,720
当然是有两种东西

120
00:04:03,880 --> 00:04:05,680
一种是说你用一个

121
00:04:06,200 --> 00:04:08,080
真的用一个小一点的

122
00:04:08,080 --> 00:04:08,960
就是说被改

123
00:04:08,960 --> 00:04:11,520
就是说被精简过的BERT

124
00:04:12,160 --> 00:04:14,520
就是说BERT large是比较大

125
00:04:14,560 --> 00:04:16,200
但是现在有很多新的模型

126
00:04:16,200 --> 00:04:18,400
说我可以把你的模型做小一点

127
00:04:18,919 --> 00:04:19,799
怎么样

128
00:04:20,079 --> 00:04:20,959
这是一种方法

129
00:04:20,959 --> 00:04:22,719
就对在模型上的改变

130
00:04:22,719 --> 00:04:24,759
使得你的GPU比较小

131
00:04:25,079 --> 00:04:26,799
第二个是说你就用

132
00:04:28,479 --> 00:04:30,159
一般来说用的是

133
00:04:30,839 --> 00:04:32,560
就多弄几个GPU

134
00:04:33,599 --> 00:04:35,000
就是说一般来说

135
00:04:35,000 --> 00:04:37,719
大家会在单机会做model parallelism

136
00:04:37,759 --> 00:04:39,199
我们没有讲model parallelism

137
00:04:39,239 --> 00:04:40,599
model parallelism意思是说

138
00:04:40,639 --> 00:04:43,359
假设你有12个transformer block的话

139
00:04:43,399 --> 00:04:44,959
那么你有4块GPU的话

140
00:04:44,959 --> 00:04:47,799
那么每块GPU会拿三个transformer block

141
00:04:48,199 --> 00:04:48,560
这样子

142
00:04:48,560 --> 00:04:49,920
我就只要存1 1的

143
00:04:49,920 --> 00:04:50,879
1 1的模型

144
00:04:50,879 --> 00:04:51,240
对吧

145
00:04:51,240 --> 00:04:52,360
中间结果了

146
00:04:52,680 --> 00:04:53,720
这样子的话

147
00:04:53,720 --> 00:04:55,520
我确实是可以做到

148
00:04:57,400 --> 00:04:58,600
我只要加GPU

149
00:04:58,600 --> 00:05:00,199
我的显存总是能做大的

150
00:05:02,560 --> 00:05:05,360
BERT的预训练任务有什么改进吗

151
00:05:05,360 --> 00:05:06,800
BERT的预训任务有很多

152
00:05:07,000 --> 00:05:08,480
就是我们讲的BERT

153
00:05:08,720 --> 00:05:12,040
比如说GBT模型就会不一样

154
00:05:12,040 --> 00:05:13,800
就是说之后的模型会有

155
00:05:14,400 --> 00:05:15,920
还是有很多改进的

156
00:05:16,439 --> 00:05:19,759
我觉得我们就不会给大家一一讲了

157
00:05:19,759 --> 00:05:20,720
就是之后的模型

158
00:05:20,720 --> 00:05:22,720
会在预训练任务的本身上

159
00:05:22,720 --> 00:05:23,680
会做很多改进

160
00:05:26,480 --> 00:05:27,120
问题8

161
00:05:27,120 --> 00:05:29,759
不同数量级别的数据

162
00:05:29,759 --> 00:05:31,080
用BERT做预训练

163
00:05:31,080 --> 00:05:31,920
大概要训练多久

164
00:05:31,920 --> 00:05:32,840
怎么评估

165
00:05:33,439 --> 00:05:36,240
我太太general了

166
00:05:40,040 --> 00:05:41,040
你可以去看一下

167
00:05:41,040 --> 00:05:42,120
就是说最近那些

168
00:05:42,120 --> 00:05:43,560
从BERT之后那些论文

169
00:05:44,319 --> 00:05:45,439
大家都报告了

170
00:05:45,480 --> 00:05:46,959
说我用了多少数据

171
00:05:46,959 --> 00:05:47,839
用了多少GPU

172
00:05:47,839 --> 00:05:48,719
训练了多久

173
00:05:48,719 --> 00:05:49,399
你可以看一下

174
00:05:49,399 --> 00:05:50,920
大概能够了解一下

175
00:05:50,920 --> 00:05:52,719
大家一般是用训练多久

