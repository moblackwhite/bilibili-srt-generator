1
00:00:00,000 --> 00:00:01,159
问题15

2
00:00:01,159 --> 00:00:04,560
为什么learnnet的batch size大于1000收敛会有问题

3
00:00:04,560 --> 00:00:06,320
它不是收敛有问题

4
00:00:06,320 --> 00:00:08,400
但是收敛的比较慢

5
00:00:09,560 --> 00:00:10,720
你可以试一下

6
00:00:11,759 --> 00:00:14,439
就是说你可以去试一下

7
00:00:14,439 --> 00:00:16,440
就是说你核心思想是说

8
00:00:16,440 --> 00:00:18,879
当你batch size等于1000的时候

9
00:00:18,879 --> 00:00:22,120
你那里面大部分图片是很相似的图片

10
00:00:22,120 --> 00:00:23,719
所以你的多样性没了的话

11
00:00:23,719 --> 00:00:25,440
你就是重复的图片

12
00:00:25,440 --> 00:00:26,400
就重复的图片

13
00:00:26,400 --> 00:00:27,600
你就是在重复计算

14
00:00:27,600 --> 00:00:29,359
所以就是影响你的收敛进度

15
00:00:30,000 --> 00:00:35,759
就是说问题16

16
00:00:35,759 --> 00:00:36,640
我觉得这个问题挺好的

17
00:00:36,640 --> 00:00:38,799
就是说我说我的函数是一个

18
00:00:38,799 --> 00:00:42,320
直接input加上的GX

19
00:00:42,320 --> 00:00:44,200
这样能保持不会变坏吗

20
00:00:44,200 --> 00:00:45,679
如果GX不是变好

21
00:00:45,679 --> 00:00:46,920
那是不是什么都不干

22
00:00:46,920 --> 00:00:47,960
还是变坏了呢

23
00:00:47,960 --> 00:00:50,439
其实这东西是说你要这么理解

24
00:00:50,439 --> 00:00:52,200
就是说你是怎么训练了

25
00:00:52,840 --> 00:00:54,480
就我去训练这个FX

26
00:00:55,200 --> 00:00:56,400
如果我的GX

27
00:00:56,400 --> 00:00:59,359
我的我的我的模型发现GX很难训练

28
00:00:59,359 --> 00:01:01,240
或者训练没有什么好处的话

29
00:01:01,240 --> 00:01:03,519
他的你会他就会拿不到T度

30
00:01:04,599 --> 00:01:06,439
因为就是说这个东西是我训练出来的

31
00:01:06,439 --> 00:01:07,079
对吧

32
00:01:07,079 --> 00:01:08,719
如果发现我的模型发现说

33
00:01:08,719 --> 00:01:10,920
我直接拿X过去

34
00:01:10,920 --> 00:01:13,120
那FX效果就很好了

35
00:01:13,120 --> 00:01:14,959
然后我不需要再去

36
00:01:14,959 --> 00:01:17,039
我加上一个GX

37
00:01:17,039 --> 00:01:20,519
我对我的loss下降没什么明显的话

38
00:01:20,519 --> 00:01:22,640
他在做T度反传的时候

39
00:01:22,640 --> 00:01:23,680
拿不到什么T度

40
00:01:23,680 --> 00:01:25,920
所以他的权重就不会被更新

41
00:01:25,920 --> 00:01:26,960
不会被更新很有可能

42
00:01:26,960 --> 00:01:28,280
他就是一件很小的权重

43
00:01:28,280 --> 00:01:28,840
随机权重

44
00:01:28,840 --> 00:01:31,120
所以最后就跟你不会做什么贡献

45
00:01:31,120 --> 00:01:32,359
这就是啊

46
00:01:32,359 --> 00:01:35,120
所以你可能甚至他的T度会变成一些零了

47
00:01:36,240 --> 00:01:37,800
他的他T度变成零

48
00:01:37,800 --> 00:01:40,400
然后他的可能权重也会慢慢的变得很小

49
00:01:40,400 --> 00:01:42,040
所以他就是啊

50
00:01:42,040 --> 00:01:44,960
所以这就是说rest net在加深的时候

51
00:01:44,960 --> 00:01:46,840
通常不会让你的模型变坏

52
00:01:51,600 --> 00:01:52,719
问题是7

53
00:01:52,719 --> 00:01:55,840
Cosine的学习率会被step的学习率好吗

54
00:01:55,840 --> 00:01:58,560
我们发现Cosine学习率挺好的

55
00:01:58,600 --> 00:02:00,240
就是说

56
00:02:01,799 --> 00:02:04,159
就是说你怎么弄呢

57
00:02:04,159 --> 00:02:04,799
就是说

58
00:02:05,719 --> 00:02:06,640
就举个例子啊

59
00:02:06,640 --> 00:02:08,759
你就是说你学习率

60
00:02:08,759 --> 00:02:10,159
我们现在是怎么画的啊

61
00:02:10,159 --> 00:02:14,159
学习率就是说假设我这个是实践的话

62
00:02:14,159 --> 00:02:15,640
我的是LR的后的学习率

63
00:02:15,640 --> 00:02:17,319
现在是这样子做的对吧

64
00:02:17,319 --> 00:02:18,360
还有一种做法是说

65
00:02:18,360 --> 00:02:19,920
你通常是说这样子

66
00:02:19,920 --> 00:02:21,480
然后这么下来对吧

67
00:02:22,439 --> 00:02:27,439
但Cosine是说你做一个这样子的东西

68
00:02:27,439 --> 00:02:30,439
就是你这是一个Cosine函数的一块

69
00:02:30,439 --> 00:02:31,800
你可以这么下来

70
00:02:31,800 --> 00:02:36,599
这是他的好处是说你在前面这一块啊

71
00:02:36,599 --> 00:02:39,159
有足够多的时间去那个

72
00:02:39,159 --> 00:02:40,240
然后中间快速下降

73
00:02:40,240 --> 00:02:42,639
后面也足够多时间去做微调

74
00:02:42,639 --> 00:02:44,520
但你还甚至可以做个这样子的东西

75
00:02:44,520 --> 00:02:46,439
就是说你甚至可以做一个啊

76
00:02:46,439 --> 00:02:49,520
这样子的曲线也是可以的

77
00:02:49,520 --> 00:02:49,919
OK

78
00:02:49,919 --> 00:02:52,719
这就是Cosine的LR它的

79
00:02:53,800 --> 00:02:53,960
啊

80
00:02:53,960 --> 00:02:55,039
是怎么用的

81
00:02:55,039 --> 00:02:56,400
就是说

82
00:02:56,400 --> 00:02:58,480
大家发现这个学习率还挺好用的

83
00:02:58,480 --> 00:02:59,560
而且他比较简单

84
00:02:59,560 --> 00:03:00,360
主要是简单

85
00:03:00,360 --> 00:03:01,080
你最简单

86
00:03:01,080 --> 00:03:02,520
你不要重复就是一个下来

87
00:03:02,520 --> 00:03:04,800
就不要调那个你就调一个最大值

88
00:03:04,800 --> 00:03:05,719
这个最小的中间

89
00:03:05,719 --> 00:03:07,439
你不要说step要多少下

90
00:03:07,439 --> 00:03:08,159
就比较简单

91
00:03:12,040 --> 00:03:14,200
残差就recedure是什么意思呢

92
00:03:14,200 --> 00:03:15,439
recedure的意思是说

93
00:03:16,879 --> 00:03:19,879
就是说我在fit这个函数的时候啊

94
00:03:19,879 --> 00:03:22,240
假设这个X也是一个小模型过来的

95
00:03:22,240 --> 00:03:25,280
那么首先说我会去fitX里面的那个小模型

96
00:03:25,319 --> 00:03:27,199
就它是一个小模型输出

97
00:03:27,199 --> 00:03:28,159
然后呢

98
00:03:28,159 --> 00:03:31,039
如果这个小模型fit差不多了之后

99
00:03:31,039 --> 00:03:34,199
我就训我就劝一个叫底层训的差不多之后

100
00:03:34,199 --> 00:03:36,360
我再会就是说啊

101
00:03:36,360 --> 00:03:38,920
把GX能够给我improve的东西

102
00:03:38,920 --> 00:03:40,599
那些recedure就是说啊

103
00:03:40,599 --> 00:03:41,680
等于是我要

104
00:03:42,960 --> 00:03:43,920
啊

105
00:03:43,920 --> 00:03:44,479
这么说吧

106
00:03:44,479 --> 00:03:45,039
就是说

107
00:03:48,039 --> 00:03:50,280
我再换换一下啊

108
00:03:50,280 --> 00:03:50,840
就这么说吧

109
00:03:50,840 --> 00:03:52,080
就是我要训练一个

110
00:03:52,960 --> 00:03:54,080
啊

111
00:03:54,080 --> 00:03:57,040
我要我要我要我要训练一个这样子的曲线的话

112
00:03:57,040 --> 00:03:58,760
这个曲线就很难训练

113
00:03:58,760 --> 00:04:01,920
我的一个做法是说我先去训练一个

114
00:04:01,920 --> 00:04:03,960
我先训练一个平滑的东西

115
00:04:03,960 --> 00:04:05,000
就是说这个就是我的

116
00:04:05,000 --> 00:04:06,520
比如说layer一能干的事情

117
00:04:07,600 --> 00:04:10,800
而我的结果是反正是layer一加上啊

118
00:04:10,800 --> 00:04:12,439
layer一加上layer二吧

119
00:04:12,439 --> 00:04:14,040
就是说啊

120
00:04:14,040 --> 00:04:15,320
然后在layer二的

121
00:04:16,600 --> 00:04:16,800
哎

122
00:04:16,800 --> 00:04:18,520
我写的挺奇怪的

123
00:04:18,520 --> 00:04:20,720
反正就是说我先去那这个东西

124
00:04:20,720 --> 00:04:22,120
然后剩下那些误差

125
00:04:22,120 --> 00:04:25,959
那些误差我就是layer二再去训练这个说这个点跟这个点的那些误差

126
00:04:25,959 --> 00:04:29,480
他就layer二在在layer一的基础上做叠加

127
00:04:29,480 --> 00:04:30,680
这样子做出了效果

128
00:04:30,680 --> 00:04:31,920
就是说啊

129
00:04:31,920 --> 00:04:35,360
就是说rest net就先会训练一下比较基础的一些下层的一些

130
00:04:35,360 --> 00:04:38,720
比如说你去一个rest net 152的话

131
00:04:38,720 --> 00:04:41,160
他可能会先在直观上来说

132
00:04:41,160 --> 00:04:43,680
他可能先训练一个rest net 18

133
00:04:43,680 --> 00:04:45,199
就下面那几个block

134
00:04:45,199 --> 00:04:49,480
然后剩下的没有fit好的东西上面存在会去帮你fit

135
00:04:49,480 --> 00:04:51,240
所以这就是他们啊

136
00:04:51,240 --> 00:04:53,960
论文rest issue那个名字的由来

137
00:04:56,360 --> 00:04:57,879
固定19是新是什么意思

138
00:04:57,879 --> 00:04:59,680
新是你的python里面的东西

139
00:04:59,680 --> 00:05:02,680
新是把python里面那个list展开

140
00:05:02,680 --> 00:05:04,879
就去看一下python那个东西是怎么干的

141
00:05:04,879 --> 00:05:07,160
叫residual block return的是一个list对吧

142
00:05:07,160 --> 00:05:09,600
我新的话就把这list展开之后

143
00:05:09,600 --> 00:05:12,759
变成了一个一堆这样子的输入

144
00:05:17,720 --> 00:05:19,200
就unit为什么要两个B

145
00:05:19,199 --> 00:05:20,399
两个边参数是一样的

146
00:05:20,399 --> 00:05:21,519
两个边参数不一样

147
00:05:21,519 --> 00:05:24,959
为什么两个边是因为每个边有自己的batchform

148
00:05:24,959 --> 00:05:26,759
在选自己的参数要学的

149
00:05:26,759 --> 00:05:28,399
所以你不能像啊

150
00:05:28,399 --> 00:05:28,920
red lieu一样

151
00:05:28,920 --> 00:05:30,159
只要定一个就行了

152
00:05:30,159 --> 00:05:32,959
所以你需要定义两个每个人有自己的参数

153
00:05:35,800 --> 00:05:35,959
啊

154
00:05:35,959 --> 00:05:38,560
为什么定义了self.red lieu没用啊

155
00:05:40,360 --> 00:05:40,639
啊

156
00:05:40,639 --> 00:05:41,199
没用吗

157
00:05:42,199 --> 00:05:42,360
啊

158
00:05:42,360 --> 00:05:44,919
你可以改成你可以改成self.red lieu吗

159
00:05:46,079 --> 00:05:47,560
没用吗

160
00:05:47,560 --> 00:05:48,000
可能吧

161
00:05:48,000 --> 00:05:48,879
就你改回就是了

162
00:05:48,879 --> 00:05:52,319
反正你用就in place的in place的参数

163
00:05:52,319 --> 00:05:56,519
就是说我就是red lieu就不要再新建一个就不要

164
00:05:56,519 --> 00:05:58,279
那正常情况下就新建一个output

165
00:05:58,279 --> 00:06:02,439
然后把输入把它做max 0放过去

166
00:06:02,439 --> 00:06:04,639
in place就是我就不要新建这个参数

167
00:06:04,639 --> 00:06:07,600
就用就改改写input的那个东西

168
00:06:07,600 --> 00:06:08,959
把里面的指揮器换掉

169
00:06:08,959 --> 00:06:10,399
能省一点点内存

170
00:06:13,920 --> 00:06:16,759
就是这里输入尺寸96x962

171
00:06:16,759 --> 00:06:18,839
24x24是怎么确定的

172
00:06:19,240 --> 00:06:21,600
不是调参就是24和24

173
00:06:21,600 --> 00:06:23,320
就是image net默认的尺寸

174
00:06:23,320 --> 00:06:27,160
我用那个东西是说我的调参的原因是说你在训练

175
00:06:27,160 --> 00:06:29,240
我希望你能在一两分钟之内完成

176
00:06:29,240 --> 00:06:31,120
如果一两分钟之外的完不成的话

177
00:06:31,120 --> 00:06:32,760
我就把你的input size搞小一点

178
00:06:33,600 --> 00:06:36,120
这是我的调参的是我调参了

179
00:06:36,120 --> 00:06:38,000
但是我的调参的不是根据accuracy

180
00:06:38,000 --> 00:06:40,520
我纯粹是做demo的时候方便一点

181
00:06:43,920 --> 00:06:47,080
训练精度是不是正常情况下会稍微大于测试精度

182
00:06:47,079 --> 00:06:49,079
是不是意味着永远达不到百分百识别率

183
00:06:49,959 --> 00:06:50,279
不一定

184
00:06:50,279 --> 00:06:54,120
测试精度可以大于训练精度了

185
00:06:54,120 --> 00:06:55,039
就你会发现

186
00:06:56,959 --> 00:06:59,599
训练精度是可以大于测试精度

187
00:06:59,599 --> 00:07:00,719
我们在之后会看到

188
00:07:00,719 --> 00:07:03,639
假设你做了大量的data的augmentation

189
00:07:03,639 --> 00:07:05,959
就是说数据里面大量噪音的情况下

190
00:07:05,959 --> 00:07:09,039
你会发现你的测试精度是比你的训练精度高的

191
00:07:09,039 --> 00:07:11,439
因为你测试的时候不会做augmentation

192
00:07:11,439 --> 00:07:13,000
训练时候你加入大量噪音

193
00:07:13,000 --> 00:07:17,920
使得你看上去精度可能会低一点

194
00:07:17,920 --> 00:07:22,160
所以就说你会在rest net经常在真正的训练时候

195
00:07:22,160 --> 00:07:25,000
你会发现经常的测试精度是比你的训练精度要高的

196
00:07:27,199 --> 00:07:29,680
所以意味着达不到百分百识别率

197
00:07:29,680 --> 00:07:33,439
而我们其实现在没有达到过百分百的识别率

198
00:07:35,120 --> 00:07:37,120
为什么是因为有很多原因

199
00:07:37,120 --> 00:07:40,560
一个是说确实完全分类正确是很难的

200
00:07:40,560 --> 00:07:43,040
第二个是说我们的数据级其实是不对的

201
00:07:43,040 --> 00:07:44,720
就是所谓的image net

202
00:07:44,720 --> 00:07:47,439
它的测试级里面其实也有12%

203
00:07:47,439 --> 00:07:50,879
我听说还是5%的图片是标错的

204
00:07:50,879 --> 00:07:52,920
就因为你那个数据是人标的

205
00:07:52,920 --> 00:07:54,319
人是有错误率的

206
00:07:55,439 --> 00:07:57,079
所以数据级就是不对的

207
00:07:57,079 --> 00:07:59,040
你做到百分百是没意义的

208
00:07:59,040 --> 00:07:59,399
OK

209
00:08:01,639 --> 00:08:03,319
问题23

210
00:08:03,319 --> 00:08:06,319
组队的话一组是每个人一本书吗

211
00:08:06,319 --> 00:08:08,439
是倒是的

212
00:08:08,439 --> 00:08:10,079
大家不要去hack这个事情了

213
00:08:10,079 --> 00:08:14,279
就是说当你组一个有50个人的一组对吧

214
00:08:14,639 --> 00:08:18,399
当然你们所有人如果说我就把所有人组一个队参加

215
00:08:18,399 --> 00:08:19,639
那么你所有人就排第一

216
00:08:19,639 --> 00:08:21,039
当然你可以怎么干这个事情

217
00:08:21,039 --> 00:08:22,159
是的

218
00:08:22,159 --> 00:08:25,519
我们是说每个一组每个人会拿到一本书

219
00:08:25,519 --> 00:08:27,759
但是我觉得还是说你组队

220
00:08:27,759 --> 00:08:32,279
纯粹是因为你可以从对方那里学到什么东西

221
00:08:32,279 --> 00:08:33,639
就是说你有些idea

222
00:08:33,639 --> 00:08:35,279
大家也可以一起进步

223
00:08:35,319 --> 00:08:36,519
如果你就是组个队

224
00:08:36,519 --> 00:08:37,879
纯粹是为了刷个分的话

225
00:08:37,879 --> 00:08:39,799
我觉得当然你也可以这么做了

226
00:08:39,799 --> 00:08:42,679
所以确实是反正是这样子的

227
00:08:42,679 --> 00:08:43,000
吧

228
00:08:45,399 --> 00:08:46,240
问题24

229
00:08:46,240 --> 00:08:49,319
我们数据级里面有没有错误的label

230
00:08:50,839 --> 00:08:53,479
就fashion list吗

231
00:08:53,519 --> 00:08:54,279
我不知道

232
00:08:54,279 --> 00:08:56,039
也许有吧

233
00:08:56,039 --> 00:08:56,959
我没仔细看过

234
00:08:56,959 --> 00:08:58,159
就是说

235
00:08:58,839 --> 00:09:00,519
一般来说你

236
00:09:02,439 --> 00:09:06,639
一般来说你不能假设一个数据里面是完全正确的

237
00:09:06,679 --> 00:09:09,359
你可以去看一下fashion list有没有

238
00:09:09,399 --> 00:09:11,519
你可以也许去看

239
00:09:11,519 --> 00:09:13,240
最近不是有篇paper来讲这个东西

240
00:09:13,240 --> 00:09:14,960
我都忘了这paper叫什么名字了

241
00:09:15,000 --> 00:09:17,600
但是label相对来说应该是没有的

242
00:09:17,600 --> 00:09:19,000
数字看起来比较简单

243
00:09:19,000 --> 00:09:20,399
但fashion list的

244
00:09:20,440 --> 00:09:22,480
就是说很多时候你没有

245
00:09:22,519 --> 00:09:23,680
可能没有错误

246
00:09:23,680 --> 00:09:25,840
但是有一些就特别难分

247
00:09:26,440 --> 00:09:27,360
就是说你这个东西

248
00:09:27,360 --> 00:09:30,360
你可能人看都觉得不知道这个到底是一个衣服

249
00:09:30,360 --> 00:09:31,560
还是一个什么东西

250
00:09:31,800 --> 00:09:33,759
就是说这些图片就很模棱两可

251
00:09:33,759 --> 00:09:35,360
你觉得从人的角度来讲

252
00:09:35,360 --> 00:09:38,039
你都可以是说可以是这个可以是那个

253
00:09:38,839 --> 00:09:41,240
就是这个东西叫做hard case

254
00:09:41,279 --> 00:09:43,399
很多数据是有这种情况的

255
00:09:43,439 --> 00:09:46,240
而且你就看你的模型做错的地方很多

256
00:09:46,279 --> 00:09:48,719
绝大部分时候你的模型分错的地方

257
00:09:48,719 --> 00:09:50,839
就是一些这样子的难的一些数据

258
00:09:50,839 --> 00:09:52,839
难的一些样本点

259
00:09:52,879 --> 00:09:56,120
对于容易的样本点模型很容易就训练好了

260
00:09:56,599 --> 00:09:57,199
OK

261
00:09:57,439 --> 00:10:02,079
所以确实是说大家需要去关心数据里面的误差

262
00:10:02,199 --> 00:10:06,120
所以就为什么data scientist的80%是现在搞数据

263
00:10:06,159 --> 00:10:07,840
就是去看数据里面

264
00:10:07,879 --> 00:10:09,080
你找人去标数据

265
00:10:09,080 --> 00:10:11,240
很有可能标的大量都是噪音

266
00:10:11,240 --> 00:10:13,320
或者数据本身其实很多噪音

267
00:10:13,519 --> 00:10:14,080
OK

268
00:10:14,720 --> 00:10:19,279
所以但是你的模型不会说要求你数据完全这些没关系

269
00:10:19,480 --> 00:10:21,960
所以大家也不用特别纠结

270
00:10:22,000 --> 00:10:24,759
而且我觉得大家不要去追求100%的正确识别率

271
00:10:24,759 --> 00:10:27,279
这个是很难做到的

272
00:10:27,399 --> 00:10:30,159
而且如果你能做到就表示你的数据太简单了

273
00:10:30,159 --> 00:10:32,360
没什么没什么意思

