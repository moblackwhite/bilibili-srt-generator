1
00:00:00,000 --> 00:00:03,560
在信息回归的从零开始实现之后

2
00:00:03,560 --> 00:00:05,960
我们来讲一下它的简洁实现

3
00:00:06,320 --> 00:00:07,560
所谓的简洁实现

4
00:00:07,560 --> 00:00:10,279
就是使用PyTorch的NN的Module

5
00:00:10,279 --> 00:00:13,439
使用它提供的一些数据处理的模块

6
00:00:13,439 --> 00:00:15,759
来让我们的实现更加简单

7
00:00:16,679 --> 00:00:17,000
好

8
00:00:17,000 --> 00:00:18,600
首先我们来看一下

9
00:00:18,760 --> 00:00:21,199
我们唯一的区别是说

10
00:00:21,199 --> 00:00:23,480
我们从Torch的UTLs里面

11
00:00:23,800 --> 00:00:25,560
import一个data的Module

12
00:00:26,000 --> 00:00:28,719
这会有一些处理数据的一些模块

13
00:00:29,160 --> 00:00:30,879
试一下我们还是跟之前一样

14
00:00:30,879 --> 00:00:33,439
我们构造一个真实的W和一个B

15
00:00:33,439 --> 00:00:36,600
然后通过人工数据合成的函数

16
00:00:36,600 --> 00:00:39,519
生成我们要的features和labels

17
00:00:43,159 --> 00:00:45,519
就是说这里有一点不一样的是说

18
00:00:45,519 --> 00:00:47,679
假设我们已经有我们的features

19
00:00:47,679 --> 00:00:48,640
和labels了

20
00:00:48,640 --> 00:00:52,439
然后我们可以把它做成一个list

21
00:00:52,640 --> 00:00:55,239
传到一个Tensor的data set里面

22
00:00:55,519 --> 00:00:57,480
就是说把我们的X和Y传进去

23
00:00:57,480 --> 00:01:00,040
会得到一个PyTorch的一个data set

24
00:01:00,600 --> 00:01:03,760
data set里拿到数据集之后

25
00:01:03,760 --> 00:01:06,400
那么我们可以调用data load的这个函数

26
00:01:06,400 --> 00:01:10,359
每一次从里面随机的挑选B个样本出来

27
00:01:10,719 --> 00:01:12,320
就是说batch size给定

28
00:01:12,480 --> 00:01:13,560
然后shuffle就是说

29
00:01:13,560 --> 00:01:16,080
是不是要随机去打乱它的顺序

30
00:01:16,320 --> 00:01:17,600
如果是training的话

31
00:01:17,600 --> 00:01:18,960
那我们当时需要的

32
00:01:20,159 --> 00:01:22,400
它的使用跟我们之前使用差不多

33
00:01:22,719 --> 00:01:24,439
比如说我们batch size等于10

34
00:01:24,600 --> 00:01:26,120
我们构造了一个这样子的

35
00:01:26,120 --> 00:01:27,359
iterator出来之后

36
00:01:27,640 --> 00:01:30,320
我们把它转成一个python的iterator

37
00:01:30,880 --> 00:01:34,159
通过next这个函数来得到一个X和一个Y

38
00:01:34,159 --> 00:01:36,560
就跟我们其实之前的使用是差不多的

39
00:01:41,000 --> 00:01:41,240
好

40
00:01:41,240 --> 00:01:42,960
接下来是我们模型的定义

41
00:01:43,640 --> 00:01:47,439
模型定义是说Torch里面NN这个module里面

42
00:01:47,640 --> 00:01:50,080
就是代表是neural network的缩写

43
00:01:50,200 --> 00:01:53,600
定义了大量的定义好的层

44
00:01:54,120 --> 00:01:57,080
对于我们的线性回归那一用的

45
00:01:57,120 --> 00:02:00,679
其实等价于它里面的线性层

46
00:02:00,679 --> 00:02:02,359
或者是说全连接层

47
00:02:03,000 --> 00:02:05,079
它唯一要指定的是说

48
00:02:05,079 --> 00:02:06,759
我的输入的维度是多少

49
00:02:06,759 --> 00:02:08,359
我的输出的维度是多少

50
00:02:08,560 --> 00:02:10,879
因为输入我们是R输出是E

51
00:02:11,400 --> 00:02:14,400
然后当然我们可以直接用linear层就行了

52
00:02:14,680 --> 00:02:16,680
我们也讲过说线性回归

53
00:02:16,680 --> 00:02:18,960
就是一个简单的单层神经网络

54
00:02:19,400 --> 00:02:21,199
但是为了之后方便

55
00:02:21,479 --> 00:02:25,159
我们把它放到一个sequential的一个容器里面

56
00:02:25,240 --> 00:02:27,840
你可理解就是一个list of layers

57
00:02:27,840 --> 00:02:31,879
就是我们把一些层按顺序一个放在一起

58
00:02:33,519 --> 00:02:36,560
然后因为它就是一个layer对吧

59
00:02:36,560 --> 00:02:39,479
我的network可以通过0来访问到这个layer

60
00:02:39,719 --> 00:02:42,879
然后通过点wait来访问到它的W

61
00:02:43,199 --> 00:02:45,519
它的data里面就是它的真实data

62
00:02:45,840 --> 00:02:47,680
normal下滑线就是说

63
00:02:47,719 --> 00:02:52,199
使用正台分布来替换掉data的值

64
00:02:53,200 --> 00:02:54,000
跟之前一样

65
00:02:54,000 --> 00:02:55,360
我们使用的均值为0

66
00:02:55,360 --> 00:02:57,520
方程为0.01来替换它

67
00:02:58,320 --> 00:03:01,440
同样的bias的话就是偏差

68
00:03:02,280 --> 00:03:04,920
最好我们直接把它设成0了

69
00:03:06,920 --> 00:03:09,480
所以这两个模块等价于之前

70
00:03:09,480 --> 00:03:11,960
我们手动实现W和B

71
00:03:11,960 --> 00:03:15,120
以及实现我们的network的function

72
00:03:17,600 --> 00:03:20,360
因为均放误差是一个非常常用的误差

73
00:03:20,360 --> 00:03:23,000
所以当然是已经有自定义好了

74
00:03:23,160 --> 00:03:26,440
它在NN里面叫做MSE loss

75
00:03:28,200 --> 00:03:30,440
所以我们就直接把它拿出来

76
00:03:30,760 --> 00:03:31,720
同样的话

77
00:03:31,920 --> 00:03:34,520
SGD也是一个非常常用的算法

78
00:03:34,560 --> 00:03:35,840
所以肯定是有的

79
00:03:36,080 --> 00:03:39,560
所以它在另外一个叫optimizer的

80
00:03:39,560 --> 00:03:40,360
module里面

81
00:03:40,400 --> 00:03:42,120
把它SGD拿出来

82
00:03:42,480 --> 00:03:45,560
它需要传入至少两个参数

83
00:03:46,200 --> 00:03:49,120
第一个参数是所有network parameters

84
00:03:49,120 --> 00:03:50,640
就包括里面所有的参数

85
00:03:50,640 --> 00:03:51,960
包括了W和B

86
00:03:53,159 --> 00:03:55,200
就这一次拿出它所有的参数

87
00:03:55,400 --> 00:03:57,039
第二个是我要制定学习率

88
00:03:57,039 --> 00:03:58,120
就是0.03

89
00:04:00,439 --> 00:04:00,680
好

90
00:04:00,680 --> 00:04:01,319
这样子的话

91
00:04:01,319 --> 00:04:03,000
我们就跟之前一样

92
00:04:03,000 --> 00:04:04,319
定义了所有的模块

93
00:04:04,319 --> 00:04:05,640
我们可以开始训练了

94
00:04:06,120 --> 00:04:08,840
训练模块跟之前其实是很像的

95
00:04:09,280 --> 00:04:10,439
就是我们首先说

96
00:04:10,439 --> 00:04:12,599
我们要迭代三个周期

97
00:04:12,879 --> 00:04:15,400
对每一次扫一遍数据里面

98
00:04:16,399 --> 00:04:17,000
在里面

99
00:04:17,000 --> 00:04:18,319
在个data iterator里面

100
00:04:18,319 --> 00:04:21,319
我们一次一次的把我们的mini batch

101
00:04:21,319 --> 00:04:22,000
拿出来

102
00:04:22,000 --> 00:04:23,159
小批量拿出来

103
00:04:23,439 --> 00:04:25,199
然后我们放进我们的net里面

104
00:04:25,319 --> 00:04:26,879
跟之前不一样是说

105
00:04:27,159 --> 00:04:29,599
net这里本身自己带了模型参数

106
00:04:29,599 --> 00:04:31,959
所以我们不需要把W和B弄进去了

107
00:04:33,120 --> 00:04:34,159
拿到预测值

108
00:04:34,159 --> 00:04:35,879
更真实的y做loss

109
00:04:37,399 --> 00:04:39,319
然后这里是说

110
00:04:39,439 --> 00:04:40,639
我们的trainer

111
00:04:40,879 --> 00:04:41,600
我告诉你

112
00:04:41,600 --> 00:04:43,039
就是我们的优化器

113
00:04:43,039 --> 00:04:45,199
告诉你说先把t2清零

114
00:04:45,199 --> 00:04:46,279
叫zero grad

115
00:04:46,319 --> 00:04:47,839
要把t2清完零之后

116
00:04:47,920 --> 00:04:50,040
我们来计算backward

117
00:04:51,800 --> 00:04:52,960
这里其实已经

118
00:04:53,280 --> 00:04:54,760
PyTorch已经把你做了sum

119
00:04:54,760 --> 00:04:56,320
你就不需要再求sum了

120
00:04:57,280 --> 00:04:58,000
最后来说

121
00:04:58,000 --> 00:04:59,920
我们有了t2之后

122
00:05:00,120 --> 00:05:01,800
调用step这个函数

123
00:05:02,000 --> 00:05:03,800
来进行一次模型的更新

124
00:05:05,040 --> 00:05:06,360
这个是跟之前一样的

125
00:05:06,640 --> 00:05:08,080
最后我们跟之前一样

126
00:05:08,120 --> 00:05:10,640
对一档扫完一遍数据之后

127
00:05:10,880 --> 00:05:13,080
然后我们来把整个

128
00:05:14,640 --> 00:05:16,560
所有的feature放到network里面

129
00:05:16,560 --> 00:05:18,759
跟所谓的label上做一次loss

130
00:05:18,759 --> 00:05:19,839
然后打印一次

131
00:05:20,199 --> 00:05:21,199
可以看到是说

132
00:05:21,959 --> 00:05:23,280
每一次这一次

133
00:05:23,839 --> 00:05:25,360
基本上一开始就很小

134
00:05:25,759 --> 00:05:28,160
因为是因为随机初始化的问题

135
00:05:28,840 --> 00:05:29,800
所以每一次的结果

136
00:05:29,800 --> 00:05:31,199
会有稍微的不一样

137
00:05:31,240 --> 00:05:32,800
但是这里的实现

138
00:05:32,800 --> 00:05:34,560
应该跟之前的从零开始

139
00:05:34,560 --> 00:05:36,240
是完全等价的

140
00:05:37,519 --> 00:05:38,319
这样子的话

141
00:05:38,319 --> 00:05:40,199
就给大家一个体验是说

142
00:05:40,800 --> 00:05:43,840
我们使用高级的一些模块的时候

143
00:05:43,840 --> 00:05:44,639
大家要知道

144
00:05:44,639 --> 00:05:45,720
其实里面的实现

145
00:05:45,720 --> 00:05:47,080
其实也是很简单的

146
00:05:47,120 --> 00:05:50,480
不用被他一些封装给吓到

147
00:05:51,480 --> 00:05:52,440
同样的来说

148
00:05:52,480 --> 00:05:54,360
我们之所以对于现行回归

149
00:05:54,360 --> 00:05:56,280
做一个很彻底的实现

150
00:05:56,280 --> 00:05:57,040
是因为

151
00:05:57,080 --> 00:05:58,920
它虽然是一个很小的模型

152
00:05:58,960 --> 00:06:00,760
但是麻雀虽小

153
00:06:00,760 --> 00:06:02,200
五脏俱全

154
00:06:02,360 --> 00:06:04,520
所以它包括了所有的数据

155
00:06:04,520 --> 00:06:05,560
如何读取

156
00:06:05,720 --> 00:06:07,040
模型的定义

157
00:06:07,160 --> 00:06:08,600
参数的初始化

158
00:06:09,200 --> 00:06:10,200
损失函数

159
00:06:10,240 --> 00:06:11,560
以及训练模块

160
00:06:11,800 --> 00:06:14,120
所以不管是后面是

161
00:06:14,439 --> 00:06:16,759
上百层的深度神经网络

162
00:06:16,759 --> 00:06:18,759
还是最简单的现行回归

163
00:06:18,879 --> 00:06:21,040
其实它的每个模块都是一样的

164
00:06:21,040 --> 00:06:22,840
都是很模板化的一个过程

165
00:06:23,040 --> 00:06:24,800
所以我们通过一个很简单的例子

166
00:06:24,800 --> 00:06:26,639
给大家解释了每个模块

167
00:06:26,720 --> 00:06:28,680
在接下来的一系列课程里面

168
00:06:28,680 --> 00:06:31,120
我们会在各个模块做一些

169
00:06:31,160 --> 00:06:32,560
让它变得越来越复杂

170
00:06:32,560 --> 00:06:36,199
最后得到我们现在的当下最好的

171
00:06:36,199 --> 00:06:37,160
深度神经网络

172
00:06:37,160 --> 00:06:38,199
是怎么实现的

173
00:06:38,600 --> 00:06:38,920
好

174
00:06:38,920 --> 00:06:40,879
这就是现行回归

175
00:06:40,879 --> 00:06:42,160
我们讲到这里

