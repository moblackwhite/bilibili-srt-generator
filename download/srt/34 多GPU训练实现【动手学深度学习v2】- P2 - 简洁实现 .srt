1
00:00:00,000 --> 00:00:05,240
好 我们今天的任务是讲多GPU训练的实现

2
00:00:06,000 --> 00:00:10,919
大家回忆一下我们在两个星期前讲的多GPU的数据

3
00:00:10,919 --> 00:00:12,120
比起是怎么做的

4
00:00:12,320 --> 00:00:13,640
大家回忆一下

5
00:00:13,800 --> 00:00:16,559
就是你有一个你有几块卡

6
00:00:16,719 --> 00:00:18,199
然后来一个数据的话

7
00:00:18,199 --> 00:00:21,719
其实核心思想是说你对一个小批量数据

8
00:00:21,719 --> 00:00:24,240
我把你分成你有4块卡的话

9
00:00:24,240 --> 00:00:25,440
我把你分成4块

10
00:00:25,839 --> 00:00:27,519
就你有百个样本的话

11
00:00:27,519 --> 00:00:30,039
那么每一块卡拿到25个样本

12
00:00:30,559 --> 00:00:33,560
然后这块每个GPU拿到25个样本之后

13
00:00:33,560 --> 00:00:35,039
去计算你的梯度

14
00:00:35,159 --> 00:00:37,439
最后你把梯度加起来做更新

15
00:00:37,480 --> 00:00:39,200
其实这就是多GPU的训练

16
00:00:39,399 --> 00:00:42,600
我们现在来给大家讲一下

17
00:00:42,600 --> 00:00:43,920
它是如何实现的

18
00:00:45,840 --> 00:00:48,879
OK 首先我们是说我们破了一些东西

19
00:00:49,039 --> 00:00:50,000
我们就跳过了

20
00:00:50,359 --> 00:00:54,239
我们这里用一个Nernet来做实验

21
00:00:54,520 --> 00:00:57,400
这个是基本上就是我们在讲

22
00:00:57,440 --> 00:00:59,920
Nernet实现的那一节的时候

23
00:00:59,920 --> 00:01:01,640
把代码操了过来

24
00:01:01,800 --> 00:01:03,400
就是说基本上是没有改变的

25
00:01:04,480 --> 00:01:06,520
所以我们就直接跳过去了

26
00:01:08,200 --> 00:01:09,439
大家知道这是一个Nernet

27
00:01:09,439 --> 00:01:11,520
这是一个非常小的网络

28
00:01:11,560 --> 00:01:13,480
我们用它来做

29
00:01:13,920 --> 00:01:15,200
作为我们的样本网络

30
00:01:15,200 --> 00:01:16,840
来测试多GPU训练

31
00:01:16,840 --> 00:01:19,120
当然这个选择会带来很多问题

32
00:01:19,120 --> 00:01:20,680
我们之后会给大家讲

33
00:01:21,000 --> 00:01:26,960
就接下来就是涉及到多GPU的一些东西了

34
00:01:27,640 --> 00:01:29,880
首先说要怎么办

35
00:01:30,080 --> 00:01:32,400
就是说我给你一个参数

36
00:01:32,400 --> 00:01:33,880
这些模型的参数

37
00:01:34,040 --> 00:01:36,840
它应该是默认是放在CPU上的

38
00:01:36,840 --> 00:01:38,400
就是说你什么都不管的情况下

39
00:01:38,440 --> 00:01:41,360
应该是创建在你的主类存里

40
00:01:41,880 --> 00:01:43,240
我们有知道

41
00:01:43,280 --> 00:01:45,160
在多GPU训练的时候

42
00:01:45,160 --> 00:01:49,120
你需要把你的参数挪到GPU上

43
00:01:49,200 --> 00:01:50,200
才能做计算

44
00:01:51,200 --> 00:01:52,880
所以这个函数干的事情

45
00:01:52,880 --> 00:01:55,000
就是说你给我一些参数

46
00:01:55,040 --> 00:01:55,840
这是一个list

47
00:01:55,880 --> 00:01:58,320
就所有的层的那些参数

48
00:01:58,440 --> 00:01:59,360
然后告诉我说

49
00:01:59,360 --> 00:02:04,280
我要去把这个参数放到哪一个GPU上

50
00:02:04,720 --> 00:02:07,040
这个函数就是说把你这个参数挪过去

51
00:02:07,800 --> 00:02:10,240
可以看到是说新的参数

52
00:02:10,240 --> 00:02:13,560
就是我对老参数的column

53
00:02:13,719 --> 00:02:15,240
就column你不写没关系

54
00:02:15,879 --> 00:02:19,240
就是说之所以写column

55
00:02:19,240 --> 00:02:21,280
就是说你to这个函数

56
00:02:21,680 --> 00:02:24,040
假设我这个P已经在GPU0上了

57
00:02:24,040 --> 00:02:26,560
你再to到GPU0上的话

58
00:02:26,560 --> 00:02:28,120
它是不会做任何操作的

59
00:02:28,159 --> 00:02:29,719
如果你column在to的话

60
00:02:29,719 --> 00:02:31,320
它永远是帮你复制

61
00:02:31,800 --> 00:02:33,320
就是说从保险起见

62
00:02:33,320 --> 00:02:34,159
你可以column

63
00:02:34,280 --> 00:02:36,560
但是实际上在我们这个情况里面

64
00:02:36,560 --> 00:02:38,120
你不用这个是没关系的

65
00:02:39,960 --> 00:02:41,719
然后我们把它放到

66
00:02:41,719 --> 00:02:43,320
就每一个就是说

67
00:02:43,360 --> 00:02:45,280
在Priority里面

68
00:02:45,280 --> 00:02:48,439
每一个P把它复制到我们要的GPU上

69
00:02:49,360 --> 00:02:50,920
接下来我们知道我们要做计算

70
00:02:50,920 --> 00:02:52,680
因为我们需要t度

71
00:02:52,680 --> 00:02:54,159
所以我们就对每一个P

72
00:02:54,159 --> 00:02:56,080
说我们需要计算t度

73
00:02:56,640 --> 00:02:58,439
最后把我们参数拿出去就行了

74
00:02:59,800 --> 00:03:01,200
所以看到是说

75
00:03:01,560 --> 00:03:02,719
开始的我们的prime

76
00:03:02,719 --> 00:03:04,240
就是nonet这些参数

77
00:03:04,360 --> 00:03:06,040
全部是放在CPU上的

78
00:03:06,439 --> 00:03:07,719
现在我们尝试说

79
00:03:07,719 --> 00:03:10,760
我们把它放在我们的GPU0上

80
00:03:11,520 --> 00:03:13,879
那么我得到的newprime

81
00:03:13,879 --> 00:03:14,680
就是说

82
00:03:14,719 --> 00:03:15,760
那就是

83
00:03:17,000 --> 00:03:18,319
你这个是

84
00:03:18,680 --> 00:03:19,400
打印一下

85
00:03:19,400 --> 00:03:20,960
你看到这个是在哪里

86
00:03:20,960 --> 00:03:22,319
在cuda0上面

87
00:03:23,319 --> 00:03:23,920
OK

88
00:03:24,120 --> 00:03:26,120
然后点grad

89
00:03:26,280 --> 00:03:27,480
因为我们还没有做计算

90
00:03:27,599 --> 00:03:28,760
所以grad是none

91
00:03:28,800 --> 00:03:30,159
就是你打出来是个none

92
00:03:30,599 --> 00:03:31,159
OK

93
00:03:32,159 --> 00:03:33,879
所以这是我们实现的第一个函数

94
00:03:33,879 --> 00:03:37,360
把一个参数复制到另外一个GPU上

95
00:03:40,520 --> 00:03:43,520
那么第二个函数是叫做oreduce

96
00:03:43,520 --> 00:03:46,640
就是说这个是大家如果学过

97
00:03:46,640 --> 00:03:48,320
就是说高性能计算里面

98
00:03:48,320 --> 00:03:51,600
oreduce是一个非常有名的一个操作

99
00:03:51,840 --> 00:03:52,640
就是说

100
00:03:53,320 --> 00:03:54,040
他干嘛呢

101
00:03:54,040 --> 00:03:54,800
就是说

102
00:03:55,200 --> 00:03:58,080
我在所有的设备上

103
00:03:58,080 --> 00:04:00,280
就GPU上有一些t度

104
00:04:00,480 --> 00:04:01,320
有一些data

105
00:04:02,240 --> 00:04:04,080
我把这些data全部加起来

106
00:04:05,280 --> 00:04:09,160
然后再把这个结果复制回去

107
00:04:10,240 --> 00:04:11,400
就你的操作

108
00:04:11,400 --> 00:04:13,360
就是说最简单

109
00:04:13,360 --> 00:04:14,440
我们实现了一个操作

110
00:04:14,440 --> 00:04:15,600
就是说你怎么做

111
00:04:15,720 --> 00:04:18,000
就是把你有4个GPU的话

112
00:04:18,319 --> 00:04:19,240
我把GPU2

113
00:04:19,280 --> 00:04:19,840
GPU3

114
00:04:19,840 --> 00:04:22,199
GPU4的数据

115
00:04:22,240 --> 00:04:23,400
先复制到

116
00:04:23,400 --> 00:04:25,480
先发送到GPU0上面

117
00:04:26,000 --> 00:04:26,680
就GPU1

118
00:04:26,879 --> 00:04:27,960
就第一个GPU上面

119
00:04:29,040 --> 00:04:30,680
然后我在上面做相加

120
00:04:30,680 --> 00:04:33,879
因为相加必须要在同一个GPU上

121
00:04:34,480 --> 00:04:35,240
的结果

122
00:04:35,240 --> 00:04:37,480
然后我们又把它发送回去

123
00:04:38,439 --> 00:04:39,360
所以大家可以看一下

124
00:04:39,360 --> 00:04:40,360
这是什么操作

125
00:04:41,160 --> 00:04:42,720
就oreduce给一个data

126
00:04:43,000 --> 00:04:44,440
data是一个list

127
00:04:44,880 --> 00:04:45,640
就里面就是说

128
00:04:45,640 --> 00:04:47,600
你有4个GPU的话

129
00:04:47,720 --> 00:04:50,560
那么它就是list of 4

130
00:04:50,560 --> 00:04:51,920
就是4个元素在里面

131
00:04:51,920 --> 00:04:54,120
每一个元素是在各自的GPU上的

132
00:04:55,760 --> 00:04:56,800
我们怎么做呢

133
00:04:57,240 --> 00:04:58,160
就是说

134
00:04:59,040 --> 00:05:00,920
我们这就是把所有的数据

135
00:05:00,920 --> 00:05:02,160
放到GPU0上

136
00:05:02,320 --> 00:05:03,200
那么就是说

137
00:05:03,400 --> 00:05:04,480
GPU0自己不用动

138
00:05:05,120 --> 00:05:06,720
对于所有的1

139
00:05:06,720 --> 00:05:07,760
从1到后面

140
00:05:07,760 --> 00:05:08,400
就是说

141
00:05:08,960 --> 00:05:10,720
GPU0以外的GPU

142
00:05:11,560 --> 00:05:12,840
这个data i就是

143
00:05:12,840 --> 00:05:14,160
第2个GPU上

144
00:05:14,520 --> 00:05:17,560
我先把它复制到GPU0

145
00:05:17,560 --> 00:05:18,640
就是说data0

146
00:05:18,640 --> 00:05:19,880
就第0个

147
00:05:21,120 --> 00:05:22,440
Tensor的GPU上面

148
00:05:23,400 --> 00:05:26,320
然后再把它对它相加

149
00:05:26,600 --> 00:05:28,440
就把它加到GPU0

150
00:05:28,440 --> 00:05:29,480
对应的东西上面

151
00:05:31,640 --> 00:05:32,920
做完这个操作之后

152
00:05:32,920 --> 00:05:34,280
data0就是

153
00:05:34,400 --> 00:05:36,400
所有的data的求和

154
00:05:36,840 --> 00:05:37,880
接下来我们怎么办

155
00:05:38,000 --> 00:05:41,199
就是把新的结果复制回去

156
00:05:41,240 --> 00:05:41,639
就是说

157
00:05:41,639 --> 00:05:43,759
对于GPU0以外的GPU

158
00:05:44,199 --> 00:05:45,360
我把它data

159
00:05:45,360 --> 00:05:47,240
重新写成它的

160
00:05:47,920 --> 00:05:48,920
加后的结果

161
00:05:49,279 --> 00:05:50,600
把data0复制回去

162
00:05:51,879 --> 00:05:52,560
所以可以看一下

163
00:05:52,560 --> 00:05:53,560
就是说这个操作

164
00:05:53,560 --> 00:05:56,199
其实是干了一件很简单的事情

165
00:05:57,519 --> 00:05:59,159
我们说data等于

166
00:06:00,319 --> 00:06:01,560
我们创建两

167
00:06:01,839 --> 00:06:03,639
我们这也是创建两个

168
00:06:03,959 --> 00:06:05,120
我们假设两个GPU

169
00:06:05,759 --> 00:06:06,199
然后

170
00:06:08,680 --> 00:06:09,680
我们的

171
00:06:12,800 --> 00:06:14,040
第0个GPU

172
00:06:14,279 --> 00:06:16,439
我们创建的是一个

173
00:06:16,480 --> 00:06:18,000
11的张梁

174
00:06:18,360 --> 00:06:19,719
然后第1个GPU

175
00:06:19,839 --> 00:06:21,600
我们创建的22的张梁

176
00:06:23,000 --> 00:06:25,199
然后在这是all reduce之前

177
00:06:25,879 --> 00:06:27,000
就data是

178
00:06:27,240 --> 00:06:30,240
data是一个list of两个张梁

179
00:06:31,120 --> 00:06:32,800
然后all reduce之后

180
00:06:32,959 --> 00:06:35,240
就是我们把data里面的数据替换掉了

181
00:06:35,400 --> 00:06:36,680
那么就是说变成了

182
00:06:36,720 --> 00:06:38,160
1加上22

183
00:06:38,360 --> 00:06:39,079
就变成33

184
00:06:39,079 --> 00:06:39,680
对不是

185
00:06:39,879 --> 00:06:42,560
然后要把每一个GPU上的数据

186
00:06:42,560 --> 00:06:44,040
全部更新成33

187
00:06:44,960 --> 00:06:46,639
这就是all reduce做的操作

188
00:06:47,720 --> 00:06:49,000
OK这个操作就是

189
00:06:49,000 --> 00:06:49,960
大家可以知道干嘛

190
00:06:50,120 --> 00:06:50,400
干嘛

191
00:06:50,400 --> 00:06:52,879
就是把我们的t度全部加起来

192
00:06:52,879 --> 00:06:54,759
然后每个继续拉到自己的t度

193
00:06:55,840 --> 00:06:56,360
OK

194
00:06:58,840 --> 00:06:59,240
好

195
00:06:59,240 --> 00:07:00,560
第3个函数

196
00:07:00,560 --> 00:07:02,240
就是说我们有讲过

197
00:07:02,280 --> 00:07:04,439
我们需要加一个小批量

198
00:07:05,240 --> 00:07:06,639
把它切开

199
00:07:07,360 --> 00:07:10,840
然后放到不同的GPU上面

200
00:07:12,120 --> 00:07:13,879
我们这里用的一个

201
00:07:13,879 --> 00:07:14,800
我们就偷个懒

202
00:07:14,960 --> 00:07:16,199
这里用的是

203
00:07:16,480 --> 00:07:19,040
PyTorch M parallel module里面

204
00:07:19,040 --> 00:07:20,519
有个scatter这个函数

205
00:07:21,480 --> 00:07:22,360
scatter这个函数

206
00:07:22,360 --> 00:07:23,040
就是说

207
00:07:23,079 --> 00:07:23,920
我有一个data

208
00:07:23,920 --> 00:07:25,160
data就是一个tensor

209
00:07:26,120 --> 00:07:27,120
然后我给你说

210
00:07:27,120 --> 00:07:29,360
我要所有的GPU是什么样子

211
00:07:29,920 --> 00:07:31,720
他就把你均匀的切开

212
00:07:31,920 --> 00:07:33,319
切到不同的GPU上

213
00:07:33,319 --> 00:07:34,680
我们直接看一下结果

214
00:07:34,879 --> 00:07:36,079
当然实现也可以实现

215
00:07:36,079 --> 00:07:37,879
你就直接写一个for loop就行了

216
00:07:38,040 --> 00:07:38,920
我们这就偷个懒

217
00:07:38,920 --> 00:07:39,759
就没有写了

218
00:07:40,519 --> 00:07:41,160
就是说

219
00:07:41,160 --> 00:07:42,800
假设我们创建一个tensor

220
00:07:42,800 --> 00:07:43,480
这个是什么

221
00:07:43,480 --> 00:07:45,439
这个是有5列

222
00:07:45,480 --> 00:07:46,840
然后有4行

223
00:07:48,000 --> 00:07:48,800
就是

224
00:07:49,160 --> 00:07:50,160
就这个tensor

225
00:07:50,199 --> 00:07:52,280
然后我们给你两个GPU的话

226
00:07:52,480 --> 00:07:53,639
那么这是input

227
00:07:53,800 --> 00:07:54,800
给完两个的话

228
00:07:54,800 --> 00:07:56,240
那么你的output是什么

229
00:07:56,280 --> 00:07:57,840
你的output就是

230
00:07:58,120 --> 00:07:59,439
你的GPU0上面

231
00:07:59,439 --> 00:08:01,280
就得到了前两行

232
00:08:02,199 --> 00:08:02,840
看见没有

233
00:08:02,840 --> 00:08:03,800
在前两行

234
00:08:03,960 --> 00:08:06,400
然后GPU1上是得到了后两行

235
00:08:06,439 --> 00:08:08,240
这里是格式会是

236
00:08:08,360 --> 00:08:09,280
长成这样子

237
00:08:10,160 --> 00:08:10,560
OK

238
00:08:10,560 --> 00:08:12,480
就是把你均匀切开

239
00:08:12,639 --> 00:08:14,879
当然如果你不能被整除的话

240
00:08:14,879 --> 00:08:16,319
那就是最后一个GPU得到

241
00:08:16,319 --> 00:08:17,240
会少一点点

242
00:08:19,800 --> 00:08:20,360
然后

243
00:08:21,080 --> 00:08:22,360
split batch

244
00:08:22,360 --> 00:08:23,040
就是说

245
00:08:23,040 --> 00:08:24,040
我给你一个batch

246
00:08:24,040 --> 00:08:25,920
里面有一个x有个y

247
00:08:26,160 --> 00:08:27,480
我给你一些GPU

248
00:08:27,800 --> 00:08:28,600
然后你能

249
00:08:28,600 --> 00:08:29,759
当然你先看一下

250
00:08:29,759 --> 00:08:30,639
说我的

251
00:08:31,319 --> 00:08:33,039
样本的数目

252
00:08:33,039 --> 00:08:34,120
和你的标号的数目

253
00:08:34,120 --> 00:08:34,839
是一样的

254
00:08:35,039 --> 00:08:36,159
这个shape是一样的

255
00:08:36,279 --> 00:08:37,039
然后你干嘛

256
00:08:37,039 --> 00:08:39,199
就是你就把样本

257
00:08:39,199 --> 00:08:39,839
也

258
00:08:40,879 --> 00:08:42,399
均匀分配到

259
00:08:42,399 --> 00:08:44,000
不同的GPU上

260
00:08:44,000 --> 00:08:46,240
你的y也是一样的

261
00:08:46,319 --> 00:08:47,519
你的标号也是一样的

262
00:08:47,519 --> 00:08:49,559
均匀的分配到不同GPU上

263
00:08:49,919 --> 00:08:52,000
这就是怎么将一个小批量

264
00:08:52,639 --> 00:08:53,439
均匀切开

265
00:08:53,439 --> 00:08:55,439
然后发送到各个的GPU上

266
00:08:55,439 --> 00:08:56,199
这个函数

267
00:09:00,439 --> 00:09:00,759
好

268
00:09:00,759 --> 00:09:02,719
我们接下来看一下说

269
00:09:03,439 --> 00:09:05,519
怎么样在多GPU上

270
00:09:05,519 --> 00:09:07,079
训练一个小批量

271
00:09:09,159 --> 00:09:10,319
大家看一下这个函数

272
00:09:11,599 --> 00:09:13,639
首先我们拿到的是

273
00:09:14,399 --> 00:09:15,319
你的输入

274
00:09:15,319 --> 00:09:17,039
你的小批量xy

275
00:09:17,199 --> 00:09:18,000
然后就是说

276
00:09:18,000 --> 00:09:18,839
这个是说

277
00:09:18,839 --> 00:09:21,919
我在所有的GPU上的parameters

278
00:09:21,919 --> 00:09:23,599
就是那些参数

279
00:09:24,000 --> 00:09:25,000
和我要知道

280
00:09:25,000 --> 00:09:26,439
我在用哪些GPU

281
00:09:27,000 --> 00:09:28,719
最后是我们的一个learning rate

282
00:09:30,279 --> 00:09:31,360
那么首先干的操作

283
00:09:31,360 --> 00:09:31,719
就是说

284
00:09:31,720 --> 00:09:33,120
我们先把我们的小批量

285
00:09:33,120 --> 00:09:34,600
均匀分配到各个GPU上

286
00:09:34,600 --> 00:09:35,879
用刚刚实现的

287
00:09:35,879 --> 00:09:37,360
split batch的函数

288
00:09:38,519 --> 00:09:39,560
接下来就是说

289
00:09:39,800 --> 00:09:41,560
这个换行有点奇怪

290
00:09:41,720 --> 00:09:42,879
我们来改一下

291
00:09:43,480 --> 00:09:45,000
这个换行是自动换行的

292
00:09:45,000 --> 00:09:47,240
用它自动的格式整理的换行的

293
00:09:47,240 --> 00:09:48,519
所以经常它就

294
00:09:48,519 --> 00:09:50,720
退比较长的

295
00:09:50,720 --> 00:09:52,320
它就比较傻一点

296
00:09:55,879 --> 00:09:56,519
OK

297
00:09:56,519 --> 00:09:57,639
所以接下来看

298
00:09:57,639 --> 00:09:58,680
就是说干什么事情

299
00:09:58,840 --> 00:10:00,040
这个事情就是说

300
00:10:00,599 --> 00:10:01,199
说白了

301
00:10:01,199 --> 00:10:03,519
就是对于每个GPU

302
00:10:04,919 --> 00:10:07,639
然后去算自己的

303
00:10:08,639 --> 00:10:10,559
去算一下

304
00:10:12,039 --> 00:10:14,559
自己小批量上的

305
00:10:15,639 --> 00:10:17,439
样本的损失

306
00:10:17,439 --> 00:10:18,360
函数算出来

307
00:10:19,240 --> 00:10:19,919
所以可以看一下

308
00:10:19,919 --> 00:10:20,799
这个是干嘛

309
00:10:21,240 --> 00:10:23,399
首先对于所有的xy

310
00:10:23,399 --> 00:10:24,759
和你的parameters

311
00:10:24,839 --> 00:10:26,360
我们在上面迭代一次

312
00:10:26,360 --> 00:10:27,399
就是每一次拿到

313
00:10:27,399 --> 00:10:29,319
是某一个GPU上的x

314
00:10:29,320 --> 00:10:31,120
y和对应的w

315
00:10:31,320 --> 00:10:32,640
w是完整的

316
00:10:33,000 --> 00:10:35,520
x和y是里面的一块

317
00:10:36,480 --> 00:10:38,200
然后我们就把它的

318
00:10:38,520 --> 00:10:41,400
把x和w放进nullat里面

319
00:10:41,400 --> 00:10:42,600
然后再对应的y

320
00:10:42,600 --> 00:10:44,160
我们算一个loss出来

321
00:10:45,040 --> 00:10:46,120
然后再求和

322
00:10:47,320 --> 00:10:47,879
OK

323
00:10:47,879 --> 00:10:49,240
这样子我就返回了

324
00:10:49,240 --> 00:10:51,320
这个ls就是返回了

325
00:10:51,480 --> 00:10:52,840
对于它是一个list

326
00:10:52,920 --> 00:10:54,440
对每一个GPU上

327
00:10:54,440 --> 00:10:55,520
它对应的损失

328
00:10:57,240 --> 00:10:58,680
接下来怎么做backward

329
00:10:58,840 --> 00:10:59,800
backward就是说

330
00:10:59,800 --> 00:11:01,200
对里面每一个GPU

331
00:11:01,200 --> 00:11:02,680
做l.backward

332
00:11:04,400 --> 00:11:04,720
OK

333
00:11:04,720 --> 00:11:05,840
就是说对每个GPU

334
00:11:05,840 --> 00:11:07,240
然后算一下t度

335
00:11:09,480 --> 00:11:11,840
算出来t度之后

336
00:11:12,280 --> 00:11:13,200
那么就是说

337
00:11:13,200 --> 00:11:14,520
对每一个t度

338
00:11:14,680 --> 00:11:15,480
就是说你看

339
00:11:15,480 --> 00:11:16,720
可以看一下这个事情

340
00:11:16,720 --> 00:11:18,560
就是说它这一行干的事情

341
00:11:18,560 --> 00:11:19,560
是说

342
00:11:21,720 --> 00:11:23,560
对于每一个层

343
00:11:25,520 --> 00:11:26,720
i是你的层

344
00:11:27,200 --> 00:11:28,680
i是你说你要多少层

345
00:11:28,680 --> 00:11:29,560
每一层

346
00:11:29,800 --> 00:11:32,360
然后对所有的GPU

347
00:11:32,440 --> 00:11:34,240
然后我们把t度拿出来

348
00:11:35,680 --> 00:11:37,360
然后对它做all reduce

349
00:11:38,800 --> 00:11:40,560
干完之后的意思是说

350
00:11:40,600 --> 00:11:42,720
那么做完all reduce之后

351
00:11:43,080 --> 00:11:45,440
那么每一层的t度

352
00:11:45,440 --> 00:11:46,080
那个grad

353
00:11:46,080 --> 00:11:47,879
我们是一个替换操作

354
00:11:47,920 --> 00:11:51,000
那么它不仅仅是我自己GPU

355
00:11:51,000 --> 00:11:53,080
拿的小块样本的t度

356
00:11:53,120 --> 00:11:55,399
是加所有的加起来的

357
00:11:55,919 --> 00:11:58,879
那就是说是整个小批量

358
00:11:58,919 --> 00:12:01,159
它的t度的完整的t度

359
00:12:01,879 --> 00:12:02,079
好

360
00:12:02,079 --> 00:12:03,079
这是通讯

361
00:12:04,360 --> 00:12:05,000
最后的话

362
00:12:05,000 --> 00:12:06,799
我们每个device

363
00:12:06,799 --> 00:12:07,639
就是每个GPU

364
00:12:07,639 --> 00:12:09,759
然后我们就做自己的SGD

365
00:12:10,559 --> 00:12:12,879
就是我们对每一个GPU做SGD

366
00:12:12,959 --> 00:12:13,840
所以这样子的话

367
00:12:13,840 --> 00:12:14,840
其实是说

368
00:12:14,879 --> 00:12:16,679
我们这里做一点重复的计算

369
00:12:16,840 --> 00:12:17,439
就是说

370
00:12:17,480 --> 00:12:18,559
当然你可以说

371
00:12:18,559 --> 00:12:20,759
我在某一个GPU上做SGD

372
00:12:20,759 --> 00:12:22,439
然后把结果复制给别人

373
00:12:22,439 --> 00:12:24,319
但我们现在这里是说

374
00:12:24,320 --> 00:12:25,560
因为每一个GPU

375
00:12:25,560 --> 00:12:26,840
自己完整的t度

376
00:12:27,080 --> 00:12:28,440
那么有完整的参数

377
00:12:28,440 --> 00:12:29,520
那么就可以自己做

378
00:12:29,520 --> 00:12:31,080
自己的SGD的更新

379
00:12:31,440 --> 00:12:31,720
好

380
00:12:31,720 --> 00:12:34,000
这就是我们最核心的

381
00:12:34,000 --> 00:12:36,080
怎么样训练一个batch

382
00:12:39,080 --> 00:12:40,120
当然另外一个是说

383
00:12:40,440 --> 00:12:41,920
这里有个问题是说

384
00:12:43,400 --> 00:12:45,320
你会发现这个东西

385
00:12:45,320 --> 00:12:46,440
有一点点串行

386
00:12:46,440 --> 00:12:47,520
就是说你

387
00:12:48,600 --> 00:12:49,840
这个事情就是说

388
00:12:49,840 --> 00:12:50,640
写起来

389
00:12:50,640 --> 00:12:51,680
就是说你发现是说

390
00:12:51,680 --> 00:12:52,560
这个写法是说

391
00:12:52,760 --> 00:12:54,440
GPU0先算

392
00:12:55,480 --> 00:12:56,920
先算寻思

393
00:12:56,920 --> 00:12:59,200
然后GPU1算完之后

394
00:12:59,200 --> 00:13:01,480
最后GPU0算t度

395
00:13:01,480 --> 00:13:02,640
GPU1算t度

396
00:13:02,640 --> 00:13:03,560
就是说你这个代码

397
00:13:03,560 --> 00:13:04,960
看上去是这个样子

398
00:13:05,360 --> 00:13:06,200
这个东西

399
00:13:06,200 --> 00:13:07,560
所以我们没有让

400
00:13:08,720 --> 00:13:09,560
明显的说

401
00:13:09,560 --> 00:13:10,960
我要怎么样并行

402
00:13:11,360 --> 00:13:12,480
这个是说

403
00:13:13,040 --> 00:13:14,440
这个就是看你的框架

404
00:13:14,440 --> 00:13:15,080
在背后

405
00:13:15,080 --> 00:13:16,880
能不能帮你自动做并行了

406
00:13:17,600 --> 00:13:18,520
框架做得好的话

407
00:13:18,520 --> 00:13:19,120
这一段代码

408
00:13:19,120 --> 00:13:20,280
是可以被自动并行的

409
00:13:20,280 --> 00:13:21,320
这样子能加速

410
00:13:21,720 --> 00:13:23,680
如果你框架没帮你做的事情

411
00:13:23,680 --> 00:13:24,960
这个事情可能是

412
00:13:24,960 --> 00:13:26,440
你得不到特别好的加速

413
00:13:27,240 --> 00:13:27,840
OK

414
00:13:27,840 --> 00:13:29,560
这好像是在MSnet

415
00:13:29,560 --> 00:13:30,280
和TensorFlow

416
00:13:30,280 --> 00:13:31,320
应该是没问题的

417
00:13:31,680 --> 00:13:32,320
PyTorch

418
00:13:32,520 --> 00:13:33,320
其实

419
00:13:34,240 --> 00:13:35,560
我其实有一点悬念

420
00:13:35,560 --> 00:13:35,879
觉得

421
00:13:36,879 --> 00:13:37,280
没关系

422
00:13:37,280 --> 00:13:38,440
就是说给大家讲一下

423
00:13:39,520 --> 00:13:41,000
主要是讲一下思路

424
00:13:42,000 --> 00:13:42,200
好

425
00:13:42,200 --> 00:13:43,879
最后我们讲完整的训练模型

426
00:13:43,879 --> 00:13:44,560
怎么样子

427
00:13:44,920 --> 00:13:46,200
完整的训练模型

428
00:13:46,200 --> 00:13:47,800
跟之前是没什么区别的

429
00:13:48,080 --> 00:13:48,920
就是我们创建

430
00:13:48,920 --> 00:13:50,440
我们的数据

431
00:13:50,440 --> 00:13:52,760
然后这样子的话

432
00:13:53,040 --> 00:13:53,760
不同的是说

433
00:13:53,760 --> 00:13:54,360
我告诉你说

434
00:13:54,360 --> 00:13:55,600
我要用多少个GPU

435
00:13:55,920 --> 00:13:57,160
这是Number of GPUs

436
00:13:57,560 --> 00:14:00,320
然后说我就把这些GPU拿出来

437
00:14:00,640 --> 00:14:01,440
就有两个的话

438
00:14:01,440 --> 00:14:02,440
就是GPU0

439
00:14:02,440 --> 00:14:03,200
GPU1

440
00:14:04,400 --> 00:14:06,120
那么这一个操作就是说

441
00:14:06,960 --> 00:14:09,560
我这个Parents

442
00:14:09,720 --> 00:14:11,200
就是说我需要把它

443
00:14:11,200 --> 00:14:13,160
在训练开始之前

444
00:14:13,720 --> 00:14:15,080
初始化的Parents

445
00:14:15,080 --> 00:14:16,800
复制到每个GPU上

446
00:14:18,120 --> 00:14:19,040
这样子的话

447
00:14:19,439 --> 00:14:21,719
一开始GPU每个人拿到的

448
00:14:22,000 --> 00:14:23,639
T度都是一样的

449
00:14:23,839 --> 00:14:25,279
而且可以看到

450
00:14:25,279 --> 00:14:26,039
我们之后

451
00:14:26,039 --> 00:14:27,559
对T度做更新的时候

452
00:14:27,559 --> 00:14:28,319
你的

453
00:14:28,959 --> 00:14:30,799
对参数做更新的时候

454
00:14:30,799 --> 00:14:32,079
你的T度也是一样

455
00:14:32,079 --> 00:14:35,039
所以在以后所有的时候

456
00:14:35,039 --> 00:14:36,399
你的每个GPU上

457
00:14:36,399 --> 00:14:37,919
你权重理论上

458
00:14:37,919 --> 00:14:39,519
大家是一样的

459
00:14:39,639 --> 00:14:42,039
这就是同步的SGD的思路

460
00:14:43,039 --> 00:14:44,240
下面就没什么区别了

461
00:14:44,399 --> 00:14:45,599
下面我们就是一些

462
00:14:45,919 --> 00:14:46,879
我们画一个东西

463
00:14:46,879 --> 00:14:48,719
然后记个时

464
00:14:48,879 --> 00:14:51,320
然后每一次

465
00:14:51,560 --> 00:14:53,480
第一个Followup就是Epoch

466
00:14:53,600 --> 00:14:54,639
然后下面就是

467
00:14:54,639 --> 00:14:55,840
对每一个Iterator里面

468
00:14:55,840 --> 00:14:57,040
抽一个XY

469
00:14:57,080 --> 00:14:58,200
用刚刚实际的函数

470
00:14:58,200 --> 00:14:59,120
做一次

471
00:15:00,680 --> 00:15:01,519
刚刚实际函数

472
00:15:01,519 --> 00:15:04,560
做一次小批量的更新

473
00:15:05,200 --> 00:15:08,440
最后其实用了一个Synchronize

474
00:15:08,440 --> 00:15:09,680
Synchronize就是说

475
00:15:10,160 --> 00:15:12,399
你保证每个人都做完了

476
00:15:12,399 --> 00:15:13,120
在这个时候

477
00:15:13,160 --> 00:15:14,200
就同步一次

478
00:15:15,440 --> 00:15:15,759
OK

479
00:15:15,759 --> 00:15:16,480
这个同步一次

480
00:15:16,480 --> 00:15:17,560
主要的目的是干嘛

481
00:15:17,560 --> 00:15:18,960
主要目的是算一下时间

482
00:15:19,480 --> 00:15:19,920
OK

483
00:15:19,920 --> 00:15:20,560
主要是这样子

484
00:15:20,600 --> 00:15:21,840
算时间转移一点点

485
00:15:22,560 --> 00:15:23,640
最后是打印点东西

486
00:15:24,280 --> 00:15:25,760
所以跟之前没什么区别

487
00:15:25,800 --> 00:15:26,920
因为主要的

488
00:15:27,200 --> 00:15:28,840
这里跟之前主要的区别是说

489
00:15:28,840 --> 00:15:31,240
我们要将参数复制到每个GPU上

490
00:15:31,600 --> 00:15:33,120
然后不一样的地方

491
00:15:33,120 --> 00:15:34,880
我们之前已经实现过了

492
00:15:36,880 --> 00:15:37,360
好

493
00:15:37,360 --> 00:15:38,320
我们来

494
00:15:40,200 --> 00:15:40,920
就可以看一下

495
00:15:40,920 --> 00:15:42,120
就是说

496
00:15:43,320 --> 00:15:44,840
在单GPU上运行

497
00:15:45,440 --> 00:15:46,000
可以看一下

498
00:15:46,000 --> 00:15:48,200
单GPU上是number of GPU等于1

499
00:15:48,639 --> 00:15:50,480
你的batch size等于256

500
00:15:51,799 --> 00:15:54,039
然后你的learning rate等于0.2

501
00:15:54,320 --> 00:15:56,879
我们就不直接不给大家重新运行了

502
00:15:56,919 --> 00:15:58,679
就是我们刚刚运行过的结果

503
00:15:58,879 --> 00:16:00,360
就可以看到是说

504
00:16:00,720 --> 00:16:03,879
你的test accuracy是0.84

505
00:16:03,879 --> 00:16:06,000
然后你是每一秒

506
00:16:06,000 --> 00:16:07,799
每一个epoch

507
00:16:07,840 --> 00:16:09,600
划的时间是2.6秒

508
00:16:10,440 --> 00:16:11,039
OK

509
00:16:12,879 --> 00:16:15,559
然后你看到是两个GPU的时候

510
00:16:17,000 --> 00:16:19,919
首先你要看到的是说

511
00:16:19,960 --> 00:16:21,639
我的batch size没变

512
00:16:21,639 --> 00:16:23,480
我的learning rate也没变

513
00:16:23,480 --> 00:16:24,679
就是还是256

514
00:16:25,039 --> 00:16:26,519
和learning rate是0.2

515
00:16:27,240 --> 00:16:30,919
然后所以我们知道同步SGD

516
00:16:30,919 --> 00:16:32,559
理论上是不会改变

517
00:16:32,799 --> 00:16:33,960
你的训练结果的

518
00:16:33,960 --> 00:16:35,480
不管你用多少GPU的结果

519
00:16:35,480 --> 00:16:36,480
应该是一样的

520
00:16:36,600 --> 00:16:38,519
所以看到这里的测试

521
00:16:38,559 --> 00:16:39,919
精度是0.82

522
00:16:39,919 --> 00:16:41,080
跟之前是类似的

523
00:16:41,320 --> 00:16:43,639
跟之前的0.84是类似的

524
00:16:43,919 --> 00:16:44,519
它不一样

525
00:16:44,519 --> 00:16:45,600
是因为你的

526
00:16:45,600 --> 00:16:47,159
你现在的

527
00:16:48,200 --> 00:16:50,480
模型参数的初始化有不一样

528
00:16:50,840 --> 00:16:52,759
所以这个就是会导致一些不一样的

529
00:16:52,759 --> 00:16:53,279
地方

530
00:16:53,720 --> 00:16:54,960
你的随机种子

531
00:16:55,120 --> 00:16:56,600
就是说有随机性在里面

532
00:16:56,639 --> 00:16:58,920
就是说你把这随机性都固定住的话

533
00:16:58,920 --> 00:17:01,320
理论上这个是应该会得到一样的

534
00:17:01,320 --> 00:17:01,840
结果

535
00:17:02,680 --> 00:17:04,400
另外一个可以看到是说

536
00:17:04,440 --> 00:17:08,200
其实你两个GPU没有变快

537
00:17:08,240 --> 00:17:11,440
就是说刚刚我们一个GPU跑一个

538
00:17:11,440 --> 00:17:13,599
data epoch是2.6秒

539
00:17:14,079 --> 00:17:15,719
两个还是2.6秒

540
00:17:18,119 --> 00:17:19,839
就说你看我们是用两个device

541
00:17:20,639 --> 00:17:21,559
这就比较有意思了

542
00:17:21,559 --> 00:17:22,119
对吧

543
00:17:22,599 --> 00:17:23,439
就是说

544
00:17:25,039 --> 00:17:26,599
为什么没变快呢

545
00:17:26,599 --> 00:17:28,759
这个就是说多GPU不变快

546
00:17:28,879 --> 00:17:30,199
或者分布式不变快

547
00:17:30,199 --> 00:17:33,599
这是一个非常常见的一个现象

548
00:17:33,959 --> 00:17:35,679
就你会去想为什么不变快

549
00:17:35,719 --> 00:17:37,559
通常不变快有很多种原因

550
00:17:37,879 --> 00:17:39,159
一个是说

551
00:17:39,240 --> 00:17:40,759
很有可能你的

552
00:17:41,480 --> 00:17:43,480
data读起来就很慢

553
00:17:43,839 --> 00:17:45,799
可能你data读一下就2.6秒

554
00:17:46,679 --> 00:17:47,159
对吧

555
00:17:47,399 --> 00:17:48,199
所以你怎么测

556
00:17:48,359 --> 00:17:48,839
你怎么测

557
00:17:48,839 --> 00:17:50,519
就是你就写一个for loop

558
00:17:50,519 --> 00:17:52,119
然后把所有的data拿出来看一下

559
00:17:52,119 --> 00:17:53,000
print一下

560
00:17:53,119 --> 00:17:54,119
然后看一看

561
00:17:54,159 --> 00:17:56,000
就是说时间是多少

562
00:17:56,039 --> 00:17:58,759
那个数应该是要远远小于2.6秒才行

563
00:17:59,879 --> 00:18:00,519
其次

564
00:18:01,279 --> 00:18:03,279
其次这个事情就是说

565
00:18:03,639 --> 00:18:05,559
我们等会讲分布式的时候

566
00:18:05,559 --> 00:18:07,359
会给大家再仔细讲一下

567
00:18:07,359 --> 00:18:08,319
这个性能问题

568
00:18:08,439 --> 00:18:10,240
其次通常的问题是说

569
00:18:10,240 --> 00:18:12,000
你把GPU增加了

570
00:18:12,000 --> 00:18:14,000
但你的back size没增加

571
00:18:14,640 --> 00:18:15,559
意味着就是说

572
00:18:15,559 --> 00:18:16,799
之前我的GPU

573
00:18:16,799 --> 00:18:19,839
一个GPU能够处理256个样本

574
00:18:20,200 --> 00:18:21,079
每一次

575
00:18:21,720 --> 00:18:23,440
这时候你两个GPU

576
00:18:23,440 --> 00:18:26,039
那每个GPU只能处理128个样本

577
00:18:27,759 --> 00:18:28,640
那么这个有问题

578
00:18:28,640 --> 00:18:29,000
为什么

579
00:18:29,000 --> 00:18:31,759
是因为你本来nullnet就很小了

580
00:18:31,880 --> 00:18:33,759
你把back size变低的话

581
00:18:33,759 --> 00:18:35,880
那就是你的计算性能会变低

582
00:18:36,279 --> 00:18:37,519
我们有讲过说GPU

583
00:18:37,519 --> 00:18:38,960
你有几千个线程

584
00:18:40,279 --> 00:18:41,880
现在你P量变小了

585
00:18:41,880 --> 00:18:43,280
那么每一次做计算的时候

586
00:18:43,280 --> 00:18:45,160
你矩阵卷机

587
00:18:45,280 --> 00:18:46,560
它的输入会变小

588
00:18:49,880 --> 00:18:52,080
那么它的你变小了的话

589
00:18:52,080 --> 00:18:54,320
那么做同样的操作的话

590
00:18:54,320 --> 00:18:55,600
它的效率就会变低

591
00:18:55,600 --> 00:18:57,360
就是说你无法那么高效的

592
00:18:57,360 --> 00:18:58,680
使用你的多线程

593
00:18:59,000 --> 00:19:00,840
就是说你没有打满我的GPU

594
00:19:02,200 --> 00:19:04,080
所以遇到这种情况怎么办呢

595
00:19:04,480 --> 00:19:06,000
通常的做法是说

596
00:19:06,000 --> 00:19:06,600
你

597
00:19:07,640 --> 00:19:10,320
你最好保证每个GPU上的back size

598
00:19:10,359 --> 00:19:11,960
就是你增加GPU的时候

599
00:19:11,960 --> 00:19:13,159
你最好保证

600
00:19:13,159 --> 00:19:15,599
每个GPU还是拿到同样多的back size

601
00:19:15,599 --> 00:19:16,960
就是说你这里最好的办法

602
00:19:16,960 --> 00:19:17,879
是乘个2

603
00:19:19,439 --> 00:19:21,759
乘个2你会碰到别的问题

604
00:19:21,919 --> 00:19:22,919
我们可以运行一下

605
00:19:22,919 --> 00:19:24,480
看一看这个要跑

606
00:19:26,480 --> 00:19:27,639
你直接乘2的话

607
00:19:27,639 --> 00:19:29,960
你会发现你的精度可能会变低

608
00:19:33,839 --> 00:19:35,799
这个是我们的

609
00:19:36,799 --> 00:19:41,599
就是说你可以看到是说

610
00:19:41,599 --> 00:19:43,399
我们还没有跑完

611
00:19:43,399 --> 00:19:46,519
基本上就是说你变成512了

612
00:19:46,519 --> 00:19:49,240
然后你的nullary还是0.2的话

613
00:19:49,240 --> 00:19:52,000
很有可能你就收敛会变慢

614
00:19:54,799 --> 00:19:56,240
我们可以等一下等它跑完

615
00:19:58,519 --> 00:19:59,960
就是说跑完之后

616
00:19:59,960 --> 00:20:01,639
你的精度差了很多

617
00:20:01,639 --> 00:20:02,359
看见没有

618
00:20:02,839 --> 00:20:03,919
刚刚是0.82

619
00:20:03,919 --> 00:20:05,639
现在变成0.69了

620
00:20:06,240 --> 00:20:09,599
你的性能稍微好了一点点

621
00:20:09,599 --> 00:20:10,839
2.4

622
00:20:11,039 --> 00:20:11,799
second

623
00:20:12,279 --> 00:20:13,519
从2.6变到2.4

624
00:20:13,559 --> 00:20:15,279
就远远没有到我们要的结果

625
00:20:15,279 --> 00:20:16,839
我们想要的是1.3

626
00:20:17,839 --> 00:20:18,519
OK

627
00:20:18,519 --> 00:20:23,000
所以解决测试精度这个问题

628
00:20:23,159 --> 00:20:24,240
通常你的做法是

629
00:20:24,240 --> 00:20:25,879
我可以把nullary调大一点

630
00:20:25,879 --> 00:20:27,359
就是我可以调成

631
00:20:27,519 --> 00:20:30,039
或者我把nullary也double一下

632
00:20:30,039 --> 00:20:31,680
就是乘2

633
00:20:32,119 --> 00:20:33,960
如果你会发现乘2太大的话

634
00:20:33,960 --> 00:20:35,079
你有可能会减少点

635
00:20:35,079 --> 00:20:37,039
乘个1.5或者什么样子的

636
00:20:37,360 --> 00:20:38,640
就是对1.5

637
00:20:38,640 --> 00:20:39,319
比如说

638
00:20:40,600 --> 00:20:42,200
就是说当然你可以

639
00:20:42,200 --> 00:20:43,039
通常来说

640
00:20:43,240 --> 00:20:44,240
你这个

641
00:20:46,600 --> 00:20:48,079
你可以就nullary

642
00:20:48,079 --> 00:20:50,000
可以在一定程度上加一点东西

643
00:20:50,000 --> 00:20:51,840
在随着batch size的增大

644
00:20:51,840 --> 00:20:53,120
但是也不能无限增加

645
00:20:53,120 --> 00:20:53,799
无限

646
00:20:54,480 --> 00:20:55,240
你这个batch size

647
00:20:55,240 --> 00:20:56,200
如果特别大

648
00:20:56,200 --> 00:20:57,400
你加到nullary

649
00:20:57,400 --> 00:20:59,000
也不会让你的测试精度

650
00:20:59,000 --> 00:21:01,960
回到原来的效果

651
00:21:02,759 --> 00:21:03,880
第二个是说

652
00:21:04,400 --> 00:21:05,400
这个东西

653
00:21:05,400 --> 00:21:07,279
假设你已经保证

654
00:21:07,279 --> 00:21:09,039
每一个GPU的batch size

655
00:21:09,039 --> 00:21:10,759
和单GPU的是一样的情况下

656
00:21:10,759 --> 00:21:11,600
就这个情况下

657
00:21:12,400 --> 00:21:13,519
理论上来说

658
00:21:13,519 --> 00:21:14,279
这个情况下

659
00:21:14,279 --> 00:21:16,600
是一个比较好的情况

660
00:21:17,120 --> 00:21:18,720
就是你的系统的

661
00:21:20,200 --> 00:21:21,680
性能应该是不错了

662
00:21:21,799 --> 00:21:23,279
但是我们这里还是有问题

663
00:21:23,440 --> 00:21:24,480
有两个原因

664
00:21:24,880 --> 00:21:26,319
第一个是Python对你的

665
00:21:26,319 --> 00:21:27,400
并行的加速不够好

666
00:21:27,400 --> 00:21:28,440
就是说我们这种

667
00:21:28,480 --> 00:21:30,000
罗写的写法

668
00:21:30,000 --> 00:21:31,480
对Python是不友好的

669
00:21:32,680 --> 00:21:33,360
第二个原因是

670
00:21:33,360 --> 00:21:36,480
Nernet这个东西太小了

671
00:21:36,840 --> 00:21:39,039
它的性能确实没那么好

672
00:21:39,559 --> 00:21:40,120
OK

