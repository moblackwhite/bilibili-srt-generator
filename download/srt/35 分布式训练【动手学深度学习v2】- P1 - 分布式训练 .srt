1
00:00:00,000 --> 00:00:02,280
27

2
00:00:02,280 --> 00:00:04,719
PyTorch怎么设置不同GPU的Batch Size

3
00:00:04,719 --> 00:00:06,879
有的

4
00:00:07,360 --> 00:00:08,599
有个参数的

5
00:00:08,599 --> 00:00:10,000
我记得是叫什么名字

6
00:00:10,000 --> 00:00:10,759
我忘了

7
00:00:10,880 --> 00:00:14,679
叫做应该是一个Device Workload

8
00:00:14,679 --> 00:00:15,800
就是Device Load

9
00:00:15,839 --> 00:00:18,719
一个这样子的一个Archive可以设的

10
00:00:18,760 --> 00:00:20,559
你可以去看一下

11
00:00:20,559 --> 00:00:21,960
应该是在DataPilot里面

12
00:00:21,960 --> 00:00:24,000
有一个外来参数可以设

13
00:00:26,440 --> 00:00:28,359
分布式的目的也是和DataPilot一样

14
00:00:28,359 --> 00:00:29,320
提升计算速度吗

15
00:00:29,320 --> 00:00:30,920
为什么不用DataPilot去写这种问题

16
00:00:30,920 --> 00:00:34,240
就DataPilot是一种并行的模式

17
00:00:34,280 --> 00:00:38,039
就是说在我们其实主要讲的是说

18
00:00:38,039 --> 00:00:40,960
DataPilot那种模式

19
00:00:41,000 --> 00:00:43,480
怎么样应用在单机多卡

20
00:00:43,480 --> 00:00:45,240
和怎么用在分布式上

21
00:00:45,960 --> 00:00:46,519
OK

22
00:00:49,320 --> 00:00:50,320
就是一个同样东西

23
00:00:50,320 --> 00:00:53,960
怎么作用在一个CPU连的多个GPU

24
00:00:53,960 --> 00:00:56,240
还是说有多个机器的情况下

25
00:00:56,240 --> 00:00:57,320
是怎么要应用的

26
00:00:57,640 --> 00:00:58,840
本质上是一个东西

27
00:01:00,320 --> 00:01:03,719
就是说问题要是

28
00:01:03,719 --> 00:01:06,079
就是说参数服务器求完T2之后

29
00:01:06,079 --> 00:01:07,879
要汇总到一个主服务器

30
00:01:07,879 --> 00:01:09,960
然后再发送到各个服务器

31
00:01:09,960 --> 00:01:12,159
然后再传送给计算节点吗

32
00:01:12,640 --> 00:01:13,680
其实不是的

33
00:01:14,400 --> 00:01:16,439
参数服务器其实是说

34
00:01:16,640 --> 00:01:18,200
它没有主服务器这个概念

35
00:01:18,200 --> 00:01:19,840
就是说最简单

36
00:01:19,840 --> 00:01:22,039
就是我就一台机器做服务器

37
00:01:22,480 --> 00:01:24,359
就所谓的T2在我这汇总

38
00:01:24,400 --> 00:01:26,240
然后在实际情况下来说

39
00:01:26,240 --> 00:01:27,719
就是说你有三台机器的话

40
00:01:27,719 --> 00:01:29,319
假设你参数服务器有三台

41
00:01:29,319 --> 00:01:30,159
Server的话

42
00:01:31,200 --> 00:01:33,000
那么每一个Server会负责

43
00:01:33,000 --> 00:01:35,840
1 1 3的模型参数

44
00:01:37,000 --> 00:01:37,359
OK

45
00:01:37,359 --> 00:01:38,719
所以三台机拼起来

46
00:01:38,719 --> 00:01:40,200
就变成一个完整的模型参数

47
00:01:40,239 --> 00:01:43,079
所以每一个worker去问参数服务器

48
00:01:43,079 --> 00:01:45,359
要服务器要模型的时候

49
00:01:45,359 --> 00:01:47,759
他会问三个人去请求各自的模型

50
00:01:47,759 --> 00:01:49,159
然后在本地汇总

51
00:01:52,159 --> 00:01:53,560
问题三是每个GPU得到

52
00:01:53,560 --> 00:01:54,799
模型的所有参数吗

53
00:01:54,840 --> 00:01:55,879
在数据并行里面

54
00:01:55,879 --> 00:01:56,159
是的

55
00:01:56,159 --> 00:01:58,159
每个GPU会得到模型的所有参数

56
00:02:02,840 --> 00:02:04,719
为什么N个GPU会相对

57
00:02:04,719 --> 00:02:06,359
单个GPU加速N倍

58
00:02:06,359 --> 00:02:07,679
我的理解是一个Batch Size

59
00:02:07,679 --> 00:02:08,319
有N个样本

60
00:02:08,319 --> 00:02:08,919
那么

61
00:02:12,919 --> 00:02:14,519
就是说简单是说

62
00:02:14,519 --> 00:02:16,439
你比如说一个GPU处理

63
00:02:16,439 --> 00:02:19,079
100个样本花时间0.1秒

64
00:02:20,000 --> 00:02:21,639
那么假设我有10个GPU的话

65
00:02:21,639 --> 00:02:23,199
我每个GPU还是处理

66
00:02:23,920 --> 00:02:27,400
还是处理100个样本的话

67
00:02:27,400 --> 00:02:29,600
那么在0.1秒之内之前

68
00:02:29,600 --> 00:02:30,760
我是处理100个样本

69
00:02:30,760 --> 00:02:32,240
现在我会处理1000个样本

70
00:02:32,360 --> 00:02:33,440
所以性能是10倍

71
00:02:33,440 --> 00:02:35,280
就throughput是变成10倍了

72
00:02:35,720 --> 00:02:37,440
它不是Latency是throughput

73
00:02:37,440 --> 00:02:41,760
就是不是完成一个批量

74
00:02:41,760 --> 00:02:42,640
你说的时间

75
00:02:42,640 --> 00:02:44,440
是说我要同样的时间内

76
00:02:44,440 --> 00:02:46,480
我能计算多少个样本会增加了

77
00:02:47,800 --> 00:02:49,240
后面会讲Transformer吗

78
00:02:49,240 --> 00:02:50,160
我们会讲Transformer

79
00:02:50,400 --> 00:02:51,400
我们会讲

80
00:02:54,200 --> 00:02:57,240
为什么Batch Size变大

81
00:02:57,240 --> 00:02:59,560
训练的有效性是下降的

82
00:03:02,560 --> 00:03:03,400
就是说这个东西

83
00:03:03,680 --> 00:03:05,720
这个东西从理论上是可以解释的

84
00:03:08,120 --> 00:03:09,600
我从直观上给大家解释一下

85
00:03:09,600 --> 00:03:10,360
就是说

86
00:03:11,040 --> 00:03:12,360
你为什么会变低

87
00:03:12,520 --> 00:03:14,040
举个极端情况

88
00:03:14,440 --> 00:03:15,560
极端情况是说

89
00:03:15,560 --> 00:03:17,040
你整个数据集里面

90
00:03:17,040 --> 00:03:18,840
所有的样本都是一个样本

91
00:03:18,880 --> 00:03:20,120
就一个图片

92
00:03:20,160 --> 00:03:21,480
我复制1万份

93
00:03:21,880 --> 00:03:24,520
那么你的Batch Size

94
00:03:24,520 --> 00:03:25,760
你不管取多大

95
00:03:26,040 --> 00:03:26,800
都是一样的

96
00:03:26,800 --> 00:03:28,160
因为你的梯度

97
00:03:28,200 --> 00:03:31,000
每个样本的梯度是一样的

98
00:03:31,000 --> 00:03:32,400
那么你这个批量的梯度

99
00:03:32,400 --> 00:03:35,160
就是你这个梯度的加起来

100
00:03:35,200 --> 00:03:35,960
最后还除以

101
00:03:35,960 --> 00:03:37,160
我们还做迭代

102
00:03:37,160 --> 00:03:38,640
是我们还除以了你的批量大小

103
00:03:38,640 --> 00:03:40,600
所以你不管Batch Size多大

104
00:03:40,640 --> 00:03:42,640
那么我算的梯度都是一样

105
00:03:43,160 --> 00:03:46,440
所以假设我要到我某一个accuracy的话

106
00:03:46,440 --> 00:03:49,160
我需要批量大小为1的时候

107
00:03:49,160 --> 00:03:50,520
我需要100次迭代

108
00:03:50,520 --> 00:03:52,360
到一个特定的accuracy

109
00:03:53,200 --> 00:03:54,080
那么我批量大小

110
00:03:54,080 --> 00:03:55,240
变成10的时候怎么办

111
00:03:56,240 --> 00:03:58,000
我还是需要100个Batch

112
00:03:59,080 --> 00:03:59,360
对吧

113
00:03:59,360 --> 00:04:00,640
因为你不管Batch Size多大

114
00:04:00,640 --> 00:04:02,560
我的梯度都是一个东西

115
00:04:02,560 --> 00:04:03,840
因为数据是一样的

116
00:04:04,160 --> 00:04:05,480
那么就导致说

117
00:04:05,520 --> 00:04:07,160
这从性能上来看怎么办

118
00:04:07,200 --> 00:04:08,000
性能上来说

119
00:04:08,000 --> 00:04:10,719
就是说之前我到一个accuracy

120
00:04:10,719 --> 00:04:11,680
Batch Size为1的时候

121
00:04:11,680 --> 00:04:12,560
我只要

122
00:04:12,719 --> 00:04:13,160
计算

123
00:04:13,160 --> 00:04:15,000
我只要处理100个样本就行了

124
00:04:15,480 --> 00:04:16,600
现在Batch Size变成10

125
00:04:16,600 --> 00:04:18,080
我需要处理1000个样本

126
00:04:18,080 --> 00:04:19,480
才能达到我的accuracy

127
00:04:20,319 --> 00:04:21,520
我的收敛就变10倍

128
00:04:21,520 --> 00:04:22,560
就慢了10倍了

129
00:04:22,560 --> 00:04:23,080
对吧

130
00:04:23,960 --> 00:04:25,640
你收敛你看的是

131
00:04:26,520 --> 00:04:28,360
你要多个样本

132
00:04:28,360 --> 00:04:29,560
多个data Epoch

133
00:04:29,759 --> 00:04:30,920
或多个Epoch

134
00:04:30,920 --> 00:04:32,960
来达到我的accuracy

135
00:04:33,400 --> 00:04:34,600
所以在实际情况下

136
00:04:34,600 --> 00:04:35,400
当然你不会说

137
00:04:35,400 --> 00:04:36,759
所有的图片都是一样

138
00:04:36,800 --> 00:04:38,120
但情况下就是说

139
00:04:38,120 --> 00:04:40,000
假设你这个批量里面

140
00:04:40,000 --> 00:04:41,120
有很多

141
00:04:41,200 --> 00:04:43,759
重复的比较冗余的图片的话

142
00:04:43,800 --> 00:04:45,680
那么它对你计算梯度

143
00:04:45,879 --> 00:04:47,000
没有太多影响

144
00:04:47,840 --> 00:04:48,120
对吧

145
00:04:48,120 --> 00:04:49,879
因为两个差不多图片

146
00:04:49,879 --> 00:04:51,759
它给你贡献的梯度就差不多

147
00:04:52,000 --> 00:04:53,360
等于说你就浪费了

148
00:04:53,399 --> 00:04:55,040
所以最好的情况是说

149
00:04:55,040 --> 00:04:56,959
你的小批量里面

150
00:04:56,959 --> 00:04:58,639
你的数据非常diverse

151
00:04:59,920 --> 00:05:01,399
就是说每个人都不一样

152
00:05:01,399 --> 00:05:02,560
所以对你的

153
00:05:03,040 --> 00:05:05,560
这样子给你的信息争议是最大的

154
00:05:05,600 --> 00:05:06,720
这样子你就不浪费

155
00:05:07,600 --> 00:05:08,519
所以就是说

156
00:05:08,519 --> 00:05:09,240
所以为什么说

157
00:05:09,240 --> 00:05:11,160
大家拥有比较大的数据器

158
00:05:11,160 --> 00:05:12,280
因为大的数据器里面

159
00:05:12,280 --> 00:05:14,280
数据的多样性会增加

160
00:05:14,280 --> 00:05:15,879
所以你用大批量的时候

161
00:05:15,879 --> 00:05:17,600
你批量大小里面

162
00:05:17,600 --> 00:05:19,320
图片的多样性还是有的

163
00:05:19,960 --> 00:05:21,000
但是如果小数据

164
00:05:21,000 --> 00:05:22,080
就像M list

165
00:05:22,080 --> 00:05:23,439
这种最简单的

166
00:05:23,480 --> 00:05:24,840
你用到512

167
00:05:24,840 --> 00:05:27,680
就用个128就很大了

168
00:05:27,720 --> 00:05:28,720
因为你每一次的

169
00:05:28,720 --> 00:05:30,280
里面那数字都长得差不多

170
00:05:30,280 --> 00:05:32,040
所以你用120

171
00:05:32,040 --> 00:05:34,560
就是说批量大小是128和512

172
00:05:34,560 --> 00:05:36,920
算出来的梯度的质量是差不多的

173
00:05:37,400 --> 00:05:38,240
差不多的情况下

174
00:05:38,240 --> 00:05:41,800
那么我当然想批量大小越小越好吧

175
00:05:44,520 --> 00:05:46,520
好分布式集群

176
00:05:46,560 --> 00:05:47,159
下训练时

177
00:05:47,159 --> 00:05:50,279
为什么计算和数据通讯可以并行

178
00:05:50,319 --> 00:05:51,719
单机上不是要等

179
00:05:53,599 --> 00:05:53,919
对的

180
00:05:53,919 --> 00:05:55,039
你这个问题问的挺好的

181
00:05:55,399 --> 00:05:56,399
我们没仔细讲

182
00:05:56,560 --> 00:05:58,240
因为我们没时间仔细讲这个事情

183
00:05:58,240 --> 00:05:58,879
就是说

184
00:05:59,799 --> 00:06:02,120
计算通讯不是完美并行的

185
00:06:02,639 --> 00:06:03,680
通常是说什么

186
00:06:04,159 --> 00:06:05,079
就是说

187
00:06:06,319 --> 00:06:07,839
你我给大家画一下

188
00:06:07,959 --> 00:06:08,879
你

189
00:06:10,159 --> 00:06:11,680
就是说你在做

190
00:06:11,680 --> 00:06:12,439
你算一个

191
00:06:12,439 --> 00:06:13,959
比如说这是你三个层

192
00:06:15,240 --> 00:06:16,399
这是data

193
00:06:16,799 --> 00:06:18,120
这个是loss的话

194
00:06:18,319 --> 00:06:19,519
你算fold的时候

195
00:06:19,519 --> 00:06:20,319
你是这么算过去

196
00:06:20,319 --> 00:06:20,879
对吧

197
00:06:21,159 --> 00:06:22,079
算梯度的时候

198
00:06:22,079 --> 00:06:22,879
你是这么算回来

199
00:06:22,879 --> 00:06:23,439
对吧

200
00:06:23,560 --> 00:06:24,439
所以

201
00:06:24,799 --> 00:06:26,199
算到

202
00:06:26,399 --> 00:06:27,919
这个梯度的时候

203
00:06:28,479 --> 00:06:29,439
在算这个梯度的时候

204
00:06:29,439 --> 00:06:30,439
我就可以发出去了

205
00:06:30,439 --> 00:06:31,399
我就给send

206
00:06:32,680 --> 00:06:34,079
接下来我算这个梯度

207
00:06:34,120 --> 00:06:35,519
就是说接下来就是说我可以

208
00:06:35,519 --> 00:06:37,240
就是说在上fold的时候

209
00:06:37,240 --> 00:06:38,399
你无法并行

210
00:06:39,039 --> 00:06:40,199
往前算的时候

211
00:06:40,199 --> 00:06:41,079
你是被

212
00:06:42,599 --> 00:06:43,560
往前算的时候

213
00:06:43,560 --> 00:06:44,680
你得等着

214
00:06:44,680 --> 00:06:46,480
但是在算梯度

215
00:06:46,840 --> 00:06:48,360
算backwards那个时候

216
00:06:48,680 --> 00:06:49,400
算一个梯度

217
00:06:49,400 --> 00:06:50,040
就可以发一个

218
00:06:50,040 --> 00:06:50,879
算到这个梯度的时候

219
00:06:50,879 --> 00:06:52,199
我就可以发一个send

220
00:06:52,720 --> 00:06:53,480
接下来我下来

221
00:06:53,480 --> 00:06:55,120
就是说在backwards和前面

222
00:06:55,120 --> 00:06:56,680
是可以做一定的并行的

223
00:06:56,960 --> 00:06:57,560
OK

224
00:06:57,560 --> 00:06:58,759
所以我们其实是

225
00:06:59,879 --> 00:07:01,800
精确来讲是这么解释的

226
00:07:04,399 --> 00:07:06,399
好分布式多GPU相当于增大

227
00:07:06,399 --> 00:07:06,879
Batch size

228
00:07:06,879 --> 00:07:07,360
一般情况下

229
00:07:07,360 --> 00:07:08,439
Batch size超过1.2

230
00:07:08,439 --> 00:07:08,920
是就不好了

231
00:07:08,920 --> 00:07:09,240
为什么

232
00:07:10,160 --> 00:07:11,240
为什么分布式

233
00:07:12,680 --> 00:07:14,120
所以Batch size这个东西

234
00:07:14,560 --> 00:07:16,360
取决于你的数据级

235
00:07:16,399 --> 00:07:17,439
在ImageNet

236
00:07:17,439 --> 00:07:19,000
ImageNet有1000个类

237
00:07:19,000 --> 00:07:20,759
Batch size可以做到10万

238
00:07:20,759 --> 00:07:21,600
都是可以的

239
00:07:22,319 --> 00:07:25,959
一取决于你的数据级的大小的

240
00:07:25,959 --> 00:07:26,639
多样性

241
00:07:27,399 --> 00:07:29,280
第二个是取决于你的优厚算法

242
00:07:29,840 --> 00:07:30,840
好的优厚算法

243
00:07:30,840 --> 00:07:32,160
可以做到10万

244
00:07:32,160 --> 00:07:33,079
或者甚至更大

245
00:07:33,759 --> 00:07:36,079
我给大家一个直观的理解

246
00:07:36,240 --> 00:07:37,639
就是说直观上来说

247
00:07:37,840 --> 00:07:39,360
假设你一个图片级

248
00:07:39,360 --> 00:07:41,519
你有N个类

249
00:07:42,199 --> 00:07:43,920
你的你有N个类的话

250
00:07:44,319 --> 00:07:45,160
你的P量大小

251
00:07:45,160 --> 00:07:46,840
最好不要超过10成

252
00:07:47,360 --> 00:07:49,080
这是我的通常做法

253
00:07:49,640 --> 00:07:50,319
这10成

254
00:07:50,319 --> 00:07:53,040
就是你做到100成是很难的

255
00:07:53,040 --> 00:07:54,920
10成或20成是不错的

256
00:07:55,120 --> 00:07:56,280
那就是Fashion MNIST

257
00:07:56,879 --> 00:07:57,400
10个类

258
00:07:57,400 --> 00:07:57,840
对吧

259
00:07:58,319 --> 00:07:59,200
那么做到100

260
00:07:59,200 --> 00:08:00,560
P量大小是128

261
00:08:00,560 --> 00:08:02,080
就是说做了是不错的

262
00:08:02,400 --> 00:08:04,319
ImageNet1000个类

263
00:08:04,319 --> 00:08:06,000
P量大小能做到10万

264
00:08:06,000 --> 00:08:06,920
也是差不多的

265
00:08:06,920 --> 00:08:07,680
也是能做的

266
00:08:07,920 --> 00:08:09,520
但更大就很难了

267
00:08:10,359 --> 00:08:14,319
就你给直观这么讲

268
00:08:14,680 --> 00:08:17,519
但很多时候你做不了特别大的

269
00:08:17,519 --> 00:08:19,319
很多时候你也不要担心这个事情

270
00:08:20,120 --> 00:08:21,680
很多时候你拿那么多GPU用

271
00:08:21,680 --> 00:08:22,120
对

272
00:08:23,079 --> 00:08:24,279
所以很多时候你要担心的是

273
00:08:24,279 --> 00:08:25,639
我那么大一个P量大小

274
00:08:25,639 --> 00:08:27,799
能不能fit到我的GPU的memory里面

275
00:08:27,799 --> 00:08:29,120
这是我要关心的事情

276
00:08:29,879 --> 00:08:31,919
所以一般的做法是说

277
00:08:32,840 --> 00:08:35,439
我一个GPU里面

278
00:08:35,439 --> 00:08:37,360
尽量的增大我的P量大小

279
00:08:37,360 --> 00:08:38,439
使得能够算

280
00:08:38,840 --> 00:08:40,600
然后接下来再看一下

281
00:08:40,600 --> 00:08:41,880
我有多少个机器

282
00:08:42,240 --> 00:08:43,360
然后有机器我就加

283
00:08:43,360 --> 00:08:44,720
但加到一定程度

284
00:08:44,720 --> 00:08:45,920
说加机器没用的情况

285
00:08:45,920 --> 00:08:46,640
就不加机器了

286
00:08:46,640 --> 00:08:47,680
我这剩下机器

287
00:08:47,680 --> 00:08:48,840
我不用去年

288
00:08:48,840 --> 00:08:49,920
我可以跑一个别的任务

289
00:08:49,920 --> 00:08:50,280
对吧

290
00:08:50,720 --> 00:08:52,000
所以这个烦恼

291
00:08:52,000 --> 00:08:53,480
就是说通常烦恼

292
00:08:53,480 --> 00:08:54,640
只是我们写论文的烦恼

293
00:08:54,800 --> 00:08:56,440
实际情况下不会发生的

294
00:08:56,720 --> 00:08:58,520
所以大家不该太担心

