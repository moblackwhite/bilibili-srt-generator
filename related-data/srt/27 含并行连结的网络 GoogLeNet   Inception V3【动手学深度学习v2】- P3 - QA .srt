1
00:00:00,000 --> 00:00:05,759
第一個問題是說D2L這個名字是Deep to Lonely的意思嗎?

2
00:00:05,759 --> 00:00:13,560
其實不是的,D2L這個名字我是隨便選的,就是想短一點

3
00:00:13,560 --> 00:00:16,640
我們的名字叫做Dive into Deep Learning

4
00:00:16,640 --> 00:00:20,000
你看這個Dive就是個D

5
00:00:20,000 --> 00:00:24,440
into就是在英語裡面叫做to,就是2

6
00:00:24,440 --> 00:00:29,080
但是我有兩個D,當然我可以叫D2DL

7
00:00:29,480 --> 00:00:31,640
D2DL我就覺得名字有點長

8
00:00:31,640 --> 00:00:35,039
我們之前吃虧就吃在MStat這個名字上

9
00:00:35,039 --> 00:00:37,160
一直我就覺得發這個名字就很難發

10
00:00:37,160 --> 00:00:39,840
你看人家TensorFlow名字多好

11
00:00:39,840 --> 00:00:41,280
所以我就想短的

12
00:00:41,280 --> 00:00:43,320
所以我就把那個D去掉了

13
00:00:43,320 --> 00:00:48,040
這個2的意思就是D2L的to就表示又是一個into的意思

14
00:00:48,040 --> 00:00:49,560
我也沒有兩個D

15
00:00:49,560 --> 00:00:52,040
就D2L,就簡單一點

16
00:00:52,040 --> 00:00:53,280
所以沒什麼

17
00:00:53,280 --> 00:00:56,480
D2L的全稱是Dive into Deep Learning,是這個名字

18
00:00:57,480 --> 00:01:00,440
但實際上我就想短一點而已

19
00:01:03,160 --> 00:01:04,400
第二個問題是說

20
00:01:04,400 --> 00:01:08,719
不同的1乘1的捲機後通道不一樣是用了不同的不符嗎?

21
00:01:08,719 --> 00:01:11,480
通道數不一樣不是不符造成的

22
00:01:11,480 --> 00:01:15,560
通道數不一樣是1乘1的捲機輸出的通道數造成的

23
00:01:15,560 --> 00:01:19,400
大家可以回去看一下那個捲機層那一節的錄像

24
00:01:20,400 --> 00:01:28,880
Inception塊裡面的一個Path的Max pooling在Conf之前比較少見

25
00:01:28,880 --> 00:01:30,560
為什麼這麼設計

26
00:01:30,560 --> 00:01:34,440
就是說我沒有覺得它

27
00:01:34,440 --> 00:01:36,400
我沒有覺得這個是一個少見的case

28
00:01:36,400 --> 00:01:37,760
給大家講一下這個

29
00:01:37,760 --> 00:01:50,320
為什麼我現在調這個地方來

30
00:01:50,320 --> 00:01:53,800
等我一下

31
00:01:54,799 --> 00:02:09,439
這個同學問說你這個

32
00:02:09,439 --> 00:02:13,280
為什麼這個Max pooling

33
00:02:13,280 --> 00:02:16,479
這個東西為什麼放在這個東西之前

34
00:02:16,479 --> 00:02:19,120
這個不少見是因為你

35
00:02:19,120 --> 00:02:22,719
其實你認為這個input是你的捲機層的input

36
00:02:22,879 --> 00:02:24,800
這個Conf它不是一個

37
00:02:24,800 --> 00:02:26,680
這個Conf你可以認為是一個全面階層

38
00:02:26,680 --> 00:02:29,039
它就是用來給你的通道數給你降一降用的

39
00:02:29,039 --> 00:02:31,639
所以捲機層這個Max pooling

40
00:02:31,639 --> 00:02:34,319
其實還是作用在之前的捲機層的輸出上

41
00:02:34,319 --> 00:02:37,680
是使得你那個像素更加的對位移更加魯邦一點

42
00:02:37,680 --> 00:02:41,479
所以這個東西其實你可以認為它是一個捲機層的作用

43
00:02:41,479 --> 00:02:43,919
這就是我們在NIN解釋的這一塊原理

44
00:02:43,919 --> 00:02:46,639
也就是說為什麼我把它畫成一個白色的地方的原因

45
00:02:53,039 --> 00:02:56,919
就3×3和5×5的捲機層也可以降低通道數

46
00:02:56,919 --> 00:02:58,439
為什麼要從1×1呢

47
00:02:58,439 --> 00:03:00,560
就是說不行的

48
00:03:00,560 --> 00:03:03,000
就是說它能夠降低通道數

49
00:03:03,000 --> 00:03:04,120
就是說我它

50
00:03:04,520 --> 00:03:06,520
就是說如果你不降通道數的話

51
00:03:06,520 --> 00:03:08,159
它的計算量太大了

52
00:03:08,879 --> 00:03:11,199
就這個1×1的捲機層是把你的通道數

53
00:03:11,199 --> 00:03:14,840
從比如說256降到128再輸入給它

54
00:03:14,960 --> 00:03:17,000
這樣子它拿到的是128通道

55
00:03:17,000 --> 00:03:18,599
它可以變高變低都沒關係

56
00:03:18,599 --> 00:03:19,719
都是但是我的

57
00:03:19,759 --> 00:03:22,240
我就把你的複雜度給降下來了

58
00:03:22,520 --> 00:03:25,480
不然的話如果你沒有1×1給你降通道數的話

59
00:03:25,480 --> 00:03:26,960
你後面計算量太大了

60
00:03:26,960 --> 00:03:27,760
就是這個原因

61
00:03:30,000 --> 00:03:31,760
就是說問題5是說

62
00:03:31,760 --> 00:03:35,120
裡面很多超參數都是2的n次方有什麼聯繫嗎

63
00:03:35,159 --> 00:03:37,280
就大家一般需要2的n次方

64
00:03:37,280 --> 00:03:39,480
是因為2的n次方算起來快一點

65
00:03:40,320 --> 00:03:43,320
就是說因為這是

66
00:03:43,640 --> 00:03:46,520
就是你用2的n次方這種做通道數的東西

67
00:03:46,520 --> 00:03:49,000
就是說在GPU上算的比較好

68
00:03:49,000 --> 00:03:50,480
而且人算起來比較簡單

69
00:03:50,480 --> 00:03:52,680
但是你不要用100 200 300這個東西

70
00:03:53,840 --> 00:03:57,000
就是要並行起來不那麼方便

71
00:03:59,600 --> 00:04:00,960
就問題6是說

72
00:04:00,960 --> 00:04:03,200
正常做深度學習模型時候

73
00:04:03,200 --> 00:04:04,880
是根據經典模型寫出來

74
00:04:04,880 --> 00:04:06,240
GPU還是交給

75
00:04:06,400 --> 00:04:08,680
直接調用

76
00:04:08,880 --> 00:04:10,840
好實現好的network

77
00:04:11,160 --> 00:04:12,920
就你正常做深度學習的時候

78
00:04:12,920 --> 00:04:14,280
是你不要去

79
00:04:14,400 --> 00:04:17,120
不要最好先不要去改經典的網絡

80
00:04:17,240 --> 00:04:19,640
就用別人的網絡

81
00:04:19,759 --> 00:04:22,599
就除非你的數據特別不一樣

82
00:04:22,599 --> 00:04:23,079
你可以改

83
00:04:23,079 --> 00:04:27,000
但是我建議除非你特別的懂這一塊是專家

84
00:04:27,000 --> 00:04:29,360
你不然盡量不要去改他的一些架構

85
00:04:29,360 --> 00:04:31,159
或者你可以稍微調一下通道數

86
00:04:31,439 --> 00:04:33,479
你把通道數所有的通道數

87
00:04:33,959 --> 00:04:35,039
除以個2

88
00:04:35,079 --> 00:04:36,560
我們在VGG幹過對吧

89
00:04:36,560 --> 00:04:39,199
我們在VGG把所有通道數除了個4

90
00:04:39,240 --> 00:04:40,879
就是說讓它變得窄一點點

91
00:04:40,879 --> 00:04:41,639
你是可以幹的

92
00:04:41,639 --> 00:04:42,599
就這樣子

93
00:04:42,599 --> 00:04:43,680
你計算量低一點

94
00:04:44,479 --> 00:04:46,159
但是別的那種

95
00:04:46,279 --> 00:04:49,279
或者你可以把輸入輸出也拉寬一點

96
00:04:49,280 --> 00:04:50,400
拉窄一點都行

97
00:04:50,600 --> 00:04:53,800
但是你別的你就最好不要去吸

98
00:04:54,000 --> 00:04:55,560
喬納裡面的結構

99
00:04:57,800 --> 00:05:00,160
就問題其實說你這個裡面

100
00:05:01,000 --> 00:05:04,000
你這個裡面它的最強是哪個

101
00:05:06,200 --> 00:05:07,880
就是說我們不是有個

102
00:05:12,280 --> 00:05:13,280
這邊沒電了

103
00:05:15,040 --> 00:05:15,480
OK

104
00:05:16,480 --> 00:05:20,480
就是說我們最強是誰

105
00:05:20,480 --> 00:05:25,080
最強我給大家看一下最強是誰

106
00:05:28,560 --> 00:05:30,560
大家不要迷信最強是誰

107
00:05:31,080 --> 00:05:32,360
最強那個是誰

108
00:05:33,000 --> 00:05:35,280
最強是resnest229

109
00:05:35,280 --> 00:05:36,400
這是我們的mobile

110
00:05:36,400 --> 00:05:38,840
最強是我們自己的mobile

111
00:05:39,200 --> 00:05:41,040
給大家看一下最強是哪個

112
00:05:41,280 --> 00:05:43,120
是resnest229

113
00:05:44,120 --> 00:05:46,079
它的accuracy是

114
00:05:46,079 --> 00:05:49,879
你看最強的系列是我們自己的工作

115
00:05:49,879 --> 00:05:54,199
就叫做就最強是這個系列

116
00:05:54,319 --> 00:05:56,160
你可以看到是黃色的系列

117
00:05:56,160 --> 00:05:58,360
它的名字叫做resnest

118
00:05:58,360 --> 00:05:59,840
就大家可以搜一下

119
00:06:05,240 --> 00:06:06,360
resnest

120
00:06:08,800 --> 00:06:09,519
就這個網絡

121
00:06:09,519 --> 00:06:18,319
我名字還在裡面

122
00:06:18,319 --> 00:06:19,680
雖然我的貢獻不是很多

123
00:06:19,680 --> 00:06:20,759
但我名字放在裡面

124
00:06:21,199 --> 00:06:24,519
這個是一個我們自己設計的一個網絡

125
00:06:25,560 --> 00:06:27,399
就用了大量的技巧

126
00:06:27,399 --> 00:06:28,680
用了大量的新的東西

127
00:06:28,680 --> 00:06:30,879
但我們這個課我們就不去特別去講

128
00:06:30,879 --> 00:06:32,079
這個網絡長什麼樣子了

129
00:06:32,719 --> 00:06:34,479
我們目前算下來

130
00:06:34,479 --> 00:06:36,560
就這個網絡確實是在

131
00:06:36,680 --> 00:06:38,959
ImageNet上它其實不一定是最好

132
00:06:38,960 --> 00:06:40,160
它主要是在

133
00:06:40,360 --> 00:06:42,680
遷移學習效果非常好

134
00:06:43,720 --> 00:06:44,840
所以

135
00:06:45,760 --> 00:06:46,760
但是反過來講

136
00:06:46,760 --> 00:06:49,080
這個文章我們就投了兩次都被拒了

137
00:06:49,360 --> 00:06:51,800
所以現在就是說效果好的

138
00:06:51,840 --> 00:06:53,160
還不一定發的出來

139
00:06:53,160 --> 00:06:56,560
OK問題8

140
00:06:56,840 --> 00:06:58,680
InceptionBlock是不是可以看得出

141
00:06:58,680 --> 00:07:01,000
輸入同樣的原生網絡

142
00:07:01,320 --> 00:07:03,000
你可以認為是你可以

143
00:07:03,440 --> 00:07:06,000
你可以認為它是parallel的網絡

144
00:07:06,160 --> 00:07:09,040
它是確實有幾個並行的模塊

145
00:07:09,319 --> 00:07:11,000
你可以認為是原生網絡

146
00:07:11,000 --> 00:07:13,079
就那種

147
00:07:13,600 --> 00:07:14,480
什麼

148
00:07:14,639 --> 00:07:15,279
True Tower

149
00:07:15,279 --> 00:07:15,879
但是

150
00:07:17,399 --> 00:07:19,639
你可以認為就是兩條不同的path去看

151
00:07:19,639 --> 00:07:20,920
就是說把你的通道

152
00:07:21,160 --> 00:07:22,319
最終的通道數裡面

153
00:07:22,319 --> 00:07:23,839
有些通道數是這麼過來

154
00:07:23,839 --> 00:07:25,199
有些通道數那麼過去

155
00:07:25,639 --> 00:07:26,759
就你可以這麼認為

156
00:07:27,639 --> 00:07:30,480
之後其實還有很多論文用類似的思想

157
00:07:32,360 --> 00:07:32,879
問題9

158
00:07:32,879 --> 00:07:35,360
3乘3改1乘3和3乘1的好處是什麼

159
00:07:36,040 --> 00:07:38,959
就3乘3改成兩個1乘3和3乘1的好處

160
00:07:38,959 --> 00:07:40,639
是可以降低你的計算量

161
00:07:42,480 --> 00:07:42,920
對吧

162
00:07:42,920 --> 00:07:44,079
3乘3的話

163
00:07:44,079 --> 00:07:44,839
你就是

164
00:07:45,959 --> 00:07:48,759
3乘3改1乘1乘3和3乘1的話

165
00:07:48,759 --> 00:07:52,120
你的計算量會減個1

166
00:07:52,680 --> 00:07:53,959
就它的主要的好處

167
00:07:54,720 --> 00:07:56,879
壞處就是說它可能效果沒那麼好一點

168
00:07:59,319 --> 00:07:59,879
問題10

169
00:07:59,879 --> 00:08:02,079
改進版的module內部的path不對稱

170
00:08:02,079 --> 00:08:03,879
是不是在提取不同層的信息

171
00:08:03,879 --> 00:08:06,199
因為最後一層和開始層的信息不一樣

172
00:08:06,680 --> 00:08:07,319
就是說

173
00:08:08,199 --> 00:08:10,319
改進版的module裡面不對稱

174
00:08:10,560 --> 00:08:12,480
這個東西也沒有什麼對稱不對稱的

175
00:08:12,480 --> 00:08:13,159
就是說

176
00:08:14,519 --> 00:08:17,879
我覺得對稱性在inception裡面

177
00:08:17,879 --> 00:08:19,279
它不是最關鍵一個東西

178
00:08:20,159 --> 00:08:21,600
我倒是覺得說

179
00:08:22,600 --> 00:08:23,879
其實我也不好解釋

180
00:08:23,879 --> 00:08:25,600
它到底改成那個樣子到底怎麼樣

181
00:08:25,600 --> 00:08:27,360
因為我們覺得你應該這麼做

182
00:08:27,360 --> 00:08:29,079
實際上你never know

183
00:08:29,079 --> 00:08:30,360
那個network幹嘛

184
00:08:32,759 --> 00:08:33,480
問題11

185
00:08:33,600 --> 00:08:35,680
linear flatten和dance有什麼區別

186
00:08:35,680 --> 00:08:36,840
感覺全是全連接

187
00:08:37,159 --> 00:08:38,519
linear和dance都是全連接

188
00:08:38,519 --> 00:08:40,879
就是你可以call linear也可以call dance

189
00:08:41,159 --> 00:08:42,200
這都是一個東西

190
00:08:42,200 --> 00:08:43,279
但flatten不是

191
00:08:43,639 --> 00:08:47,039
flatten就是把一個4D的一個tensor變成一個2D

192
00:08:47,840 --> 00:08:49,879
因為你全連接必須是2D的輸入

193
00:08:49,879 --> 00:08:51,320
所以你捲接是4D

194
00:08:51,879 --> 00:08:53,320
flatten就是說把你的

195
00:08:53,759 --> 00:08:55,120
PID大小維保持住

196
00:08:55,120 --> 00:08:56,800
剩下東西全部拉成一條橡樑

197
00:08:57,600 --> 00:08:58,720
就flatten乾的事情

198
00:09:01,360 --> 00:09:01,879
問題12

199
00:09:01,879 --> 00:09:03,879
GNCV的model2裡面有GoogleNet

200
00:09:03,879 --> 00:09:04,759
Inception v3

201
00:09:04,759 --> 00:09:05,639
這個兩個模型

202
00:09:05,759 --> 00:09:07,000
它們什麼區別

203
00:09:07,240 --> 00:09:08,360
就是說區別就是說

204
00:09:08,360 --> 00:09:11,200
我們今天詳細講的那個模型是GoogleNet

205
00:09:11,439 --> 00:09:14,360
GoogleNet是最早的一個版本

206
00:09:14,519 --> 00:09:16,799
因此v3就是我們後面有講過說

207
00:09:16,799 --> 00:09:18,399
對GoogleNet做各種改進

208
00:09:18,439 --> 00:09:20,519
而且加上了應該是加上了

209
00:09:20,519 --> 00:09:22,720
Batch Normalization

210
00:09:22,879 --> 00:09:25,279
就是說GoogleV3是GoogleNet的改進

211
00:09:25,840 --> 00:09:27,720
現在很少用GoogleNet之間

212
00:09:27,759 --> 00:09:28,720
大家一般是用v3

213
00:09:28,720 --> 00:09:31,159
因為GoogleNet效果現在不是很好

214
00:09:31,879 --> 00:09:37,039
就是說問題13是說

215
00:09:37,039 --> 00:09:39,279
你的模型裡面都是一個樣本

216
00:09:39,279 --> 00:09:40,240
一個通道

217
00:09:40,240 --> 00:09:42,919
然後在實際正常情況是N個樣本

218
00:09:42,919 --> 00:09:43,759
三個通道

219
00:09:45,759 --> 00:09:47,039
就那樣的話

220
00:09:47,039 --> 00:09:49,799
每次是一起進入還是一個圖片

221
00:09:49,799 --> 00:09:50,639
一個圖片計算

222
00:09:50,799 --> 00:09:52,639
就是說我們在這個地方

223
00:09:52,639 --> 00:09:53,759
為什麼通道數是1

224
00:09:53,919 --> 00:09:57,080
是因為我們圖片級是一個RGB的

225
00:09:57,240 --> 00:09:58,600
是一個灰度圖

226
00:09:58,759 --> 00:09:59,519
沒有RGB

227
00:09:59,519 --> 00:10:01,320
所以是通道為1而不是3

228
00:10:02,000 --> 00:10:02,799
另外一個是說

229
00:10:02,799 --> 00:10:04,480
我們為什麼通道數選1

230
00:10:04,720 --> 00:10:05,439
我們通道數

231
00:10:05,439 --> 00:10:07,120
我們批量大小為什麼選1

232
00:10:07,120 --> 00:10:07,759
我們不是1

233
00:10:08,439 --> 00:10:10,120
我只是在給你看

234
00:10:10,960 --> 00:10:13,759
他要對高寬和通道數做變換的時候

235
00:10:13,759 --> 00:10:14,600
我們用的是1

236
00:10:15,600 --> 00:10:17,559
實際上我們訓練的時候用的128

237
00:10:17,559 --> 00:10:19,279
是一次性算128的圖片

238
00:10:19,279 --> 00:10:20,279
不是一個個算

239
00:10:20,519 --> 00:10:22,320
一個個算就基本上算不出來了

240
00:10:22,320 --> 00:10:23,080
性能太差

241
00:10:23,080 --> 00:10:24,240
都是一起算的

242
00:10:26,679 --> 00:10:27,759
問題14

243
00:10:29,159 --> 00:10:30,480
既然模型的調參

244
00:10:30,480 --> 00:10:32,519
是靠不同的方法對比得到的

245
00:10:32,519 --> 00:10:34,399
在計算資源有限的情況下

246
00:10:34,399 --> 00:10:35,960
有什麼辦法降低成本

247
00:10:36,480 --> 00:10:38,920
比如說把fashion list尺寸減小

248
00:10:39,680 --> 00:10:41,720
問題14是說你怎麼調參

249
00:10:42,680 --> 00:10:44,800
就是叫做hyperparameter optimization

250
00:10:44,800 --> 00:10:45,800
叫HPO

251
00:10:45,960 --> 00:10:47,600
就這一塊已經有很多種辦法

252
00:10:47,600 --> 00:10:48,840
一個辦法是說

253
00:10:48,879 --> 00:10:52,000
確實你不能在fashion list上調

254
00:10:52,039 --> 00:10:54,440
通常是說你在一個image net的

255
00:10:54,440 --> 00:10:55,759
一個小子級上調

256
00:10:55,759 --> 00:10:57,480
可以是什麼SIFAR10

257
00:10:57,960 --> 00:10:59,720
或者說你的image net

258
00:10:59,720 --> 00:11:02,200
可以把你的輸入輸出搞小一點

259
00:11:02,360 --> 00:11:05,120
然後把你的樣本減少一點

260
00:11:05,160 --> 00:11:06,759
然後你就說你這樣子

261
00:11:06,759 --> 00:11:08,399
我有10個不同的網絡

262
00:11:08,399 --> 00:11:09,879
我在後面訓練一次

263
00:11:09,879 --> 00:11:11,000
我能排個序

264
00:11:11,040 --> 00:11:14,080
把全面就是說雖然圖片級變少了

265
00:11:14,080 --> 00:11:18,200
但是大家排序可能還是相對說比較穩定

266
00:11:18,240 --> 00:11:20,279
所以你在小數據上去測一下

267
00:11:20,320 --> 00:11:21,720
然後再挑好的

268
00:11:21,720 --> 00:11:24,759
比如說我有1000個不一樣的想法

269
00:11:24,879 --> 00:11:27,080
我在一個小數據上選個10個

270
00:11:27,120 --> 00:11:28,879
再在完整數據上圈個

271
00:11:29,720 --> 00:11:30,680
圈10個

272
00:11:30,720 --> 00:11:33,200
但這裡面有一大套這樣子工具

273
00:11:33,399 --> 00:11:37,000
我們大家特別想聽的話

274
00:11:37,000 --> 00:11:37,519
當然可以

275
00:11:37,519 --> 00:11:38,519
我們可以講一次

276
00:11:38,680 --> 00:11:40,080
就是講HPO的東西

277
00:11:40,120 --> 00:11:41,440
就怎麼樣收參

278
00:11:41,440 --> 00:11:44,279
這不是現在也不是那麼簡簡單單的

279
00:11:44,279 --> 00:11:45,560
可以有很多種好

280
00:11:45,560 --> 00:11:47,320
很好的方法幫你收參

281
00:11:50,080 --> 00:11:50,960
問題15

282
00:11:50,960 --> 00:11:54,080
感覺現在講的網絡的輸出通道越來越多

283
00:11:54,120 --> 00:11:55,080
為什麼越來越多

284
00:11:55,080 --> 00:11:58,360
是因為輸出通道數多是沒問題的

285
00:11:58,360 --> 00:11:58,759
就是說

286
00:11:58,759 --> 00:12:01,200
你可以認為輸出通道數越多

287
00:12:01,200 --> 00:12:04,360
我在圖片裡面識別的pattern就越多

288
00:12:04,399 --> 00:12:06,600
但是你通道數你不能多到

289
00:12:06,600 --> 00:12:09,000
你的網絡根本就訓練不了

290
00:12:09,000 --> 00:12:10,320
就是說你通道數太多

291
00:12:10,320 --> 00:12:11,360
很容易overfitting

292
00:12:11,480 --> 00:12:13,240
所以就是說通道數越多

293
00:12:13,240 --> 00:12:15,480
是因為我們現在的網絡的設計

294
00:12:15,480 --> 00:12:16,519
越來越精巧

295
00:12:16,519 --> 00:12:17,240
你發現沒有

296
00:12:17,279 --> 00:12:19,360
比Alexander幾個大的全聯界

297
00:12:19,360 --> 00:12:20,680
幾個大的礦物一搞

298
00:12:20,680 --> 00:12:21,639
精巧多了

299
00:12:22,279 --> 00:12:25,039
所以當你的越來越深的情況下

300
00:12:25,039 --> 00:12:26,560
你的捲機層越來越深

301
00:12:26,560 --> 00:12:28,600
然後你捲機層的識別的模式

302
00:12:28,600 --> 00:12:29,760
會變得越來越多

303
00:12:29,760 --> 00:12:32,480
然後通常你可以把通常會也通道

304
00:12:32,480 --> 00:12:33,640
數會變得越來越多

305
00:12:33,680 --> 00:12:36,400
但目前來看1024就差不多了

306
00:12:36,440 --> 00:12:38,440
我可能有你可以2048

307
00:12:38,440 --> 00:12:40,040
但是一般一般情況下

308
00:12:40,040 --> 00:12:40,840
通用情況下

309
00:12:40,840 --> 00:12:42,360
imageNet這種數據機上

310
00:12:42,400 --> 00:12:43,680
這100萬張圖片的話

311
00:12:43,680 --> 00:12:45,600
1024是一個不錯的一個維度

312
00:12:48,360 --> 00:12:51,160
就是說大家說跟101000類

313
00:12:51,160 --> 00:12:51,800
是不是相關

314
00:12:51,800 --> 00:12:53,760
其實跟1000類也沒有那麼關

315
00:12:53,760 --> 00:12:54,800
也就是說我覺得

316
00:12:54,800 --> 00:12:57,600
如果你是2000類

317
00:12:57,600 --> 00:12:59,480
3000類1024也是不錯

318
00:12:59,640 --> 00:13:00,880
就是說

319
00:13:01,440 --> 00:13:03,760
當然是說如果你變成10萬類的話

320
00:13:03,760 --> 00:13:05,320
也許你要變成2048

321
00:13:05,320 --> 00:13:06,760
或者是更大一點

322
00:13:06,800 --> 00:13:09,600
但是確實一定關係

323
00:13:09,600 --> 00:13:11,360
但是不是完全對應的關係

324
00:13:16,400 --> 00:13:18,280
就是Inception塊裡面不同的路徑

325
00:13:18,280 --> 00:13:21,240
PyTorch的時間會自動並行嗎

326
00:13:21,240 --> 00:13:21,760
不行

327
00:13:21,760 --> 00:13:23,680
PyTorch好像不會幫你幹這個事情

328
00:13:23,720 --> 00:13:25,520
它就是一條一條算下去

329
00:13:25,560 --> 00:13:27,760
你在那樣symbolic graph裡面

330
00:13:27,760 --> 00:13:29,120
可能會幫你做這個事情

331
00:13:29,120 --> 00:13:32,040
就ImageNet TensorFlow應該是幫你做並行

332
00:13:32,080 --> 00:13:36,440
但是也沒可能也區別不大

333
00:13:37,000 --> 00:13:38,160
也沒那麼

334
00:13:40,520 --> 00:13:42,240
也沒那麼大的區別

335
00:13:43,120 --> 00:13:44,040
問題17

336
00:13:44,040 --> 00:13:46,880
RESTnest用的是Tension的架構嗎

337
00:13:46,880 --> 00:13:49,280
想聽一下跟有什麼不一樣

338
00:13:49,280 --> 00:13:52,840
我們講我們model組裡面最好的模型

339
00:13:52,840 --> 00:13:53,759
是這個模型

340
00:13:53,800 --> 00:13:54,560
就是說

341
00:13:55,759 --> 00:13:57,200
設計思想

342
00:13:58,200 --> 00:14:00,120
這個東西解釋起來比較麻煩

343
00:14:00,120 --> 00:14:01,960
就是為什麼是他用了大量的

344
00:14:01,960 --> 00:14:02,720
就是說

345
00:14:02,720 --> 00:14:04,519
首先我們當然承認一點

346
00:14:04,600 --> 00:14:06,160
就是說這個東西效果好

347
00:14:06,160 --> 00:14:07,800
不僅僅是用了Tension

348
00:14:08,000 --> 00:14:09,639
就是說我們當然寫paper

349
00:14:09,639 --> 00:14:10,440
當然是這麼說

350
00:14:10,639 --> 00:14:12,519
就是用Split Tension的東西

351
00:14:12,759 --> 00:14:14,960
實際上它效果是用了大量的

352
00:14:14,960 --> 00:14:18,920
我們因為開發GuramCV做類似的工作

353
00:14:18,920 --> 00:14:20,920
所以我們積累了大量的

354
00:14:21,120 --> 00:14:22,160
Trick在裡面

355
00:14:22,159 --> 00:14:23,919
所謂的Trick再加在一起

356
00:14:23,919 --> 00:14:24,759
加上這些東西

357
00:14:24,759 --> 00:14:27,159
就在裡面最主要的確實Tension的東西

358
00:14:27,159 --> 00:14:29,159
但它之所以比別人好很多

359
00:14:29,159 --> 00:14:30,759
它不僅僅是一個東西在作用

360
00:14:30,759 --> 00:14:32,679
它是很多別的東西在作用

361
00:14:33,279 --> 00:14:36,559
所以大家有興趣給大家講一下

362
00:14:36,559 --> 00:14:38,319
這裡面Trick都長什麼樣子

363
00:14:38,959 --> 00:14:41,279
就是說在我剛剛的slide

364
00:14:41,279 --> 00:14:42,199
我把它刪掉了

365
00:14:42,199 --> 00:14:44,279
我覺得這個東西是不是講起來太長了

366
00:14:44,279 --> 00:14:46,319
我們就課程時間控制不了

367
00:14:46,399 --> 00:14:49,240
如果大家特別想知道那些Trick

368
00:14:49,279 --> 00:14:52,360
大家就說模型設計是一塊

369
00:14:52,519 --> 00:14:54,000
數據怎麼弄

370
00:14:54,720 --> 00:14:55,799
優化怎麼優化

371
00:14:55,799 --> 00:14:56,680
Learning rate怎麼聊

372
00:14:56,680 --> 00:14:58,279
這東西都很重要

373
00:14:58,320 --> 00:14:59,759
模型其實不僅僅是

374
00:14:59,759 --> 00:15:01,120
可能不一定是最重要的

375
00:15:01,120 --> 00:15:03,799
很多時候那些東西可以是

376
00:15:04,960 --> 00:15:05,639
你可以看一下

377
00:15:05,960 --> 00:15:07,320
就是說比如說舉個例子

378
00:15:12,639 --> 00:15:15,639
你可以看到說這個框是個

379
00:15:15,639 --> 00:15:17,320
或者我可以做Detection

380
00:15:17,519 --> 00:15:18,600
就Detection好看一點

381
00:15:18,639 --> 00:15:19,240
就是說

382
00:15:20,519 --> 00:15:21,200
這個是什麼意思

383
00:15:21,600 --> 00:15:23,519
這個箭頭就是說你在這個地方

384
00:15:23,519 --> 00:15:25,759
本來你這個模型它的論文裡面

385
00:15:25,759 --> 00:15:27,360
Report的精度是在這個地方

386
00:15:27,920 --> 00:15:30,040
然後你再加了一些Trick進去的時候

387
00:15:30,040 --> 00:15:31,080
你的Data怎麼搞

388
00:15:31,080 --> 00:15:32,440
你的Learning rate怎麼設

389
00:15:32,440 --> 00:15:33,680
你的優化怎麼搞的話

390
00:15:33,720 --> 00:15:35,560
你可以把這個模型從這個精度

391
00:15:35,560 --> 00:15:37,040
再不改變計算複雜度

392
00:15:37,040 --> 00:15:39,000
因為你沒有改太多模型

393
00:15:39,320 --> 00:15:41,040
你改的是訓練的那些東西

394
00:15:41,080 --> 00:15:43,639
你可以把精度從這個地方升到這個地方

395
00:15:44,480 --> 00:15:45,800
就EURO3

396
00:15:46,080 --> 00:15:48,880
就是說這些訓練的數據的Trick

397
00:15:48,880 --> 00:15:51,680
對整個模型的非常重要

398
00:15:51,680 --> 00:15:53,920
所以就是說當你知道大量的Trick的話

399
00:15:53,920 --> 00:15:54,800
你不改變模型

400
00:15:54,800 --> 00:15:55,920
你也可以把它做得很牛逼

401
00:15:57,120 --> 00:15:58,440
就是說這個是一些

402
00:15:59,360 --> 00:16:02,080
Defraining的一些Dirty的一個地方在裡面

403
00:16:03,720 --> 00:16:06,720
而且GoogleNet V3為什麼厲害

404
00:16:06,720 --> 00:16:09,320
它也不僅僅是GoogleNet V3這個架構

405
00:16:09,320 --> 00:16:12,520
它裡面加入了大量的新的Softmax

406
00:16:12,520 --> 00:16:15,080
它叫做Soft Labeling

407
00:16:15,200 --> 00:16:17,480
就是把你那個01變成了一個0.9和0.1

408
00:16:17,480 --> 00:16:18,560
我們之前有講過這個

409
00:16:18,759 --> 00:16:19,840
就是他們提出來的

410
00:16:19,840 --> 00:16:22,920
這個東西給你帶來了很大的性能提升

411
00:16:22,920 --> 00:16:26,400
但是他們把它歸空於Inception V3的改進

412
00:16:26,400 --> 00:16:27,360
但也不一定是的

413
00:16:27,360 --> 00:16:29,480
所以在讀Paper的時候

414
00:16:29,480 --> 00:16:32,280
這裡面有很多很細節的東西在裡面

