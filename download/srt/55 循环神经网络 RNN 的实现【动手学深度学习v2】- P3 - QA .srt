1
00:00:00,000 --> 00:00:01,840
好 我们看一下简洁视线

2
00:00:01,840 --> 00:00:04,320
简洁视线其实跟之前的从零开始

3
00:00:04,320 --> 00:00:05,360
也没太多区别

4
00:00:05,360 --> 00:00:06,719
就是说首先第一个

5
00:00:06,719 --> 00:00:09,240
我们先把我们的数据load进来

6
00:00:11,240 --> 00:00:12,280
然后定义模型

7
00:00:12,280 --> 00:00:13,880
主要区别在这个地方

8
00:00:14,400 --> 00:00:16,480
定义模型就是PyTorch的RNN

9
00:00:16,480 --> 00:00:18,839
Module里面有个RNN的类

10
00:00:19,440 --> 00:00:20,920
就直接说告诉你说

11
00:00:21,160 --> 00:00:23,519
我这个WCAP的是多少

12
00:00:23,519 --> 00:00:25,199
就输入输出是多少

13
00:00:25,199 --> 00:00:27,719
和我的隐藏大小是多少

14
00:00:27,960 --> 00:00:28,719
就行了

15
00:00:28,719 --> 00:00:31,079
就会拿到我的RNN的layer

16
00:00:32,560 --> 00:00:33,159
OK

17
00:00:33,280 --> 00:00:33,759
说白了

18
00:00:33,759 --> 00:00:35,839
这就是我们之前所有那一堆函数

19
00:00:35,960 --> 00:00:37,879
全部藏在里面了

20
00:00:39,239 --> 00:00:40,719
然后拿到RNN layer之后

21
00:00:41,239 --> 00:00:43,560
当然你得自己手动来初始化

22
00:00:43,560 --> 00:00:44,359
你的隐藏状态

23
00:00:44,600 --> 00:00:46,200
就是初始化的是一个

24
00:00:46,719 --> 00:00:48,640
它这里会不一样一点

25
00:00:48,640 --> 00:00:49,960
它就有个1在这个地方

26
00:00:50,359 --> 00:00:51,679
然后这是一个

27
00:00:52,200 --> 00:00:53,679
说白了是1是因为

28
00:00:54,120 --> 00:00:56,519
你到LSTM的话就变成了

29
00:00:56,840 --> 00:00:57,960
我们刚刚是有一个

30
00:00:57,960 --> 00:00:59,399
我们用的是一个tuple来存

31
00:00:59,520 --> 00:01:01,120
它这用的是把多加了一个

32
00:01:01,120 --> 00:01:02,079
维度1在这个地方

33
00:01:02,600 --> 00:01:04,480
然后BatchSize和NumberHidden

34
00:01:04,480 --> 00:01:05,840
就是你的初始化状态

35
00:01:07,840 --> 00:01:08,640
调用它的话

36
00:01:08,840 --> 00:01:12,000
其实就是我先初始化一个X出来

37
00:01:12,319 --> 00:01:14,000
调用它的话跟之前是一样的

38
00:01:14,000 --> 00:01:15,040
一个N layer

39
00:01:15,079 --> 00:01:16,159
我们把X丢进来

40
00:01:16,159 --> 00:01:17,159
把state丢进来

41
00:01:17,159 --> 00:01:18,560
会返回出一个Y

42
00:01:18,560 --> 00:01:19,799
返回出一个

43
00:01:20,640 --> 00:01:23,000
一个这样子的state new

44
00:01:24,280 --> 00:01:26,359
它的Y跟我们之前不一样的是说

45
00:01:26,359 --> 00:01:27,879
它的Y并没有做

46
00:01:31,039 --> 00:01:32,719
它的Y存的东西

47
00:01:32,719 --> 00:01:34,239
跟我们稍微有一点不一样

48
00:01:34,680 --> 00:01:37,359
它的Y里面第一个维度是你的

49
00:01:40,159 --> 00:01:41,840
第一个维度是

50
00:01:42,759 --> 00:01:44,280
第一个维度是你的时间

51
00:01:44,280 --> 00:01:45,400
第二个维度是你的p

52
00:01:45,400 --> 00:01:47,200
第三个维度是你的输出

53
00:01:47,200 --> 00:01:47,920
所以它是一个

54
00:01:47,920 --> 00:01:48,680
我们就没有

55
00:01:48,680 --> 00:01:51,000
它不像我们就concate成一个2D的

56
00:01:51,000 --> 00:01:52,120
它其实还是

57
00:01:52,799 --> 00:01:54,799
把它弄成一个3D的东西出来

58
00:01:55,000 --> 00:01:56,359
state跟之前是一样的

59
00:01:56,359 --> 00:01:58,120
就是1312256的东西

60
00:02:00,719 --> 00:02:02,200
好 我们的RNN模型

61
00:02:02,319 --> 00:02:04,399
我们看一下RNN模型是什么样子的

62
00:02:07,599 --> 00:02:10,800
首先你看一下RNN这个东西

63
00:02:10,919 --> 00:02:12,039
我们是RNN layer

64
00:02:12,039 --> 00:02:13,240
然后cap the size

65
00:02:13,240 --> 00:02:14,159
然后number of hindrances

66
00:02:14,159 --> 00:02:15,359
就全部存起来

67
00:02:15,400 --> 00:02:18,560
然后我们这个地方有

68
00:02:19,159 --> 00:02:20,599
之后会讲到bidirectional

69
00:02:20,599 --> 00:02:22,680
就双向的东西会怎么样做的

70
00:02:23,000 --> 00:02:24,199
OK 先不管的话

71
00:02:24,240 --> 00:02:25,560
就是说他们这个地方

72
00:02:25,560 --> 00:02:28,400
跟我们之前的一个主要的不一样

73
00:02:28,400 --> 00:02:30,560
我们现在刚刚我也讲错一点点

74
00:02:30,640 --> 00:02:31,920
我们再纠正一下

75
00:02:31,920 --> 00:02:33,680
就跟我们之前的RNN

76
00:02:33,680 --> 00:02:34,560
不一样的地方在于

77
00:02:34,920 --> 00:02:36,640
我们刚刚RNN的模型

78
00:02:36,800 --> 00:02:39,720
包括了我的输出层

79
00:02:41,000 --> 00:02:42,080
在这个地方没有

80
00:02:42,080 --> 00:02:44,400
就Pytorch RNN layer

81
00:02:44,400 --> 00:02:45,840
只包括隐藏层

82
00:02:45,840 --> 00:02:47,080
没有包括输出层

83
00:02:47,480 --> 00:02:48,320
因为输出层的话

84
00:02:48,320 --> 00:02:50,200
你可以任意给定别的东西

85
00:02:50,320 --> 00:02:51,840
我们反正就偷个懒

86
00:02:51,840 --> 00:02:53,960
就是把全部放隐藏层放在里面

87
00:02:53,960 --> 00:02:55,040
输出层放在里面

88
00:02:55,159 --> 00:02:57,240
所以你用Pytorch RNN的

89
00:02:57,240 --> 00:02:57,960
layer的时候

90
00:02:58,280 --> 00:03:00,080
你得构造自己的输出层

91
00:03:00,080 --> 00:03:00,960
就是linear

92
00:03:01,840 --> 00:03:03,159
所以就是说

93
00:03:03,159 --> 00:03:04,120
你可以看到我们的输出层

94
00:03:04,120 --> 00:03:05,240
就是number of hindrances

95
00:03:05,240 --> 00:03:07,120
乘以我的cap the size

96
00:03:07,280 --> 00:03:07,800
OK

97
00:03:08,400 --> 00:03:09,480
然后可以看到forward

98
00:03:10,400 --> 00:03:11,400
forward的话

99
00:03:11,520 --> 00:03:13,080
就是跟之前差不多

100
00:03:13,080 --> 00:03:14,960
就是我的input x进来

101
00:03:15,000 --> 00:03:16,040
做one hot

102
00:03:16,360 --> 00:03:19,040
one hot就换成float

103
00:03:19,319 --> 00:03:20,920
然后丢到RNN里面

104
00:03:21,159 --> 00:03:22,520
这个y我们纠正一下

105
00:03:22,520 --> 00:03:23,719
刚刚我说错了

106
00:03:23,960 --> 00:03:25,280
y其实就是你的

107
00:03:27,280 --> 00:03:28,680
其实是你的

108
00:03:29,359 --> 00:03:30,599
中间的隐藏层

109
00:03:30,800 --> 00:03:31,080
y

110
00:03:31,599 --> 00:03:33,800
就每一个就是你的时间部署

111
00:03:33,800 --> 00:03:34,680
乘以P量大小

112
00:03:34,680 --> 00:03:36,120
乘以隐藏大小

113
00:03:36,960 --> 00:03:38,120
然后我们要做的话

114
00:03:38,240 --> 00:03:39,159
我们就reshape一下

115
00:03:39,599 --> 00:03:41,079
就把它做成一个2D

116
00:03:41,400 --> 00:03:44,120
就把时间长度

117
00:03:44,240 --> 00:03:45,560
时间部署和P量

118
00:03:45,560 --> 00:03:47,079
那个维度放在一起

119
00:03:47,079 --> 00:03:48,199
后面最后一个维度

120
00:03:48,199 --> 00:03:49,599
就是你的隐藏大小

121
00:03:49,599 --> 00:03:50,479
放在二维

122
00:03:50,960 --> 00:03:52,919
最后我们有点像看卷机

123
00:03:52,920 --> 00:03:54,440
我们卷机是不是最后一层

124
00:03:54,440 --> 00:03:56,280
要到输出的时候

125
00:03:56,280 --> 00:03:57,760
也要做一次reshape出来

126
00:03:58,120 --> 00:03:59,160
然后丢到Linear里面

127
00:03:59,160 --> 00:04:00,120
就得到我们的输出了

128
00:04:01,560 --> 00:04:02,160
另外的话

129
00:04:02,160 --> 00:04:02,920
就是说

130
00:04:04,480 --> 00:04:05,280
差不多的话

131
00:04:06,200 --> 00:04:07,760
基本上是如果是

132
00:04:08,440 --> 00:04:09,360
我们之后会来讲

133
00:04:09,480 --> 00:04:10,760
如果是LSTM的话

134
00:04:10,760 --> 00:04:13,080
会有两个状态

135
00:04:13,080 --> 00:04:13,760
如果不然的话

136
00:04:13,760 --> 00:04:15,680
就是一个我们之前的状态

137
00:04:16,400 --> 00:04:17,759
所以我们就先跳过

138
00:04:17,840 --> 00:04:19,560
就是跟我们之前是没区别

139
00:04:19,560 --> 00:04:21,800
但是这个地方考虑了之后的

140
00:04:23,280 --> 00:04:25,480
双向的和LSTM它的区别

141
00:04:27,240 --> 00:04:28,800
所以基本上可以看到是说

142
00:04:29,400 --> 00:04:30,439
用了RN的话

143
00:04:30,439 --> 00:04:31,759
你主要的好处是说

144
00:04:31,759 --> 00:04:33,360
你RN等Fold的函数

145
00:04:33,360 --> 00:04:34,160
你不用管它

146
00:04:34,840 --> 00:04:36,640
但是在训练模型

147
00:04:36,720 --> 00:04:37,600
整个模型上面

148
00:04:37,600 --> 00:04:39,360
其实跟我们的空Scratch的

149
00:04:39,360 --> 00:04:40,280
没有太多区别

150
00:04:40,280 --> 00:04:41,160
就Fold的函数

151
00:04:41,160 --> 00:04:41,960
没有太多区别

152
00:04:42,400 --> 00:04:44,200
所以而且你还得把

153
00:04:44,200 --> 00:04:46,439
记得把输出层给加进来

154
00:04:47,680 --> 00:04:48,240
OK

155
00:04:48,639 --> 00:04:52,160
然后基本上就是初始化一下

156
00:04:52,160 --> 00:04:54,040
然后看一下

157
00:04:54,040 --> 00:04:54,600
Predict

158
00:04:54,600 --> 00:04:55,520
就没有训练之后

159
00:04:55,520 --> 00:04:56,240
Predict怎么样

160
00:04:56,240 --> 00:04:57,439
就是基本上就是

161
00:04:58,280 --> 00:04:59,600
全部输出了一个E

162
00:05:00,320 --> 00:05:02,200
然后就训练一遍

163
00:05:02,920 --> 00:05:04,120
我就是跟刚刚是一样的

164
00:05:04,120 --> 00:05:05,640
我们就不太讲了

165
00:05:05,840 --> 00:05:07,680
就唯一可以看到是说

166
00:05:08,080 --> 00:05:08,960
我们这个地方

167
00:05:10,600 --> 00:05:12,120
Populacity虽然是1.3

168
00:05:12,320 --> 00:05:14,000
就是说比我们刚刚1.0

169
00:05:14,000 --> 00:05:14,920
要高那么一点点

170
00:05:14,920 --> 00:05:16,240
但是基本上你可以忽略不计

171
00:05:16,240 --> 00:05:16,960
那点误差

172
00:05:17,080 --> 00:05:17,720
而另外一个说

173
00:05:17,720 --> 00:05:18,680
你可以看到是

174
00:05:19,080 --> 00:05:20,440
速度比我们快

175
00:05:21,360 --> 00:05:23,120
就之前我们大概跑了1分多钟

176
00:05:23,120 --> 00:05:23,360
对吧

177
00:05:23,360 --> 00:05:24,839
这里大概跑了24秒

178
00:05:24,839 --> 00:05:25,759
就是快个

179
00:05:26,040 --> 00:05:27,120
两到三倍的样子

180
00:05:27,120 --> 00:05:27,639
三倍吧

181
00:05:27,639 --> 00:05:28,240
将近

182
00:05:28,639 --> 00:05:30,000
这是因为为什么呢

183
00:05:30,600 --> 00:05:32,839
因为我们从零开始实现的时候

184
00:05:32,839 --> 00:05:34,800
我们是用矩阵

185
00:05:34,800 --> 00:05:36,079
有一堆小矩阵

186
00:05:36,079 --> 00:05:37,680
惩罚做的实现

187
00:05:37,920 --> 00:05:40,680
然后在框架的实现

188
00:05:40,680 --> 00:05:41,839
框架的RM实现

189
00:05:41,879 --> 00:05:43,399
一般会把这些小矩阵

190
00:05:43,439 --> 00:05:43,879
惩罚

191
00:05:44,279 --> 00:05:45,439
变成一个大矩阵

192
00:05:45,439 --> 00:05:46,000
惩罚

193
00:05:46,399 --> 00:05:48,720
就把这些输入全部放在一起

194
00:05:48,720 --> 00:05:51,200
先concatenate成一个大矩阵

195
00:05:51,200 --> 00:05:53,080
然后做一次矩阵惩罚就出去了

196
00:05:53,640 --> 00:05:56,680
因为做小矩阵惩罚的

197
00:05:58,080 --> 00:05:58,800
开销

198
00:05:59,080 --> 00:06:00,680
多次小矩阵惩罚的开销

199
00:06:00,680 --> 00:06:01,800
要大于

200
00:06:02,600 --> 00:06:03,480
同样的计算量

201
00:06:03,480 --> 00:06:05,280
但做一次大矩阵惩罚

202
00:06:05,280 --> 00:06:07,120
就因为Python一有开销

203
00:06:07,120 --> 00:06:07,680
二呢

204
00:06:07,680 --> 00:06:09,560
GPU自己可能有一定开销

205
00:06:09,560 --> 00:06:10,840
而且大矩阵的话

206
00:06:11,480 --> 00:06:13,480
多线程的优化会更好一点

207
00:06:13,880 --> 00:06:15,720
我们在计算机性能也讲过

208
00:06:15,720 --> 00:06:17,080
这也是为什么说

209
00:06:17,279 --> 00:06:19,039
利用框架的RM的实现

210
00:06:19,199 --> 00:06:20,279
可以效果很好

211
00:06:20,319 --> 00:06:21,000
当然反过来讲

212
00:06:21,000 --> 00:06:23,560
你也可以做实现这种空开的操作

213
00:06:23,680 --> 00:06:25,199
你可以把我们scratch版本

214
00:06:25,199 --> 00:06:26,039
提升一下性能

215
00:06:26,039 --> 00:06:27,879
也是基本上也可以达到

216
00:06:28,079 --> 00:06:28,919
差不多的好处

217
00:06:30,319 --> 00:06:30,439
好

218
00:06:30,479 --> 00:06:31,599
另外看一下效果

219
00:06:31,639 --> 00:06:32,199
就是说

220
00:06:32,319 --> 00:06:33,959
因为我们没有训练到零

221
00:06:34,120 --> 00:06:35,199
所以还是看到那边

222
00:06:35,479 --> 00:06:36,199
这个是什么词

223
00:06:36,199 --> 00:06:36,879
我也不认识

224
00:06:36,919 --> 00:06:38,039
这个词我也不认识

225
00:06:38,079 --> 00:06:38,560
对吧

226
00:06:39,120 --> 00:06:41,839
所以效果没有训练到零

227
00:06:41,839 --> 00:06:44,360
完全记住你样本效果好

228
00:06:45,439 --> 00:06:46,039
OK

229
00:06:46,840 --> 00:06:47,120
好

230
00:06:47,120 --> 00:06:50,120
这就是使用高级的API

231
00:06:50,120 --> 00:06:51,640
就是用Torch RN的

232
00:06:51,640 --> 00:06:52,920
RN的layer API

233
00:06:54,040 --> 00:06:55,160
就是基本上可以

234
00:06:56,120 --> 00:06:57,319
避免到我们去定义

235
00:06:57,319 --> 00:07:00,040
RN函数初始状态的

236
00:07:00,040 --> 00:07:00,960
不能避免

237
00:07:01,120 --> 00:07:02,439
主要是避免初始化

238
00:07:02,439 --> 00:07:04,600
那些权重和RN怎么计算

239
00:07:04,759 --> 00:07:06,520
两个函数可以被用RN layer

240
00:07:06,520 --> 00:07:07,520
来替代掉

241
00:07:07,759 --> 00:07:08,120
OK

242
00:07:08,120 --> 00:07:09,680
别的都是差不多的

