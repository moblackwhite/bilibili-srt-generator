1
00:00:00,000 --> 00:00:01,280
好,Bert

2
00:00:01,280 --> 00:00:05,160
Bert这个模型就是大家知道

3
00:00:05,160 --> 00:00:09,359
Bert是芝麻街里面一个人物的名字

4
00:00:09,359 --> 00:00:10,759
芝麻街怎么说呢

5
00:00:10,759 --> 00:00:12,839
叫Sesame Street

6
00:00:12,839 --> 00:00:14,599
在美国的话

7
00:00:14,599 --> 00:00:19,760
它是一个针对小朋友的英语启蒙的一个节目

8
00:00:19,760 --> 00:00:21,199
还播出了挺多季的

9
00:00:21,199 --> 00:00:22,240
国内的话

10
00:00:22,240 --> 00:00:23,280
如果讲对应的话

11
00:00:23,280 --> 00:00:27,080
我觉得有一点点像当年何久老师

12
00:00:27,080 --> 00:00:27,960
金贵子

13
00:00:28,080 --> 00:00:29,760
那个项目叫CCTV做的

14
00:00:29,760 --> 00:00:32,439
但如果大家可能年纪轻

15
00:00:32,439 --> 00:00:33,480
可能没见过这个项目

16
00:00:33,920 --> 00:00:35,640
后来就没做了

17
00:00:35,640 --> 00:00:38,399
或者你可以认为是有点像西游记

18
00:00:38,399 --> 00:00:41,480
虽然西游记一直没有拍很多续集

19
00:00:41,600 --> 00:00:42,840
但是会一直放

20
00:00:43,280 --> 00:00:46,760
当然芝麻街是没有西游记在美国的影响力了

21
00:00:46,760 --> 00:00:47,960
但是你多多少少认为

22
00:00:47,960 --> 00:00:51,200
它确实一个大家都知道的剧

23
00:00:51,400 --> 00:00:53,159
比如说我小孩4岁

24
00:00:53,159 --> 00:00:54,760
在学校里幼儿园

25
00:00:54,760 --> 00:00:57,719
老师就可能会给大家讲一讲这个故事

26
00:00:58,000 --> 00:00:59,600
Bert是里面一个主人公

27
00:00:59,759 --> 00:01:01,920
所以有点像你说你写个

28
00:01:01,920 --> 00:01:03,439
你写篇诺文

29
00:01:03,439 --> 00:01:05,960
把你的模型的名字叫孙空

30
00:01:05,960 --> 00:01:08,239
我取个模型

31
00:01:08,239 --> 00:01:09,239
我在你后面做一个

32
00:01:09,359 --> 00:01:11,400
我得想办法取一个沙和尚

33
00:01:11,400 --> 00:01:13,159
或者唐僧或者猪八戒

34
00:01:13,159 --> 00:01:13,560
对吧

35
00:01:13,920 --> 00:01:15,359
如果你名字都取完了

36
00:01:15,359 --> 00:01:15,920
那我怎么办

37
00:01:15,920 --> 00:01:18,599
那我就取什么牛魔王什么

38
00:01:18,599 --> 00:01:19,159
对吧

39
00:01:19,159 --> 00:01:21,439
你要把西游记里面所有的名字给你取一遍

40
00:01:21,640 --> 00:01:26,120
所以Bert也是拉开了一扇大门

41
00:01:26,280 --> 00:01:27,439
让大家取名字

42
00:01:27,439 --> 00:01:28,679
突然上了一个档子

43
00:01:28,879 --> 00:01:30,319
基本上Bert之后

44
00:01:30,319 --> 00:01:33,879
有整个芝麻街一条街的一系列模型

45
00:01:34,319 --> 00:01:34,920
OK

46
00:01:34,959 --> 00:01:37,280
所以我们就直接给大家讲一下

47
00:01:37,280 --> 00:01:39,319
Bert后续的很多模型

48
00:01:39,319 --> 00:01:41,280
我们就因为时间关系

49
00:01:41,280 --> 00:01:42,280
我们就不给大家讲了

50
00:01:42,280 --> 00:01:44,519
但是实际上长得都差不多

51
00:01:44,879 --> 00:01:45,640
OK

52
00:01:46,159 --> 00:01:47,920
所以我们来讲Bert的时候

53
00:01:48,039 --> 00:01:49,799
我们要讲到一个东西

54
00:01:49,799 --> 00:01:53,280
就是什么是NLP里面的迁移学习

55
00:01:53,439 --> 00:01:54,519
Transfer Learning

56
00:01:54,719 --> 00:01:56,439
我们在讲计算机视觉的时候

57
00:01:56,439 --> 00:01:56,879
有讲过

58
00:01:57,079 --> 00:01:59,119
就是说你在ImageNet上

59
00:01:59,119 --> 00:02:00,640
预训上的模型

60
00:02:00,679 --> 00:02:02,640
然后用到别的地方去

61
00:02:02,839 --> 00:02:03,839
你可以做别的任务

62
00:02:03,839 --> 00:02:06,719
就是说做你的小数据的预测也好

63
00:02:06,719 --> 00:02:07,640
图片分类也好

64
00:02:08,359 --> 00:02:10,519
做你的目标检测也好

65
00:02:10,799 --> 00:02:13,319
基本上你是不是从头开始训练的

66
00:02:13,319 --> 00:02:14,439
你可能是拿一个

67
00:02:14,439 --> 00:02:16,400
在ImageNet获得更大的数据

68
00:02:16,400 --> 00:02:19,000
是具体上训练好的一个模型

69
00:02:19,759 --> 00:02:23,519
所以在计算机视觉是很流行这个事情

70
00:02:23,719 --> 00:02:27,439
而且是AlexNet也开创了整个

71
00:02:27,480 --> 00:02:28,400
ImageNet

72
00:02:28,600 --> 00:02:33,240
在用深度学习做迁移学习的一个开始

73
00:02:33,240 --> 00:02:36,200
所以也是引领了整个深度学习的一个

74
00:02:36,240 --> 00:02:37,520
热潮的开始

75
00:02:38,080 --> 00:02:39,320
但NLP的话

76
00:02:39,520 --> 00:02:42,480
其实在我觉得在2016年

77
00:02:42,840 --> 00:02:45,120
2016年ResNet已经出来了

78
00:02:45,240 --> 00:02:46,360
在2016年的时候

79
00:02:46,360 --> 00:02:47,439
大家都觉得

80
00:02:47,480 --> 00:02:51,240
好像深度学习在NLP没那么有用

81
00:02:51,240 --> 00:02:52,640
感觉比别的来讲

82
00:02:52,800 --> 00:02:54,280
WordVec早就有了

83
00:02:54,680 --> 00:02:57,360
然后好像那些模型

84
00:02:57,680 --> 00:02:58,840
就语言模型也有

85
00:02:58,840 --> 00:03:01,600
但是没有觉得深度学习有那么的有用

86
00:03:02,000 --> 00:03:04,400
直到比如说BERT之类的

87
00:03:04,400 --> 00:03:05,480
这一系列的模型

88
00:03:05,560 --> 00:03:07,280
在17年18年出来

89
00:03:08,080 --> 00:03:10,240
所以它就是能解决

90
00:03:10,240 --> 00:03:12,080
很好的解决NLP里面的

91
00:03:12,120 --> 00:03:13,200
迁移学习的模型

92
00:03:13,480 --> 00:03:15,520
我们先看在它之前怎么办呢

93
00:03:15,800 --> 00:03:16,560
之前的话

94
00:03:16,760 --> 00:03:17,880
就是说一般来说

95
00:03:17,880 --> 00:03:19,600
你会用一个预序量的模型

96
00:03:19,600 --> 00:03:22,040
来抽取词或者句子的特征

97
00:03:22,400 --> 00:03:23,599
比如说你用work2vec

98
00:03:23,719 --> 00:03:24,759
我们没有讲这个模型

99
00:03:25,039 --> 00:03:27,039
其实也比较简单的模型

100
00:03:27,039 --> 00:03:27,799
就是一个

101
00:03:27,799 --> 00:03:28,879
你可能认为是一个

102
00:03:30,719 --> 00:03:32,840
一个多多少少是有的

103
00:03:32,840 --> 00:03:33,639
像一个语言模型

104
00:03:33,879 --> 00:03:35,079
就是用一个词

105
00:03:35,120 --> 00:03:38,000
去用边上的词来预测中心词

106
00:03:38,280 --> 00:03:40,919
或者用一个词来预测边上几个词

107
00:03:42,079 --> 00:03:44,159
然后因为它训练起来特别快

108
00:03:44,399 --> 00:03:46,799
所以它其实就是一个

109
00:03:47,039 --> 00:03:48,519
你可能有点像一个线性网络

110
00:03:48,959 --> 00:03:51,239
然后它训练起来比较快

111
00:03:51,240 --> 00:03:53,000
所以你能够训练一个比较大的模型

112
00:03:53,400 --> 00:03:54,719
用它来抽特征

113
00:03:55,240 --> 00:03:57,159
或者你也可以用语言模型来抽特征

114
00:03:57,320 --> 00:03:57,640
就是说

115
00:03:57,640 --> 00:03:59,480
你用在一个很大的数据机上

116
00:03:59,520 --> 00:04:01,040
训练一个比较大的语言模型

117
00:04:01,040 --> 00:04:02,159
来抽取特征

118
00:04:02,640 --> 00:04:03,439
一般来说

119
00:04:03,640 --> 00:04:04,920
你抽完特征之后

120
00:04:05,360 --> 00:04:08,480
你去做迁移学习的时候

121
00:04:08,760 --> 00:04:11,560
一般不会去更新预训练好的模型

122
00:04:11,719 --> 00:04:13,760
比如说你要work2vec的特征的话

123
00:04:13,760 --> 00:04:17,400
你不会再去fine tune work2vec

124
00:04:18,720 --> 00:04:19,400
另外一个

125
00:04:19,400 --> 00:04:20,960
它有一点点问题是说

126
00:04:22,079 --> 00:04:25,800
如果你用它来做抽取特征的话

127
00:04:26,000 --> 00:04:28,759
它给你的是一些比较底层的特征

128
00:04:29,439 --> 00:04:31,079
就是说很多时候你用来

129
00:04:31,639 --> 00:04:33,280
把它当一个embedding层用

130
00:04:33,280 --> 00:04:34,560
就是一个嵌入层

131
00:04:35,040 --> 00:04:36,400
大家知道做NLP的时候

132
00:04:36,400 --> 00:04:37,480
你一个词过来

133
00:04:37,480 --> 00:04:39,480
第一个先进入嵌入层

134
00:04:39,680 --> 00:04:40,720
embedding layer

135
00:04:40,720 --> 00:04:41,800
拿到一个项量

136
00:04:41,920 --> 00:04:43,879
所以 work2vec或者语言模型

137
00:04:43,879 --> 00:04:45,199
很多时候就是替换掉

138
00:04:45,199 --> 00:04:46,439
你那个embedding层

139
00:04:46,600 --> 00:04:47,879
你后面该怎么办

140
00:04:47,879 --> 00:04:48,680
还是得怎么办

141
00:04:48,680 --> 00:04:49,879
就是说你用RN也好

142
00:04:49,879 --> 00:04:51,000
用什么模型也好

143
00:04:51,000 --> 00:04:53,480
你还是得设计一个比较复杂的模型

144
00:04:54,959 --> 00:04:55,600
所以的话

145
00:04:55,600 --> 00:04:57,720
就是说你换一个新的任务的话

146
00:04:57,720 --> 00:04:59,959
你还是得重新设计一下

147
00:04:59,959 --> 00:05:02,199
跟你任务比较相关的网络

148
00:05:02,199 --> 00:05:05,079
来抽取你要的特征

149
00:05:05,680 --> 00:05:07,000
另外一块就是说

150
00:05:07,519 --> 00:05:08,360
之所以这么做

151
00:05:08,360 --> 00:05:10,160
是因为确实work2vec

152
00:05:10,160 --> 00:05:11,639
只看了一些局部的方向

153
00:05:11,839 --> 00:05:13,759
没有看特别多的时序信息

154
00:05:14,120 --> 00:05:14,759
语言模型

155
00:05:15,680 --> 00:05:17,240
也就是看了一个方向

156
00:05:17,240 --> 00:05:19,600
而且也训练的模型不那么大

157
00:05:19,640 --> 00:05:22,080
因为我们知道RN不好处理

158
00:05:22,080 --> 00:05:23,840
特别长的序列

159
00:05:23,840 --> 00:05:27,120
所以他也就看了那么短短的一段东西

160
00:05:27,480 --> 00:05:30,120
使得你如果想做一个新的任务的话

161
00:05:30,439 --> 00:05:32,280
你可能还得真的设计

162
00:05:32,280 --> 00:05:34,040
自己的网络来做这个事情

163
00:05:35,280 --> 00:05:37,800
所以这个是说在BERT之前

164
00:05:37,800 --> 00:05:39,560
整个NLP是怎么样子

165
00:05:40,480 --> 00:05:42,240
但是BERT的动机是什么

166
00:05:42,600 --> 00:05:43,680
BERT的动机就是说

167
00:05:43,680 --> 00:05:47,840
你看在图片里面已经做得很好了

168
00:05:48,160 --> 00:05:50,640
那么我现在NLP也需要有个类似的东西

169
00:05:51,000 --> 00:05:52,280
就是说他想说

170
00:05:52,280 --> 00:05:55,480
我想基于微调的来做NLP的模型

171
00:05:56,080 --> 00:05:58,160
我们知道在图片的时候

172
00:05:58,160 --> 00:06:01,360
你看我们图片有一只猫

173
00:06:02,240 --> 00:06:02,760
我拿个笔

174
00:06:04,080 --> 00:06:05,440
图片你这只猫进来

175
00:06:05,840 --> 00:06:07,160
你放到一些层里面

176
00:06:07,480 --> 00:06:09,080
然后最后拿到一个输出层

177
00:06:09,400 --> 00:06:10,160
做一个分类器

178
00:06:10,760 --> 00:06:13,960
如果你是做微调的话

179
00:06:13,960 --> 00:06:15,320
那么你下面这个层

180
00:06:15,320 --> 00:06:17,000
你的全都是可以复用的

181
00:06:17,240 --> 00:06:18,680
你只要改你最后一个

182
00:06:18,680 --> 00:06:20,439
output layer就行了

183
00:06:21,120 --> 00:06:21,360
对吧

184
00:06:21,360 --> 00:06:22,360
就是说你换一个数据机

185
00:06:22,360 --> 00:06:23,279
我就改最后一层

186
00:06:23,680 --> 00:06:25,000
这些层我都是可以复用的

187
00:06:25,399 --> 00:06:27,199
而且这个东西整个这一块

188
00:06:27,199 --> 00:06:29,439
是能够挪到别的任务去

189
00:06:29,920 --> 00:06:30,920
你做一个别的

190
00:06:30,920 --> 00:06:32,279
可能不那么一样的任务

191
00:06:32,639 --> 00:06:34,199
你可能就整个那一块拿过去

192
00:06:34,199 --> 00:06:36,480
后面就接一个比较小一点的网络就行了

193
00:06:37,040 --> 00:06:38,560
不需要说特别的

194
00:06:38,839 --> 00:06:41,319
另外的网络去再去抽取图片里面的

195
00:06:41,319 --> 00:06:44,439
那些比较low level的一些特征

196
00:06:45,439 --> 00:06:47,360
NOP也想做类似的事情

197
00:06:47,519 --> 00:06:48,759
就是说我给你一个句子

198
00:06:49,279 --> 00:06:51,000
我想说我有个比较深的网络

199
00:06:51,000 --> 00:06:51,839
我进来

200
00:06:52,079 --> 00:06:54,120
然后最后来一个输出层

201
00:06:54,319 --> 00:06:57,360
到说这是一个正的评价

202
00:06:57,360 --> 00:06:58,560
还是一个负类评价

203
00:06:59,519 --> 00:07:00,920
那么下一个是说

204
00:07:00,920 --> 00:07:02,639
假设我换了一个任务的话

205
00:07:02,720 --> 00:07:03,639
那么一样的

206
00:07:03,639 --> 00:07:05,240
我希望说我用这些层

207
00:07:05,639 --> 00:07:07,199
还是搬过来

208
00:07:07,439 --> 00:07:09,319
只需要改最后那一层

209
00:07:10,159 --> 00:07:13,560
而且这些权重我都可以复用

210
00:07:14,000 --> 00:07:15,120
所以就是说

211
00:07:15,480 --> 00:07:17,199
你想BERT就想说

212
00:07:17,199 --> 00:07:20,480
我想把整个NOP的训练

213
00:07:20,480 --> 00:07:24,120
也做的跟计算机视觉训练是一样的

214
00:07:24,519 --> 00:07:26,000
我的预训练的模型

215
00:07:26,160 --> 00:07:27,360
就pre-trained的模型

216
00:07:27,399 --> 00:07:30,279
抽取了足够多的语言的信息

217
00:07:30,480 --> 00:07:32,279
使得我抽出来的feature

218
00:07:32,519 --> 00:07:33,680
已经足够好

219
00:07:33,680 --> 00:07:36,840
能够抓住里面很多的语义信息

220
00:07:37,759 --> 00:07:39,240
那么你做一个新的任务的时候

221
00:07:39,600 --> 00:07:42,560
你只需要增加一个简单的输出层就行了

222
00:07:42,680 --> 00:07:44,600
你不需要再去给我再加一个RN

223
00:07:44,800 --> 00:07:46,519
加一个什么transformer layer

224
00:07:46,519 --> 00:07:47,560
再加什么

225
00:07:47,560 --> 00:07:48,720
别的都不需要了

226
00:07:48,759 --> 00:07:49,600
只要一个

227
00:07:49,600 --> 00:07:51,879
只要把我的特征转换成

228
00:07:51,879 --> 00:07:53,199
你的要的语义

229
00:07:53,759 --> 00:07:55,199
Label的空间就行了

230
00:07:55,480 --> 00:07:57,680
这就是BERT的它的一个动机

231
00:07:58,920 --> 00:08:00,279
所以它的架构

232
00:08:01,399 --> 00:08:03,040
BERT虽然这个名字取的

233
00:08:03,079 --> 00:08:05,040
叫Bi-directional什么

234
00:08:05,279 --> 00:08:07,439
它其实就是个transformer

235
00:08:07,439 --> 00:08:09,959
把解码器给砍了

236
00:08:10,040 --> 00:08:10,760
就是说

237
00:08:10,800 --> 00:08:14,840
BERT就是一个只有编码器的transformer

238
00:08:16,040 --> 00:08:16,360
OK

239
00:08:16,360 --> 00:08:19,080
所以BERT那篇paper也挺好意思

240
00:08:19,080 --> 00:08:20,760
就是说作者就有这个idea

241
00:08:20,760 --> 00:08:21,480
然后试了一下

242
00:08:21,480 --> 00:08:22,680
也就几天就写完了

243
00:08:22,680 --> 00:08:24,920
然后可能就整个训练模型

244
00:08:25,040 --> 00:08:27,560
到写论文

245
00:08:27,560 --> 00:08:28,640
可能就很短时间

246
00:08:28,640 --> 00:08:31,720
就是因为它确实它从

247
00:08:32,560 --> 00:08:33,400
结构上来讲

248
00:08:33,400 --> 00:08:33,960
没有

249
00:08:34,600 --> 00:08:36,080
就是一个把

250
00:08:36,600 --> 00:08:39,200
只保留编码器的一个transformer就行了

251
00:08:39,720 --> 00:08:42,040
所以更多是说它做个实验

252
00:08:43,080 --> 00:08:44,960
然后证明了这个效果很好

253
00:08:44,960 --> 00:08:46,080
这是它的

254
00:08:46,520 --> 00:08:47,440
idea很简单

255
00:08:47,440 --> 00:08:49,040
但是它的效果非常好

256
00:08:50,800 --> 00:08:52,480
这个也是说音乐效果好

257
00:08:52,480 --> 00:08:53,280
所以大家去看

258
00:08:53,560 --> 00:08:55,080
原来有了它之后

259
00:08:55,080 --> 00:08:56,320
我真的就可以

260
00:08:57,080 --> 00:08:58,840
在后续的fine tuning的任务上

261
00:08:58,840 --> 00:09:00,360
就可以做的比较简单

262
00:09:01,040 --> 00:09:03,600
所以BERT它在原始的论文里面

263
00:09:03,640 --> 00:09:05,480
它提供了两个模型的版本

264
00:09:06,400 --> 00:09:07,840
一个版本叫Base

265
00:09:07,960 --> 00:09:09,160
一个版本叫Lodge

266
00:09:09,840 --> 00:09:11,160
Base的话是什么

267
00:09:11,360 --> 00:09:15,320
Base的话就是用了12个transformer block

268
00:09:16,160 --> 00:09:18,280
记得我们上节课讲的transformer block

269
00:09:18,400 --> 00:09:20,320
就是一个里面有一个

270
00:09:20,360 --> 00:09:21,920
它的encoder的block

271
00:09:22,160 --> 00:09:23,879
就是编码器的blocker

272
00:09:23,879 --> 00:09:25,560
就是进来是一个

273
00:09:26,240 --> 00:09:29,080
自self的tension

274
00:09:29,320 --> 00:09:31,280
然后加了一些layer-long

275
00:09:31,360 --> 00:09:31,920
resizure

276
00:09:31,960 --> 00:09:33,560
然后到一个ffn

277
00:09:33,600 --> 00:09:34,720
就是一个全连接了

278
00:09:34,759 --> 00:09:35,759
然后再出去

279
00:09:36,000 --> 00:09:36,960
这是一个block

280
00:09:37,280 --> 00:09:40,160
然后BERT的base是用了12个block

281
00:09:40,200 --> 00:09:41,000
已经很多了

282
00:09:42,480 --> 00:09:44,759
它的hidden size是768

283
00:09:45,120 --> 00:09:47,720
就是说所有东西都是768过去

284
00:09:47,960 --> 00:09:50,560
然后multi head用的是那个head

285
00:09:50,560 --> 00:09:52,240
就是那个头用的12

286
00:09:53,120 --> 00:09:54,759
所以它的模型量大概是

287
00:09:55,519 --> 00:09:56,280
这个是一个亿

288
00:09:57,360 --> 00:09:59,639
所以已经很大了

289
00:09:59,840 --> 00:10:02,920
就是说在当时候transformer那一块

290
00:10:02,920 --> 00:10:04,120
根本就没有做那么大

291
00:10:04,120 --> 00:10:06,240
BERT确实是刚出来的时候

292
00:10:06,240 --> 00:10:08,519
已经觉得这个模型真的很大

293
00:10:08,519 --> 00:10:08,799
很大

294
00:10:08,799 --> 00:10:09,759
就NLP里面

295
00:10:09,759 --> 00:10:11,200
终于大家可以做到

296
00:10:11,519 --> 00:10:14,279
你不看最下的embedding层

297
00:10:14,600 --> 00:10:15,639
就中间那些层

298
00:10:15,680 --> 00:10:17,360
真的可以做到很大

299
00:10:17,360 --> 00:10:18,200
而且很深了

300
00:10:19,399 --> 00:10:20,759
然后BERT large的话

301
00:10:21,399 --> 00:10:24,120
它用的是24个block

302
00:10:24,120 --> 00:10:26,759
就是24个transformer块

303
00:10:27,360 --> 00:10:28,639
然后你的hidden size

304
00:10:28,639 --> 00:10:29,919
涨到了1024

305
00:10:29,919 --> 00:10:31,720
你的头涨到了16

306
00:10:31,879 --> 00:10:33,080
这个head是16

307
00:10:33,480 --> 00:10:36,800
然后你的模型规模到了三个亿

308
00:10:38,800 --> 00:10:40,720
这个是当然是说

309
00:10:40,720 --> 00:10:42,520
你可以认为base效果不错

310
00:10:42,520 --> 00:10:44,639
large当然效果会更好

311
00:10:45,040 --> 00:10:46,720
计算量大概是你的

312
00:10:46,759 --> 00:10:48,480
基本上是三四倍的样子

313
00:10:49,800 --> 00:10:51,160
然后另外一个是说

314
00:10:51,520 --> 00:10:54,200
BERT用了比较大的训练机

315
00:10:54,200 --> 00:10:56,080
它用的是整个wikipedia

316
00:10:56,320 --> 00:10:57,920
然后还用了一个book

317
00:10:57,960 --> 00:10:58,759
就是一些书

318
00:10:58,759 --> 00:10:59,879
加上wikipedia

319
00:11:00,080 --> 00:11:01,280
加整个训练机上

320
00:11:01,279 --> 00:11:02,240
它的训练数据

321
00:11:02,240 --> 00:11:04,799
大概有三十一个次

322
00:11:05,559 --> 00:11:08,279
所以这个是你可认为

323
00:11:08,279 --> 00:11:09,919
这个真的是第一个

324
00:11:09,919 --> 00:11:13,000
在NLP上做的很深的网络

325
00:11:13,639 --> 00:11:15,959
而且用了很大的数据训练的网络

326
00:11:15,959 --> 00:11:17,199
而且被证明说

327
00:11:17,399 --> 00:11:18,600
这个网络做出来的模型

328
00:11:18,600 --> 00:11:21,319
确实是非常适合做签语学习

329
00:11:21,480 --> 00:11:24,759
它真的是能够抽取比较好的特征

330
00:11:24,759 --> 00:11:27,360
使得之后的任务变得非常简单

331
00:11:27,600 --> 00:11:29,639
这个是它的主要的贡献

332
00:11:31,759 --> 00:11:32,639
然后当然是说

333
00:11:32,639 --> 00:11:34,079
它也不是说

334
00:11:34,079 --> 00:11:35,279
它是模型上来讲

335
00:11:35,279 --> 00:11:36,360
就是直接改了一下

336
00:11:36,360 --> 00:11:37,000
transformer

337
00:11:37,000 --> 00:11:38,399
但它还是有别的创新的

338
00:11:38,559 --> 00:11:42,159
它有在输入和loss上是有创新的

339
00:11:42,279 --> 00:11:43,959
我们来看一下它的创新

340
00:11:45,120 --> 00:11:46,439
第一个就是说

341
00:11:46,600 --> 00:11:48,720
之前我们看transformer的时候

342
00:11:48,720 --> 00:11:51,159
我们的输入也是一个句子对

343
00:11:51,720 --> 00:11:53,639
它是一个source的sentence

344
00:11:53,639 --> 00:11:55,159
到一个target的sentence

345
00:11:55,679 --> 00:11:57,000
你原句子就是说

346
00:11:57,000 --> 00:11:58,000
做翻译的时候

347
00:11:58,079 --> 00:11:59,799
原句子进入了你encoder

348
00:11:59,840 --> 00:12:01,160
然后你的target的句子

349
00:12:01,160 --> 00:12:02,520
进的是你的decoder

350
00:12:03,040 --> 00:12:05,000
现在你只有一个encoder了

351
00:12:05,000 --> 00:12:06,600
你只有一个编码器怎么办

352
00:12:07,840 --> 00:12:09,000
如果因为NLP里面

353
00:12:09,000 --> 00:12:10,240
很多是两个句子的

354
00:12:10,760 --> 00:12:12,800
就是说比如说QA都是两个句子

355
00:12:12,800 --> 00:12:14,360
就是一个句子进去

356
00:12:14,360 --> 00:12:15,320
一个句子出来

357
00:12:15,560 --> 00:12:16,440
你怎么办呢

358
00:12:17,240 --> 00:12:17,800
就是说

359
00:12:17,800 --> 00:12:19,880
或者说我就把两个句子

360
00:12:19,880 --> 00:12:20,920
把它拼起来

361
00:12:20,920 --> 00:12:22,480
放到我的decoder里面

362
00:12:23,320 --> 00:12:24,560
所以这样的话

363
00:12:24,560 --> 00:12:28,320
它的样本它是一个句子对

364
00:12:29,320 --> 00:12:30,480
可以看见这个样子

365
00:12:30,640 --> 00:12:31,280
就是说

366
00:12:31,280 --> 00:12:32,440
比如说

367
00:12:33,080 --> 00:12:34,560
这个电影很好

368
00:12:34,560 --> 00:12:36,520
我喜欢它

369
00:12:36,640 --> 00:12:38,080
这是两个句子

370
00:12:38,920 --> 00:12:40,720
两个句子怎么放进去呢

371
00:12:40,720 --> 00:12:42,560
它首先加了一个叫

372
00:12:42,560 --> 00:12:45,160
special symbol叫classification

373
00:12:45,320 --> 00:12:46,320
就是叫

374
00:12:47,280 --> 00:12:49,000
就是叫分类

375
00:12:49,040 --> 00:12:50,000
这class

376
00:12:50,480 --> 00:12:53,200
然后作为开头

377
00:12:53,240 --> 00:12:56,240
作为句子对的开头

378
00:12:56,280 --> 00:12:57,520
然后句子之间

379
00:12:57,679 --> 00:12:59,319
是用了一个叫separate

380
00:12:59,319 --> 00:13:01,079
就是分格符的一个东西

381
00:13:01,120 --> 00:13:02,360
把两个句子分开

382
00:13:02,600 --> 00:13:03,639
就是说这一个分格符

383
00:13:04,000 --> 00:13:04,840
那一个分格符

384
00:13:05,319 --> 00:13:06,360
当然你可以用

385
00:13:06,360 --> 00:13:07,319
你可以做更长

386
00:13:07,480 --> 00:13:09,079
你给做三个四个都OK

387
00:13:09,079 --> 00:13:10,639
其实你都是可以做的

388
00:13:10,799 --> 00:13:12,639
但是一般来说

389
00:13:12,639 --> 00:13:13,439
你没有太多

390
00:13:13,480 --> 00:13:16,360
要你需要三个四个句子的应用

391
00:13:16,360 --> 00:13:18,879
所以你用两个也挺好的

392
00:13:20,360 --> 00:13:21,679
第二个是说你

393
00:13:22,319 --> 00:13:23,960
它另外一个是说

394
00:13:24,279 --> 00:13:25,639
因为你有两个句子

395
00:13:26,039 --> 00:13:27,360
而且我们知道

396
00:13:27,360 --> 00:13:29,199
如果你就这个东西的话

397
00:13:29,199 --> 00:13:31,000
它确实对transformer来讲

398
00:13:31,120 --> 00:13:32,799
它不是那么好的区分

399
00:13:32,799 --> 00:13:34,319
你到底哪个句子是谁的

400
00:13:34,480 --> 00:13:35,799
所以它额外的加了一个

401
00:13:35,799 --> 00:13:36,720
告诉你说

402
00:13:36,840 --> 00:13:38,120
segment embedding

403
00:13:38,279 --> 00:13:39,480
就对于它来讲

404
00:13:39,679 --> 00:13:40,799
这是第一个句子

405
00:13:40,960 --> 00:13:42,199
它给你是0

406
00:13:42,799 --> 00:13:44,360
就它给你一个ID是0

407
00:13:44,759 --> 00:13:45,919
那么对第二个句子

408
00:13:45,919 --> 00:13:46,919
它给你的ID是1

409
00:13:46,919 --> 00:13:48,080
就segment是1

410
00:13:48,319 --> 00:13:49,199
这样子的话

411
00:13:49,199 --> 00:13:50,080
就是说这一块

412
00:13:50,480 --> 00:13:52,840
对于所有前面一个句子的话

413
00:13:52,840 --> 00:13:54,080
我给的是一个

414
00:13:54,720 --> 00:13:55,280
向量

415
00:13:55,280 --> 00:13:56,200
然后后面是给的

416
00:13:56,200 --> 00:13:56,960
另外一个向量

417
00:13:57,000 --> 00:13:58,280
这样子来区分它

418
00:14:00,759 --> 00:14:01,920
第三个是说

419
00:14:01,920 --> 00:14:04,080
我们在上节课也讲过

420
00:14:04,080 --> 00:14:05,680
就是它的position embedding

421
00:14:05,680 --> 00:14:07,920
就是位置编码是可以学的

422
00:14:07,960 --> 00:14:09,240
它不再用那个sin

423
00:14:09,240 --> 00:14:10,200
cosine函数了

424
00:14:10,240 --> 00:14:11,639
他说反正我就让你学

425
00:14:11,840 --> 00:14:13,879
反正你学怎么样就怎么样了

426
00:14:13,879 --> 00:14:14,840
我也不去

427
00:14:15,400 --> 00:14:16,759
手工给你设计了

428
00:14:17,000 --> 00:14:17,200
好

429
00:14:17,200 --> 00:14:19,240
这就是对于输入的修改

430
00:14:21,360 --> 00:14:23,280
第二个是说它的任务

431
00:14:23,600 --> 00:14:24,720
就是说你

432
00:14:24,759 --> 00:14:26,199
因为刚刚我们讲的是

433
00:14:26,199 --> 00:14:28,000
比如说讲transformer的时候

434
00:14:28,000 --> 00:14:30,720
还是说我做翻译的时候

435
00:14:30,720 --> 00:14:32,600
我那么就做一个翻译的任务

436
00:14:33,000 --> 00:14:34,480
现在Bert说我不行

437
00:14:34,600 --> 00:14:36,919
我得做一个通用的任务

438
00:14:37,319 --> 00:14:38,639
因为它是预训练模型

439
00:14:38,759 --> 00:14:41,039
做一个很常见的通用的任务

440
00:14:42,159 --> 00:14:43,480
然后使得说

441
00:14:43,480 --> 00:14:45,919
我用这个任务训练出来的数据够好

442
00:14:45,919 --> 00:14:47,799
然后做别的任务的时候都能做

443
00:14:48,600 --> 00:14:50,679
在文本里面

444
00:14:50,919 --> 00:14:52,679
就是说最通用的任务是谁

445
00:14:52,880 --> 00:14:54,080
就是语言模型了

446
00:14:54,920 --> 00:14:56,360
你每次预测一个句子

447
00:14:56,480 --> 00:14:57,760
给你看前面一些词

448
00:14:57,760 --> 00:14:58,960
去预测下一个词

449
00:14:59,160 --> 00:15:01,040
这是整个NLP里面

450
00:15:01,720 --> 00:15:03,280
最常见的一个模型

451
00:15:03,520 --> 00:15:05,400
当然如果你要做一训练的话

452
00:15:05,600 --> 00:15:07,280
你不管是wordvec还是语言模型

453
00:15:07,280 --> 00:15:09,600
都多多少少都是预测下一个词

454
00:15:09,600 --> 00:15:10,640
因为它是一个顺序的

455
00:15:11,920 --> 00:15:13,920
但是Bert不能直接这么做

456
00:15:14,480 --> 00:15:14,880
为什么

457
00:15:15,680 --> 00:15:17,000
因为你的encoder

458
00:15:17,000 --> 00:15:18,000
它是个encoder

459
00:15:18,160 --> 00:15:18,680
encoder

460
00:15:18,680 --> 00:15:20,640
它是能看到后面的东西的

461
00:15:21,200 --> 00:15:21,880
就记得吗

462
00:15:21,919 --> 00:15:22,919
就在former里面

463
00:15:23,519 --> 00:15:26,200
encoder是每一个中间那个东西

464
00:15:26,200 --> 00:15:27,960
输出是又看前又看后

465
00:15:27,960 --> 00:15:29,279
所以它是双向的

466
00:15:29,919 --> 00:15:30,840
它的decoder

467
00:15:30,879 --> 00:15:33,039
它的解码器才可以是做到单向的

468
00:15:33,200 --> 00:15:35,639
所以为什么叫Bert是bi-directional的

469
00:15:36,080 --> 00:15:38,000
它的那个B就是双向的意思

470
00:15:38,639 --> 00:15:40,000
所以就是说

471
00:15:40,000 --> 00:15:41,720
他就是要说

472
00:15:41,720 --> 00:15:44,240
我就是要去看双向信息

473
00:15:44,240 --> 00:15:46,000
能抽取比较好的特征

474
00:15:46,440 --> 00:15:48,759
所以如果我还要去训练语言模型的话

475
00:15:48,759 --> 00:15:49,679
当然会有问题

476
00:15:50,200 --> 00:15:51,080
所以他怎么办

477
00:15:51,400 --> 00:15:54,120
他就做了一个修改

478
00:15:54,240 --> 00:15:56,160
叫做带研码的

479
00:15:56,160 --> 00:15:57,800
叫Mask的语言模型

480
00:15:58,680 --> 00:15:59,200
什么意思

481
00:15:59,200 --> 00:15:59,639
很简单

482
00:15:59,639 --> 00:16:01,920
就是给一个句子

483
00:16:02,400 --> 00:16:05,960
我随机把中间一些词遮起来

484
00:16:06,240 --> 00:16:07,800
让你去预测这些词

485
00:16:08,280 --> 00:16:10,840
不是说我看到前些词预测后面

486
00:16:10,840 --> 00:16:12,680
就是说现在是变成玩心填空了

487
00:16:13,000 --> 00:16:13,879
我给你一个句子

488
00:16:13,879 --> 00:16:14,800
中间挖掉一些词

489
00:16:14,800 --> 00:16:17,600
让你看下面一些词长什么样子

490
00:16:18,720 --> 00:16:20,080
所以他就是说

491
00:16:20,080 --> 00:16:21,120
我对每个句子

492
00:16:21,480 --> 00:16:24,000
我用比如说15%的概率

493
00:16:24,120 --> 00:16:26,200
将一些token一些词源

494
00:16:26,560 --> 00:16:28,680
换成叫做Mask的一个

495
00:16:28,680 --> 00:16:29,759
Special的symbol

496
00:16:30,280 --> 00:16:32,400
然后你的任务就是去说

497
00:16:33,000 --> 00:16:35,040
去对这些Mask掉的那些东西

498
00:16:35,040 --> 00:16:36,160
去看他去预测

499
00:16:36,160 --> 00:16:37,480
他是真的是谁

500
00:16:38,000 --> 00:16:40,120
这就是带研码的语言模型

501
00:16:40,440 --> 00:16:41,000
这样的话

502
00:16:41,000 --> 00:16:42,800
就是说我不是个语言模型

503
00:16:42,800 --> 00:16:43,920
我不是用来预测未来

504
00:16:43,920 --> 00:16:45,320
我就是给你做玩心填空

505
00:16:45,320 --> 00:16:48,080
所以我看双向信息是没问题的

506
00:16:49,080 --> 00:16:52,720
但因为在微调任务中

507
00:16:52,879 --> 00:16:54,639
你没有Mask这个东西

508
00:16:54,840 --> 00:16:55,879
就你的data里面

509
00:16:55,879 --> 00:16:57,120
没有Mask这个东西

510
00:16:57,120 --> 00:16:58,080
所以就是说

511
00:16:58,080 --> 00:16:59,600
你现在BERT训练时候

512
00:16:59,600 --> 00:17:00,680
你说我在数据里面

513
00:17:00,680 --> 00:17:01,960
加了很多Mask进去

514
00:17:01,960 --> 00:17:03,720
但是你在Find2的时候

515
00:17:03,720 --> 00:17:04,559
你没有这个东西

516
00:17:04,559 --> 00:17:05,600
那怎么办呢

517
00:17:06,120 --> 00:17:07,200
他就是说BERT说

518
00:17:07,200 --> 00:17:08,759
我不要让我的模型

519
00:17:08,759 --> 00:17:10,920
总是看见Mask就去输出

520
00:17:11,680 --> 00:17:13,319
就去尝试预测

521
00:17:13,360 --> 00:17:14,920
所以他做了一点点优化

522
00:17:14,920 --> 00:17:15,640
是说

523
00:17:16,240 --> 00:17:17,440
我选中一些词

524
00:17:17,440 --> 00:17:19,080
用来把它遮掉的话

525
00:17:19,160 --> 00:17:19,960
那么这些词

526
00:17:20,160 --> 00:17:22,240
在80%的概率下

527
00:17:22,240 --> 00:17:23,680
我将那个词变成一个

528
00:17:23,680 --> 00:17:24,560
叫做Mask的

529
00:17:24,560 --> 00:17:26,520
Special的一个token

530
00:17:28,040 --> 00:17:29,600
然后有10%的概率

531
00:17:29,840 --> 00:17:31,240
我换成一个随机的

532
00:17:31,240 --> 00:17:32,200
别的一个token

533
00:17:32,280 --> 00:17:33,960
就随机sample一个token过来

534
00:17:35,000 --> 00:17:36,400
还有10%的概率

535
00:17:36,880 --> 00:17:37,800
我就什么都不干

536
00:17:37,800 --> 00:17:39,200
我就保持原来的token

537
00:17:39,200 --> 00:17:40,840
让你就是说做点b

538
00:17:40,840 --> 00:17:42,800
让你看到自己的信息

539
00:17:43,160 --> 00:17:43,759
OK

540
00:17:43,759 --> 00:17:45,640
所以这个就是他的

541
00:17:47,480 --> 00:17:48,359
他对

542
00:17:49,599 --> 00:17:50,720
因为要做Find20

543
00:17:50,720 --> 00:17:52,079
所以他对Mask这个东西

544
00:17:52,079 --> 00:17:53,559
做了一点点的改动

545
00:17:53,559 --> 00:17:54,839
使得你这个模型

546
00:17:54,839 --> 00:17:58,079
不要太看见Mask就去预测

547
00:17:59,079 --> 00:17:59,319
好

548
00:17:59,319 --> 00:18:00,079
这是第一个

549
00:18:00,079 --> 00:18:01,039
他的预训任务

550
00:18:01,879 --> 00:18:04,039
BERT其实有两个

551
00:18:04,079 --> 00:18:05,599
他做两个任务

552
00:18:05,599 --> 00:18:06,279
同时

553
00:18:06,480 --> 00:18:07,799
他第二个任务叫做什么

554
00:18:07,799 --> 00:18:10,319
叫做下一句子预测

555
00:18:11,079 --> 00:18:11,879
就他就说

556
00:18:11,880 --> 00:18:13,560
我每次给你一对句子

557
00:18:14,280 --> 00:18:15,040
我就说

558
00:18:15,040 --> 00:18:16,520
我就给你预测说

559
00:18:17,160 --> 00:18:20,120
这个句子是不是在原始的句子中

560
00:18:20,120 --> 00:18:21,240
是相邻的

561
00:18:22,080 --> 00:18:23,800
所以他在构造样本的时候

562
00:18:23,800 --> 00:18:24,520
他是说

563
00:18:24,560 --> 00:18:26,160
我有50%的概率

564
00:18:26,600 --> 00:18:28,000
采样一个句子之后

565
00:18:28,000 --> 00:18:30,080
我把后一个句子也采样进来

566
00:18:30,240 --> 00:18:31,120
做成一个pair

567
00:18:31,120 --> 00:18:32,080
这是50%概率

568
00:18:32,080 --> 00:18:32,680
就是说

569
00:18:34,320 --> 00:18:35,160
这个电影很好

570
00:18:35,320 --> 00:18:37,320
我喜欢这个就是连起来的

571
00:18:38,320 --> 00:18:39,560
然后有50%概率

572
00:18:39,760 --> 00:18:40,800
选中一个句子

573
00:18:40,799 --> 00:18:43,240
然后在别的地方随机再调一个句子

574
00:18:43,240 --> 00:18:44,440
过来放在后面

575
00:18:44,720 --> 00:18:45,879
那么他就是负累了

576
00:18:45,879 --> 00:18:46,639
这个是正的

577
00:18:46,639 --> 00:18:47,399
这是负的

578
00:18:47,519 --> 00:18:49,079
那么就是说电影很好

579
00:18:49,159 --> 00:18:51,680
然后Hello world

580
00:18:52,960 --> 00:18:55,200
然后他然后这样子的话

581
00:18:55,200 --> 00:18:55,960
他就是说

582
00:18:55,960 --> 00:18:59,119
将class就是说最早token

583
00:18:59,559 --> 00:19:00,759
对应的feature

584
00:19:01,159 --> 00:19:01,879
对应的输出

585
00:19:02,000 --> 00:19:03,000
用来做预测

586
00:19:03,079 --> 00:19:04,960
预测说这个句子是不是

587
00:19:05,639 --> 00:19:07,359
这一对句子是不是相邻的

588
00:19:07,639 --> 00:19:08,000
OK

589
00:19:08,000 --> 00:19:11,200
这就是他的两一个预训任务

590
00:19:12,240 --> 00:19:14,480
所以总结一下

591
00:19:15,480 --> 00:19:16,400
BERT是什么

592
00:19:16,519 --> 00:19:20,039
BERT是针对于NLP的微调设计

593
00:19:20,039 --> 00:19:21,200
使得它的微调

594
00:19:21,319 --> 00:19:23,519
就跟我们回到了

595
00:19:23,519 --> 00:19:24,160
熟悉的

596
00:19:24,160 --> 00:19:25,920
计算机视觉领域的微调

597
00:19:26,319 --> 00:19:27,400
就是我训练一个

598
00:19:27,400 --> 00:19:28,759
在一个大的文本上

599
00:19:28,759 --> 00:19:30,119
训练一个比较大的模型

600
00:19:30,160 --> 00:19:31,559
然后在做别的任务的时候

601
00:19:31,680 --> 00:19:32,720
这个模型搬过去

602
00:19:32,759 --> 00:19:33,759
把head

603
00:19:33,920 --> 00:19:35,599
把输出层改一改就行了

604
00:19:35,640 --> 00:19:37,200
然后在做微调的时候

605
00:19:37,200 --> 00:19:39,319
就是说你自己也会改改全种

606
00:19:39,519 --> 00:19:41,360
最后一层也改一改

607
00:19:41,400 --> 00:19:46,680
最后会比你直接落的券会好一点

608
00:19:47,799 --> 00:19:49,759
所以他也是开启了

609
00:19:49,799 --> 00:19:52,880
NLP做fine tuning的一个大的

610
00:19:52,920 --> 00:19:54,120
不是说他开启了

611
00:19:54,240 --> 00:19:56,120
就是他让这一块更加流行了

612
00:19:57,120 --> 00:19:58,519
然后变成主流了

613
00:19:58,880 --> 00:20:00,120
然后它的架构

614
00:20:00,640 --> 00:20:02,120
我们看到它其实就是一个

615
00:20:02,120 --> 00:20:03,840
transformer的编码器

616
00:20:04,519 --> 00:20:05,960
它就是变形金刚的encoder

617
00:20:06,120 --> 00:20:07,559
我们上节课讲过了

618
00:20:08,480 --> 00:20:09,840
但是他做了一点点修改

619
00:20:09,840 --> 00:20:11,120
就是他的模型更大了

620
00:20:11,160 --> 00:20:13,600
它有12层24层

621
00:20:14,000 --> 00:20:16,720
然后它hindersight 768

622
00:20:16,759 --> 00:20:17,680
1.24

623
00:20:18,120 --> 00:20:20,799
然后他用了更大的训练数据

624
00:20:21,120 --> 00:20:23,880
现在BERT这种模型训练

625
00:20:23,880 --> 00:20:26,120
一般是11个词开始

626
00:20:26,400 --> 00:20:27,600
你没有11个词

627
00:20:27,600 --> 00:20:29,079
就不要一个billion的词

628
00:20:29,200 --> 00:20:30,480
没有这个东西就

629
00:20:31,519 --> 00:20:32,319
不用来了

630
00:20:32,360 --> 00:20:33,000
因为为什么

631
00:20:33,000 --> 00:20:34,640
是因为文本这个东西

632
00:20:34,799 --> 00:20:35,559
不像图片

633
00:20:35,559 --> 00:20:36,519
图片我得标

634
00:20:36,519 --> 00:20:37,839
文本不需要标

635
00:20:38,159 --> 00:20:40,319
文本你的就是一个sequential信息

636
00:20:40,519 --> 00:20:41,839
我就给你文本就行了

637
00:20:41,839 --> 00:20:43,039
不需要标任何东西

638
00:20:43,279 --> 00:20:46,079
所以说文本你可以做到无限大

639
00:20:46,480 --> 00:20:49,559
比如说到后来现在的GPT-3

640
00:20:49,759 --> 00:20:52,799
用到了整个网页爬下来的数据

641
00:20:52,799 --> 00:20:54,319
那真的就是大概10倍

642
00:20:54,319 --> 00:20:55,519
及100倍的样子

643
00:20:56,480 --> 00:20:58,359
所以这一块就说有无限数据

644
00:20:58,399 --> 00:20:59,279
只要你有机器

645
00:20:59,279 --> 00:21:00,159
我都可以训练

646
00:21:00,200 --> 00:21:02,440
所以这一块就拉开了整个

647
00:21:03,359 --> 00:21:04,279
军备竞赛

648
00:21:04,559 --> 00:21:05,720
我就看谁的模型大

649
00:21:05,720 --> 00:21:06,759
谁的数据多

650
00:21:07,519 --> 00:21:08,519
这个是他的

651
00:21:08,920 --> 00:21:10,160
所以因为他模型更大

652
00:21:10,160 --> 00:21:11,000
训练数据越多

653
00:21:11,000 --> 00:21:13,039
所以他的特征当然就越好了

654
00:21:13,399 --> 00:21:16,000
然后他的另外一些改动是说

655
00:21:16,039 --> 00:21:17,639
他输了是一个句子对

656
00:21:18,160 --> 00:21:20,319
然后他有做了一个segment embedding

657
00:21:20,359 --> 00:21:22,920
然后他的位置编码是科学的

658
00:21:23,319 --> 00:21:24,279
训练的时候

659
00:21:24,279 --> 00:21:25,559
因为他是一个预训练人物

660
00:21:25,920 --> 00:21:27,399
所以他设计了两个人物

661
00:21:28,039 --> 00:21:30,000
一个是带有代研

662
00:21:30,160 --> 00:21:31,680
就Mask的语言模型

663
00:21:31,960 --> 00:21:33,759
第二个是说预测一对句子

664
00:21:33,759 --> 00:21:35,080
是不是一对

665
00:21:35,600 --> 00:21:39,560
这就是对于Bert的一个大概的介绍

666
00:21:40,240 --> 00:21:40,480
好

667
00:21:40,480 --> 00:21:41,759
我们接下来就是说

668
00:21:41,799 --> 00:21:43,600
给大家看一下代码实现

669
00:21:44,400 --> 00:21:46,640
给大家再更好的理解一下

670
00:21:46,640 --> 00:21:47,480
就是说这里面

671
00:21:47,480 --> 00:21:49,200
到底是发生了什么事情

