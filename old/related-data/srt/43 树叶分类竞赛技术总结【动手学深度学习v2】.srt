1
00:00:00,000 --> 00:00:03,000
首先我们快速总结一下

2
00:00:03,000 --> 00:00:10,800
看一下前面这次数页分类竞赛获奖选手的代码是怎么实现的

3
00:00:12,080 --> 00:00:14,759
首先我们稍微回顾一下我们这个竞赛

4
00:00:14,759 --> 00:00:17,320
我们这个竞赛是取分类数页

5
00:00:17,320 --> 00:00:20,240
一共有176类的数页

6
00:00:20,559 --> 00:00:24,879
然后训练样本一共是18000张

7
00:00:24,879 --> 00:00:26,400
其实样本并不多

8
00:00:26,640 --> 00:00:28,000
可以看到里面的叶子

9
00:00:28,239 --> 00:00:32,719
叶子主要是一个叶子在各个不同的角度下

10
00:00:32,719 --> 00:00:34,520
然后你的背景有可能是白

11
00:00:34,520 --> 00:00:35,359
有的是灰

12
00:00:36,240 --> 00:00:40,159
我们这次竞赛一共有165支队伍参加

13
00:00:40,399 --> 00:00:44,200
有41支队伍的进度大于98%

14
00:00:44,200 --> 00:00:46,000
我觉得这是非常好的进度了

15
00:00:46,640 --> 00:00:54,439
因为我当时候觉得98%应该是数据级的一个比较顶的分数

16
00:00:54,439 --> 00:00:58,119
但最后带了最后最好的成绩已经99%了

17
00:00:58,679 --> 00:01:02,920
有83支队伍一共进度是大于95%的

18
00:01:03,079 --> 00:01:06,319
我觉得95%的进度已经绝对够用了

19
00:01:06,519 --> 00:01:08,079
在实际的生产中

20
00:01:08,280 --> 00:01:10,599
实际你去做图片分类的时候

21
00:01:11,039 --> 00:01:12,200
在工业界的应用里面

22
00:01:12,519 --> 00:01:16,920
如果你这个数页你的算法能在数据级上拿个95%的进度

23
00:01:16,920 --> 00:01:18,879
我觉得是绝对是够用的

24
00:01:19,280 --> 00:01:21,159
你不要追求那么高的进度

25
00:01:21,159 --> 00:01:22,719
之后我们会给大家解释原因

26
00:01:25,159 --> 00:01:27,200
然后我们做一下结果的分析

27
00:01:27,480 --> 00:01:30,920
一共有16支队伍提供了他们的代码

28
00:01:30,920 --> 00:01:32,560
就是可以重复的代码

29
00:01:32,840 --> 00:01:35,400
我建议大家一定要去看一看

30
00:01:35,840 --> 00:01:38,879
大家具体是怎么样一步一步去

31
00:01:40,159 --> 00:01:41,879
获得那么好的成绩的

32
00:01:41,879 --> 00:01:43,560
然后里面有非常多的细节

33
00:01:43,560 --> 00:01:46,719
我们这里大概给大家过一下主要的一些算法

34
00:01:46,719 --> 00:01:49,120
但是细节一定要去大家去看一下代码

35
00:01:49,759 --> 00:01:53,239
而且这些代码很有可能你可以用它去参加下一次的比赛

36
00:01:53,240 --> 00:01:57,560
或者是你自己用它去做你的自己的项目

37
00:01:58,800 --> 00:02:02,520
然后我们有说过在4榜前5名

38
00:02:02,520 --> 00:02:04,840
我们是给你我们的签名书

39
00:02:04,880 --> 00:02:08,240
然后鉴于这次大家成绩那么好

40
00:02:08,240 --> 00:02:10,520
我们说前面20名同学

41
00:02:10,520 --> 00:02:14,600
只要你去提交你可以重复的代码

42
00:02:14,600 --> 00:02:16,879
然后分享给大家去学习的话

43
00:02:16,879 --> 00:02:18,719
我们也会送你签名书

44
00:02:19,000 --> 00:02:19,960
没想到的是

45
00:02:19,960 --> 00:02:23,520
其实基本上只有除了三位同学之外

46
00:02:23,520 --> 00:02:27,080
我们剩下的15名同学都提交了代码

47
00:02:27,080 --> 00:02:29,920
所以我们要送的书比较多

48
00:02:30,040 --> 00:02:30,800
这一次

49
00:02:31,360 --> 00:02:35,879
所以如果你是在4榜排名这个range里面的同学

50
00:02:37,760 --> 00:02:39,159
应该是17名同学

51
00:02:40,040 --> 00:02:42,560
你请你把你的微信号发给我的邮箱

52
00:02:42,560 --> 00:02:43,719
我会加你微信

53
00:02:43,719 --> 00:02:45,240
然后我会安排说

54
00:02:45,560 --> 00:02:47,600
之后我们怎么把书寄给你

55
00:02:47,879 --> 00:02:49,159
当然我们的书还没出版

56
00:02:49,359 --> 00:02:51,359
希望您今年能出版

57
00:02:52,120 --> 00:02:54,319
另外我们还要加上一个额外的同学

58
00:02:54,680 --> 00:02:55,479
这位同学

59
00:02:55,800 --> 00:02:57,719
其实我也不知道怎么发这个音

60
00:02:58,359 --> 00:03:02,599
但是虽然你没有拿到前20

61
00:03:02,639 --> 00:03:05,400
但是我觉得你的代码挺好的

62
00:03:06,120 --> 00:03:07,280
分数也挺好的

63
00:03:07,800 --> 00:03:08,960
超过了95%

64
00:03:09,120 --> 00:03:11,639
然后最主要的是

65
00:03:11,639 --> 00:03:14,039
我发现很多同学是用了你的代码

66
00:03:14,120 --> 00:03:15,599
在上面进行了改进

67
00:03:15,639 --> 00:03:18,120
我觉得非常有意义

68
00:03:18,120 --> 00:03:19,800
因为最后的最后

69
00:03:19,800 --> 00:03:21,200
其实整个深度学习的领域

70
00:03:21,200 --> 00:03:23,800
真的就是靠大家不断的分享自己的代码

71
00:03:23,800 --> 00:03:26,240
然后大家在别人的代码的基础上去改

72
00:03:26,280 --> 00:03:28,599
因为里面有太多的技术细节

73
00:03:29,039 --> 00:03:31,599
你如果在论文里面可能一句话带过了

74
00:03:31,599 --> 00:03:34,039
但你就只要在代码里面才看得比较清楚

75
00:03:34,360 --> 00:03:37,240
这个也是我们整个这门课要强调的是

76
00:03:37,280 --> 00:03:38,840
大家一定要去学习代码

77
00:03:38,879 --> 00:03:39,960
而不仅仅是我告诉你

78
00:03:40,120 --> 00:03:41,000
这是什么东西

79
00:03:41,640 --> 00:03:45,120
所以我们也会额外给这位同学送上一本书

80
00:03:45,319 --> 00:03:47,640
请你也把你的微信发给我的邮箱

81
00:03:47,640 --> 00:03:49,440
我们之后会安排

82
00:03:50,280 --> 00:03:50,560
OK

83
00:03:50,560 --> 00:03:53,480
这就是这一次竞赛的获奖

84
00:03:53,640 --> 00:03:56,280
虽然奖品不大

85
00:03:56,520 --> 00:03:58,480
就是一本书加一个签名

86
00:03:58,680 --> 00:04:00,480
但我觉得主要还是说

87
00:04:00,760 --> 00:04:04,800
给大家一个动力去调一调东西

88
00:04:05,280 --> 00:04:06,640
而且另外一块是

89
00:04:06,640 --> 00:04:08,760
我们真的希望大家能分享自己代码

90
00:04:08,760 --> 00:04:10,840
然后让给别人学习的机会

91
00:04:11,080 --> 00:04:13,120
因为我们这个就是以学习为目的

92
00:04:13,280 --> 00:04:15,080
就大家都去体验一下

93
00:04:15,200 --> 00:04:18,800
怎么样去再给定一个数据集

94
00:04:19,240 --> 00:04:21,040
把它刷到最高

95
00:04:21,240 --> 00:04:22,120
虽然在实际中

96
00:04:22,120 --> 00:04:23,879
你可能一般不会干这种事情

97
00:04:24,199 --> 00:04:26,040
但是你得体验一次

98
00:04:26,040 --> 00:04:28,680
大概我们目的就达到了

99
00:04:29,680 --> 00:04:30,280
OK

100
00:04:31,040 --> 00:04:33,240
另外一块我们做一下技术的分析

101
00:04:34,160 --> 00:04:37,160
分析一下这些提交了代码的同学

102
00:04:37,160 --> 00:04:38,759
大概是怎么样子的

103
00:04:40,160 --> 00:04:41,199
基本上可以认为

104
00:04:41,360 --> 00:04:43,120
我们在课程里面

105
00:04:43,120 --> 00:04:46,959
基本上介绍过了所有的主要的方向

106
00:04:47,840 --> 00:04:51,399
同学们主要是针对于下面这些方向

107
00:04:51,399 --> 00:04:52,120
有所加强

108
00:04:52,240 --> 00:04:53,519
我们说我们介绍过的

109
00:04:53,519 --> 00:04:54,920
我们就不再重复了

110
00:04:55,800 --> 00:04:57,280
跟我们之前介绍的代码

111
00:04:57,280 --> 00:04:58,720
不一样的有那么几点

112
00:04:59,120 --> 00:05:00,920
一点是数据增强

113
00:05:01,920 --> 00:05:04,280
就是说你会发现说整个数据集

114
00:05:04,480 --> 00:05:06,879
跟我们之前看的数据集是不一样的

115
00:05:06,879 --> 00:05:10,040
首先比如说你叶子边上的背景

116
00:05:10,040 --> 00:05:12,920
是比较多的

117
00:05:13,480 --> 00:05:16,120
然后所以你可以用更多增强

118
00:05:16,360 --> 00:05:18,399
另外一个是说大家用的一个

119
00:05:18,399 --> 00:05:19,680
我们也提过一个的技巧

120
00:05:19,840 --> 00:05:21,680
就在测试的时候

121
00:05:21,800 --> 00:05:26,000
多次使用一个可能是较弱一点的增强

122
00:05:26,000 --> 00:05:28,120
然后取平均得到最好的结果

123
00:05:28,319 --> 00:05:31,560
我们在介绍课程是有讲过说

124
00:05:32,120 --> 00:05:34,120
为什么你在做测试的时候

125
00:05:34,120 --> 00:05:38,079
我们是在中心点出一张图片做预测

126
00:05:38,120 --> 00:05:39,639
我说你其实很多时候

127
00:05:39,639 --> 00:05:41,360
大家也会在4个角落

128
00:05:41,360 --> 00:05:44,040
再最后做5个去

129
00:05:44,040 --> 00:05:45,000
然后做平均

130
00:05:45,120 --> 00:05:46,199
然后在这里

131
00:05:46,199 --> 00:05:47,840
大家有做更多的增强

132
00:05:47,840 --> 00:05:49,560
就是说我用一个

133
00:05:49,560 --> 00:05:52,560
我也可能做一些旋转之类的增强

134
00:05:52,560 --> 00:05:54,920
但是可能会比去年稍微弱一点点

135
00:05:54,920 --> 00:05:56,720
就是说你的噪音不要那么大

136
00:05:56,759 --> 00:06:00,040
但是因为你可以选的范围更多

137
00:06:00,040 --> 00:06:03,080
所以你可以在一个稍弱的增强级上

138
00:06:03,120 --> 00:06:04,000
做增强

139
00:06:04,000 --> 00:06:07,879
然后多次对一个图片做多次增强

140
00:06:07,879 --> 00:06:09,319
然后做预测取平均

141
00:06:10,040 --> 00:06:11,080
这个是第一点

142
00:06:11,520 --> 00:06:13,360
第二点是说大家很多人

143
00:06:13,360 --> 00:06:15,160
都使用了多个模型做一次

144
00:06:15,319 --> 00:06:18,040
我们之前在第一次竞赛中也讲过

145
00:06:18,040 --> 00:06:20,120
autogram其实用的模型融合

146
00:06:20,560 --> 00:06:23,160
然后你有多个模型

147
00:06:23,160 --> 00:06:24,319
然后所有的结果

148
00:06:24,319 --> 00:06:25,920
你可以做加全平均

149
00:06:26,080 --> 00:06:26,879
当然最简单

150
00:06:26,879 --> 00:06:27,680
你就是做个平均

151
00:06:27,800 --> 00:06:29,040
你也不知道谁全中大

152
00:06:29,040 --> 00:06:30,319
谁全中小

153
00:06:30,600 --> 00:06:33,600
有同学使用了超过10种的模型

154
00:06:33,759 --> 00:06:34,800
就特别多的模型

155
00:06:35,800 --> 00:06:38,680
然后也有同学使用单一模型了

156
00:06:38,680 --> 00:06:40,000
但是可能会训练多次

157
00:06:40,000 --> 00:06:40,920
然后做平均

158
00:06:41,720 --> 00:06:42,800
另外一块是说

159
00:06:42,800 --> 00:06:46,360
在训练算法上面和学习率上有改进

160
00:06:46,600 --> 00:06:49,560
我们现在基本上用的是最简单的

161
00:06:49,560 --> 00:06:51,800
SGD或者OIUAD

162
00:06:52,000 --> 00:06:52,840
学习率的话

163
00:06:52,840 --> 00:06:55,840
我们基本上就是一条学习率过去

164
00:06:55,840 --> 00:06:58,360
然后我们在上个星期就讲过

165
00:06:58,360 --> 00:06:59,759
你可能会下降一次

166
00:07:00,560 --> 00:07:03,639
第4个就是说有几个同学用的是

167
00:07:03,639 --> 00:07:05,600
也有同学发言说

168
00:07:05,840 --> 00:07:07,680
这个数据里面有大量的

169
00:07:07,920 --> 00:07:09,000
同样一张图片

170
00:07:09,000 --> 00:07:10,439
有多个标号的情况

171
00:07:11,120 --> 00:07:12,319
这个其实太正常了

172
00:07:12,319 --> 00:07:14,600
就是说真实数据里面大量的噪音

173
00:07:14,600 --> 00:07:16,680
噪音有可能是数据本身有噪音

174
00:07:16,680 --> 00:07:18,480
第二个是说人在处理的时候

175
00:07:18,480 --> 00:07:21,160
很多时候是没注意那么多细节

176
00:07:21,160 --> 00:07:22,399
所以需要不同的

177
00:07:22,399 --> 00:07:23,560
不断的去看

178
00:07:23,720 --> 00:07:24,519
所以有同学说

179
00:07:24,639 --> 00:07:25,920
我把那些重复的

180
00:07:25,920 --> 00:07:29,159
有奇异标号的数据挪掉

181
00:07:29,800 --> 00:07:32,360
这也是做了一个数据清理

182
00:07:33,079 --> 00:07:35,360
这个就是简单来说

183
00:07:35,800 --> 00:07:39,800
同学们在这4个方向上有所加强

184
00:07:39,800 --> 00:07:41,439
然后能取到更好的成绩

185
00:07:42,439 --> 00:07:45,240
我们再稍微仔细讲一点

186
00:07:45,680 --> 00:07:46,759
在数据方向

187
00:07:46,759 --> 00:07:48,240
首先用重复的图片

188
00:07:48,240 --> 00:07:49,520
你可以手动去掉

189
00:07:49,720 --> 00:07:51,439
然后在增强的时候

190
00:07:52,240 --> 00:07:53,319
可以看到是说

191
00:07:53,319 --> 00:07:55,480
因为我们树叶放在那个地方

192
00:07:55,520 --> 00:07:57,319
然后背景是比较大的

193
00:07:57,439 --> 00:07:59,240
你像我们之前的C5

194
00:07:59,759 --> 00:08:00,920
然后Mlist

195
00:08:00,920 --> 00:08:03,879
它的背景其实比较小

196
00:08:03,879 --> 00:08:06,080
就图片已经比较占的图片

197
00:08:06,240 --> 00:08:07,439
就是说真实的图片

198
00:08:07,439 --> 00:08:10,000
占我那个image比较大了

199
00:08:10,160 --> 00:08:13,480
所以你可以使用更大的那些

200
00:08:13,719 --> 00:08:15,279
你可以在裁剪的时候

201
00:08:15,279 --> 00:08:16,160
可以取到

202
00:08:16,959 --> 00:08:19,000
可以裁出更小的区域出来

203
00:08:20,199 --> 00:08:21,199
另外一块是说

204
00:08:21,199 --> 00:08:22,800
你的你是个树叶

205
00:08:23,160 --> 00:08:25,040
因为你树叶是没有方向的

206
00:08:25,240 --> 00:08:26,959
所以你可以做很多的旋转

207
00:08:27,959 --> 00:08:29,120
所以这样子的话

208
00:08:29,120 --> 00:08:30,759
它确实这样子图片

209
00:08:30,759 --> 00:08:32,639
跟我们之前的图片

210
00:08:32,639 --> 00:08:37,320
比它的数据增强的空间更大

211
00:08:38,160 --> 00:08:38,880
另外一个是说

212
00:08:38,880 --> 00:08:40,200
大部分同学用的

213
00:08:40,240 --> 00:08:41,880
跨图片的增强

214
00:08:42,200 --> 00:08:43,320
一个是mixup

215
00:08:43,440 --> 00:08:44,480
我们之前有讲过

216
00:08:44,480 --> 00:08:47,080
就是说你随机拿两张图片出来

217
00:08:47,120 --> 00:08:50,280
然后给一个随机的权重

218
00:08:50,560 --> 00:08:51,200
两张图片

219
00:08:51,200 --> 00:08:52,440
按随机权重相加

220
00:08:52,480 --> 00:08:53,600
然后你的标号

221
00:08:53,600 --> 00:08:55,600
也按照同样的权重相加

222
00:08:55,760 --> 00:08:58,560
要得到一个新的训练图片

223
00:08:58,560 --> 00:08:59,920
然后参加训练

224
00:09:00,560 --> 00:09:02,600
另外一个大家用的比较多的

225
00:09:02,600 --> 00:09:05,520
而且我们没有特别去

226
00:09:05,799 --> 00:09:08,079
讲的一个叫cut mix

227
00:09:08,279 --> 00:09:09,399
跟mixup有点像

228
00:09:10,000 --> 00:09:11,399
就是说它的意思是说

229
00:09:11,439 --> 00:09:13,919
你去不同图片里面

230
00:09:13,919 --> 00:09:15,600
随机的去采样一些块

231
00:09:15,600 --> 00:09:17,480
就抠出一些块

232
00:09:17,600 --> 00:09:20,039
然后把这些块随机的组合起来

233
00:09:20,279 --> 00:09:21,319
最后你的标号

234
00:09:21,319 --> 00:09:25,399
取决于说你也是你随机组合的权重

235
00:09:25,519 --> 00:09:27,120
你会认为是这么相加

236
00:09:27,840 --> 00:09:30,439
就是说这个东西的意思是说

237
00:09:30,720 --> 00:09:33,639
希望你尽量的去看一些局部的信息

238
00:09:33,799 --> 00:09:35,199
因为对数据来讲

239
00:09:35,199 --> 00:09:36,600
局部信息还是挺重要的

240
00:09:37,519 --> 00:09:39,439
然后当然是说跨图片增强

241
00:09:39,439 --> 00:09:41,279
还有很多别的一个

242
00:09:41,279 --> 00:09:42,439
现在论文上挺多

243
00:09:42,679 --> 00:09:45,519
但是这两个大家是用的比较多的

244
00:09:46,759 --> 00:09:48,519
就数据方面的一些

245
00:09:48,679 --> 00:09:49,759
大家的一些工作

246
00:09:50,080 --> 00:09:51,159
然后模型方面

247
00:09:52,360 --> 00:09:53,360
基本上可以认为

248
00:09:53,360 --> 00:09:57,279
模型大多是resnet的变种

249
00:09:57,759 --> 00:09:59,799
有使用原始resnet的

250
00:10:00,360 --> 00:10:02,639
大家但是更多的是使用

251
00:10:02,879 --> 00:10:03,840
比如说dance net

252
00:10:04,480 --> 00:10:06,080
我们本来打算讲dance net

253
00:10:06,080 --> 00:10:07,480
后来还是说没有讲

254
00:10:07,480 --> 00:10:09,840
就是说resnet不是说相加

255
00:10:10,519 --> 00:10:12,319
在通道上相加

256
00:10:13,159 --> 00:10:15,919
dance net是说我在通道上是并在一起

257
00:10:15,919 --> 00:10:16,439
不相加

258
00:10:16,439 --> 00:10:17,879
所以保留了更多的信息

259
00:10:18,319 --> 00:10:19,679
比如说resnet

260
00:10:19,679 --> 00:10:20,720
next

261
00:10:20,720 --> 00:10:21,600
什么resnest

262
00:10:21,759 --> 00:10:24,000
就是基本上各种res

263
00:10:24,879 --> 00:10:26,399
net什么

264
00:10:26,399 --> 00:10:27,439
傻傻的分不清楚

265
00:10:27,439 --> 00:10:27,919
对吧

266
00:10:28,399 --> 00:10:29,799
所以其实我自己很多时候

267
00:10:29,799 --> 00:10:31,439
也搞不清这些名字是干嘛的

268
00:10:32,240 --> 00:10:33,040
所以基本上是说

269
00:10:33,040 --> 00:10:34,720
在resnet上加了一些

270
00:10:34,720 --> 00:10:35,800
更多的一些细节

271
00:10:36,840 --> 00:10:39,160
对模型做了更多细节的调整

272
00:10:39,160 --> 00:10:42,520
使得它的精度可以高了一点点

273
00:10:43,040 --> 00:10:45,160
另外一个是efficient net

274
00:10:45,160 --> 00:10:47,320
efficient net这个东西是在

275
00:10:48,160 --> 00:10:49,920
在开国上用的还挺多的

276
00:10:50,400 --> 00:10:51,840
其实我也不是那么理解

277
00:10:51,840 --> 00:10:53,320
就是说为什么大家那么喜欢

278
00:10:53,320 --> 00:10:54,400
要efficient net

279
00:10:54,400 --> 00:10:55,600
efficient net你可以认为

280
00:10:57,240 --> 00:10:59,840
基本上就是resnet的调三

281
00:11:00,000 --> 00:11:02,639
就是说你可以把它拉长一点

282
00:11:02,680 --> 00:11:03,680
变胖一点

283
00:11:03,680 --> 00:11:05,399
然后图片大小变大一点

284
00:11:05,399 --> 00:11:06,800
就是把它这种

285
00:11:06,800 --> 00:11:07,399
你可以当

286
00:11:07,399 --> 00:11:09,519
如果是你当resnet是一张图片的话

287
00:11:09,519 --> 00:11:10,800
你efficient net就是说

288
00:11:10,800 --> 00:11:12,040
我可以把你拉大一点

289
00:11:13,000 --> 00:11:14,120
就是大家可以看一下论文

290
00:11:14,280 --> 00:11:16,160
这个是就具体拉多大

291
00:11:16,160 --> 00:11:17,960
然后是搜出来的

292
00:11:19,879 --> 00:11:21,440
另外一块优化算法的话

293
00:11:21,680 --> 00:11:23,759
大家用的是

294
00:11:24,080 --> 00:11:26,639
基本上是adam或者它的变种

295
00:11:28,280 --> 00:11:29,400
我们还没有讲adam

296
00:11:29,519 --> 00:11:31,399
具体没有讲adam到底怎么回事

297
00:11:31,399 --> 00:11:33,039
但是我还是给大家可以

298
00:11:33,399 --> 00:11:34,120
大概讲一下

299
00:11:34,120 --> 00:11:35,480
就是adam就是说

300
00:11:35,559 --> 00:11:36,199
这个算法

301
00:11:37,639 --> 00:11:38,600
它不是给你

302
00:11:38,600 --> 00:11:40,639
不会给你最好的成绩

303
00:11:40,639 --> 00:11:43,279
就是说你SGD通过仔细的调整

304
00:11:43,399 --> 00:11:44,840
比如说调SGD

305
00:11:44,840 --> 00:11:46,240
momentum这些参数

306
00:11:46,279 --> 00:11:47,279
可以

307
00:11:47,600 --> 00:11:49,919
通常可以使得比adam的结果更好

308
00:11:50,000 --> 00:11:52,679
但是adam对这些学习率不那么敏感

309
00:11:52,679 --> 00:11:54,600
就是说在你不那么调

310
00:11:54,600 --> 00:11:55,960
仔细调整的情况下

311
00:11:55,960 --> 00:11:57,559
它的结果都还不错

312
00:11:57,799 --> 00:11:59,079
它更平滑一点

313
00:11:59,080 --> 00:12:00,520
SGD就是说你的

314
00:12:00,560 --> 00:12:03,040
最好的参数的区域更窄一点

315
00:12:03,040 --> 00:12:05,160
但是可能结果会更好

316
00:12:05,360 --> 00:12:06,920
所以很多时候大家说

317
00:12:07,320 --> 00:12:09,360
如果你优化算法不那么关键的时候

318
00:12:09,800 --> 00:12:11,160
就是说我可以去

319
00:12:11,480 --> 00:12:12,840
在数据上做工作

320
00:12:12,840 --> 00:12:14,200
模型上做工作时候

321
00:12:14,200 --> 00:12:15,000
那么优化算法

322
00:12:15,000 --> 00:12:16,360
我可以用的稍微简单一点

323
00:12:16,360 --> 00:12:17,200
就是不要

324
00:12:17,759 --> 00:12:19,960
调整那么调整

325
00:12:20,240 --> 00:12:21,680
但adam也有很多变种

326
00:12:21,680 --> 00:12:23,520
adam论文出来之后

327
00:12:25,080 --> 00:12:27,920
也是有大量的几十个变种在后面

328
00:12:27,919 --> 00:12:29,799
大家有用它的一些变种

329
00:12:30,679 --> 00:12:31,799
另外一个学习率

330
00:12:32,039 --> 00:12:33,319
学习率我们

331
00:12:33,439 --> 00:12:35,079
这两个其实我们都有讲过

332
00:12:35,079 --> 00:12:37,959
一个是基于cos的下降的一个方式

333
00:12:38,559 --> 00:12:39,319
它的好处是说

334
00:12:39,319 --> 00:12:40,959
你不怎么需要调参

335
00:12:41,199 --> 00:12:42,519
也挺稳定的

336
00:12:43,039 --> 00:12:44,360
另外一个叫做

337
00:12:44,360 --> 00:12:46,360
当你训练不动时往下调

338
00:12:46,439 --> 00:12:49,079
这也是当年resnet那些paper

339
00:12:49,199 --> 00:12:50,839
他们常用的方法

340
00:12:50,919 --> 00:12:52,839
就是说比如说resnet那个paper

341
00:12:52,839 --> 00:12:53,799
应该是说

342
00:12:53,799 --> 00:12:55,759
它每隔30个epoch

343
00:12:55,759 --> 00:12:56,919
还是多少个epoch

344
00:12:56,919 --> 00:12:58,039
往下降一次

345
00:12:58,360 --> 00:12:59,559
就是往下乘个0.1

346
00:12:59,559 --> 00:13:00,399
然后往下降

347
00:13:00,600 --> 00:13:02,559
实际上大家发现是说

348
00:13:02,559 --> 00:13:04,000
你更好的是

349
00:13:04,000 --> 00:13:05,120
你就一直训练

350
00:13:05,879 --> 00:13:06,799
就当时的手动

351
00:13:06,960 --> 00:13:09,679
就是取个学习率0.1

352
00:13:09,679 --> 00:13:10,639
你要一直往下跑

353
00:13:10,639 --> 00:13:11,519
一直往下跑

354
00:13:11,519 --> 00:13:13,039
然后你就盯着它的

355
00:13:13,840 --> 00:13:15,439
validation loss

356
00:13:15,439 --> 00:13:17,000
就是你的验证的loss

357
00:13:17,000 --> 00:13:18,960
或者验证的accuracy都OK

358
00:13:19,000 --> 00:13:20,399
如果你发现训练

359
00:13:20,399 --> 00:13:21,720
它开始变平的时候

360
00:13:22,159 --> 00:13:23,639
你就把训练停掉

361
00:13:24,039 --> 00:13:25,559
然后把学习率往下调一下

362
00:13:25,560 --> 00:13:26,280
就比如说

363
00:13:27,160 --> 00:13:28,640
调成以前的0.1

364
00:13:28,680 --> 00:13:30,520
就调成10%

365
00:13:30,520 --> 00:13:31,520
然后继续训练

366
00:13:31,680 --> 00:13:33,440
然后等到你眼睛看着

367
00:13:33,440 --> 00:13:35,400
它越来越平的时候

368
00:13:35,600 --> 00:13:36,800
你就把它停掉

369
00:13:36,800 --> 00:13:37,920
然后再往下调

370
00:13:38,240 --> 00:13:40,600
这个是当年最原始的做法

371
00:13:41,560 --> 00:13:42,360
这个的好处是说

372
00:13:42,360 --> 00:13:43,360
你相对来说

373
00:13:43,360 --> 00:13:44,320
你不会那么浪费

374
00:13:44,320 --> 00:13:45,840
因为你反正你每一个点

375
00:13:45,960 --> 00:13:47,520
都做了checkpoint

376
00:13:47,600 --> 00:13:48,600
所以你其实你可以

377
00:13:48,600 --> 00:13:51,040
任何回到以前的一个点都OK

378
00:13:51,800 --> 00:13:52,560
那个是我觉得

379
00:13:52,560 --> 00:13:54,240
那个是我们5年前经常用的

380
00:13:54,360 --> 00:13:54,840
5年前

381
00:13:54,840 --> 00:13:56,440
反正你半眼瘦的

382
00:13:56,560 --> 00:13:58,280
脑袋里就装着那个东西在跑

383
00:13:58,320 --> 00:13:59,560
然后时不时去看一下

384
00:13:59,680 --> 00:14:00,519
跑的怎么样了

385
00:14:00,680 --> 00:14:01,720
如果发现它平了

386
00:14:01,720 --> 00:14:02,920
赶紧上线

387
00:14:02,920 --> 00:14:05,680
去SS迟到那机器上

388
00:14:05,680 --> 00:14:06,440
把它停掉

389
00:14:06,440 --> 00:14:07,560
然后手动往下调

390
00:14:08,399 --> 00:14:09,440
就现在当然是做的

391
00:14:09,440 --> 00:14:10,360
比较智能一点

392
00:14:10,480 --> 00:14:12,280
就是说现在框架都提供

393
00:14:12,280 --> 00:14:13,800
这样子的学习率的schedule

394
00:14:13,800 --> 00:14:14,560
就是说

395
00:14:14,879 --> 00:14:16,160
就是说当你可以

396
00:14:16,160 --> 00:14:17,160
大家看一下代码

397
00:14:17,160 --> 00:14:18,200
具体怎么实现了

398
00:14:18,200 --> 00:14:19,519
给大家讲一下idea

399
00:14:19,519 --> 00:14:20,200
就是说

400
00:14:20,280 --> 00:14:21,920
当你发现你的

401
00:14:22,759 --> 00:14:23,800
比较平的时候

402
00:14:24,120 --> 00:14:25,520
它就自动帮你往下调

403
00:14:25,520 --> 00:14:27,440
但是你可以去设什么

404
00:14:27,480 --> 00:14:28,800
什么叫做比较平

405
00:14:29,240 --> 00:14:31,080
什么下调多少

406
00:14:31,080 --> 00:14:32,080
可以让你自动去设

407
00:14:32,080 --> 00:14:33,760
你就不要去眼睛盯着了

408
00:14:34,120 --> 00:14:34,440
OK

409
00:14:34,440 --> 00:14:36,000
这是模型方面的

410
00:14:37,360 --> 00:14:39,280
所以基本上就是这两大方面

411
00:14:39,680 --> 00:14:41,280
所以我们在之前

412
00:14:41,280 --> 00:14:43,240
也给大家多多少少有讲过

413
00:14:43,400 --> 00:14:45,120
大家QA也问到过这些问题

414
00:14:45,360 --> 00:14:48,720
但是这里有仔细的代码

415
00:14:48,720 --> 00:14:50,280
大家欢迎大家去看一看

416
00:14:50,280 --> 00:14:51,240
去学习一下

417
00:14:51,240 --> 00:14:52,120
仔细体会一下

418
00:14:52,120 --> 00:14:54,039
各个每一个细节是怎么用的

419
00:14:55,440 --> 00:14:56,480
大家也有问到

420
00:14:56,480 --> 00:14:58,120
autogram是怎么用

421
00:14:58,279 --> 00:14:58,840
autogram

422
00:14:58,840 --> 00:15:00,919
我们有同学提供了一个样本

423
00:15:01,039 --> 00:15:02,240
我其实没有试

424
00:15:02,919 --> 00:15:03,759
是5行代码

425
00:15:03,919 --> 00:15:05,399
就是安装加训练

426
00:15:05,399 --> 00:15:07,039
大概是花时100分钟

427
00:15:07,080 --> 00:15:08,120
就还是比较快的

428
00:15:08,279 --> 00:15:11,320
我觉得是比较容易用

429
00:15:11,320 --> 00:15:12,960
而且运行起来比较快

430
00:15:12,960 --> 00:15:14,200
就是从你什么都不用管

431
00:15:14,200 --> 00:15:15,240
就是100分钟

432
00:15:16,200 --> 00:15:17,080
因为别的同学

433
00:15:17,080 --> 00:15:18,759
我看你们提交还挺多的

434
00:15:18,919 --> 00:15:20,399
就是大家基本上

435
00:15:20,399 --> 00:15:21,960
也确实花了好几个星期

436
00:15:21,960 --> 00:15:24,600
然后都提交了几十次

437
00:15:24,840 --> 00:15:26,560
我觉得在这一点上

438
00:15:26,560 --> 00:15:28,080
autogram就automl

439
00:15:28,519 --> 00:15:29,680
还是做的比较好的

440
00:15:29,680 --> 00:15:30,400
就是说

441
00:15:30,400 --> 00:15:31,680
一用起来简单

442
00:15:31,680 --> 00:15:33,519
二运行也不长

443
00:15:33,720 --> 00:15:34,960
但它进度并不高

444
00:15:35,160 --> 00:15:36,120
96%

445
00:15:36,240 --> 00:15:38,879
我觉得进度还不错

446
00:15:38,920 --> 00:15:40,920
但是当然是说大家调的比较狠

447
00:15:40,920 --> 00:15:42,920
所以在基本上分数

448
00:15:43,080 --> 00:15:44,879
进个40名都进不了

449
00:15:44,879 --> 00:15:45,720
我觉得是

450
00:15:45,840 --> 00:15:47,200
记得是应该是进个

451
00:15:47,200 --> 00:15:49,160
可能七八十名的样子

452
00:15:50,120 --> 00:15:51,160
因为大家比较强

453
00:15:51,679 --> 00:15:53,279
当然你可以通过定制化

454
00:15:53,279 --> 00:15:54,000
来提升进度

455
00:15:54,000 --> 00:15:55,319
就是automl

456
00:15:55,319 --> 00:15:55,879
autogram

457
00:15:55,879 --> 00:15:57,639
你可以告诉他用哪个模型

458
00:15:58,319 --> 00:15:59,879
在哪几种参数里面去搜

459
00:15:59,879 --> 00:16:01,639
然后当然你可以这么做

460
00:16:01,639 --> 00:16:02,199
就是说

461
00:16:02,399 --> 00:16:03,959
基本上大概所有的

462
00:16:04,360 --> 00:16:05,399
这些

463
00:16:06,439 --> 00:16:07,399
提供的代码

464
00:16:07,399 --> 00:16:08,439
我都可以在知道

465
00:16:08,439 --> 00:16:09,199
你是怎么做的时候

466
00:16:09,199 --> 00:16:10,679
我就把你参数放进去

467
00:16:10,679 --> 00:16:11,719
当然是可以做的

468
00:16:11,919 --> 00:16:13,079
但这个就是

469
00:16:14,039 --> 00:16:15,319
那就是你需要

470
00:16:15,600 --> 00:16:18,480
更多的专家的知识去定制

471
00:16:19,240 --> 00:16:20,959
但我们可能会在下一个版本

472
00:16:20,960 --> 00:16:22,080
会让你去搜索

473
00:16:22,080 --> 00:16:23,840
更多的模型

474
00:16:23,840 --> 00:16:24,600
超参数

475
00:16:24,879 --> 00:16:27,120
有可能在同样的时间里面

476
00:16:27,120 --> 00:16:28,360
可以进度会更好

477
00:16:28,759 --> 00:16:29,480
但反过来讲

478
00:16:29,480 --> 00:16:30,639
其实我觉得

479
00:16:30,680 --> 00:16:31,840
对于一个

480
00:16:31,879 --> 00:16:33,800
我觉得automl主要的好处

481
00:16:33,800 --> 00:16:37,759
它其实不应该是给你去刷一个榜的

482
00:16:38,200 --> 00:16:40,080
因为你刷榜的话

483
00:16:40,080 --> 00:16:41,360
真的我可以说

484
00:16:41,400 --> 00:16:42,720
我真的很有钱的话

485
00:16:43,160 --> 00:16:44,040
我可以像Google

486
00:16:44,040 --> 00:16:45,519
他们刷automl

487
00:16:45,519 --> 00:16:47,200
真的就是几千个tpu

488
00:16:47,200 --> 00:16:48,440
可以跑一个星期

489
00:16:48,960 --> 00:16:50,120
但是我觉得

490
00:16:50,919 --> 00:16:52,200
有点浪费能源

491
00:16:53,159 --> 00:16:54,200
因为绝大部分

492
00:16:54,200 --> 00:16:55,639
因为你去打比赛

493
00:16:55,639 --> 00:16:57,000
是一个学习的目的

494
00:16:57,399 --> 00:16:58,120
不经济

495
00:16:58,120 --> 00:17:00,360
不是让你真的要去赢多少钱

496
00:17:00,360 --> 00:17:01,440
你赢个3000块钱

497
00:17:01,440 --> 00:17:02,240
30,000块钱

498
00:17:02,240 --> 00:17:04,039
你可能你的电费还花不来

499
00:17:04,960 --> 00:17:07,480
所以我觉得autogrow更多是说

500
00:17:07,759 --> 00:17:10,279
让你在解决实际的问题的时候

501
00:17:10,279 --> 00:17:11,920
可以不用那么去关心

502
00:17:11,920 --> 00:17:14,160
你的模型细节

503
00:17:14,160 --> 00:17:15,559
而是关注于说

504
00:17:15,559 --> 00:17:16,960
去解决你的问题

505
00:17:17,240 --> 00:17:18,400
理解你的问题

506
00:17:18,640 --> 00:17:19,640
去解决你的问题

507
00:17:19,640 --> 00:17:22,240
更多是关心业务上的一些逻辑

508
00:17:22,240 --> 00:17:24,960
还不仅仅是去花太多时间去调查

509
00:17:25,160 --> 00:17:26,680
所以我们觉得还是autogrow

510
00:17:26,680 --> 00:17:29,360
我们目前还是关注在工业界的应用上

511
00:17:29,360 --> 00:17:30,440
就是说意味着说

512
00:17:30,440 --> 00:17:31,240
不会给

513
00:17:31,720 --> 00:17:34,400
可能不会给大家去做大量的model

514
00:17:34,400 --> 00:17:34,880
injumbo

515
00:17:35,280 --> 00:17:36,600
在tabula之前

516
00:17:36,600 --> 00:17:37,320
我们可以做

517
00:17:37,320 --> 00:17:38,840
是因为每个模型都很小

518
00:17:38,840 --> 00:17:40,320
你是做一做没关系的

519
00:17:40,400 --> 00:17:42,920
但是对img那种比较大的神经网络

520
00:17:43,280 --> 00:17:45,960
你做了大量的模型的injumbo的话

521
00:17:45,960 --> 00:17:47,520
还而且你在测试的时候

522
00:17:47,520 --> 00:17:48,880
用data augmentation的话

523
00:17:49,080 --> 00:17:50,680
这个东西测试太贵了

524
00:17:51,200 --> 00:17:52,600
而且目前来说

525
00:17:52,600 --> 00:17:54,440
这样子应用上线

526
00:17:54,760 --> 00:17:55,800
代价非常大

527
00:17:55,800 --> 00:17:58,600
就是说我要用户提交一个图片

528
00:17:58,600 --> 00:18:00,680
我真的跑一个很贵的神经网络的话

529
00:18:00,680 --> 00:18:03,200
我们很有可能线上的代价是非常大的

530
00:18:03,560 --> 00:18:05,560
所以很有可能autogrow

531
00:18:05,560 --> 00:18:06,280
在这一点上

532
00:18:06,280 --> 00:18:08,200
更多的是关注工业界应用上

533
00:18:08,200 --> 00:18:10,440
我们还目前没有打算去说

534
00:18:10,440 --> 00:18:12,240
用来给大家去赢比赛

535
00:18:12,520 --> 00:18:15,200
当然有一天如果发现计算真的足够便宜

536
00:18:15,520 --> 00:18:16,000
我们的

537
00:18:16,880 --> 00:18:18,440
越来越多的定制芯片出来

538
00:18:18,759 --> 00:18:19,920
NVIDIA加强价

539
00:18:19,920 --> 00:18:21,200
然后更好的算法出来

540
00:18:21,200 --> 00:18:22,640
说不定automate还是有可能的

541
00:18:24,080 --> 00:18:24,680
OK

542
00:18:25,360 --> 00:18:25,720
好

543
00:18:25,720 --> 00:18:27,240
我们大概总结一下

544
00:18:27,240 --> 00:18:28,080
就是说

545
00:18:28,759 --> 00:18:31,640
提升精度的思路有那么几个

546
00:18:31,680 --> 00:18:34,559
一个是说你去看那个数据

547
00:18:34,720 --> 00:18:37,279
然后去寻找比较好的增强

548
00:18:37,519 --> 00:18:40,759
大家都用了一个增强的包里面

549
00:18:40,759 --> 00:18:42,519
提供了大量的图片的增强

550
00:18:42,519 --> 00:18:44,320
就是说你先大概去看一下

551
00:18:44,320 --> 00:18:45,440
人家有什么增强

552
00:18:45,440 --> 00:18:46,720
然后看一眼你的数据

553
00:18:46,720 --> 00:18:48,160
然后看一看你觉得怎么样

554
00:18:48,240 --> 00:18:50,320
然后你可以在图片上去

555
00:18:50,360 --> 00:18:52,080
试几个增强

556
00:18:52,080 --> 00:18:53,480
然后看一看出来的东西

557
00:18:53,480 --> 00:18:55,800
是不是人觉得还靠谱

558
00:18:56,120 --> 00:18:57,080
你可以去

559
00:18:57,080 --> 00:18:59,600
这个是根据数据挑选增强

560
00:19:00,480 --> 00:19:02,840
第二个是说你选新模型

561
00:19:03,400 --> 00:19:05,519
就是说RESNET已经是老模型了

562
00:19:05,519 --> 00:19:06,440
我们讲到的

563
00:19:06,480 --> 00:19:08,680
应该是2016 17年的工作

564
00:19:08,680 --> 00:19:11,279
所以已经是四五年前工作了

565
00:19:11,680 --> 00:19:12,880
现在新的工作

566
00:19:13,200 --> 00:19:14,880
已经有很多新的工作出来

567
00:19:14,880 --> 00:19:15,800
新的模型

568
00:19:16,160 --> 00:19:17,680
在细节上更好一点

569
00:19:17,799 --> 00:19:19,080
所以你用新的模型

570
00:19:19,080 --> 00:19:21,840
通常会给你带来一些提升

571
00:19:22,440 --> 00:19:23,240
另外一个是说

572
00:19:23,240 --> 00:19:24,519
你用新的优化算法

573
00:19:24,519 --> 00:19:26,640
不管是你的优化算法

574
00:19:26,640 --> 00:19:28,560
还是你的学习率的调整

575
00:19:28,799 --> 00:19:30,560
新的都可以

576
00:19:30,560 --> 00:19:31,000
新的

577
00:19:31,000 --> 00:19:32,920
我觉得目前主要追求的是说

578
00:19:32,920 --> 00:19:36,960
让你的相对来说调参更少一点

579
00:19:36,960 --> 00:19:37,560
所以这样子

580
00:19:37,560 --> 00:19:39,160
你可以关注在数据增强

581
00:19:39,160 --> 00:19:40,279
核心模型上

582
00:19:40,680 --> 00:19:42,400
而且打比赛大家常用的

583
00:19:42,400 --> 00:19:44,080
就是说多模型融合

584
00:19:44,360 --> 00:19:45,799
你可以挑很多模型

585
00:19:46,000 --> 00:19:48,559
然后另外一个是说

586
00:19:48,559 --> 00:19:51,079
真的为了提升测试精度

587
00:19:51,079 --> 00:19:53,599
大家会在测试时也使用增强

588
00:19:53,960 --> 00:19:56,919
这是那么几个提升的精度的思路

589
00:19:58,079 --> 00:19:59,039
但反过来讲

590
00:19:59,159 --> 00:20:00,240
我们这次数据

591
00:20:00,240 --> 00:20:02,119
相对来说比较简单

592
00:20:02,119 --> 00:20:03,240
就是一个叶子

593
00:20:03,599 --> 00:20:05,240
就你的背景比较简单

594
00:20:05,240 --> 00:20:06,599
而且就是光照

595
00:20:07,480 --> 00:20:09,039
旋转还好

596
00:20:09,039 --> 00:20:11,000
就我们拍的叶子里面

597
00:20:11,159 --> 00:20:13,839
基本上都主体比较明确

598
00:20:13,879 --> 00:20:15,959
而且清晰度也还不错

599
00:20:16,119 --> 00:20:18,799
所以数据相对来说是比较简单的

600
00:20:18,839 --> 00:20:21,240
然后排名有一定的学习性

601
00:20:22,039 --> 00:20:23,480
就是说大家去看说

602
00:20:23,480 --> 00:20:24,959
你为什么用那么多

603
00:20:24,959 --> 00:20:26,199
为什么用这个用那个

604
00:20:26,199 --> 00:20:27,039
用这个的时候

605
00:20:27,039 --> 00:20:28,240
其实很多时候

606
00:20:28,559 --> 00:20:29,519
你换一个数据

607
00:20:29,519 --> 00:20:31,839
不代表你是一样的

608
00:20:32,039 --> 00:20:33,720
超限数能够用过去

609
00:20:34,359 --> 00:20:35,279
所以是说

610
00:20:35,319 --> 00:20:36,000
更多的时候

611
00:20:36,000 --> 00:20:38,039
大家知道有那么几个选项

612
00:20:38,039 --> 00:20:39,519
你碰到一个新的数据的时候

613
00:20:39,519 --> 00:20:40,759
你可以去试一下

614
00:20:40,759 --> 00:20:41,359
这些选项

615
00:20:41,359 --> 00:20:42,720
是不是还行

616
00:20:43,640 --> 00:20:45,079
而且说很多时候

617
00:20:45,079 --> 00:20:46,400
你说我们说

618
00:20:46,400 --> 00:20:47,519
把前20名同学

619
00:20:47,519 --> 00:20:48,880
拿出来代码看一下

620
00:20:48,880 --> 00:20:50,600
而且其实我觉得

621
00:20:51,799 --> 00:20:55,240
95%或者98%以上同学的代码

622
00:20:55,240 --> 00:20:56,319
都挺好的

623
00:20:56,640 --> 00:20:58,440
就是说大家也不要太纠结

624
00:20:58,440 --> 00:21:01,360
说我们这一次为什么没有进多少

625
00:21:01,360 --> 00:21:04,039
真的就是一定的随机性在里面

626
00:21:06,079 --> 00:21:06,920
另外一个是说

627
00:21:06,920 --> 00:21:09,079
我们做这个比赛

628
00:21:09,240 --> 00:21:10,600
也是给大家体会一下

629
00:21:10,600 --> 00:21:12,840
怎么样去刷精度

630
00:21:13,000 --> 00:21:14,720
但是在工业界应用里面

631
00:21:14,920 --> 00:21:16,800
这个是有一定不一样的

632
00:21:17,080 --> 00:21:18,760
大家在吐槽说

633
00:21:18,760 --> 00:21:20,360
你在开国上打比赛

634
00:21:20,360 --> 00:21:21,200
打得特别好

635
00:21:21,200 --> 00:21:22,720
但是你去工业界应用的时候

636
00:21:22,720 --> 00:21:24,800
你还是会发现会很不一样

637
00:21:24,880 --> 00:21:26,000
就工业界的应用

638
00:21:26,000 --> 00:21:29,960
和你的去打比赛的方法论

639
00:21:29,960 --> 00:21:31,520
是特别不一样的

640
00:21:31,600 --> 00:21:32,560
而且我们这门课

641
00:21:32,560 --> 00:21:34,920
其实更多的在QA的关节里面

642
00:21:34,920 --> 00:21:36,120
我们其实回答

643
00:21:36,120 --> 00:21:38,040
都是在工业界应用上的一些东西

644
00:21:38,720 --> 00:21:39,720
比如说简单来说

645
00:21:40,120 --> 00:21:41,000
简单说就是说

646
00:21:41,000 --> 00:21:42,200
它的不同在于说

647
00:21:42,600 --> 00:21:43,880
工业界的应用里面

648
00:21:43,880 --> 00:21:47,200
其实挺少使用模型融合

649
00:21:47,200 --> 00:21:49,200
和测试式数据增强的

650
00:21:49,759 --> 00:21:52,480
这是因为计算代价过大

651
00:21:52,839 --> 00:21:54,240
我们也讲过说

652
00:21:54,400 --> 00:21:55,799
你一张图片过来

653
00:21:55,799 --> 00:21:57,960
你做一次inference的开销

654
00:21:57,960 --> 00:21:59,600
可能是可能一分钱

655
00:21:59,600 --> 00:22:01,920
或者0.1分钱

656
00:22:01,960 --> 00:22:03,799
但是你如果图片过大的话

657
00:22:03,799 --> 00:22:05,519
你有很多用户在用

658
00:22:05,759 --> 00:22:08,440
每秒钟你可以有1000张图片的话

659
00:22:08,480 --> 00:22:09,400
那么每秒钟

660
00:22:09,400 --> 00:22:10,840
你可能花几块钱

661
00:22:10,880 --> 00:22:12,080
或者几十块钱

662
00:22:12,160 --> 00:22:12,880
这样子的话

663
00:22:12,880 --> 00:22:14,480
你的开销其实够大的

664
00:22:14,519 --> 00:22:16,840
而且你很难是说

665
00:22:16,840 --> 00:22:17,799
你一个图片

666
00:22:17,799 --> 00:22:19,200
就是一张一张图片

667
00:22:19,200 --> 00:22:20,640
你一个用户请求里面

668
00:22:20,640 --> 00:22:23,320
可能有保存很多张图片

669
00:22:23,440 --> 00:22:24,600
而且你可能会猜测

670
00:22:24,600 --> 00:22:27,080
很多个不一样的子问题

671
00:22:27,120 --> 00:22:28,480
这样子你都去做的话

672
00:22:28,480 --> 00:22:30,120
你的代价就会非常大

673
00:22:32,640 --> 00:22:33,920
另外一个是说

674
00:22:34,840 --> 00:22:36,200
在工业界应用里面

675
00:22:36,360 --> 00:22:38,080
有一个非常不一样的是说

676
00:22:38,079 --> 00:22:40,399
大家不会去真的去试

677
00:22:40,399 --> 00:22:41,960
那么新的模型

678
00:22:42,079 --> 00:22:43,599
就是大家在这里用的模型

679
00:22:43,759 --> 00:22:46,000
不见得在工业界上会经常用

680
00:22:47,319 --> 00:22:49,319
就是说各种特别新的模型

681
00:22:50,119 --> 00:22:51,960
因为为什么是因为说

682
00:22:52,240 --> 00:22:53,919
很多时候在工业界里面

683
00:22:53,960 --> 00:22:55,480
你我先拿

684
00:22:55,519 --> 00:22:56,679
弄一个数据出来

685
00:22:56,799 --> 00:22:57,720
标一个数据也行

686
00:22:57,720 --> 00:22:58,759
怎么样也行

687
00:22:59,039 --> 00:23:01,079
然后我去试一些

688
00:23:01,359 --> 00:23:02,279
调一次

689
00:23:03,199 --> 00:23:04,079
那调一次的话

690
00:23:04,079 --> 00:23:05,559
我不会为了说

691
00:23:05,559 --> 00:23:07,319
一定要调到多少

692
00:23:07,319 --> 00:23:09,200
去把精度刷的特别高

693
00:23:09,200 --> 00:23:11,079
或通常来说

694
00:23:11,079 --> 00:23:13,079
不会把我的整个超参数

695
00:23:13,240 --> 00:23:15,000
整个模型弄得特别复杂

696
00:23:15,039 --> 00:23:16,359
越简单越好

697
00:23:16,399 --> 00:23:17,720
因为相对来说

698
00:23:17,720 --> 00:23:18,399
越简单

699
00:23:18,399 --> 00:23:19,519
你的泛滑性越强

700
00:23:20,039 --> 00:23:22,159
因为你越调的越狠

701
00:23:22,159 --> 00:23:23,839
就是你的overfeeling的

702
00:23:23,879 --> 00:23:25,039
概率就越大

703
00:23:25,679 --> 00:23:27,559
然后当你调的特别狠的时候

704
00:23:27,559 --> 00:23:29,599
你的验证数据机

705
00:23:29,599 --> 00:23:31,079
也变成训练数据机了

706
00:23:31,200 --> 00:23:32,399
就是说你是你的模型

707
00:23:32,399 --> 00:23:34,240
加软一起训练出来结果

708
00:23:34,439 --> 00:23:35,919
所以很多时候在工业界的时候

709
00:23:35,920 --> 00:23:37,640
我会说我会调一个

710
00:23:37,640 --> 00:23:39,000
也相对来说

711
00:23:39,000 --> 00:23:42,320
比较简单的一组超参数

712
00:23:42,480 --> 00:23:44,200
数据增强

713
00:23:44,640 --> 00:23:45,160
模型

714
00:23:45,880 --> 00:23:46,320
学习率

715
00:23:46,400 --> 00:23:49,360
都不会选择那么的调的那么彻底

716
00:23:49,360 --> 00:23:50,600
相对来说比较简单

717
00:23:50,840 --> 00:23:52,000
然后很多时候

718
00:23:52,000 --> 00:23:54,000
我就会把超参数固定住

719
00:23:54,000 --> 00:23:55,440
就之后不变了

720
00:23:56,039 --> 00:23:56,880
所以就是说

721
00:23:56,880 --> 00:23:57,920
然后接下来就干嘛

722
00:23:58,279 --> 00:24:00,800
接下来就是不断的去提升数据的质量

723
00:24:01,160 --> 00:24:03,320
加入更多新的类别

724
00:24:03,360 --> 00:24:05,279
标记更多的数据

725
00:24:05,359 --> 00:24:08,319
然后增加数据的多样性

726
00:24:08,879 --> 00:24:10,079
为什么你会这么想了

727
00:24:10,079 --> 00:24:12,559
是因为当你一个模型

728
00:24:12,559 --> 00:24:14,559
在应用部署的时候

729
00:24:14,720 --> 00:24:16,519
你就会有更多的用户来用

730
00:24:16,680 --> 00:24:18,319
然后你就获取更多的数据

731
00:24:18,319 --> 00:24:19,759
就是说你在工业界应用里头

732
00:24:19,759 --> 00:24:21,440
你的数据是不断的过来的

733
00:24:22,160 --> 00:24:24,599
而且你会去关心新的数据

734
00:24:24,639 --> 00:24:25,599
而不是旧的数据

735
00:24:25,720 --> 00:24:27,960
因为你的整个流行趋势在变

736
00:24:28,440 --> 00:24:30,079
就是说用户的行为各种东西

737
00:24:30,079 --> 00:24:31,759
它都会随着时间的变化

738
00:24:31,879 --> 00:24:34,200
所以你要不断的去看你的新的数据

739
00:24:34,200 --> 00:24:35,480
可能去对它标注

740
00:24:35,480 --> 00:24:36,720
然后同样的话

741
00:24:36,720 --> 00:24:37,759
你花的时间越多

742
00:24:37,920 --> 00:24:39,640
你会对之前的数据做清理

743
00:24:39,960 --> 00:24:40,840
然后这样子的话

744
00:24:40,840 --> 00:24:42,960
你的数据的质量是不断提升的

745
00:24:43,120 --> 00:24:44,400
所以很多时候说

746
00:24:44,640 --> 00:24:46,640
我固定住超参数

747
00:24:46,640 --> 00:24:48,759
然后还能够很好的去算

748
00:24:49,039 --> 00:24:51,200
我这个数据加了这个东西之后

749
00:24:51,200 --> 00:24:52,480
对我们模型会怎么样

750
00:24:52,480 --> 00:24:55,080
就固定住一块纸条数据

751
00:24:55,240 --> 00:24:56,680
我就可以有个清晰的知道

752
00:24:56,680 --> 00:24:57,480
怎么样的数据

753
00:24:57,480 --> 00:24:58,640
对我的模型更重要

754
00:25:00,160 --> 00:25:01,039
所以的话

755
00:25:01,240 --> 00:25:02,519
就是说你可能会不断的

756
00:25:02,519 --> 00:25:04,599
每周每个月去更新的数据

757
00:25:04,599 --> 00:25:05,480
但你的模型

758
00:25:05,480 --> 00:25:06,920
你会重新训练模型

759
00:25:07,240 --> 00:25:09,279
但是你不会去改你的模型超参数

760
00:25:09,359 --> 00:25:11,400
就是说假设你用了RESNET50的话

761
00:25:11,400 --> 00:25:12,680
可能一直用RESNET50

762
00:25:12,680 --> 00:25:14,240
用很长一阵子

763
00:25:14,400 --> 00:25:16,279
但也许可能过一年

764
00:25:16,279 --> 00:25:16,799
过两年

765
00:25:16,799 --> 00:25:17,559
你会去想说

766
00:25:17,799 --> 00:25:19,519
是不是有更好的模型

767
00:25:19,960 --> 00:25:21,400
更好的模型相对来说

768
00:25:21,400 --> 00:25:25,599
我不会去找那些计算的特别大的

769
00:25:25,639 --> 00:25:27,480
通常你会去找那个模型

770
00:25:27,519 --> 00:25:29,279
至少在计算复杂度上

771
00:25:29,279 --> 00:25:30,799
不会比现在模型更高

772
00:25:31,399 --> 00:25:31,960
所以就是说

773
00:25:31,960 --> 00:25:33,240
你会去找那个模型

774
00:25:33,279 --> 00:25:35,519
在计算复杂度差不多

775
00:25:35,519 --> 00:25:37,000
或者更低的情况下

776
00:25:37,000 --> 00:25:39,079
能在你的数据上能提升结果

777
00:25:39,759 --> 00:25:41,000
他就说你固定住数据

778
00:25:41,000 --> 00:25:42,200
然后再去调字层

779
00:25:42,240 --> 00:25:44,319
这次调成可能不见得是

780
00:25:44,319 --> 00:25:45,440
你每个月都去调

781
00:25:45,480 --> 00:25:47,159
可能是一年调一两次

782
00:25:47,159 --> 00:25:47,759
这样子

783
00:25:48,519 --> 00:25:48,839
OK

784
00:25:48,839 --> 00:25:50,919
这就是工业界应用

785
00:25:50,960 --> 00:25:53,319
和你的硬赛的区别

786
00:25:53,680 --> 00:25:55,480
工业界关心数据

787
00:25:55,519 --> 00:25:57,759
80%的时间在弄数据

788
00:25:58,039 --> 00:25:59,000
20%时间

789
00:25:59,000 --> 00:26:01,599
我觉得是在跟业务逻辑做一些

790
00:26:02,759 --> 00:26:04,200
一些融合

791
00:26:04,599 --> 00:26:06,160
所以你调模型的时间

792
00:26:06,160 --> 00:26:07,079
其实并不多

793
00:26:07,640 --> 00:26:08,759
在竞赛中

794
00:26:08,759 --> 00:26:10,079
你固定住了数据

795
00:26:10,119 --> 00:26:11,440
你就是调你的模型

796
00:26:12,039 --> 00:26:12,319
OK

797
00:26:12,319 --> 00:26:13,680
所以说这两个

798
00:26:13,680 --> 00:26:16,519
而且你在学术界

799
00:26:16,519 --> 00:26:18,119
很多时候也是固定住数据

800
00:26:18,160 --> 00:26:19,079
然后去调模型

801
00:26:19,079 --> 00:26:22,519
所以学术界和你的竞赛界

802
00:26:22,519 --> 00:26:23,920
是有一点相同的

803
00:26:24,079 --> 00:26:26,200
在工业界确实是有一点不一样

804
00:26:26,200 --> 00:26:28,279
就大家知道这个东西就很好

