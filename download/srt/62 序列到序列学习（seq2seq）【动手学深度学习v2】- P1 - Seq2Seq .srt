1
00:00:00,000 --> 00:00:02,280
问一下我们的图是怎么做的

2
00:00:04,639 --> 00:00:05,759
我们图是怎么做

3
00:00:05,759 --> 00:00:08,759
我们图是用一个叫做

4
00:00:11,599 --> 00:00:15,320
我们用一个Ormanly Graph的软件做的

5
00:00:16,320 --> 00:00:17,199
就是说你可以去看一下

6
00:00:17,199 --> 00:00:19,400
我们的图其实放在我们的原代码里面

7
00:00:19,920 --> 00:00:21,600
就是说你可以去看一下

8
00:00:21,600 --> 00:00:24,519
我们的图其实放在这个地方

9
00:00:24,560 --> 00:00:26,760
我们的图用的还挺多人用的

10
00:00:26,760 --> 00:00:29,280
就是很多写教科书的人用了我们的图

11
00:00:30,600 --> 00:00:33,679
如果你想用的话

12
00:00:33,679 --> 00:00:34,280
你可以去看一下

13
00:00:34,280 --> 00:00:35,560
我们原代码图放在哪里

14
00:00:35,679 --> 00:00:38,079
放在Graph里面

15
00:00:38,079 --> 00:00:41,320
就是这些都是我们的原始图的那些文件

16
00:00:41,359 --> 00:00:43,920
用的是一个叫Ormanly Graph的一个软件化的

17
00:00:47,600 --> 00:00:49,920
encoder的输出和decoder的输入

18
00:00:49,920 --> 00:00:52,679
拼接和安慰相加起来有区别吗

19
00:00:53,600 --> 00:00:54,519
你不能安慰加

20
00:00:54,519 --> 00:00:57,280
是因为你的输入

21
00:00:57,679 --> 00:00:59,560
就你的decoder的输入

22
00:00:59,560 --> 00:01:01,960
它是它的embedding size

23
00:01:02,399 --> 00:01:04,480
然后你的encoder输出是hidden size

24
00:01:04,680 --> 00:01:05,920
我们现在是取的是一样

25
00:01:05,920 --> 00:01:07,159
但实际上可能是不一样

26
00:01:07,159 --> 00:01:07,920
所以你不能相加

27
00:01:07,920 --> 00:01:09,000
因为长度不一样

28
00:01:09,079 --> 00:01:10,400
所以你拼的是最好的

29
00:01:12,599 --> 00:01:14,640
embedding是用word2vec做的

30
00:01:14,640 --> 00:01:15,079
不是的

31
00:01:15,200 --> 00:01:18,040
embedding我们就不准备讲word2vec了

32
00:01:18,040 --> 00:01:21,359
有的东西现在用的也不怎么太多用

33
00:01:22,400 --> 00:01:24,400
所以基本都用BERT了

34
00:01:24,799 --> 00:01:26,719
所以我们就跳过了word2vec

35
00:01:26,760 --> 00:01:27,599
它不是用word2vec

36
00:01:27,599 --> 00:01:29,760
它就是从头开始训练的

37
00:01:30,120 --> 00:01:30,920
就没有用

38
00:01:30,920 --> 00:01:33,959
我们也没有用运训练的这种东西

39
00:01:33,959 --> 00:01:35,359
我们embedding是一个

40
00:01:35,359 --> 00:01:36,120
随机输出化

41
00:01:36,120 --> 00:01:37,519
重新开始训练的东西

42
00:01:39,560 --> 00:01:42,000
encoder的output和decoder的output

43
00:01:42,000 --> 00:01:43,519
是不是都是预测

44
00:01:43,640 --> 00:01:45,159
encoder的output你没预测

45
00:01:45,280 --> 00:01:46,879
你encoder的output没意义

46
00:01:46,879 --> 00:01:48,680
就是说因为你没有loss

47
00:01:48,680 --> 00:01:51,039
我拿不出encoder

48
00:01:51,039 --> 00:01:52,120
当然你可以说

49
00:01:52,120 --> 00:01:53,799
encoder可以预测下一个句子

50
00:01:53,799 --> 00:01:54,960
下一个词长什么样

51
00:01:54,960 --> 00:01:55,960
但是没有太多意义

52
00:01:57,079 --> 00:01:58,079
因为一般来说

53
00:01:58,079 --> 00:01:59,159
你一般来说

54
00:01:59,159 --> 00:02:00,679
你可以用一个language model

55
00:02:00,679 --> 00:02:01,640
做预训练

56
00:02:01,640 --> 00:02:02,560
我们之后会讲

57
00:02:02,640 --> 00:02:04,239
就BERT做function怎么做

58
00:02:08,120 --> 00:02:10,280
就decoder是预测的发育

59
00:02:10,280 --> 00:02:11,479
就decoder对

60
00:02:11,479 --> 00:02:12,520
decoder就是

61
00:02:12,560 --> 00:02:13,800
因为在真实情况下

62
00:02:13,800 --> 00:02:15,400
我只给你英语不给你发育

63
00:02:15,520 --> 00:02:16,800
所以你要去给个运输

64
00:02:16,800 --> 00:02:17,840
decoder全部

65
00:02:17,840 --> 00:02:19,000
decoder都是预测

66
00:02:19,199 --> 00:02:21,680
在应用的时候

67
00:02:23,680 --> 00:02:24,680
valid length

68
00:02:24,719 --> 00:02:25,919
两个都是valid length

69
00:02:27,400 --> 00:02:28,080
valid length

70
00:02:28,080 --> 00:02:30,840
就是你句子实际是多长的

71
00:02:31,240 --> 00:02:32,280
这不是选择的

72
00:02:32,280 --> 00:02:34,039
它是valid length

73
00:02:34,039 --> 00:02:35,719
就是你发育句子有多长

74
00:02:36,400 --> 00:02:37,199
它不是选的

75
00:02:37,199 --> 00:02:38,919
它是原始句子有多长

76
00:02:39,000 --> 00:02:40,639
这样子我要存下来

77
00:02:40,639 --> 00:02:43,439
因为后面那些都是是padding

78
00:02:43,439 --> 00:02:45,360
成的

79
00:02:45,800 --> 00:02:46,879
pad的符号

80
00:02:46,879 --> 00:02:48,520
那个东西我是不去管的

81
00:02:48,520 --> 00:02:50,079
就算loss的时候

82
00:02:50,079 --> 00:02:51,199
我是不要去看的

83
00:02:51,240 --> 00:02:52,879
以及我们之后做attention的时候

84
00:02:52,879 --> 00:02:55,240
我就是不去看那些是pad的

85
00:02:55,240 --> 00:02:55,840
那些东西

86
00:02:55,840 --> 00:02:57,879
只看你原始句子长什么样子

87
00:03:00,360 --> 00:03:01,920
number of hindrances32

88
00:03:02,000 --> 00:03:02,640
为什么那么的

89
00:03:02,640 --> 00:03:03,560
没什么为什么

90
00:03:03,560 --> 00:03:04,800
就是跟你的hinder size

91
00:03:04,800 --> 00:03:05,319
为什么

92
00:03:05,480 --> 00:03:07,120
就是跟你的mlp

93
00:03:07,120 --> 00:03:08,159
你的number of hindrances

94
00:03:08,159 --> 00:03:09,560
要定成32什么意思

95
00:03:13,199 --> 00:03:15,360
现在sequence to sequence

96
00:03:15,360 --> 00:03:16,400
都用transformer

97
00:03:16,400 --> 00:03:18,080
实现了RNN和iOS10

98
00:03:18,080 --> 00:03:19,360
还有什么应用场景

99
00:03:19,640 --> 00:03:20,719
你这个问题问的挺好的

100
00:03:21,640 --> 00:03:22,000
是的

101
00:03:22,000 --> 00:03:23,080
sequence to sequence

102
00:03:23,080 --> 00:03:24,080
可以存transformer

103
00:03:24,080 --> 00:03:26,200
我们之后会过两天会讲

104
00:03:26,480 --> 00:03:29,840
但是你也别说RNN和iOS10

105
00:03:29,840 --> 00:03:30,280
有什么用

106
00:03:30,400 --> 00:03:32,000
虽然现在是说

107
00:03:33,280 --> 00:03:34,040
这是潮流

108
00:03:34,160 --> 00:03:36,240
就是说现在说我不需要RNN

109
00:03:36,439 --> 00:03:38,080
我就直接用transformer了

110
00:03:38,080 --> 00:03:40,080
你但是你可能过一过的

111
00:03:40,080 --> 00:03:41,680
过个一两个月又回来

112
00:03:41,680 --> 00:03:43,240
就大家又发现RNN好

113
00:03:43,840 --> 00:03:44,160
对吧

114
00:03:44,160 --> 00:03:46,760
这个东西就是深度学习

115
00:03:46,880 --> 00:03:47,880
就是一波又一波

116
00:03:47,880 --> 00:03:50,480
然后现在还不是说

117
00:03:50,720 --> 00:03:52,000
我说我卷机都不用了

118
00:03:52,000 --> 00:03:53,160
我就用mlp了

119
00:03:53,160 --> 00:03:53,680
对吧

120
00:03:54,480 --> 00:03:55,960
都是操作

121
00:03:57,400 --> 00:03:58,040
就是说

122
00:04:00,040 --> 00:04:01,960
随着大家的研究的

123
00:04:01,960 --> 00:04:03,040
不断的进步

124
00:04:03,240 --> 00:04:04,960
就是说你突然突然跑过来说

125
00:04:05,080 --> 00:04:06,200
我发现我transformer

126
00:04:06,200 --> 00:04:07,439
调出来效果很好

127
00:04:07,439 --> 00:04:09,000
然后我就不需要RNN了

128
00:04:09,040 --> 00:04:10,200
但是有可能过一阵子

129
00:04:10,200 --> 00:04:11,160
又人跑过来说

130
00:04:11,160 --> 00:04:12,240
RNN调出来效果

131
00:04:12,240 --> 00:04:13,120
比transformer更好

132
00:04:13,120 --> 00:04:14,439
所以我还是用RNN

133
00:04:14,480 --> 00:04:15,120
和这东西

134
00:04:15,880 --> 00:04:16,520
就是

135
00:04:17,680 --> 00:04:20,319
基本上大家喜欢这种

136
00:04:20,800 --> 00:04:23,120
为了夸一下自己的论文

137
00:04:23,120 --> 00:04:24,000
有多厉害

138
00:04:24,000 --> 00:04:26,519
所以我需要制造噱头

139
00:04:26,720 --> 00:04:27,800
我写了一篇transformer

140
00:04:27,800 --> 00:04:29,160
我就会去狂吹

141
00:04:29,160 --> 00:04:30,040
transformer不需要了

142
00:04:30,040 --> 00:04:30,560
因为这样子

143
00:04:30,560 --> 00:04:31,680
你大家都来看我的论文

144
00:04:33,079 --> 00:04:34,759
如果我过两天我写了篇RNN

145
00:04:34,759 --> 00:04:36,800
发现能够把transformer

146
00:04:36,800 --> 00:04:37,279
干赢了

147
00:04:37,279 --> 00:04:38,839
那就是RNNis all your need

148
00:04:38,839 --> 00:04:39,120
对吧

149
00:04:39,120 --> 00:04:40,240
我就说你们都错了

150
00:04:40,240 --> 00:04:42,319
我发现RNN还是很好的

151
00:04:42,360 --> 00:04:43,240
都是噱头

152
00:04:43,800 --> 00:04:45,759
就是说这不是做研究的

153
00:04:45,759 --> 00:04:46,759
学术的状态

154
00:04:46,879 --> 00:04:47,879
学术状态是

155
00:04:47,920 --> 00:04:49,959
我们会大家在transformer

156
00:04:49,959 --> 00:04:50,920
就大概给大家讲一下

157
00:04:50,920 --> 00:04:52,680
就是他们之间区别是什么东西

158
00:04:54,519 --> 00:04:57,399
我用BERT XLNet的模型

159
00:04:57,399 --> 00:04:57,959
都学习过

160
00:04:57,959 --> 00:04:59,439
但完全不知道开过怎么做

161
00:04:59,439 --> 00:05:05,759
这个论文是说

162
00:05:05,759 --> 00:05:07,000
我道理我都懂

163
00:05:07,000 --> 00:05:08,279
我就是过不好这一生

164
00:05:08,279 --> 00:05:08,639
对吧

165
00:05:08,639 --> 00:05:12,800
就是说

166
00:05:12,800 --> 00:05:13,560
所以这种话

167
00:05:13,560 --> 00:05:15,079
你知道你学过了

168
00:05:15,079 --> 00:05:16,879
和你会用还是有区别的

169
00:05:17,519 --> 00:05:19,000
我们这个课还是说

170
00:05:19,000 --> 00:05:20,000
基本上会告诉你

171
00:05:20,000 --> 00:05:21,199
对这是什么

172
00:05:21,199 --> 00:05:22,279
和大概怎么用

173
00:05:22,920 --> 00:05:25,040
所以我们还算是

174
00:05:25,640 --> 00:05:26,760
两边都做

175
00:05:26,920 --> 00:05:27,520
就是说

176
00:05:28,200 --> 00:05:29,200
就是说你学一个东西

177
00:05:29,200 --> 00:05:29,760
就三个东西

178
00:05:29,880 --> 00:05:30,520
what

179
00:05:30,560 --> 00:05:31,720
how和why

180
00:05:31,720 --> 00:05:33,160
就what是什么东西

181
00:05:33,200 --> 00:05:33,920
怎么用它

182
00:05:33,920 --> 00:05:34,880
为什么是这么

183
00:05:35,280 --> 00:05:36,680
我们基本上

184
00:05:36,680 --> 00:05:38,520
what和how还是讲的比较多

185
00:05:38,680 --> 00:05:39,560
两个一半一半

186
00:05:39,600 --> 00:05:41,080
我们先讲slides上

187
00:05:41,080 --> 00:05:41,560
我讲what

188
00:05:41,560 --> 00:05:43,560
how就是基本上讲讲code

189
00:05:43,720 --> 00:05:45,840
然后给大家劝劝模型

190
00:05:45,920 --> 00:05:47,360
以后搞搞竞赛

191
00:05:47,800 --> 00:05:48,520
why的东西

192
00:05:48,520 --> 00:05:49,440
我们讲了一些些

193
00:05:49,600 --> 00:05:49,920
就是说

194
00:05:49,920 --> 00:05:50,720
但是这不好讲

195
00:05:50,720 --> 00:05:52,200
因为确实没那么多

196
00:05:52,199 --> 00:05:52,639
那么多why

197
00:05:52,680 --> 00:05:53,000
就是说

198
00:05:53,000 --> 00:05:53,839
我跟你讲的why

199
00:05:53,839 --> 00:05:54,920
可能过两天就不是了

200
00:05:54,920 --> 00:05:55,959
就是被打脸

201
00:05:56,399 --> 00:05:58,439
所以这个东西就真的

202
00:05:59,240 --> 00:06:00,639
没有很正常

203
00:06:00,839 --> 00:06:01,759
不要着急

204
00:06:02,000 --> 00:06:02,879
就是反正慢慢学

205
00:06:05,079 --> 00:06:05,879
问题11

206
00:06:05,879 --> 00:06:07,199
实际句子的长度

207
00:06:07,199 --> 00:06:08,240
超过了设定句子

208
00:06:08,240 --> 00:06:09,319
是不是截掉不用了

209
00:06:09,319 --> 00:06:10,199
还是放到下一个句子

210
00:06:10,199 --> 00:06:11,599
是截掉不用了

211
00:06:11,599 --> 00:06:12,839
你不能放到下一个句子

212
00:06:12,839 --> 00:06:13,519
因为下一个句子

213
00:06:13,519 --> 00:06:15,240
是另外一个跟你不一样的句子

214
00:06:15,680 --> 00:06:16,240
翻译

215
00:06:16,439 --> 00:06:17,000
翻译

216
00:06:17,039 --> 00:06:19,839
我们翻译通常不是一篇文章

217
00:06:19,839 --> 00:06:20,639
这么翻下来

218
00:06:20,639 --> 00:06:22,439
翻译的每个句子是随机的

219
00:06:22,599 --> 00:06:24,000
所以你不能放到下一个

220
00:06:25,560 --> 00:06:26,319
通常来讲

221
00:06:26,479 --> 00:06:29,639
你取一个相对来说

222
00:06:29,639 --> 00:06:30,759
比较长的就行了

223
00:06:30,759 --> 00:06:32,039
所以让你截掉的

224
00:06:32,039 --> 00:06:33,159
不要太多就行了

225
00:06:33,199 --> 00:06:34,279
比如说我们这取10

226
00:06:34,439 --> 00:06:35,519
因为我们刚刚算了

227
00:06:35,519 --> 00:06:36,319
histogram

228
00:06:36,360 --> 00:06:37,759
我们在搞这个data set的时候

229
00:06:37,759 --> 00:06:38,479
给大家画了一下

230
00:06:38,479 --> 00:06:39,079
就是翻译

231
00:06:39,079 --> 00:06:40,399
基本上都在10这边

232
00:06:40,479 --> 00:06:40,759
对吧

233
00:06:40,759 --> 00:06:41,959
基本上都小于10

234
00:06:42,599 --> 00:06:43,919
所以我们用了一个10

235
00:06:48,319 --> 00:06:49,279
问题12

236
00:06:49,480 --> 00:06:53,200
decoder的输入和最终预测的输出

237
00:06:53,200 --> 00:06:55,320
一定是同一个东西吗

238
00:06:55,320 --> 00:06:56,560
还是两个可以不同

239
00:06:56,680 --> 00:06:58,000
如果不同怎么理解

240
00:07:02,840 --> 00:07:03,640
可以不同

241
00:07:03,800 --> 00:07:04,560
真的可以不同

242
00:07:04,560 --> 00:07:07,080
就是说我没有说decoder的

243
00:07:07,080 --> 00:07:09,440
一定是要预测自己

244
00:07:11,600 --> 00:07:12,120
比如说

245
00:07:16,800 --> 00:07:17,800
你让我举个例子

246
00:07:17,800 --> 00:07:19,160
我还不一定真能

247
00:07:20,200 --> 00:07:23,160
举个例子

248
00:07:23,160 --> 00:07:24,840
你让我举个具体说什么

249
00:07:24,840 --> 00:07:25,640
不一样的例子

250
00:07:25,640 --> 00:07:27,240
我还真不一定能举出来

251
00:07:34,200 --> 00:07:35,760
所以我还真不

252
00:07:35,760 --> 00:07:37,320
就是说理论上是可以

253
00:07:37,320 --> 00:07:38,800
但实际上好像我也举不出来

254
00:07:38,800 --> 00:07:39,160
这个例子

255
00:07:39,160 --> 00:07:40,160
就是我能举出来例子

256
00:07:40,160 --> 00:07:41,680
就是说你的in code

257
00:07:41,680 --> 00:07:42,880
可以不是rn

258
00:07:42,920 --> 00:07:43,840
就是说你可以

259
00:07:43,840 --> 00:07:45,080
比如说我要做

260
00:07:45,600 --> 00:07:47,600
图片生成文字

261
00:07:48,600 --> 00:07:49,920
就给一张图片

262
00:07:49,920 --> 00:07:51,120
我要把它caption生出来

263
00:07:51,120 --> 00:07:52,160
或者是说

264
00:07:52,200 --> 00:07:54,120
给个图片

265
00:07:54,120 --> 00:07:56,400
把你周围那一堆文字生成出来

266
00:07:56,400 --> 00:07:58,320
那么你的in code

267
00:07:58,320 --> 00:07:59,400
就是一个CNN

268
00:07:59,400 --> 00:07:59,920
对吧

269
00:08:00,879 --> 00:08:02,480
你总不能用rn做图片

270
00:08:02,480 --> 00:08:03,840
rn做图片不是那么好做

271
00:08:03,840 --> 00:08:05,320
所以你通常in code

272
00:08:05,320 --> 00:08:06,280
可以是个很不一样的东西

273
00:08:06,280 --> 00:08:07,280
你可以不是rn

274
00:08:07,320 --> 00:08:10,280
但是确实decoder

275
00:08:10,280 --> 00:08:12,000
但你反过来讲

276
00:08:12,000 --> 00:08:13,720
我被带去了

277
00:08:13,760 --> 00:08:16,040
decoder不一定是sequence

278
00:08:16,080 --> 00:08:17,680
decoder可以是生成图片

279
00:08:18,080 --> 00:08:19,480
给文字生成图片

280
00:08:19,480 --> 00:08:20,879
那么我的decoder

281
00:08:20,879 --> 00:08:22,800
当然就是直接生成图片了

282
00:08:22,800 --> 00:08:23,439
那就不需要

283
00:08:23,439 --> 00:08:24,240
我都不需要输入了

284
00:08:24,240 --> 00:08:24,680
对吧

285
00:08:24,840 --> 00:08:26,040
我给文字生成图片

286
00:08:26,040 --> 00:08:26,960
你像最近

287
00:08:27,840 --> 00:08:29,640
OpenAI做的工作就是这样子

288
00:08:29,640 --> 00:08:31,000
给我一个东西

289
00:08:31,000 --> 00:08:32,120
就可以生成一个东西

290
00:08:32,639 --> 00:08:33,920
那就是说你的decoder

291
00:08:33,920 --> 00:08:35,240
就是一个CNN

292
00:08:35,519 --> 00:08:36,440
就假设你是说

293
00:08:36,440 --> 00:08:37,879
如果decoder是rn的话

294
00:08:37,879 --> 00:08:39,759
好像还真的就是

295
00:08:39,759 --> 00:08:41,080
你把你的句子

296
00:08:41,240 --> 00:08:42,680
作为你的输入输进来

