1
00:00:00,000 --> 00:00:05,080
好 我们的问题是说

2
00:00:05,080 --> 00:00:06,600
第一个问题是说

3
00:00:06,600 --> 00:00:12,400
请问可以在colab上安装D2L进行环境片配置吗

4
00:00:12,400 --> 00:00:13,839
可以的

5
00:00:13,839 --> 00:00:17,800
就是说你在colab上pip install D2L就行了

6
00:00:17,800 --> 00:00:18,719
就是可以装的

7
00:00:18,719 --> 00:00:20,839
就是你在每一张理论上

8
00:00:20,839 --> 00:00:21,800
在每一张前面

9
00:00:21,800 --> 00:00:22,679
我们有个感叹号

10
00:00:22,679 --> 00:00:23,559
pip install D2L

11
00:00:23,559 --> 00:00:25,000
你是可以运行的

12
00:00:25,320 --> 00:00:27,320
或者说推荐一下

13
00:00:27,320 --> 00:00:29,280
对学生党比较便宜的平台

14
00:00:29,600 --> 00:00:33,960
如果你能够访问colab的话

15
00:00:33,960 --> 00:00:35,920
我就不说怎么访问

16
00:00:35,920 --> 00:00:38,400
colab还是不错的

17
00:00:38,600 --> 00:00:42,480
因为它的话就是说比较简单

18
00:00:42,480 --> 00:00:45,280
国内的话我来看一看

19
00:00:45,280 --> 00:00:46,320
我们现在还好

20
00:00:46,320 --> 00:00:47,920
我们现在没有用特别复杂的东西

21
00:00:47,920 --> 00:00:50,240
我这是说之后要用GPU的话

22
00:00:50,240 --> 00:00:51,240
确实会麻烦一点

23
00:00:51,240 --> 00:00:53,600
大家不一定要自己吃

24
00:00:53,600 --> 00:00:54,680
装一个GPU

25
00:00:54,680 --> 00:00:56,840
这个比较现在买不到

26
00:00:56,840 --> 00:00:58,920
现在被挖矿的人抢光了

27
00:00:59,240 --> 00:01:00,520
买GPU比较难

28
00:01:00,520 --> 00:01:02,560
国内的话云的话

29
00:01:02,560 --> 00:01:03,679
AW有

30
00:01:03,679 --> 00:01:04,640
阿里也有

31
00:01:04,640 --> 00:01:05,760
腾讯应该也有

32
00:01:05,760 --> 00:01:07,400
我再看一看

33
00:01:07,400 --> 00:01:07,879
好吧

34
00:01:07,879 --> 00:01:10,840
就是说云应该是一个不错的选择

35
00:01:10,840 --> 00:01:13,079
但是用云关键是不要忘了关机

36
00:01:13,079 --> 00:01:17,560
第二个问题是说

37
00:01:17,680 --> 00:01:20,120
为什么使用平方损失

38
00:01:20,120 --> 00:01:21,840
而不是用绝对差值

39
00:01:21,840 --> 00:01:25,000
其实没有太多的

40
00:01:25,359 --> 00:01:26,240
怎么说

41
00:01:26,400 --> 00:01:27,840
我们之后会讲一下

42
00:01:27,840 --> 00:01:31,320
平方损失和绝对差值之间的区别

43
00:01:31,359 --> 00:01:32,840
但区别不大

44
00:01:32,880 --> 00:01:35,200
平方损失最早大家用平方损失

45
00:01:35,200 --> 00:01:36,359
是因为绝对差值

46
00:01:36,359 --> 00:01:38,680
它是一个不可不可倒的函数

47
00:01:39,040 --> 00:01:41,800
它是一个单纽subgradient

48
00:01:41,800 --> 00:01:44,320
我们之前在矩阵计算里面

49
00:01:44,320 --> 00:01:45,600
应该讲过这个事情

50
00:01:45,600 --> 00:01:46,879
就在零点的时候

51
00:01:46,879 --> 00:01:47,960
你绝对差值

52
00:01:47,960 --> 00:01:50,320
它的导势会有一点点比较难求

53
00:01:50,480 --> 00:01:52,879
但是说实际上来说

54
00:01:52,879 --> 00:01:54,080
我们之后会讲一下区别

55
00:01:54,080 --> 00:01:55,680
但实际上没什么太多区别

56
00:01:55,680 --> 00:01:57,000
就是说你用绝对差值

57
00:01:57,000 --> 00:01:59,120
或者用平方损失都问题不大

58
00:02:00,560 --> 00:02:02,359
第三个问题是损失

59
00:02:02,359 --> 00:02:04,480
为什么要求平均

60
00:02:05,280 --> 00:02:07,280
就是说你可以不求平均没关系了

61
00:02:07,319 --> 00:02:08,199
就是说

62
00:02:08,240 --> 00:02:10,400
你求平均和不求平均

63
00:02:10,400 --> 00:02:11,879
本质上没有关系

64
00:02:11,879 --> 00:02:13,400
就是说数值是等价的

65
00:02:13,400 --> 00:02:14,840
就是说你求了平均

66
00:02:14,840 --> 00:02:15,599
你的梯度

67
00:02:15,599 --> 00:02:23,080
那就是在一个样本和scale上面

68
00:02:23,080 --> 00:02:24,400
你不求平均的话

69
00:02:24,400 --> 00:02:26,039
你的梯度的数值会比较大

70
00:02:26,039 --> 00:02:26,879
那就是说

71
00:02:27,039 --> 00:02:28,919
因为你的除以n

72
00:02:28,919 --> 00:02:30,199
在损失上除以n

73
00:02:30,199 --> 00:02:33,039
就等价于我的梯度也除以了n

74
00:02:33,919 --> 00:02:34,840
所以说

75
00:02:35,159 --> 00:02:39,039
你如果你反正是用随机梯度下降的话

76
00:02:39,039 --> 00:02:40,959
那么你如果不除以n的话

77
00:02:40,959 --> 00:02:43,799
你就把学习率除个n就是了

78
00:02:44,519 --> 00:02:46,079
我给大家演示一下

79
00:02:49,519 --> 00:02:50,280
就是说这个值

80
00:02:51,399 --> 00:02:53,719
假设我的L里面没有除个n

81
00:02:54,120 --> 00:02:55,719
那么这一块值梯度

82
00:02:55,719 --> 00:02:58,680
就会变成以前的n倍大小

83
00:02:59,000 --> 00:03:01,159
那么你想得到之前一样的

84
00:03:01,159 --> 00:03:02,039
这一块的话

85
00:03:02,039 --> 00:03:04,120
你就把你的学习率除个n就行了

86
00:03:04,120 --> 00:03:05,759
就是你想把n除在这个地方

87
00:03:05,759 --> 00:03:06,439
除在那个地方

88
00:03:06,439 --> 00:03:07,560
其实本质上没区别

89
00:03:07,599 --> 00:03:10,199
或者是说学习率是怎么选的

90
00:03:10,199 --> 00:03:11,319
使得我的梯

91
00:03:11,360 --> 00:03:13,159
使得整个这一块不要太大

92
00:03:13,159 --> 00:03:14,000
也不要太小

93
00:03:14,919 --> 00:03:17,639
所以是说除以n的好处是说

94
00:03:17,639 --> 00:03:19,719
我不管你样本

95
00:03:19,719 --> 00:03:21,000
不管你的梯量有多大

96
00:03:21,000 --> 00:03:21,840
或样本有多大

97
00:03:21,840 --> 00:03:24,200
我的梯度的值都差不多

98
00:03:24,240 --> 00:03:25,759
使得我调学习率的时候

99
00:03:25,759 --> 00:03:26,520
比较好调

100
00:03:26,520 --> 00:03:28,120
就是我不管你的梯量大小了

101
00:03:28,120 --> 00:03:29,520
就是比较节省

102
00:03:29,560 --> 00:03:31,120
但如果你没有除n的话

103
00:03:31,159 --> 00:03:33,280
那你就记得把学习率除了就行了

104
00:03:33,280 --> 00:03:34,159
所以没关系的

105
00:03:37,199 --> 00:03:39,480
线性回归损失函数

106
00:03:39,480 --> 00:03:41,240
是不是通常都是MSC

107
00:03:41,240 --> 00:03:41,840
就是MSC

108
00:03:41,840 --> 00:03:42,840
就是我们之前说的

109
00:03:44,520 --> 00:03:44,920
是的

110
00:03:44,920 --> 00:03:46,800
一般都是这样子的

111
00:03:49,520 --> 00:03:50,800
关于神经网络

112
00:03:50,800 --> 00:03:53,920
我们是通过误差反馈修改参数

113
00:03:53,960 --> 00:03:55,360
但神经元没有

114
00:03:55,400 --> 00:03:57,439
反馈误差这一说

115
00:03:57,439 --> 00:03:59,640
请问这一点是人为推导的吗

116
00:04:00,400 --> 00:04:01,800
这个是一个挺好玩的问题

117
00:04:01,800 --> 00:04:02,760
就是说

118
00:04:02,800 --> 00:04:04,520
首先我们提到两件事情

119
00:04:04,680 --> 00:04:05,800
一件事情是说

120
00:04:05,800 --> 00:04:08,560
现在其实大家对于神经网络

121
00:04:08,560 --> 00:04:09,600
对于跟人的神经

122
00:04:09,600 --> 00:04:10,800
没什么太多区别

123
00:04:11,160 --> 00:04:12,480
第二个事情是说

124
00:04:12,520 --> 00:04:14,280
确实误差反传

125
00:04:14,280 --> 00:04:16,439
是跟神经元有一定的关系

126
00:04:16,439 --> 00:04:17,560
在神经元里面

127
00:04:17,560 --> 00:04:18,840
大家可以去搜一个

128
00:04:18,880 --> 00:04:20,200
叫做赫本的

129
00:04:20,240 --> 00:04:23,680
叫做赫普的一个定理

130
00:04:23,680 --> 00:04:24,480
就是说

131
00:04:25,040 --> 00:04:26,480
它其实是说

132
00:04:26,520 --> 00:04:28,800
神经元还是会根据你的负反馈

133
00:04:28,800 --> 00:04:30,760
进行一些预计的调整

134
00:04:30,760 --> 00:04:31,840
就是说它的调整

135
00:04:31,840 --> 00:04:33,640
是通过化学元素

136
00:04:33,640 --> 00:04:35,960
神经元放电不放电的调整

137
00:04:35,960 --> 00:04:37,520
它是有一定点的关系

138
00:04:37,560 --> 00:04:39,600
我们之后会讲到一个模型

139
00:04:39,600 --> 00:04:41,400
叫做感知机

140
00:04:41,440 --> 00:04:44,120
它其实是来自于神经网络的一个

141
00:04:44,120 --> 00:04:44,960
我给大家讲一下

142
00:04:44,960 --> 00:04:46,640
它来自神经网络

143
00:04:46,639 --> 00:04:50,199
它本质上等价是一个T2下降

144
00:04:50,199 --> 00:04:52,000
我们在下一节会讲到

145
00:04:54,439 --> 00:04:55,519
在物理实验中

146
00:04:55,519 --> 00:04:58,000
通常使用n-1T的n求误差

147
00:04:58,000 --> 00:04:59,120
请问这里求误差

148
00:04:59,120 --> 00:05:00,519
也能n-1T的n吗

149
00:05:00,879 --> 00:05:02,159
就你用谁都没关系

150
00:05:02,159 --> 00:05:03,919
就是说对损失来话

151
00:05:03,919 --> 00:05:05,839
你需要最小化损失

152
00:05:05,839 --> 00:05:08,199
我不关心损失值是多少

153
00:05:08,199 --> 00:05:09,079
我都要最小化

154
00:05:09,079 --> 00:05:10,560
所以跟之前提到的一样

155
00:05:10,560 --> 00:05:11,879
你出个谁都没关系

156
00:05:11,879 --> 00:05:13,759
就是说最后你就血续率

157
00:05:13,759 --> 00:05:14,680
调一调就行了

158
00:05:17,120 --> 00:05:18,439
第7个问题是说

159
00:05:18,439 --> 00:05:19,759
不管是T2下降

160
00:05:19,759 --> 00:05:21,039
还是随机T2下降

161
00:05:21,039 --> 00:05:22,879
怎么找到合适的学习率

162
00:05:22,919 --> 00:05:24,319
有什么好办法吗

163
00:05:26,519 --> 00:05:27,560
有两个办法

164
00:05:27,599 --> 00:05:28,240
第一个是说

165
00:05:28,240 --> 00:05:30,159
你找到一个

166
00:05:30,199 --> 00:05:33,639
对学习率不那么敏感的算法

167
00:05:33,639 --> 00:05:34,879
比如说我们之后会讲的

168
00:05:34,879 --> 00:05:37,599
这种比较smooth的一些T2下降

169
00:05:37,639 --> 00:05:40,079
就是说对学习率不那么下降

170
00:05:40,120 --> 00:05:41,360
敏感

171
00:05:41,399 --> 00:05:43,439
第二个是说

172
00:05:44,439 --> 00:05:48,199
你可以通过合理的参数的初始化

173
00:05:48,199 --> 00:05:50,160
使得你整个

174
00:05:50,360 --> 00:05:51,719
你学习率就去个0.1

175
00:05:51,719 --> 00:05:53,079
或者0.01就差不多了

176
00:05:53,079 --> 00:05:54,319
就是说你一开始不要

177
00:05:54,319 --> 00:05:55,959
我们之后会讲数值稳定性

178
00:05:56,079 --> 00:05:57,720
会给大家解释一下

179
00:05:58,120 --> 00:06:00,639
然后还有一个办法是说

180
00:06:01,839 --> 00:06:04,560
有这样子的找学习率的方法

181
00:06:04,560 --> 00:06:06,480
我们可能我们课程

182
00:06:06,480 --> 00:06:08,120
这一次没有安排讲这个事情

183
00:06:08,120 --> 00:06:09,279
我大概想办法

184
00:06:09,279 --> 00:06:11,600
在我们的优化算法

185
00:06:11,600 --> 00:06:13,120
那一节插进来

186
00:06:13,120 --> 00:06:13,879
给大家说

187
00:06:13,920 --> 00:06:15,640
怎么样去调一个

188
00:06:15,640 --> 00:06:16,680
就很快的方法

189
00:06:16,680 --> 00:06:18,600
找到一个还合适的一个区间

190
00:06:18,640 --> 00:06:19,520
我们想办法

191
00:06:19,720 --> 00:06:21,200
大家如果我忘了的话

192
00:06:21,200 --> 00:06:22,400
记得提醒一下我

193
00:06:26,560 --> 00:06:28,520
So batch size是否会影响

194
00:06:28,520 --> 00:06:29,960
最终模型的结果

195
00:06:31,080 --> 00:06:34,600
其实你过小是好的

196
00:06:34,640 --> 00:06:35,879
大了不行

197
00:06:35,879 --> 00:06:37,840
就是说可能反思直觉

198
00:06:39,160 --> 00:06:40,680
就我们之后可能会讲到

199
00:06:41,600 --> 00:06:43,400
丢弃法就dropout的时候

200
00:06:43,400 --> 00:06:44,879
就是说你的

201
00:06:45,120 --> 00:06:46,760
batch size小

202
00:06:46,760 --> 00:06:49,160
其实对你在同样的计算

203
00:06:49,160 --> 00:06:50,160
就是说我扫数据

204
00:06:50,160 --> 00:06:51,600
扫10遍的情况下

205
00:06:51,720 --> 00:06:52,840
batch size越小

206
00:06:52,840 --> 00:06:54,680
其实是对收敛越好

207
00:06:55,360 --> 00:06:56,760
为什么是因为

208
00:06:57,320 --> 00:06:58,560
随机梯度下降

209
00:06:58,560 --> 00:07:00,800
理论上是说给你带来了噪音

210
00:07:02,000 --> 00:07:04,000
就是说每一次他其实在

211
00:07:04,000 --> 00:07:05,600
因为你采样的样本越小

212
00:07:05,600 --> 00:07:06,520
你的噪音越多

213
00:07:06,520 --> 00:07:08,000
因为我有100万个样本

214
00:07:08,000 --> 00:07:09,320
每次采样两张图片

215
00:07:09,320 --> 00:07:10,480
两个样本的话

216
00:07:10,520 --> 00:07:11,520
那么噪音就比较大

217
00:07:11,519 --> 00:07:13,919
就跟真实的方向肯定差很远

218
00:07:14,039 --> 00:07:16,639
但是噪音对神经网络是

219
00:07:17,560 --> 00:07:18,519
是一件好事情

220
00:07:18,680 --> 00:07:19,719
因为神经网络

221
00:07:19,719 --> 00:07:22,079
现在深度神经网络都太复杂了

222
00:07:22,560 --> 00:07:25,000
一定的噪音使得你不会走偏

223
00:07:26,240 --> 00:07:27,519
就是说大家说

224
00:07:27,519 --> 00:07:28,399
你教小孩的时候

225
00:07:28,399 --> 00:07:29,879
不要一直夸他对吧

226
00:07:30,159 --> 00:07:32,879
你也需要你也操一点

227
00:07:32,879 --> 00:07:34,680
其实对小孩不见得是件坏事情

228
00:07:34,680 --> 00:07:37,399
就是更所谓的更路棒

229
00:07:37,399 --> 00:07:38,799
就是说对于各种噪音

230
00:07:38,799 --> 00:07:40,399
你的容忍度越好

231
00:07:40,440 --> 00:07:42,600
你可能你整个模型的繁华性更好

232
00:07:42,600 --> 00:07:42,800
好

233
00:07:42,800 --> 00:07:43,800
我们还没讲繁华性

234
00:07:46,840 --> 00:07:48,760
在过拟核和欠拟核的情况下

235
00:07:48,760 --> 00:07:50,800
学习率和批次应该怎么调整

236
00:07:50,840 --> 00:07:51,960
理论上来说

237
00:07:51,960 --> 00:07:54,040
学习率和批量大小

238
00:07:54,040 --> 00:07:55,560
理论上是不会影响到

239
00:07:55,560 --> 00:07:57,880
最后的一个收敛结果

240
00:07:57,920 --> 00:07:59,000
当然是说实际上

241
00:07:59,000 --> 00:08:01,080
因为我们不能完全纠结

242
00:08:01,080 --> 00:08:02,440
所以有一丁丁的影响

243
00:08:02,480 --> 00:08:03,880
但是你会想象是说

244
00:08:03,880 --> 00:08:04,880
没有那么影响

245
00:08:04,960 --> 00:08:07,800
我们之前说到批量大小太大

246
00:08:07,800 --> 00:08:09,320
会导致收敛有问题

247
00:08:09,319 --> 00:08:11,680
但是只要你不是特别大

248
00:08:11,680 --> 00:08:12,319
一般来说

249
00:08:12,319 --> 00:08:13,879
就是说你就多花点时间

250
00:08:13,879 --> 00:08:15,279
最后还是能收敛的

251
00:08:17,159 --> 00:08:19,480
当然我们可能会在实际上

252
00:08:19,480 --> 00:08:20,879
给大家演示一下

253
00:08:21,199 --> 00:08:22,759
如果你真的特别大

254
00:08:22,759 --> 00:08:24,159
那就是比较难了

255
00:08:26,959 --> 00:08:28,920
针对批量大小的数据

256
00:08:28,920 --> 00:08:30,879
及进行网络训练的时候

257
00:08:30,879 --> 00:08:34,120
网络中每个参数更新减去的T度

258
00:08:34,120 --> 00:08:35,439
是batch size中

259
00:08:35,480 --> 00:08:37,480
每个样本对应参数的求和

260
00:08:37,480 --> 00:08:38,720
取得平均值吗

261
00:08:39,320 --> 00:08:39,760
是的

262
00:08:39,760 --> 00:08:41,000
就是说理论上

263
00:08:41,000 --> 00:08:42,240
它其实的工作原理

264
00:08:42,240 --> 00:08:44,520
因为T度你知道是个线性的

265
00:08:44,520 --> 00:08:45,879
就是说我的损失函数

266
00:08:45,879 --> 00:08:47,720
是一个每个样本相加

267
00:08:47,720 --> 00:08:49,840
其等价是每个样本求T度

268
00:08:49,840 --> 00:08:50,840
然后取均值

269
00:08:51,040 --> 00:08:52,280
就是说它是个线性关系

270
00:08:52,280 --> 00:08:53,040
是可以写的

271
00:08:54,640 --> 00:08:56,120
随机T度下降

272
00:08:56,120 --> 00:08:59,040
随机是指批量大小是随机的吗

273
00:08:59,160 --> 00:09:01,240
而不是批量大小是一样的

274
00:09:01,280 --> 00:09:03,120
就是说批量大小等于128

275
00:09:03,120 --> 00:09:05,040
那么就是我们之后实现会讲

276
00:09:05,160 --> 00:09:06,920
就是说每次我随机的

277
00:09:06,919 --> 00:09:09,679
在样本里面采样128个元素

278
00:09:10,319 --> 00:09:12,679
这就随机的随机采样的意思

279
00:09:12,919 --> 00:09:14,959
所以这是为什么我们要讲实现

280
00:09:15,079 --> 00:09:17,519
实现会有所有细节都会给大家讲

281
00:09:18,399 --> 00:09:19,639
在深度学习中

282
00:09:20,399 --> 00:09:22,199
设置损失函数的时候

283
00:09:22,199 --> 00:09:24,240
需要考虑正则吗

284
00:09:24,399 --> 00:09:25,879
需要的正则

285
00:09:25,879 --> 00:09:28,319
但是我们一般不放在损失函

286
00:09:28,839 --> 00:09:30,959
怎么说我们把损失函数

287
00:09:30,959 --> 00:09:32,599
就是L2的损失函数

288
00:09:32,599 --> 00:09:34,159
和正则是分开的

289
00:09:34,199 --> 00:09:35,719
我们之后会讲到正则项

290
00:09:36,000 --> 00:09:37,200
会需要这个东西

291
00:09:37,240 --> 00:09:39,000
但是也没有太多用

292
00:09:39,000 --> 00:09:40,399
就是说我们还有很多

293
00:09:40,399 --> 00:09:42,360
很多别的方法来做正则

294
00:09:45,240 --> 00:09:47,240
为什么机器学习的优化算法

295
00:09:47,240 --> 00:09:49,320
都采用梯度下降一阶倒算法

296
00:09:49,320 --> 00:09:50,960
而不用刘顿法二阶倒

297
00:09:51,240 --> 00:09:52,040
说的更快

298
00:09:52,040 --> 00:09:54,720
一般能算出一阶倒二阶倒也能算

299
00:09:55,440 --> 00:09:57,440
首先这句话不一定是对的

300
00:09:57,639 --> 00:09:59,800
二阶倒不好算很难算

301
00:10:01,920 --> 00:10:04,800
就是说不是总是能算出来的

302
00:10:04,920 --> 00:10:06,160
都算起来特别贵

303
00:10:07,400 --> 00:10:09,560
就是说你一阶倒是个向量

304
00:10:09,560 --> 00:10:10,680
二阶倒是一个梯度

305
00:10:11,000 --> 00:10:12,600
你想一想梯度是

306
00:10:12,760 --> 00:10:14,880
假设你的维度是100的话

307
00:10:15,120 --> 00:10:18,920
它的一阶梯度是一个100的向量

308
00:10:18,920 --> 00:10:20,920
它的二阶梯度就是100乘以百的矩阵

309
00:10:20,920 --> 00:10:21,720
当你很大的时候

310
00:10:21,720 --> 00:10:22,560
当你很大了

311
00:10:22,600 --> 00:10:24,320
有很多算法可以做近似

312
00:10:24,840 --> 00:10:25,640
是可以做的

313
00:10:25,640 --> 00:10:27,920
但是真正的刘顿法是做不了的

314
00:10:27,920 --> 00:10:29,840
我们可以做一点近似的刘顿法

315
00:10:30,360 --> 00:10:32,240
第二个是说为什么不用刘顿法

316
00:10:32,360 --> 00:10:32,960
就是说

317
00:10:35,680 --> 00:10:38,639
这个会涉及到比较大的一个话题

318
00:10:38,919 --> 00:10:41,519
我不知道要不要特别展开

319
00:10:41,840 --> 00:10:44,759
就是说首先大家要理解的是说

320
00:10:45,840 --> 00:10:47,560
这里面有两个模型

321
00:10:48,000 --> 00:10:49,279
一个是我的统计模型

322
00:10:49,279 --> 00:10:51,360
就是我的损失函数长什么样子

323
00:10:52,840 --> 00:10:54,399
一个是我的优化模型

324
00:10:55,000 --> 00:10:57,080
就是我用什么样的算法来求解

325
00:10:57,919 --> 00:11:01,240
这两个东西其实都是错的

326
00:11:02,039 --> 00:11:05,120
就是说我的统计模型是错的

327
00:11:05,120 --> 00:11:07,360
我优化模型也不会求到最终解

328
00:11:07,519 --> 00:11:10,200
我们当然现在是可以求最终解

329
00:11:10,200 --> 00:11:13,080
但是对一般的机器学习函数

330
00:11:13,080 --> 00:11:14,080
是求不到最终解的

331
00:11:14,440 --> 00:11:16,279
所以说两个东西都是错的

332
00:11:16,279 --> 00:11:17,279
所以说

333
00:11:19,000 --> 00:11:22,399
所以说我要把一个统计模型的最终解

334
00:11:22,399 --> 00:11:24,200
求出来意义并不大

335
00:11:24,399 --> 00:11:25,840
因为它是一个错误的模型

336
00:11:26,120 --> 00:11:27,919
你求到最终解或者不求到最终解

337
00:11:27,919 --> 00:11:28,759
都无所谓

338
00:11:29,879 --> 00:11:30,519
这是第一点

339
00:11:31,079 --> 00:11:32,759
所以是说你收敛快不快

340
00:11:32,759 --> 00:11:36,199
其实我真的不那么关心

341
00:11:36,759 --> 00:11:37,919
第二点是说

342
00:11:38,159 --> 00:11:41,240
我关心的是说你收敛到哪一个地方

343
00:11:41,919 --> 00:11:42,840
我们之后会讲到

344
00:11:42,840 --> 00:11:44,519
是说你的真实的损失函数

345
00:11:44,519 --> 00:11:47,480
是一个非常复杂的平面

346
00:11:47,759 --> 00:11:48,960
一个曲面

347
00:11:49,679 --> 00:11:51,960
你通过你是牛顿法说我收敛很快

348
00:11:51,960 --> 00:11:54,279
但是你可能找到那个点是一个

349
00:11:54,279 --> 00:11:55,759
次有解不是一个最有解

350
00:11:55,759 --> 00:11:57,799
那个点比较不平坦

351
00:11:58,280 --> 00:12:01,520
所以是说你不见得就算你收敛快

352
00:12:01,520 --> 00:12:02,680
你很快就能得了解

353
00:12:02,680 --> 00:12:05,320
不见得你最后你的训练出来的模型的

354
00:12:05,320 --> 00:12:08,320
泛化性比我用随机梯度下降走的好

355
00:12:09,880 --> 00:12:11,640
人生也是这样感叹一下

356
00:12:11,640 --> 00:12:13,760
就是说你不要每次步子迈太快

357
00:12:14,120 --> 00:12:14,680
没必要

358
00:12:14,680 --> 00:12:18,240
就是说你20岁走上人生巅峰

359
00:12:18,280 --> 00:12:19,000
so what

360
00:12:20,000 --> 00:12:21,320
你小步小步走

361
00:12:21,320 --> 00:12:22,640
你每次往前走一点

362
00:12:22,640 --> 00:12:23,840
50岁到人生巅峰

363
00:12:23,840 --> 00:12:24,800
说不定还高一点的

364
00:12:24,800 --> 00:12:25,240
对吧

365
00:12:25,920 --> 00:12:27,600
所以是说这个是玄学

366
00:12:27,600 --> 00:12:29,519
我们可以之后再来看

367
00:12:30,759 --> 00:12:33,759
所以一般来说我们虽然二阶梯度

368
00:12:33,759 --> 00:12:36,639
二阶岛算法在学术上

369
00:12:36,639 --> 00:12:38,639
确实最近一些年有很多工作

370
00:12:38,639 --> 00:12:41,399
但是它还没有到像一阶法

371
00:12:41,399 --> 00:12:42,879
那么实用的一个阶段

372
00:12:43,120 --> 00:12:44,759
计算比较复杂

373
00:12:44,759 --> 00:12:45,159
第一点

374
00:12:45,159 --> 00:12:47,360
第二点是说收敛不一定快

375
00:12:47,399 --> 00:12:49,279
但就算快结果不一定好

376
00:12:49,320 --> 00:12:52,320
所以是说大家在你有那么多的

377
00:12:52,360 --> 00:12:53,080
不好的情况下

378
00:12:53,080 --> 00:12:54,680
大家先学最简单的

379
00:12:55,200 --> 00:12:58,440
学习率怎么除以n

380
00:12:58,720 --> 00:12:59,920
设置学习率的时候

381
00:12:59,920 --> 00:13:00,960
是的

382
00:13:00,960 --> 00:13:03,080
就是说假设n等于100的话

383
00:13:03,520 --> 00:13:05,880
假设我的损失还是没有除以n

384
00:13:06,040 --> 00:13:07,600
我的学习率等于0.1

385
00:13:07,960 --> 00:13:10,920
那么如果你损失还是没有除以n的

386
00:13:10,920 --> 00:13:11,160
好

387
00:13:11,160 --> 00:13:12,840
我就把0.1除以n

388
00:13:13,080 --> 00:13:13,840
就是除以100

389
00:13:14,120 --> 00:13:15,760
我们之后会可以给大家解释一下

390
00:13:15,760 --> 00:13:17,480
对

391
00:13:17,480 --> 00:13:18,760
低太奇是干嘛

392
00:13:18,880 --> 00:13:21,040
就是刚刚我们最早有个低太奇函数

393
00:13:21,040 --> 00:13:22,440
就是说告诉你说

394
00:13:22,440 --> 00:13:25,120
我们就不要再不要算t度了

395
00:13:25,120 --> 00:13:25,920
就是说

396
00:13:26,320 --> 00:13:27,400
就是因为

397
00:13:29,520 --> 00:13:30,800
你可以认为

398
00:13:31,600 --> 00:13:33,760
就是说我要传到浪派去

399
00:13:33,760 --> 00:13:36,280
我就不要去参加自动求t度这个事情

400
00:13:36,280 --> 00:13:39,000
就是说从t度计算图里面给拿出来

401
00:13:44,440 --> 00:13:47,560
就低太奇和拍touch的低太奇用法

402
00:13:47,560 --> 00:13:47,800
信仰

403
00:13:47,800 --> 00:13:49,480
这个就是拍touch的低太奇了

404
00:13:49,680 --> 00:13:50,560
就是说

405
00:13:51,560 --> 00:13:52,960
就是说你可以看一下

406
00:13:52,960 --> 00:13:54,760
就是说取决拍touch版本

407
00:13:55,120 --> 00:13:57,720
有些版本新的版本好像是不需要了

408
00:13:57,720 --> 00:13:59,400
就旧版本去低太奇一下

409
00:13:59,520 --> 00:14:01,000
就是说你到转到浪派之前

410
00:14:01,000 --> 00:14:02,080
要干一个这个事情

411
00:14:04,960 --> 00:14:05,280
好

412
00:14:05,280 --> 00:14:07,200
我们来这一个就是我们

413
00:14:07,400 --> 00:14:10,200
接下来的跟这一节相关的问题

414
00:14:10,240 --> 00:14:11,120
就是说

415
00:14:11,400 --> 00:14:13,400
这样的data iterator的写法

416
00:14:13,440 --> 00:14:15,960
每次把所有的漏的进来

417
00:14:16,000 --> 00:14:17,400
如果数据多的话

418
00:14:17,440 --> 00:14:19,120
最后的内存会爆掉

419
00:14:19,160 --> 00:14:20,520
有没有好办法吗

420
00:14:20,800 --> 00:14:21,160
是的

421
00:14:21,160 --> 00:14:22,760
这样子内存会爆掉

422
00:14:22,760 --> 00:14:23,600
就是说

423
00:14:24,800 --> 00:14:26,320
如果你的数据特别大的话

424
00:14:26,320 --> 00:14:28,240
假设你数据要100个GB的话

425
00:14:28,440 --> 00:14:30,280
你达到数据就你这样子

426
00:14:30,280 --> 00:14:31,880
就要先漏的内存

427
00:14:31,920 --> 00:14:33,120
肯定是不对的

428
00:14:33,440 --> 00:14:35,160
但是我们在整本书

429
00:14:35,400 --> 00:14:36,880
整本书我们用的data set

430
00:14:36,880 --> 00:14:37,480
都不会很大

431
00:14:37,480 --> 00:14:38,160
就是

432
00:14:38,440 --> 00:14:39,440
就几百兆的样子

433
00:14:39,840 --> 00:14:41,960
这几百兆东西写进内存

434
00:14:41,960 --> 00:14:43,080
其实问题不大了

435
00:14:43,480 --> 00:14:45,000
其实你真的要在

436
00:14:45,600 --> 00:14:47,560
实际情况中你的

437
00:14:48,040 --> 00:14:49,000
就是说你

438
00:14:49,160 --> 00:14:51,160
你真实的有GPU的机器的话

439
00:14:51,160 --> 00:14:53,280
你的内存可能几十G也是有的

440
00:14:53,320 --> 00:14:56,040
所以你做一个10G20G的数据

441
00:14:56,040 --> 00:14:57,560
你漏的进去也问题不大

442
00:14:57,600 --> 00:14:58,200
所以

443
00:14:58,360 --> 00:14:59,760
你不要太担心这个问题

444
00:14:59,880 --> 00:15:01,600
但一般的做法是说

445
00:15:02,000 --> 00:15:03,080
因为我们是

446
00:15:03,280 --> 00:15:04,360
真的就告诉你说

447
00:15:04,360 --> 00:15:05,680
从头实现怎么样子

448
00:15:05,720 --> 00:15:07,040
但实际情况中

449
00:15:07,040 --> 00:15:08,800
你是数据是放在硬盘上的

450
00:15:08,880 --> 00:15:09,760
每一次读

451
00:15:09,760 --> 00:15:10,160
就是说

452
00:15:10,160 --> 00:15:11,840
比如说我有100万张图片

453
00:15:11,880 --> 00:15:12,840
放在硬盘上

454
00:15:13,080 --> 00:15:15,720
每次我是随机去把图片给抓出来

455
00:15:16,200 --> 00:15:17,240
我是不需要

456
00:15:17,560 --> 00:15:19,480
去真的去把整个数据

457
00:15:19,480 --> 00:15:20,680
填漏的进内存

458
00:15:20,720 --> 00:15:22,759
我每一次我只要读到

459
00:15:22,759 --> 00:15:25,080
批量大小的数据进内存就行了

460
00:15:25,279 --> 00:15:27,040
但如果说性能有问题的话

461
00:15:27,040 --> 00:15:29,120
我会往前读几个批量大小

462
00:15:29,120 --> 00:15:29,800
就是说

463
00:15:29,840 --> 00:15:32,040
假设我的批量大小是128的话

464
00:15:32,120 --> 00:15:32,879
我

465
00:15:33,639 --> 00:15:34,720
读一个再读一个

466
00:15:34,720 --> 00:15:36,240
每次往前读三个

467
00:15:36,279 --> 00:15:37,040
早一点开始读

468
00:15:37,040 --> 00:15:39,160
就要就预取prefetch

469
00:15:42,120 --> 00:15:43,840
这里的index要转成

470
00:15:44,399 --> 00:15:45,879
tensor指向列表不行吗

471
00:15:45,879 --> 00:15:46,800
你可以试一下

472
00:15:47,280 --> 00:15:48,040
应该是不行的

473
00:15:48,080 --> 00:15:49,200
你可以试一下

474
00:15:51,759 --> 00:15:54,920
每次都随机取出一部分

475
00:15:54,920 --> 00:15:57,320
最后保证所有数据都被拿过了

476
00:15:57,320 --> 00:15:58,920
我们这个实现是会保证

477
00:15:58,920 --> 00:16:00,440
所有数据被拿过了

478
00:16:00,480 --> 00:16:01,480
就是说你看到是说

479
00:16:01,480 --> 00:16:03,480
我们把0一直到最后一个的

480
00:16:03,480 --> 00:16:04,680
所有的index

481
00:16:04,680 --> 00:16:06,000
全部生成出来

482
00:16:06,040 --> 00:16:07,759
然后randomly shuffle一下

483
00:16:07,800 --> 00:16:09,480
然后一次读每次读的话

484
00:16:09,480 --> 00:16:11,600
我能保证每扫一次数据

485
00:16:11,600 --> 00:16:13,440
把所有的数据都拿过一次

486
00:16:13,960 --> 00:16:15,200
但是另外有一些

487
00:16:15,200 --> 00:16:17,160
另外一个实践是说

488
00:16:17,160 --> 00:16:18,879
我可以不要这么做

489
00:16:18,879 --> 00:16:20,920
每次真的去随机采样一些数据

490
00:16:20,920 --> 00:16:21,759
也是可以的

491
00:16:21,960 --> 00:16:25,560
所以说其实你只要数据

492
00:16:25,560 --> 00:16:28,720
你叠代次数够多的话

493
00:16:29,800 --> 00:16:32,320
你总会拿看到所有的数据

494
00:16:32,360 --> 00:16:33,399
而且你真的

495
00:16:33,399 --> 00:16:34,680
你有一百万个数据的话

496
00:16:34,680 --> 00:16:35,399
比如举个例子

497
00:16:35,399 --> 00:16:37,320
少看一些数据没关系

498
00:16:37,320 --> 00:16:38,360
其实没有什么关系

499
00:16:39,360 --> 00:16:43,919
这里使用生成器生成数据

500
00:16:43,919 --> 00:16:44,480
有什么优势

501
00:16:44,480 --> 00:16:45,560
相对return

502
00:16:46,080 --> 00:16:48,600
好处是说你生成数据

503
00:16:48,600 --> 00:16:50,080
说每一次就是说

504
00:16:50,240 --> 00:16:51,680
这里好处是说

505
00:16:51,680 --> 00:16:53,279
你比如return的好处是

506
00:16:53,279 --> 00:16:54,879
我不需要把所有的batch

507
00:16:54,879 --> 00:16:55,720
全部生成好

508
00:16:55,720 --> 00:16:57,159
每一次我要一个batch

509
00:16:57,159 --> 00:16:58,279
我就去run一遍

510
00:16:58,320 --> 00:16:59,600
这个是第一个好处

511
00:16:59,600 --> 00:17:01,560
第二个好处就是写起来比较好

512
00:17:01,560 --> 00:17:02,759
python都是这么写的

513
00:17:02,759 --> 00:17:04,680
就是说python都是选用iterator

514
00:17:04,720 --> 00:17:05,799
这是python的写法

515
00:17:06,399 --> 00:17:09,559
有没有就是一个python的习惯

516
00:17:11,960 --> 00:17:15,399
如果样本大小不是批量数的整数倍

517
00:17:15,399 --> 00:17:18,359
那需要随机剔除多余的样本吗

518
00:17:19,000 --> 00:17:21,599
这个也是一个细节的问题

519
00:17:21,599 --> 00:17:22,399
就是说

520
00:17:22,480 --> 00:17:24,359
假设我有一百个样本

521
00:17:24,359 --> 00:17:26,119
我批量大小取了60

522
00:17:26,119 --> 00:17:27,799
那么我第一个还能

523
00:17:27,799 --> 00:17:29,839
第二个最后40个怎么办

524
00:17:31,240 --> 00:17:32,279
有三种做法

525
00:17:33,359 --> 00:17:34,519
最常见的做法

526
00:17:34,519 --> 00:17:36,359
其实是拿到一个样本

527
00:17:36,359 --> 00:17:38,680
把后就拿到一个小一点的样本

528
00:17:38,680 --> 00:17:40,879
就是成为40的一个批量大小

529
00:17:41,079 --> 00:17:42,680
这是我们这里的做法

530
00:17:42,879 --> 00:17:43,960
是这样子做的

531
00:17:44,160 --> 00:17:45,240
第二种做法是说

532
00:17:45,240 --> 00:17:46,799
把最后一个不完整丢掉

533
00:17:46,799 --> 00:17:47,920
这是第二种做法

534
00:17:48,160 --> 00:17:49,879
第三个做法是说

535
00:17:50,160 --> 00:17:51,440
其实是说

536
00:17:51,440 --> 00:17:55,839
从下一个epoch里面补20个过来

537
00:17:57,599 --> 00:17:58,039
有理解吗

538
00:17:58,039 --> 00:17:59,879
就是说我不够20个

539
00:18:00,000 --> 00:18:01,480
我再去随在别的地方

540
00:18:01,480 --> 00:18:03,599
再随机采用20个过来补全

541
00:18:03,719 --> 00:18:05,399
这是第三种做法

542
00:18:09,799 --> 00:18:10,359
所以

543
00:18:11,639 --> 00:18:13,519
问题22优化算法里面

544
00:18:13,519 --> 00:18:14,480
除以batch size

545
00:18:14,480 --> 00:18:16,319
但是最后一个batch的样本

546
00:18:16,319 --> 00:18:17,079
没有那么多

547
00:18:17,079 --> 00:18:17,399
对的

548
00:18:17,399 --> 00:18:18,559
这个就是一个

549
00:18:18,759 --> 00:18:19,559
是的

550
00:18:19,879 --> 00:18:21,359
这是我们偷懒了

551
00:18:21,799 --> 00:18:23,639
就是说实际上你的实现

552
00:18:23,639 --> 00:18:25,639
应该是你应该除以到

553
00:18:26,240 --> 00:18:27,879
你根据你真实的

554
00:18:27,879 --> 00:18:28,959
你拿进来的批量

555
00:18:28,959 --> 00:18:29,839
多大

556
00:18:29,839 --> 00:18:30,639
你要除以它

557
00:18:30,679 --> 00:18:31,879
这是你真实的

558
00:18:31,879 --> 00:18:32,879
一个真正的实现

559
00:18:33,080 --> 00:18:33,920
这里我们就偷过来

560
00:18:33,920 --> 00:18:35,120
其实你就无所谓了

561
00:18:35,520 --> 00:18:36,640
少一点点多一点点

562
00:18:36,640 --> 00:18:38,040
而且我们这里是整除的

563
00:18:38,040 --> 00:18:38,560
OK

564
00:18:38,560 --> 00:18:40,480
我们生成了100个样本

565
00:18:40,480 --> 00:18:42,040
然后我们的p量是等于10

566
00:18:42,040 --> 00:18:43,800
所以我们除起来是整除了

567
00:18:44,680 --> 00:18:45,360
所以没关系

568
00:18:45,360 --> 00:18:46,320
但是你实际上

569
00:18:46,320 --> 00:18:48,640
你应该是要去除你的

570
00:18:48,680 --> 00:18:49,760
实际样本的个数

571
00:18:50,240 --> 00:18:52,080
你用pytorch的trainer的话

572
00:18:52,080 --> 00:18:53,280
他会帮你做这个事情

573
00:18:55,200 --> 00:18:56,200
第问题24

574
00:18:56,200 --> 00:18:58,080
学习率不做衰减吗

575
00:18:58,680 --> 00:19:00,960
有什么好的衰减方法吗

576
00:19:01,000 --> 00:19:02,160
我们没有做衰减

577
00:19:02,160 --> 00:19:05,400
我们整本书都没有讲太多衰减

578
00:19:05,400 --> 00:19:07,120
就是说衰减的意思是说

579
00:19:07,320 --> 00:19:10,640
理论上SGD要收敛的话

580
00:19:10,640 --> 00:19:12,680
那么需要不断的把学习率变小

581
00:19:12,680 --> 00:19:13,800
变小

582
00:19:14,400 --> 00:19:15,360
实际上来说

583
00:19:17,360 --> 00:19:20,279
你其实有很多种办法

584
00:19:20,279 --> 00:19:22,000
你不做衰减也问题不大

585
00:19:22,000 --> 00:19:23,519
就是说比如说你用

586
00:19:23,560 --> 00:19:24,800
我们之后会讲那种

587
00:19:24,800 --> 00:19:27,960
比较adaptive的学习方法

588
00:19:27,960 --> 00:19:29,640
就是说他会根据你的

589
00:19:29,759 --> 00:19:31,600
t2的大小来调整

590
00:19:31,640 --> 00:19:32,440
学习率

591
00:19:32,440 --> 00:19:34,640
就是说你不做衰减也问题不大

592
00:19:34,800 --> 00:19:35,920
所以你就说

593
00:19:37,120 --> 00:19:38,840
我们可以先不讲这个事情

594
00:19:38,840 --> 00:19:39,759
不讲这个细节

595
00:19:42,520 --> 00:19:45,720
老师请问这有没有收敛的判断吗

596
00:19:45,759 --> 00:19:48,680
要直接人为设置epoch大小吗

597
00:19:49,480 --> 00:19:50,600
你可以判收敛

598
00:19:50,600 --> 00:19:51,840
收敛很多种判法

599
00:19:51,840 --> 00:19:52,759
一个是说

600
00:19:52,759 --> 00:19:54,560
我发现两个目标函数

601
00:19:54,560 --> 00:19:55,600
两次迭代的

602
00:19:55,600 --> 00:19:57,040
两个epoch之间

603
00:19:57,400 --> 00:19:59,080
目标函数变化不大的时候

604
00:19:59,079 --> 00:19:59,879
我就可以收敛了

605
00:19:59,879 --> 00:20:00,759
我可以这么判

606
00:20:01,119 --> 00:20:04,079
就是说相对变化是

607
00:20:04,119 --> 00:20:05,559
比如说1%的时候

608
00:20:05,559 --> 00:20:06,599
我就说停了

609
00:20:06,799 --> 00:20:07,480
第二个是说

610
00:20:07,480 --> 00:20:10,399
我会拿一个验证数据集

611
00:20:10,599 --> 00:20:12,359
看一下验证数据的精度

612
00:20:12,359 --> 00:20:13,000
没有增加了

613
00:20:13,000 --> 00:20:13,599
我也可以停

614
00:20:15,480 --> 00:20:16,359
这里我就偷个懒

615
00:20:16,559 --> 00:20:17,000
就是说

616
00:20:17,000 --> 00:20:17,759
反正你就看一下

617
00:20:17,759 --> 00:20:18,759
你眼睛看一下

618
00:20:19,119 --> 00:20:20,319
就是说实际情况下

619
00:20:20,480 --> 00:20:25,319
实际在真实的训练中

620
00:20:25,359 --> 00:20:26,119
通常来说

621
00:20:26,119 --> 00:20:28,720
这个是大家凭直觉选的

622
00:20:28,759 --> 00:20:29,759
就取决于你

623
00:20:30,519 --> 00:20:31,120
一般来说

624
00:20:31,120 --> 00:20:32,920
第一次去训练的时候

625
00:20:32,920 --> 00:20:34,079
我会去看他

626
00:20:34,079 --> 00:20:35,319
就是说我就选一个比较大的

627
00:20:35,440 --> 00:20:36,160
设100

628
00:20:36,160 --> 00:20:36,880
比如说

629
00:20:37,519 --> 00:20:39,440
然后去看他的学习的曲线

630
00:20:39,799 --> 00:20:41,400
我们之后会来给大家画

631
00:20:41,400 --> 00:20:42,600
曲线长什么样子

632
00:20:42,640 --> 00:20:44,400
我如果感觉他平了的话

633
00:20:44,400 --> 00:20:46,960
我感觉数据集感觉有点难

634
00:20:46,960 --> 00:20:48,200
可能要100个epoch

635
00:20:48,600 --> 00:20:49,440
另外一个数据集

636
00:20:49,440 --> 00:20:51,360
可能就40个就差不多了

637
00:20:51,920 --> 00:20:52,799
下次我就知道

638
00:20:53,120 --> 00:20:53,880
这应该选100

639
00:20:53,880 --> 00:20:54,640
这个选40

640
00:20:54,640 --> 00:20:55,759
这是最简单的了

641
00:20:56,039 --> 00:20:56,880
实际情况就是说

642
00:20:56,880 --> 00:20:58,240
你当然自动化一点

643
00:20:58,240 --> 00:20:59,720
就是说根据你每一次

644
00:20:59,920 --> 00:21:00,960
精度的变化

645
00:21:00,960 --> 00:21:02,039
或者是说

646
00:21:02,359 --> 00:21:04,160
你的目标函数的变化

647
00:21:04,160 --> 00:21:04,839
变化不大了

648
00:21:04,839 --> 00:21:05,799
你就可以停

649
00:21:08,120 --> 00:21:10,039
但是你多迭代一点没错的

650
00:21:10,039 --> 00:21:10,880
就是说

651
00:21:10,920 --> 00:21:12,039
你知道你算力支持

652
00:21:12,039 --> 00:21:13,599
你就多跑也没关系

653
00:21:13,880 --> 00:21:14,759
就是说

654
00:21:14,759 --> 00:21:17,079
就算你的loss没有下降了

655
00:21:17,079 --> 00:21:18,680
可能他还在做一些微调

656
00:21:18,960 --> 00:21:19,839
多学习

657
00:21:20,120 --> 00:21:21,000
就是说

658
00:21:21,120 --> 00:21:22,319
就是说你

659
00:21:22,440 --> 00:21:23,759
就是说你读书对吧

660
00:21:23,759 --> 00:21:24,680
读10遍

661
00:21:24,680 --> 00:21:25,279
可能差不多

662
00:21:25,279 --> 00:21:26,440
你再读一遍也没关系

663
00:21:26,440 --> 00:21:27,200
就这样子

664
00:21:29,240 --> 00:21:34,039
本质上为什么没有SGD

665
00:21:34,039 --> 00:21:36,200
是因为大部分实际loss太复杂

666
00:21:36,200 --> 00:21:36,880
推导不出

667
00:21:36,880 --> 00:21:38,279
导数为0的解吗

668
00:21:38,319 --> 00:21:40,480
只能足够batch去逼近

669
00:21:40,480 --> 00:21:40,839
是的

670
00:21:40,839 --> 00:21:41,480
是这样子

671
00:21:41,480 --> 00:21:41,880
就是说

672
00:21:41,880 --> 00:21:43,160
除了线性回归之外

673
00:21:43,160 --> 00:21:43,799
有显示解

674
00:21:43,799 --> 00:21:44,799
别的所有都没有

675
00:21:45,079 --> 00:21:46,160
有显示解的模型

676
00:21:46,160 --> 00:21:48,319
我们也没必要真的花半年来

677
00:21:48,319 --> 00:21:48,960
给大家讲了

678
00:21:48,960 --> 00:21:49,720
这东西

679
00:21:49,880 --> 00:21:51,839
我就讲到今天就结束了

680
00:21:51,839 --> 00:21:54,359
T2就后面都不用讲了

681
00:21:54,599 --> 00:21:56,079
但实际上来说

682
00:21:57,120 --> 00:21:57,839
纠结不了

683
00:21:58,159 --> 00:22:00,199
就是说我们都是在解NP

684
00:22:00,199 --> 00:22:02,199
就所有我们有意思的问题

685
00:22:02,199 --> 00:22:03,519
都是NP complete

686
00:22:03,639 --> 00:22:05,319
如果你这个问题不是NP complete

687
00:22:05,319 --> 00:22:06,319
我用什么机器学习

688
00:22:06,319 --> 00:22:06,839
对吧

689
00:22:06,879 --> 00:22:09,839
就直接精确值求了

690
00:22:10,000 --> 00:22:10,919
所以的话

691
00:22:10,919 --> 00:22:13,959
你如果能够直接解求出来

692
00:22:13,959 --> 00:22:15,279
那就不是NP complete了

693
00:22:15,279 --> 00:22:17,039
对吧

694
00:22:17,399 --> 00:22:17,959
OK

695
00:22:19,599 --> 00:22:20,319
问题26

696
00:22:20,319 --> 00:22:22,839
为什么W每次要随机初始化

697
00:22:22,839 --> 00:22:24,279
不能用同样的值呢

698
00:22:24,599 --> 00:22:25,879
你可以用同样的值没关系

699
00:22:25,880 --> 00:22:28,000
就是说你可以固定随机种子

700
00:22:28,200 --> 00:22:29,120
就是说

701
00:22:29,160 --> 00:22:30,720
但是你无所谓

702
00:22:30,720 --> 00:22:31,880
就是说这个东西

703
00:22:31,920 --> 00:22:36,400
你随机或者你固定或不固定的没关系

704
00:22:36,400 --> 00:22:38,240
就是说我们就偷懒就不固定了

705
00:22:40,240 --> 00:22:42,920
实际中有时候网络会输出lot number

706
00:22:42,920 --> 00:22:44,360
lot number怎么出现的

707
00:22:44,480 --> 00:22:45,880
为什么不是int

708
00:22:46,080 --> 00:22:47,280
数值不稳

709
00:22:47,280 --> 00:22:50,120
就是说因为求导有除法

710
00:22:50,120 --> 00:22:53,280
除法会导致你除0除int

711
00:22:53,480 --> 00:22:55,080
就是说会出现这个问题

712
00:22:55,480 --> 00:22:57,279
所以这就是求导

713
00:22:57,279 --> 00:22:59,240
我们会讲数值稳定性的问题

714
00:22:59,240 --> 00:23:00,240
在后面一节

715
00:23:01,960 --> 00:23:05,240
定义层最后一定要手动设置初始值吗

716
00:23:06,639 --> 00:23:07,119
不一定

717
00:23:07,119 --> 00:23:09,519
就是说他有默认的初始值也挺好的

718
00:23:09,599 --> 00:23:12,359
就我们之所以手动设是给大家说

719
00:23:12,439 --> 00:23:15,079
我们要跟我们的之前的

720
00:23:15,079 --> 00:23:17,639
从零开始实现能匹配上

721
00:23:17,639 --> 00:23:18,519
就是这个意思

722
00:23:21,159 --> 00:23:23,039
就我们之后就不会这么手动设了

723
00:23:23,039 --> 00:23:23,960
因为设不过来了

724
00:23:23,960 --> 00:23:24,399
对吧

725
00:23:25,879 --> 00:23:28,559
L的backward就是调用它的backup propagation

726
00:23:28,559 --> 00:23:30,599
我们之前自动求导也没有讲过

727
00:23:32,559 --> 00:23:33,599
对最后一层

728
00:23:33,599 --> 00:23:35,519
for里面最后一层就是为了print

729
00:23:36,839 --> 00:23:37,919
我们就是为了print

730
00:23:37,919 --> 00:23:39,199
他不需要清零

731
00:23:39,199 --> 00:23:39,839
T2清零

732
00:23:39,839 --> 00:23:41,319
是因为我们没有更新了

733
00:23:41,319 --> 00:23:42,480
我们只run forward

734
00:23:42,480 --> 00:23:43,439
没有run backward

735
00:23:43,439 --> 00:23:44,359
所以不需要清零

736
00:23:46,359 --> 00:23:48,079
每个batch计算的时候

737
00:23:48,079 --> 00:23:49,399
为什么要把T2清零

738
00:23:49,679 --> 00:23:52,399
是因为排头就不放零星

739
00:23:52,399 --> 00:23:54,119
就是你算完T字之后

740
00:23:54,119 --> 00:23:54,879
如果你不清零

741
00:23:54,880 --> 00:23:56,880
他就在上面的T字上做了一加

742
00:23:56,920 --> 00:23:58,280
这一加下去了

743
00:24:02,280 --> 00:24:04,320
这个问题就是说还是not a number问题

744
00:24:04,320 --> 00:24:07,120
我们会有专门的一章来给大家解释

745
00:24:07,120 --> 00:24:08,520
所以大家不用急

746
00:24:08,760 --> 00:24:10,040
我会讲为什么要int

747
00:24:10,040 --> 00:24:11,080
为什么要not a number

748
00:24:11,080 --> 00:24:11,960
为什么会变成零

749
00:24:11,960 --> 00:24:12,360
怎么

750
00:24:12,360 --> 00:24:13,880
所以这一块我们会

751
00:24:13,880 --> 00:24:16,480
因为这个问题还挺难的

752
00:24:16,480 --> 00:24:17,080
挺严重的

753
00:24:17,080 --> 00:24:19,520
所以我们会真的给大家解释一下

754
00:24:20,120 --> 00:24:21,320
专门花一点时间

