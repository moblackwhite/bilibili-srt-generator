1
00:00:00,000 --> 00:00:00,920
好

2
00:00:00,920 --> 00:00:04,719
接下来我们来讲我们的第一个

3
00:00:04,719 --> 00:00:07,679
对于序列模型的升级网络

4
00:00:08,199 --> 00:00:10,120
叫做循环升级网络

5
00:00:10,120 --> 00:00:12,679
或者叫做Recurrent Neural Network

6
00:00:12,679 --> 00:00:14,759
一般简称为RNN

7
00:00:17,120 --> 00:00:17,559
好

8
00:00:17,559 --> 00:00:19,039
就是说我们先回一下

9
00:00:19,039 --> 00:00:19,640
讲这个模型

10
00:00:19,640 --> 00:00:20,920
说回一下我们之前的

11
00:00:20,920 --> 00:00:23,960
那个潜变量自回归模型

12
00:00:23,960 --> 00:00:24,640
是怎么回事

13
00:00:26,280 --> 00:00:27,120
然后呢

14
00:00:28,199 --> 00:00:29,920
这东西我们看一下

15
00:00:30,519 --> 00:00:31,280
就是说

16
00:00:32,039 --> 00:00:33,439
我们在这个地方

17
00:00:34,000 --> 00:00:34,799
是

18
00:00:35,520 --> 00:00:36,520
用一个这样子假设

19
00:00:36,880 --> 00:00:38,480
就是说我们砍掉了一些东西

20
00:00:38,480 --> 00:00:39,399
但是剩下一些东西

21
00:00:39,399 --> 00:00:41,719
就是说首先你的X

22
00:00:43,880 --> 00:00:46,240
X是跟两个东西相关的

23
00:00:46,480 --> 00:00:50,000
跟你的当前的H

24
00:00:50,280 --> 00:00:52,719
就是XT跟当前的HT

25
00:00:52,719 --> 00:00:54,120
就是我假设我写错的话

26
00:00:54,120 --> 00:00:57,039
就是假设这是XT的话

27
00:00:57,280 --> 00:00:59,799
那么这个是HT

28
00:01:00,119 --> 00:01:02,280
这个是XT-1

29
00:01:03,600 --> 00:01:04,239
OK

30
00:01:04,239 --> 00:01:05,200
所以呢

31
00:01:06,599 --> 00:01:10,519
XT是跟你的HT和你的XT-1相关

32
00:01:10,560 --> 00:01:12,280
就是说我的概率生成的话

33
00:01:12,280 --> 00:01:13,719
是跟这两个东西相关

34
00:01:14,319 --> 00:01:16,280
然后你的HT

35
00:01:16,400 --> 00:01:18,960
就T时间那个潜变量H

36
00:01:19,200 --> 00:01:24,519
是跟你的XT-1和HT-1相关

37
00:01:24,960 --> 00:01:25,319
OK

38
00:01:25,319 --> 00:01:26,560
跟这两个接头相关

39
00:01:27,560 --> 00:01:28,280
OK

40
00:01:28,280 --> 00:01:31,000
这是我们的潜变量回归模型

41
00:01:32,159 --> 00:01:34,600
然后我们怎么把这个模型变成RN

42
00:01:35,400 --> 00:01:37,799
接下来看就是说怎么样做成RN

43
00:01:37,799 --> 00:01:38,799
RN相对来说

44
00:01:38,799 --> 00:01:42,560
它的假设稍微更加简单一点

45
00:01:42,560 --> 00:01:43,680
它的假设

46
00:01:44,680 --> 00:01:45,680
是说

47
00:01:45,920 --> 00:01:51,040
我有一个观察X

48
00:01:51,040 --> 00:01:52,680
我有一个引变量

49
00:01:52,840 --> 00:01:54,280
我们叫做Hinden variable

50
00:01:54,280 --> 00:01:55,760
我们不叫做Latent variable

51
00:01:55,760 --> 00:01:58,800
我们有解释过Latent和Hinden是什么区别

52
00:01:59,080 --> 00:02:00,680
在昨天的QA环节里面

53
00:02:01,000 --> 00:02:01,520
说白了一点

54
00:02:01,520 --> 00:02:02,359
就是Hinden variable

55
00:02:02,359 --> 00:02:03,320
通常是说

56
00:02:03,520 --> 00:02:05,840
是现实生活中存在了一个东西

57
00:02:05,840 --> 00:02:07,520
但是只是我们没观察到

58
00:02:07,880 --> 00:02:09,040
但是Latent的variable

59
00:02:09,080 --> 00:02:11,280
可以用来指代现实中不存在的东西

60
00:02:11,280 --> 00:02:12,759
就是人为加上去的东西

61
00:02:12,800 --> 00:02:13,920
比如说类别

62
00:02:14,120 --> 00:02:15,680
就是clustering那个东西

63
00:02:16,439 --> 00:02:17,280
这种标号

64
00:02:17,400 --> 00:02:18,719
这东西可以加上去

65
00:02:19,360 --> 00:02:20,240
所以我们

66
00:02:20,520 --> 00:02:23,879
但是在我们的神经领域这一块

67
00:02:24,120 --> 00:02:25,360
就是这两个是可以换的

68
00:02:25,400 --> 00:02:27,720
就Hinden Latent或者钱

69
00:02:27,800 --> 00:02:28,560
银都可以换

70
00:02:28,560 --> 00:02:29,800
就是你为了简单起见

71
00:02:29,800 --> 00:02:31,480
就都叫银都是没问题的

72
00:02:33,840 --> 00:02:34,160
好

73
00:02:34,160 --> 00:02:35,120
我们来看到是说

74
00:02:35,120 --> 00:02:37,480
我们观察到的是X

75
00:02:37,480 --> 00:02:38,520
X就是有T

76
00:02:39,400 --> 00:02:40,520
然后一样的

77
00:02:40,600 --> 00:02:42,560
比如说我们这个地方写成

78
00:02:43,440 --> 00:02:44,440
写一个

79
00:02:44,440 --> 00:02:45,440
我用个黄色

80
00:02:46,160 --> 00:02:47,480
XT的话

81
00:02:48,040 --> 00:02:49,440
那么你的引变量

82
00:02:49,640 --> 00:02:51,800
引变量其实说白了就是一个向量

83
00:02:52,320 --> 00:02:55,600
引变量也是有一个HT

84
00:02:56,439 --> 00:02:57,400
然后

85
00:03:00,760 --> 00:03:02,400
然后你根据HT

86
00:03:02,560 --> 00:03:05,280
它就能够去生成我的

87
00:03:06,040 --> 00:03:07,760
我的一个输出就是OT

88
00:03:10,760 --> 00:03:12,800
所以基本上你看到我的输出

89
00:03:12,800 --> 00:03:13,760
是怎么输出的

90
00:03:13,920 --> 00:03:15,920
我的输出是用的HT

91
00:03:15,920 --> 00:03:18,400
我就在T时间的输出OT

92
00:03:18,439 --> 00:03:21,040
是根据我的HT输出的

93
00:03:21,840 --> 00:03:25,000
HT你不能用XT的东西

94
00:03:25,000 --> 00:03:27,880
就是用的是HT用的是XT-1

95
00:03:30,920 --> 00:03:31,560
OK

96
00:03:33,920 --> 00:03:34,800
所以这东西

97
00:03:36,600 --> 00:03:39,040
我给我在讲具体更新之前

98
00:03:39,040 --> 00:03:40,240
先跟大家讲一下

99
00:03:40,240 --> 00:03:41,120
说这个东西

100
00:03:41,120 --> 00:03:43,240
到底在语言模型是怎么用的

101
00:03:43,439 --> 00:03:44,200
就这样子

102
00:03:44,200 --> 00:03:45,120
大家就是说

103
00:03:45,160 --> 00:03:46,000
不会那么误解

104
00:03:46,000 --> 00:03:48,560
就是说你看我们语言模型的时候

105
00:03:48,600 --> 00:03:50,439
我的输入里面

106
00:03:50,439 --> 00:03:51,439
比如说我是你好

107
00:03:51,439 --> 00:03:52,199
Hello world

108
00:03:52,199 --> 00:03:53,400
就你好逗号

109
00:03:53,400 --> 00:03:54,800
世界感叹号的话

110
00:03:55,439 --> 00:03:56,919
那么我的输出是怎么回事

111
00:03:57,680 --> 00:03:58,840
输出就是说你

112
00:03:58,840 --> 00:04:01,000
其实我们刚刚在讲数据生成的时候

113
00:04:01,000 --> 00:04:02,359
已经讲过是怎么回事了

114
00:04:02,400 --> 00:04:04,919
就是说你在当前时刻

115
00:04:04,960 --> 00:04:05,639
临时刻

116
00:04:05,680 --> 00:04:08,079
比如说临时刻输入到你进去

117
00:04:08,159 --> 00:04:09,039
是不需要输出

118
00:04:11,240 --> 00:04:12,719
当时当

119
00:04:12,759 --> 00:04:16,560
就是说当前临时刻输入进去的东西

120
00:04:16,600 --> 00:04:17,839
我需要的输出

121
00:04:18,000 --> 00:04:19,839
它其实是做在下一个时刻

122
00:04:19,840 --> 00:04:22,720
就是说看你下标都可以怎么说

123
00:04:22,760 --> 00:04:24,600
就是说输入你进去的时候

124
00:04:25,000 --> 00:04:26,920
我要输出一个

125
00:04:32,520 --> 00:04:33,680
这个地方

126
00:04:37,120 --> 00:04:37,640
对

127
00:04:42,320 --> 00:04:42,960
我看一下

128
00:04:42,960 --> 00:04:44,040
我觉得这个地方

129
00:04:44,040 --> 00:04:47,360
我下标有弄错了

130
00:04:49,880 --> 00:04:51,840
我觉得这个地方我下标有弄错了

131
00:04:51,840 --> 00:04:53,320
这个东西应该是

132
00:04:55,080 --> 00:04:56,160
这个东西不好意思

133
00:04:56,160 --> 00:04:56,760
我改一下

134
00:05:13,760 --> 00:05:16,560
就是说假设我的输出是你的话

135
00:05:17,240 --> 00:05:19,280
那么我会去更新

136
00:05:19,280 --> 00:05:20,520
我的影变量

137
00:05:21,120 --> 00:05:24,400
那么我就要我要去预测好字

138
00:05:25,960 --> 00:05:28,200
接下来我观察到了好

139
00:05:28,720 --> 00:05:30,080
我去更新我的影变量

140
00:05:30,080 --> 00:05:31,760
再输出下一个逗号

141
00:05:33,200 --> 00:05:33,840
OK

142
00:05:34,200 --> 00:05:37,160
就是说你在

143
00:05:37,600 --> 00:05:40,680
你其实用的是你的OT

144
00:05:40,680 --> 00:05:41,680
是用来

145
00:05:41,720 --> 00:05:44,000
match到你的XT的输入

146
00:05:44,000 --> 00:05:45,880
但是在你生成OT的时候

147
00:05:46,000 --> 00:05:47,320
你不能看到XT

148
00:05:47,920 --> 00:05:49,519
就你这个东西是你的输出

149
00:05:49,519 --> 00:05:51,279
是要去预测

150
00:05:51,279 --> 00:05:53,199
你当前就当前时刻的输出

151
00:05:53,199 --> 00:05:55,399
是预测当前时刻的观察

152
00:05:55,439 --> 00:05:58,159
但是你的输出发生在观察之前

153
00:05:59,040 --> 00:05:59,560
OK

154
00:05:59,600 --> 00:06:01,240
所以回到我们像上一个模型

155
00:06:01,240 --> 00:06:01,920
就是说

156
00:06:03,719 --> 00:06:06,279
假设你这个是XT

157
00:06:06,480 --> 00:06:07,600
HT

158
00:06:07,759 --> 00:06:09,039
OT的话

159
00:06:09,240 --> 00:06:15,319
那么我们的OT是根据HT来输出的

160
00:06:16,120 --> 00:06:19,279
但是HT用的是XT-1的东西

161
00:06:19,920 --> 00:06:21,560
所以这个东西一定要搞清楚

162
00:06:21,680 --> 00:06:23,959
然后我在计算损失的时候

163
00:06:23,959 --> 00:06:26,719
我去比较OT和XT之间的关系

164
00:06:26,719 --> 00:06:27,800
计算损失

165
00:06:28,639 --> 00:06:30,600
但是XT是用来更新

166
00:06:31,240 --> 00:06:32,039
你的HT

167
00:06:32,039 --> 00:06:33,800
使得它挪到下一个单元

168
00:06:33,959 --> 00:06:34,439
OK

169
00:06:34,439 --> 00:06:35,879
这就是跟你的

170
00:06:37,480 --> 00:06:37,920
怎么说

171
00:06:37,920 --> 00:06:38,759
跟你的

172
00:06:41,680 --> 00:06:43,439
MLP就是你的

173
00:06:44,319 --> 00:06:46,199
带一个

174
00:06:47,360 --> 00:06:48,279
多层感知机

175
00:06:48,279 --> 00:06:49,319
有一点不一样的地方

176
00:06:49,319 --> 00:06:50,439
就是有个时间轴

177
00:06:50,719 --> 00:06:51,399
多层感知机

178
00:06:51,399 --> 00:06:54,160
假设你没有时间的关系的话

179
00:06:54,160 --> 00:06:58,319
它就是会回到一个最简单的

180
00:06:58,360 --> 00:06:59,240
MLP的

181
00:06:59,680 --> 00:07:01,759
MLP的一个模型

182
00:07:02,519 --> 00:07:03,240
所以我们来看一下

183
00:07:03,240 --> 00:07:04,759
就是说它到底是怎么更新的

184
00:07:06,360 --> 00:07:07,159
就到底怎么更新

185
00:07:07,159 --> 00:07:08,879
它本身就是一个MLP了

186
00:07:09,279 --> 00:07:10,240
它是怎么呢

187
00:07:10,560 --> 00:07:12,720
假设我的HT是一个

188
00:07:13,000 --> 00:07:14,759
隐藏状态的话

189
00:07:14,920 --> 00:07:16,000
那么我的sigma

190
00:07:16,000 --> 00:07:18,199
我的我的我的phi

191
00:07:18,240 --> 00:07:19,439
phi就是一个

192
00:07:19,480 --> 00:07:20,160
我的

193
00:07:21,240 --> 00:07:22,720
我的一个激活

194
00:07:22,720 --> 00:07:24,319
还是我的一个激活函数

195
00:07:26,439 --> 00:07:28,720
然后激活函数的话

196
00:07:30,040 --> 00:07:31,319
首先我不看这一块

197
00:07:31,840 --> 00:07:33,079
我先看这个地方

198
00:07:34,400 --> 00:07:37,079
WHX是什么东西

199
00:07:37,120 --> 00:07:37,960
它其实说白了

200
00:07:37,960 --> 00:07:39,360
就是那个隐藏的

201
00:07:39,360 --> 00:07:41,080
我们MLP

202
00:07:41,360 --> 00:07:42,680
隐藏weight

203
00:07:43,280 --> 00:07:43,800
W

204
00:07:45,120 --> 00:07:46,840
你的X是你的输出

205
00:07:46,879 --> 00:07:48,560
但是你的X是你的输入

206
00:07:48,560 --> 00:07:50,319
但是你的X现在是个T-1

207
00:07:50,319 --> 00:07:51,800
就是我们都加了下标了

208
00:07:52,280 --> 00:07:54,240
就之前我们就是H是等于

209
00:07:54,960 --> 00:07:56,560
phi WX

210
00:07:56,600 --> 00:07:57,720
但是都没有这些下标

211
00:07:57,840 --> 00:07:59,360
我们现在都加上了下标

212
00:07:59,400 --> 00:08:02,400
就是说HT是有XT-1

213
00:08:02,439 --> 00:08:04,480
乘以一个权重

214
00:08:04,720 --> 00:08:08,520
再加上我的对应的偏移

215
00:08:08,519 --> 00:08:10,799
所得到的一个输出

216
00:08:10,799 --> 00:08:12,159
就是隐藏层输出

217
00:08:12,359 --> 00:08:13,959
但是这里不一样的是说

218
00:08:14,000 --> 00:08:15,599
他还HT

219
00:08:15,599 --> 00:08:17,039
他不仅跟X相关

220
00:08:17,039 --> 00:08:18,519
还跟前一刻的

221
00:08:19,959 --> 00:08:21,039
H相关

222
00:08:21,079 --> 00:08:22,319
就是这样子的话

223
00:08:22,359 --> 00:08:24,120
前一刻的H

224
00:08:24,519 --> 00:08:25,519
T-1

225
00:08:26,240 --> 00:08:28,959
它也被乘了一个W

226
00:08:30,639 --> 00:08:34,240
W我们为了跟跟X的W区分开来

227
00:08:34,279 --> 00:08:36,079
我们写的是HH

228
00:08:36,079 --> 00:08:37,840
HH就是从H到H

229
00:08:38,679 --> 00:08:39,519
H

230
00:08:40,039 --> 00:08:42,799
X就是从X到H这个东西

231
00:08:43,319 --> 00:08:43,639
OK

232
00:08:43,639 --> 00:08:46,759
所以是说它有一个WHH

233
00:08:46,759 --> 00:08:49,600
乘以前一个时刻的隐藏状态

234
00:08:49,720 --> 00:08:51,120
得到当前隐藏状态

235
00:08:52,159 --> 00:08:53,639
假设我去掉这个东西

236
00:08:53,840 --> 00:08:55,319
就假设没有这个东西的话

237
00:08:55,319 --> 00:08:57,000
说白了就是一个MLP了

238
00:08:57,079 --> 00:08:57,600
对吧

239
00:08:58,919 --> 00:09:01,559
然后拿到了隐藏状态的话

240
00:09:01,919 --> 00:09:04,919
就跟MLP一样的可以输出

241
00:09:05,120 --> 00:09:06,759
就是说这个地方

242
00:09:06,759 --> 00:09:08,120
我可能是不需要这个

243
00:09:08,320 --> 00:09:08,840
sorry

244
00:09:10,799 --> 00:09:12,039
这个地方我是不需要的

245
00:09:12,039 --> 00:09:12,679
这个东西

246
00:09:12,960 --> 00:09:14,720
所以输出还是说

247
00:09:14,720 --> 00:09:16,679
使用你的隐藏状态

248
00:09:17,320 --> 00:09:18,279
乘以一个W

249
00:09:18,279 --> 00:09:19,879
加上一个BIOS得到输出

250
00:09:22,639 --> 00:09:25,279
所以很简单来看

251
00:09:25,279 --> 00:09:26,600
就是说循环神级网络

252
00:09:26,600 --> 00:09:27,080
说白了

253
00:09:27,080 --> 00:09:29,560
就是在你的MLP里面

254
00:09:29,560 --> 00:09:30,639
再加了一项

255
00:09:30,799 --> 00:09:32,080
使得加了一个

256
00:09:32,080 --> 00:09:33,639
这么一下子的地方

257
00:09:33,680 --> 00:09:35,879
使得它能够跟前一个时间的

258
00:09:35,879 --> 00:09:37,519
HT-1发生关系

259
00:09:37,960 --> 00:09:39,679
然后别的东西

260
00:09:39,679 --> 00:09:40,039
就是说

261
00:09:40,039 --> 00:09:42,039
你假设你不看一个时间轴的情况下

262
00:09:42,039 --> 00:09:43,519
其实它本质上没区别

263
00:09:44,279 --> 00:09:46,399
你其实就是一个预测

264
00:09:46,439 --> 00:09:48,559
所以另外一块就是说

265
00:09:48,559 --> 00:09:50,399
我没有去对X解模

266
00:09:50,600 --> 00:09:52,960
所有的X之间的

267
00:09:52,960 --> 00:09:53,919
sequence信息

268
00:09:53,919 --> 00:09:55,720
都存在我的H里面

269
00:09:57,199 --> 00:09:59,799
所以这个东西说白了

270
00:09:59,799 --> 00:10:01,519
就是用来核心状态

271
00:10:01,519 --> 00:10:04,039
是说我给你一个你的时候

272
00:10:04,319 --> 00:10:06,360
这个W说给你一个你

273
00:10:06,360 --> 00:10:08,039
你对应的印象

274
00:10:08,039 --> 00:10:10,680
对我能可以去把它转化到一个

275
00:10:10,680 --> 00:10:12,039
的好对应的状态

276
00:10:12,039 --> 00:10:12,639
就是说

277
00:10:12,759 --> 00:10:14,320
WHH说白了

278
00:10:14,320 --> 00:10:15,840
就是把你的时序信息

279
00:10:15,840 --> 00:10:17,279
把你存在里面了

280
00:10:17,399 --> 00:10:19,680
就是说所有的时序信息

281
00:10:19,680 --> 00:10:20,800
都是存在这个地方

282
00:10:21,039 --> 00:10:22,600
这一个箭头里面

283
00:10:23,120 --> 00:10:23,800
所以这个东西

284
00:10:23,800 --> 00:10:26,039
它存的就是WHH的东西

285
00:10:26,200 --> 00:10:28,480
所以RNN是最简单的

286
00:10:28,480 --> 00:10:32,039
RNN是通过WHH来存你的

287
00:10:32,039 --> 00:10:33,039
时序信息的

288
00:10:34,039 --> 00:10:36,240
所以跟MLP比

289
00:10:36,240 --> 00:10:39,199
就是我是用一个额外的WHH

290
00:10:39,199 --> 00:10:40,799
来存我的时序信息

291
00:10:40,799 --> 00:10:42,199
就是就这么点区别

292
00:10:42,199 --> 00:10:47,959
好

293
00:10:47,959 --> 00:10:50,719
我们这个是我们就已经讲过了

294
00:10:50,719 --> 00:10:52,439
然后

295
00:10:52,639 --> 00:10:53,639
另外一个是说

296
00:10:53,639 --> 00:10:57,480
我们就怎么样衡量一个语言模型的好坏

297
00:10:57,480 --> 00:11:00,399
就我们刚刚一讲模型什么回事

298
00:11:00,399 --> 00:11:01,079
另外一个是说

299
00:11:01,080 --> 00:11:02,000
我们怎么样

300
00:11:02,040 --> 00:11:05,680
通常用什么样的evaluation matrix

301
00:11:05,920 --> 00:11:07,680
所以这个地方其实也挺简单的

302
00:11:07,680 --> 00:11:09,400
就是说你可以认为就是语言模型

303
00:11:09,400 --> 00:11:10,560
说白了就是个分类模型

304
00:11:10,560 --> 00:11:11,000
对吧

305
00:11:11,680 --> 00:11:13,000
就是说我去我的输出

306
00:11:13,000 --> 00:11:14,960
就是我去判断下一个词

307
00:11:15,000 --> 00:11:18,320
假设我的字点大小是N的话

308
00:11:18,320 --> 00:11:21,320
我换一个

309
00:11:21,320 --> 00:11:22,840
就不用N

310
00:11:23,040 --> 00:11:25,400
就是说用M的话

311
00:11:25,400 --> 00:11:27,520
就用M大小的话

312
00:11:27,640 --> 00:11:29,720
就是做一个M类的分类问题

313
00:11:29,920 --> 00:11:31,680
每次你预测下一个词是多少

314
00:11:31,680 --> 00:11:33,040
那个词是一个类别

315
00:11:33,040 --> 00:11:33,519
对吧

316
00:11:33,960 --> 00:11:35,480
所以你是一个分类问题的话

317
00:11:35,480 --> 00:11:37,639
当然我可以用加超商

318
00:11:38,920 --> 00:11:41,279
我可以用加超商来去做这个事情

319
00:11:42,720 --> 00:11:43,920
然后你有N个

320
00:11:43,920 --> 00:11:46,840
现在我是要做N个要常为N

321
00:11:47,440 --> 00:11:49,680
就是说我要做N次预测的话

322
00:11:49,680 --> 00:11:51,200
那么这个N我就说

323
00:11:51,240 --> 00:11:51,800
说白了

324
00:11:51,800 --> 00:11:53,080
就是我给一个序列的话

325
00:11:53,080 --> 00:11:56,200
我得做N次预测

326
00:11:56,200 --> 00:11:57,399
那就做N次分类

327
00:11:57,960 --> 00:11:59,519
那我衡量一个语言模型好坏

328
00:11:59,679 --> 00:12:00,840
我就是取个平均

329
00:12:01,000 --> 00:12:03,559
就是N次加超商的平均

330
00:12:03,919 --> 00:12:04,799
就像这个公式

331
00:12:06,079 --> 00:12:07,439
这个东西就是

332
00:12:07,679 --> 00:12:08,879
就是那个

333
00:12:09,120 --> 00:12:10,960
P就是语言模型的预测概率

334
00:12:11,000 --> 00:12:13,720
然后你的X是你的真实值

335
00:12:13,919 --> 00:12:14,639
就是说

336
00:12:14,799 --> 00:12:16,039
在给定真实值的

337
00:12:16,039 --> 00:12:17,039
我的预测概率多少

338
00:12:17,039 --> 00:12:19,199
就是我的softmax输出

339
00:12:19,240 --> 00:12:20,000
所谓的这个东西

340
00:12:20,000 --> 00:12:24,079
就是我对于真实值的词的

341
00:12:24,079 --> 00:12:25,879
softmax输出是什么东西

342
00:12:25,919 --> 00:12:26,759
然后取个log

343
00:12:27,039 --> 00:12:27,720
取个负号

344
00:12:27,759 --> 00:12:29,120
就是我们的cross

345
00:12:30,399 --> 00:12:31,120
entropy对吧

346
00:12:31,120 --> 00:12:32,720
就是叫这个就是交叉商

347
00:12:32,920 --> 00:12:34,519
然后但是我们做了N个

348
00:12:34,519 --> 00:12:35,720
就是假设sequence

349
00:12:35,720 --> 00:12:36,720
长度是N的话

350
00:12:37,200 --> 00:12:37,720
我们的N

351
00:12:38,120 --> 00:12:39,200
就是之前我们用的T

352
00:12:39,320 --> 00:12:40,240
我们现在用的N

353
00:12:41,160 --> 00:12:42,279
然后这个东西

354
00:12:42,320 --> 00:12:43,560
我们记为π的话

355
00:12:43,600 --> 00:12:45,120
所谓的就是我们

356
00:12:45,160 --> 00:12:46,800
语言模型就做了一个

357
00:12:47,759 --> 00:12:48,639
你要多长

358
00:12:48,639 --> 00:12:49,440
假设你一个序列

359
00:12:49,440 --> 00:12:50,120
常为N的话

360
00:12:50,120 --> 00:12:51,160
就做N次分类

361
00:12:51,200 --> 00:12:52,399
那么我当然可以说

362
00:12:52,440 --> 00:12:54,160
每次分类平均的交叉商

363
00:12:54,160 --> 00:12:55,360
作为我的evaluation

364
00:12:56,200 --> 00:12:58,279
所以这个东西叫做平均交叉商

365
00:12:59,360 --> 00:13:00,399
但历史原因

366
00:13:00,720 --> 00:13:01,759
历史原因NOP

367
00:13:01,919 --> 00:13:03,360
他不是用了这个东西

368
00:13:03,480 --> 00:13:05,480
他选择叫做困惑度

369
00:13:05,480 --> 00:13:07,159
叫做perplexity的东西

370
00:13:07,560 --> 00:13:08,840
这perplexity这个东西

371
00:13:08,840 --> 00:13:09,399
怎么回事

372
00:13:09,840 --> 00:13:11,720
他就是做了一个指数在里面

373
00:13:13,560 --> 00:13:15,399
就是说他就是对这个东西

374
00:13:15,560 --> 00:13:17,080
做个指数来衡量

375
00:13:18,240 --> 00:13:20,200
他在就是说为什么这么做

376
00:13:20,360 --> 00:13:24,039
我觉得一个好处

377
00:13:24,039 --> 00:13:25,200
是说你做完指数之后

378
00:13:25,840 --> 00:13:27,800
做完指数之后

379
00:13:28,040 --> 00:13:29,600
这个值就显得比较大了

380
00:13:30,640 --> 00:13:33,960
就是说你这个东西是一个很小的值

381
00:13:34,879 --> 00:13:37,320
然后你这个东西做一个指数之后

382
00:13:37,320 --> 00:13:38,520
这个值就被拉开了

383
00:13:38,520 --> 00:13:42,200
就是说之前说我平均交叉商

384
00:13:42,400 --> 00:13:44,240
改进了0.001

385
00:13:44,240 --> 00:13:45,400
这个东西看上去不好看

386
00:13:45,400 --> 00:13:45,800
对吧

387
00:13:45,840 --> 00:13:47,160
但是我用困惑度来讲

388
00:13:47,160 --> 00:13:49,560
我就会变成可能变改进了2

389
00:13:49,600 --> 00:13:51,200
看上去更大一点

390
00:13:51,240 --> 00:13:52,800
所以这个数值更大了

391
00:13:52,800 --> 00:13:55,040
所以比较容易看出来

392
00:13:55,039 --> 00:13:56,480
我那么一点点改进

393
00:13:56,480 --> 00:13:57,719
给你带来的好处

394
00:13:58,639 --> 00:14:01,039
然后第二个是说

395
00:14:01,120 --> 00:14:03,000
就是说你可认为这个东西

396
00:14:03,199 --> 00:14:04,879
多多少少表示了

397
00:14:04,879 --> 00:14:09,719
你每一次给一个序列

398
00:14:09,959 --> 00:14:10,759
我告诉你说

399
00:14:10,759 --> 00:14:13,039
我生成了我下面的预测

400
00:14:13,639 --> 00:14:15,599
假设你是等于1的话

401
00:14:15,599 --> 00:14:16,959
假设你这个只等于1的话

402
00:14:16,959 --> 00:14:17,480
我告诉你

403
00:14:17,480 --> 00:14:18,919
我告诉你很确信

404
00:14:19,240 --> 00:14:21,199
下面这个词一定是谁

405
00:14:22,079 --> 00:14:24,240
假设你困惑度是等于2的话

406
00:14:24,240 --> 00:14:24,960
我会告诉你说

407
00:14:25,120 --> 00:14:26,960
我觉得可能是这两个词

408
00:14:26,960 --> 00:14:27,720
直接选一个

409
00:14:27,759 --> 00:14:29,399
这两个词的概率都很高

410
00:14:30,120 --> 00:14:31,639
就是说你在困惑度

411
00:14:31,919 --> 00:14:34,320
就是说代表了你是一个

412
00:14:34,360 --> 00:14:35,720
大概是一个K的话

413
00:14:35,720 --> 00:14:37,200
就表示说我每一次预测的时候

414
00:14:37,200 --> 00:14:39,560
我都觉得这个K个词都有可能

415
00:14:40,519 --> 00:14:42,919
所以你这个地方1就表示完美

416
00:14:43,039 --> 00:14:44,159
最好的情况就是1

417
00:14:44,159 --> 00:14:45,240
就是说每次告诉你

418
00:14:45,240 --> 00:14:46,240
很确信告诉你说

419
00:14:46,560 --> 00:14:48,680
这东西就是下个词就是它

420
00:14:48,720 --> 00:14:49,879
无穷大就最差了

421
00:14:49,879 --> 00:14:51,000
就是说我根本不知道

422
00:14:51,000 --> 00:14:52,440
我觉得谁的有可能

423
00:14:52,560 --> 00:14:53,080
OK

424
00:14:53,080 --> 00:14:55,440
所以这个东西也就是说

425
00:14:55,440 --> 00:14:57,160
在直观上更好理解

426
00:14:57,160 --> 00:14:58,960
也就是NLP里面常用的

427
00:14:58,960 --> 00:14:59,800
populacity

428
00:15:00,360 --> 00:15:02,400
但是你不要被它吓到了

429
00:15:02,400 --> 00:15:07,120
就是我们的天天在用的

430
00:15:07,120 --> 00:15:07,720
那个loss

431
00:15:07,720 --> 00:15:08,759
cross entropy loss

432
00:15:08,759 --> 00:15:10,639
然后做个指数就完事了

433
00:15:10,639 --> 00:15:11,120
OK

434
00:15:13,000 --> 00:15:15,400
这是我们的预言模型

435
00:15:15,400 --> 00:15:17,280
常用的衡量的标准

436
00:15:19,200 --> 00:15:20,320
另外一个东西

437
00:15:20,480 --> 00:15:21,800
我们要讲的是一个叫做

438
00:15:21,800 --> 00:15:23,520
梯度减材的东西

439
00:15:26,040 --> 00:15:27,000
我把它跳过去了

440
00:15:27,000 --> 00:15:28,160
OK

441
00:15:30,360 --> 00:15:31,800
梯度减材是什么东西

442
00:15:32,160 --> 00:15:35,560
就是我们在数值稳定性有讲过

443
00:15:37,160 --> 00:15:38,800
就是说我在迭代的时候

444
00:15:38,800 --> 00:15:39,800
我每次要计算

445
00:15:39,800 --> 00:15:42,960
梯个时间步上的梯度

446
00:15:43,520 --> 00:15:44,600
就我做梯

447
00:15:44,720 --> 00:15:45,680
我每次算的时候

448
00:15:45,680 --> 00:15:47,880
我Rn是要做梯次

449
00:15:47,880 --> 00:15:49,240
这样是长度是梯的话

450
00:15:49,399 --> 00:15:52,560
那么我在反向传播的过程中

451
00:15:52,560 --> 00:15:55,399
会做OT次的矩阵乘法

452
00:15:55,399 --> 00:15:57,240
会导致数值的不稳定

453
00:15:58,480 --> 00:16:00,440
就是说我们在梯度包罩里面

454
00:16:00,440 --> 00:16:00,879
有讲过

455
00:16:00,960 --> 00:16:03,200
就是说你一乘一堆矩阵的话

456
00:16:03,200 --> 00:16:04,039
你要么就很小

457
00:16:04,039 --> 00:16:04,799
要么就很大

458
00:16:05,000 --> 00:16:06,879
很小的话导致训练不动

459
00:16:07,000 --> 00:16:07,840
但是

460
00:16:08,519 --> 00:16:10,320
炸掉的话就导致你就出错了

461
00:16:10,320 --> 00:16:12,360
就是一堆lot of number

462
00:16:13,279 --> 00:16:15,399
所以这里用的是在Rn

463
00:16:15,600 --> 00:16:17,480
经常在用的一个技术

464
00:16:17,480 --> 00:16:18,879
叫做梯度减材

465
00:16:18,879 --> 00:16:21,480
叫做gradient clip

466
00:16:22,080 --> 00:16:23,840
就是说这个东西是用来有效的

467
00:16:23,840 --> 00:16:25,360
预防梯度爆炸的

468
00:16:26,559 --> 00:16:27,480
其实这个东西什么意思

469
00:16:28,120 --> 00:16:30,240
就假设我梯度的G

470
00:16:32,240 --> 00:16:33,519
这个G就是表示

471
00:16:33,519 --> 00:16:36,080
我的所有层上的梯度

472
00:16:36,960 --> 00:16:38,159
所有的layer上梯度

473
00:16:38,159 --> 00:16:39,960
全部放在一个向量里面

474
00:16:40,279 --> 00:16:41,480
不是一层一层的

475
00:16:41,480 --> 00:16:43,240
通常这个G表示

476
00:16:43,240 --> 00:16:45,679
所有层的梯度放在一起

477
00:16:46,200 --> 00:16:50,840
假设我G它的长度

478
00:16:50,840 --> 00:16:51,920
就是L2 long

479
00:16:52,480 --> 00:16:54,200
超过了θ的话

480
00:16:55,960 --> 00:16:57,440
我会要把它的L2 long

481
00:16:57,440 --> 00:16:58,840
就是降回到θ

482
00:16:59,880 --> 00:17:00,440
就说白了

483
00:17:00,440 --> 00:17:01,360
就是成了一个

484
00:17:01,360 --> 00:17:03,280
这个东西成了一个数在这里面

485
00:17:03,560 --> 00:17:04,759
假设你这个东西

486
00:17:05,200 --> 00:17:07,400
你这个长度少于θ

487
00:17:07,400 --> 00:17:08,240
θ要么等于5

488
00:17:08,240 --> 00:17:09,200
要么等于10

489
00:17:09,240 --> 00:17:11,920
假设你的长度少于5的话

490
00:17:11,920 --> 00:17:13,120
θ等于5的话

491
00:17:13,160 --> 00:17:14,440
那么我不干什么事情

492
00:17:14,440 --> 00:17:17,680
因为这个东西小于等于1

493
00:17:17,880 --> 00:17:19,799
那么就是乘以1

494
00:17:24,559 --> 00:17:25,400
就是说

495
00:17:25,440 --> 00:17:29,600
如果你比它θ大的话

496
00:17:29,600 --> 00:17:30,960
那么这个值就会变成

497
00:17:31,640 --> 00:17:33,759
刚刚如果你是G的长度

498
00:17:33,759 --> 00:17:34,840
小于θ的话

499
00:17:34,840 --> 00:17:36,080
这个东西是大于等于1

500
00:17:36,200 --> 00:17:38,600
所以是小的话就得到1

501
00:17:38,840 --> 00:17:40,039
假设这个东西

502
00:17:40,960 --> 00:17:43,039
它比θ要大的话

503
00:17:43,080 --> 00:17:43,799
那么这个东西

504
00:17:44,000 --> 00:17:45,440
它就变成了

505
00:17:45,799 --> 00:17:48,119
小于等于1的一个东西

506
00:17:48,119 --> 00:17:50,159
那么这个mean就是这个地方

507
00:17:50,519 --> 00:17:51,359
就不要它了

508
00:17:51,359 --> 00:17:51,799
对吧

509
00:17:52,119 --> 00:17:55,319
就变成了θ乘以这个东西除以它

510
00:17:55,359 --> 00:17:56,399
然后这个东西

511
00:17:56,680 --> 00:17:58,680
如果你的记者G撇的话

512
00:17:58,680 --> 00:18:00,159
那么G撇的

513
00:18:00,599 --> 00:18:02,559
要它的long就等于

514
00:18:02,960 --> 00:18:04,960
θ除以G

515
00:18:04,960 --> 00:18:07,159
然后对标量是可以写出来的

516
00:18:07,599 --> 00:18:08,200
这样子的话

517
00:18:08,200 --> 00:18:09,079
这个东西消掉

518
00:18:09,079 --> 00:18:11,559
那么它就等于θ了

519
00:18:12,480 --> 00:18:12,839
就是说

520
00:18:12,839 --> 00:18:15,720
如果你这个G它的长度很大

521
00:18:15,960 --> 00:18:17,960
就超过了5或者10

522
00:18:18,000 --> 00:18:18,839
那么这个操作

523
00:18:18,839 --> 00:18:20,440
就把你东西拉回到5和10

524
00:18:20,440 --> 00:18:22,000
就是你那边值特别大的话

525
00:18:22,000 --> 00:18:23,039
我就把你压回来

526
00:18:23,159 --> 00:18:24,519
就是一个投影操作

527
00:18:24,919 --> 00:18:26,240
这样子保证我的

528
00:18:26,480 --> 00:18:29,359
长度永远不会超过θ

529
00:18:29,639 --> 00:18:31,519
就是说如果你的值正常

530
00:18:31,519 --> 00:18:32,240
我就不管你

531
00:18:32,240 --> 00:18:33,079
如果你值太大

532
00:18:33,079 --> 00:18:34,159
我就把你拉回来

533
00:18:34,200 --> 00:18:36,399
所以这样子能够有效的避免

534
00:18:36,399 --> 00:18:37,240
它剔除爆炸

535
00:18:37,240 --> 00:18:38,960
因为你的

536
00:18:39,240 --> 00:18:40,599
就是说你等于θ的话

537
00:18:40,720 --> 00:18:42,200
你的G里面的每一个值的

538
00:18:42,200 --> 00:18:43,640
绝对值都不会超过θ

539
00:18:44,040 --> 00:18:46,280
所以当然就是不会有剔除爆炸的问题

540
00:18:47,560 --> 00:18:49,000
OK我们等会会来实

541
00:18:49,000 --> 00:18:50,480
给大家实现这个东西来看一看

542
00:18:52,680 --> 00:18:54,120
另外一个是说

543
00:18:58,520 --> 00:18:59,240
更多应用

544
00:18:59,400 --> 00:19:00,760
RNN怎么做应用

545
00:19:01,720 --> 00:19:03,040
我们就是说

546
00:19:04,160 --> 00:19:05,800
最简单的是

547
00:19:05,920 --> 00:19:07,160
一个这样子应用状态

548
00:19:07,280 --> 00:19:08,840
最简单是一个一对一的

549
00:19:08,840 --> 00:19:09,600
就是说一对一

550
00:19:09,600 --> 00:19:10,280
所谓的一对一

551
00:19:10,280 --> 00:19:11,400
就是就没有RNN

552
00:19:11,520 --> 00:19:13,760
就是我们最简单的MLP了

553
00:19:13,760 --> 00:19:16,080
就给我一个样本

554
00:19:16,080 --> 00:19:17,280
输出一个label

555
00:19:18,720 --> 00:19:21,520
现在是说我们给你一个序列的话

556
00:19:21,640 --> 00:19:23,440
我们有这么几种情况

557
00:19:23,920 --> 00:19:25,240
第一个是文本生成

558
00:19:25,240 --> 00:19:27,960
就是给你一个开始的词

559
00:19:28,080 --> 00:19:30,560
然后去生成一个下一个词

560
00:19:30,600 --> 00:19:32,640
然后把它作为一个东西

561
00:19:32,640 --> 00:19:33,320
再输给它

562
00:19:33,320 --> 00:19:34,120
然后做下一个词

563
00:19:34,120 --> 00:19:35,720
就是一直生成下去

564
00:19:36,920 --> 00:19:38,080
第二个是说

565
00:19:38,080 --> 00:19:39,560
我给你一个序列

566
00:19:39,559 --> 00:19:41,440
就给一个文一个句子

567
00:19:41,679 --> 00:19:42,799
然后每一个词

568
00:19:42,799 --> 00:19:44,079
每一个时间t进入

569
00:19:44,079 --> 00:19:45,639
继续更新我的隐藏状态

570
00:19:45,679 --> 00:19:47,159
在最后的时刻输出

571
00:19:47,159 --> 00:19:48,759
就得到我的句子的分类

572
00:19:48,759 --> 00:19:49,919
就是文本分类

573
00:19:50,480 --> 00:19:51,960
如果是我们做机器

574
00:19:52,879 --> 00:19:54,000
这两个我们都会看到

575
00:19:54,359 --> 00:19:55,639
文本分类的话

576
00:19:55,639 --> 00:19:56,519
我们会是

577
00:19:56,799 --> 00:19:58,240
另外一个是问答的话

578
00:19:58,399 --> 00:19:59,480
就给一个句子

579
00:19:59,480 --> 00:20:00,319
给你一个句子

580
00:20:00,319 --> 00:20:01,440
然后先不输出

581
00:20:01,440 --> 00:20:02,599
然后在句子结束的时候

582
00:20:02,599 --> 00:20:04,079
开始输出答案

583
00:20:04,119 --> 00:20:06,359
就问答或者机器翻译

584
00:20:06,359 --> 00:20:07,000
干这个事情

585
00:20:07,159 --> 00:20:08,559
这个东西我们也会在

586
00:20:09,799 --> 00:20:11,039
接下来给大家讲

587
00:20:11,440 --> 00:20:13,000
最后一个是Tag生成

588
00:20:13,000 --> 00:20:14,519
Tag生成所谓的

589
00:20:14,879 --> 00:20:16,279
这一块就是说

590
00:20:16,519 --> 00:20:17,559
我给你

591
00:20:19,079 --> 00:20:19,759
一个句子

592
00:20:19,759 --> 00:20:21,319
你告诉我每一个词

593
00:20:21,599 --> 00:20:22,639
它是一个什么Tag

594
00:20:22,639 --> 00:20:23,799
或者是说你说我这个词

595
00:20:23,799 --> 00:20:24,440
是个名词

596
00:20:24,440 --> 00:20:25,039
是个动词

597
00:20:25,039 --> 00:20:25,759
是个形容词

598
00:20:25,759 --> 00:20:26,440
什么东西

599
00:20:26,480 --> 00:20:28,359
就对每一个词都进行输出

600
00:20:29,079 --> 00:20:29,960
我们这个东西

601
00:20:30,119 --> 00:20:32,039
我们在这个课就不会去讲

602
00:20:32,039 --> 00:20:33,559
但实际上没什么本质区别

603
00:20:33,799 --> 00:20:34,879
就是说跟前面的

604
00:20:34,879 --> 00:20:35,960
没什么本质区别

605
00:20:36,079 --> 00:20:36,639
所以我们

606
00:20:37,040 --> 00:20:39,920
这个东西已经讲过了

607
00:20:39,920 --> 00:20:40,920
就整个

608
00:20:41,360 --> 00:20:42,960
在就是一个MLP

609
00:20:43,120 --> 00:20:44,280
就没有失去信息

610
00:20:44,600 --> 00:20:45,600
我们这个东西

611
00:20:45,759 --> 00:20:46,759
就是一个

612
00:20:46,800 --> 00:20:49,160
这个东西是一个语言模型的

613
00:20:49,160 --> 00:20:51,120
一个文本生成的应用

614
00:20:51,480 --> 00:20:53,240
这个东西就是一个文本分类

615
00:20:53,280 --> 00:20:54,040
然后这个东西

616
00:20:54,160 --> 00:20:55,160
就是一个机器翻译

617
00:20:55,160 --> 00:20:56,800
我们在接下来

618
00:20:57,640 --> 00:20:58,759
一个月里面

619
00:20:58,759 --> 00:20:59,520
大概是

620
00:21:00,200 --> 00:21:00,920
会给大家讲

621
00:21:00,920 --> 00:21:02,400
这三个应用是怎么用的

622
00:21:03,200 --> 00:21:03,800
OK

623
00:21:04,399 --> 00:21:05,480
好

624
00:21:05,480 --> 00:21:07,159
最后我们总结一下

625
00:21:08,639 --> 00:21:09,240
总结一下

626
00:21:09,240 --> 00:21:09,960
就是说

627
00:21:10,279 --> 00:21:12,240
悬款审计网络的输出

628
00:21:12,879 --> 00:21:15,559
它取决于当下的输出

629
00:21:15,599 --> 00:21:19,799
和前一个时间的影变量

630
00:21:20,960 --> 00:21:22,279
然后它的影变量

631
00:21:22,480 --> 00:21:24,119
是用来存好

632
00:21:24,159 --> 00:21:25,639
我的历史的信息

633
00:21:25,639 --> 00:21:28,319
以及说影变量里面存好了

634
00:21:29,399 --> 00:21:30,440
和它矩阵

635
00:21:30,599 --> 00:21:32,480
就是h和wh

636
00:21:32,480 --> 00:21:33,119
是那个矩阵

637
00:21:33,640 --> 00:21:37,400
存的是你过去的历史信息

638
00:21:37,400 --> 00:21:39,480
和下一个历史信息

639
00:21:39,480 --> 00:21:41,680
它怎么样转过去

640
00:21:42,680 --> 00:21:46,080
所以拿到过去的输入

641
00:21:46,120 --> 00:21:48,560
和当前的隐藏状态

642
00:21:48,560 --> 00:21:50,840
我就可以去预测当前的输出

643
00:21:51,440 --> 00:21:54,000
因为wh是拥有一定的

644
00:21:54,160 --> 00:21:55,440
持续的预测目的

645
00:21:56,160 --> 00:21:59,640
然后我再根据当前的输入

646
00:22:00,080 --> 00:22:01,840
再去更新我的隐藏状态

647
00:22:01,839 --> 00:22:03,839
可以预测下一个时候的输出

648
00:22:04,279 --> 00:22:04,799
OK

649
00:22:04,799 --> 00:22:07,279
这就是影变量模型

650
00:22:07,279 --> 00:22:12,279
它的主要的一个用法

651
00:22:12,279 --> 00:22:14,480
而且RNN具体来讲

652
00:22:14,480 --> 00:22:15,919
它是一个影变量模型

653
00:22:15,959 --> 00:22:17,439
它告诉你说

654
00:22:17,439 --> 00:22:18,839
影变量状态是什么

655
00:22:18,839 --> 00:22:19,599
是一个向量

656
00:22:19,599 --> 00:22:22,199
然后它的向量是怎么样去更新的

657
00:22:22,199 --> 00:22:24,119
它主要是做这个事情

658
00:22:24,559 --> 00:22:26,639
说白了就是一个全连接层

659
00:22:27,759 --> 00:22:31,279
然后在应用到语言模型的时候

660
00:22:31,480 --> 00:22:33,639
它就根据当前的词

661
00:22:33,639 --> 00:22:34,639
和之前看过的词

662
00:22:34,839 --> 00:22:37,240
预测下一刻词是什么样子的

663
00:22:38,319 --> 00:22:40,960
然后在语言模型里面

664
00:22:40,960 --> 00:22:44,119
我们做困惑度的时候

665
00:22:44,720 --> 00:22:46,319
我们在衡量它的好坏的时候

666
00:22:46,319 --> 00:22:49,960
就是看它平均每一次预测的时候

667
00:22:49,960 --> 00:22:52,480
它的分类的cross entropy

668
00:22:53,799 --> 00:22:55,119
然后它的平均值

669
00:22:55,160 --> 00:22:58,160
再取一个指数

670
00:22:58,759 --> 00:23:00,039
这个东西叫做困惑度

671
00:23:00,039 --> 00:23:01,160
叫做publicity

672
00:23:01,720 --> 00:23:02,559
publicity

673
00:23:02,559 --> 00:23:04,000
就是基本上衡量的说

674
00:23:04,000 --> 00:23:04,960
我这个语言模型

675
00:23:04,960 --> 00:23:06,359
对下一个词

676
00:23:06,359 --> 00:23:08,079
每一次预测的时候

677
00:23:08,079 --> 00:23:09,559
有给你大概平均下来

678
00:23:09,559 --> 00:23:11,240
给你多少个候选词

679
00:23:11,559 --> 00:23:13,440
当然是说你候选词越少越好

680
00:23:13,440 --> 00:23:15,160
就是说给你一个候选词是最好的

681
00:23:15,879 --> 00:23:17,720
就是说举个例子

682
00:23:17,720 --> 00:23:18,960
你在输入法的时候

683
00:23:18,960 --> 00:23:20,799
你输一个拼音过去

684
00:23:20,839 --> 00:23:22,680
然后它给你出一堆

685
00:23:22,920 --> 00:23:24,319
出一堆当时不好的

686
00:23:24,319 --> 00:23:25,079
就告诉你说

687
00:23:25,079 --> 00:23:25,920
你就输过去

688
00:23:25,920 --> 00:23:27,759
就告诉你的

689
00:23:27,799 --> 00:23:29,240
比较基础度比较高的

690
00:23:29,240 --> 00:23:30,000
告诉你说

691
00:23:30,000 --> 00:23:30,680
我就给你

692
00:23:30,680 --> 00:23:32,240
我就猜你下一个是这个东西

693
00:23:32,240 --> 00:23:33,200
当然是最好的

694
00:23:33,880 --> 00:23:35,600
所以就是困惑度

695
00:23:35,880 --> 00:23:38,240
虽然它不是一个新的东西

696
00:23:38,240 --> 00:23:39,080
就是说我们的

697
00:23:39,080 --> 00:23:40,560
平常用的cross entropy

698
00:23:40,560 --> 00:23:41,240
取个均值

699
00:23:41,240 --> 00:23:42,400
再取个指数

700
00:23:42,440 --> 00:23:44,440
但是它是在自然语言处理里面

701
00:23:44,440 --> 00:23:46,560
它确实有一定的意义

702
00:23:46,720 --> 00:23:48,400
就是说用说语言模型

703
00:23:48,400 --> 00:23:50,279
它给你平均候选词的个数

704
00:23:50,640 --> 00:23:53,000
所以我们通常用这个词来衡量好坏

705
00:23:53,560 --> 00:23:55,000
最后一个我们有讲过

706
00:23:55,000 --> 00:23:57,560
就是说RN这个东西

707
00:23:57,559 --> 00:24:01,319
因为要对OT的时候里面

708
00:24:01,319 --> 00:24:02,559
因为它里面就是一个

709
00:24:02,559 --> 00:24:03,720
很简单的局限乘法

710
00:24:03,879 --> 00:24:06,399
就是说你可以基本上认为是

711
00:24:06,399 --> 00:24:07,879
假设一个隐藏的话

712
00:24:08,200 --> 00:24:09,440
长度为T的话

713
00:24:09,480 --> 00:24:11,000
基本上你就可以变成

714
00:24:11,039 --> 00:24:12,759
你可以差不多的

715
00:24:12,759 --> 00:24:15,559
把它当成一个长度为

716
00:24:15,960 --> 00:24:17,879
乘数为T的一个全年

717
00:24:17,879 --> 00:24:20,000
一个MLP

718
00:24:20,720 --> 00:24:21,639
一个T路很长

719
00:24:21,759 --> 00:24:22,359
128

720
00:24:22,919 --> 00:24:24,000
那就变成了一个

721
00:24:24,000 --> 00:24:26,599
128个隐藏的一个MLP

722
00:24:26,680 --> 00:24:28,360
它当然会有进度

723
00:24:28,400 --> 00:24:30,440
就是说数字稳定性的问题

724
00:24:31,200 --> 00:24:32,920
就是说你有很多局限内乘

725
00:24:32,960 --> 00:24:35,680
所以RN一般来说

726
00:24:35,680 --> 00:24:38,040
是需要通过做T图剪裁的

727
00:24:38,360 --> 00:24:40,040
这样子能使得你的

728
00:24:40,040 --> 00:24:41,240
有效的解决

729
00:24:41,240 --> 00:24:42,680
你的T图爆炸的问题

730
00:24:43,800 --> 00:24:45,360
当然我们之后会看到说

731
00:24:45,360 --> 00:24:46,640
在一些新的网络里面

732
00:24:46,800 --> 00:24:49,160
比如说LSTM就GLU

733
00:24:49,160 --> 00:24:50,080
在网络里面

734
00:24:50,120 --> 00:24:52,040
它确实对于T图爆炸

735
00:24:52,040 --> 00:24:53,480
会控制得更好一点

736
00:24:53,960 --> 00:24:55,600
就是说它尽量把你的东西

737
00:24:55,600 --> 00:24:56,600
拉回到

738
00:24:57,160 --> 00:25:00,360
0 1这种或1-1这种scale里面

739
00:25:02,280 --> 00:25:04,240
但是RN我们没有做任何东西

740
00:25:04,240 --> 00:25:05,880
所以最简单最暴力的操作

741
00:25:05,880 --> 00:25:07,200
就是做T图剪裁

742
00:25:07,640 --> 00:25:08,240
OK

743
00:25:08,280 --> 00:25:10,480
这就是我们介绍的

744
00:25:10,480 --> 00:25:12,160
第一个神经网络

745
00:25:12,160 --> 00:25:14,640
用来处理时序序列的

