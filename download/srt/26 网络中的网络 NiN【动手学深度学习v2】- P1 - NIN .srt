1
00:00:00,000 --> 00:00:04,179
一个超级宽的单影行层

2
00:00:04,179 --> 00:00:05,540
MLP难以训练

3
00:00:05,540 --> 00:00:07,700
是因为显存不够大吗

4
00:00:07,700 --> 00:00:11,220
就是说超级宽的单影行层

5
00:00:12,740 --> 00:00:14,939
不是它不是显存的问题

6
00:00:14,939 --> 00:00:17,699
它是很容易过离合

7
00:00:18,219 --> 00:00:19,219
效果很差

8
00:00:19,219 --> 00:00:23,379
它不是它确实不是京都会怎么样

9
00:00:23,379 --> 00:00:26,699
主要是在在CN上

10
00:00:26,919 --> 00:00:31,480
确实不如这种参数比较少的卷迹的效果

11
00:00:31,480 --> 00:00:35,920
就我们在之前讲为什么要用卷迹神经网络

12
00:00:35,920 --> 00:00:38,840
从CN到MLP到CN来讲

13
00:00:38,840 --> 00:00:39,920
我们有讲过这个问题

14
00:00:44,480 --> 00:00:46,280
问题二是说PyTorch跑起来

15
00:00:46,280 --> 00:00:49,439
是不是就要再用4G左右的内存

16
00:00:49,439 --> 00:00:53,519
为什么我6G的类显存VGG跑起来

17
00:00:53,519 --> 00:00:55,840
刚好跑125的Batch Size

18
00:00:57,679 --> 00:01:00,920
就是说不会应该就是说PyTorch

19
00:01:00,920 --> 00:01:02,120
或者是不管是PyTorch

20
00:01:02,120 --> 00:01:05,480
TensorFlow或者是比较M.Snap或者是别的框架

21
00:01:05,480 --> 00:01:08,439
不应该再跑起来给你占用4G的内存

22
00:01:08,439 --> 00:01:09,439
这是不会的

23
00:01:09,439 --> 00:01:13,079
这个东西你首先你看一下你的内存

24
00:01:13,079 --> 00:01:14,320
是不是有别人在用

25
00:01:14,320 --> 00:01:16,120
所以你看一下你的

26
00:01:16,120 --> 00:01:19,480
比如说你如果你的显示器是接在你的显卡上的话

27
00:01:19,480 --> 00:01:22,240
你有可能是要用一定带宽的

28
00:01:22,240 --> 00:01:23,840
第二个是说

29
00:01:23,859 --> 00:01:28,980
你不管Batch Size等于多大

30
00:01:28,980 --> 00:01:32,420
你都是要存全种的

31
00:01:32,420 --> 00:01:34,180
就是你模型的大小

32
00:01:34,939 --> 00:01:37,740
而且你要如果你要算T度的话

33
00:01:37,740 --> 00:01:38,939
T度也要存一份

34
00:01:38,939 --> 00:01:40,900
就是说等于是假设你的模型存

35
00:01:40,900 --> 00:01:41,980
你存在硬盘上

36
00:01:41,980 --> 00:01:44,180
你发现是100兆或者200兆的话

37
00:01:44,180 --> 00:01:45,340
那么它的内存里面

38
00:01:45,340 --> 00:01:47,580
它也要占200兆的GPU的显存

39
00:01:47,580 --> 00:01:49,500
然后同样T度也要占200兆

40
00:01:49,500 --> 00:01:50,780
那就400兆就没有

41
00:01:50,780 --> 00:01:53,260
然后剩下的东西几乎上你可以认为是

42
00:01:53,280 --> 00:01:55,760
等价于你的内存的

43
00:01:55,760 --> 00:01:58,080
你的P量大小的一个线性关系

44
00:01:58,080 --> 00:01:59,480
就你P量大小增大

45
00:01:59,480 --> 00:02:03,480
你剩下内存使用率应该是会增大一倍

46
00:02:04,359 --> 00:02:07,040
然后所以的话在这个地方就是说

47
00:02:07,040 --> 00:02:10,560
真的取决于你模型有多大

48
00:02:10,560 --> 00:02:12,200
如果你模型特别大的话

49
00:02:12,200 --> 00:02:14,640
那么确实你不管P量有多大

50
00:02:14,640 --> 00:02:17,120
你都是要需要占用你的显存的

51
00:02:17,120 --> 00:02:20,080
那么接下来就是说看你模型有多深

52
00:02:20,080 --> 00:02:21,599
越深的网络

53
00:02:21,699 --> 00:02:25,060
你的中间的存中间的变量要求就越高

54
00:02:25,060 --> 00:02:29,299
我们之前在自动T度求导是有讲过原理

55
00:02:29,299 --> 00:02:33,060
所以然后当然是你一个通过调节P量大小

56
00:02:33,060 --> 00:02:35,979
来使得降低它的显存的存储

57
00:02:35,979 --> 00:02:38,939
但是你别的如果你发现还有别的人在用你的话

58
00:02:38,939 --> 00:02:43,979
你要看一下到底是谁在用

59
00:02:43,979 --> 00:02:45,979
比如说你可以通过

60
00:02:45,979 --> 00:02:48,740
比如说这是我的这台机器

61
00:02:48,740 --> 00:02:50,259
我把它放大一点

62
00:02:51,599 --> 00:02:59,539
可能还得放大一点

63
00:02:59,539 --> 00:03:01,379
就是说你可以看到

64
00:03:01,379 --> 00:03:08,019
比如说这是我这台机器它的显存用户

65
00:03:08,019 --> 00:03:12,180
我在机器有大概有4个V100的一个显卡

66
00:03:12,180 --> 00:03:15,939
我其实我现在应该用的在我现在在用谁

67
00:03:15,939 --> 00:03:17,139
我一直在用零

68
00:03:17,139 --> 00:03:20,579
就是说你看到这个是我开了两个机制本

69
00:03:20,920 --> 00:03:25,880
因为我们讲到N核讲之后的exception

70
00:03:26,240 --> 00:03:32,240
所以你可以看到第一个是说我第一个用的是58862进场

71
00:03:32,240 --> 00:03:34,320
大家可以看一下58862是用什么东西

72
00:03:34,320 --> 00:03:35,360
我这就不去看了

73
00:03:35,360 --> 00:03:40,840
就是说你自己能看到就是说用的大概是3.5G的样子

74
00:03:41,120 --> 00:03:44,960
那么下一个我猜这个是GoogleNet的机制本

75
00:03:44,960 --> 00:03:50,439
然后这一个用的是258就是两个2.6G的样子

76
00:03:50,760 --> 00:03:53,520
所以最后是因为这个是你当前在用的

77
00:03:53,520 --> 00:03:59,280
这个应该是加起来就用了大概6个6G的内存的样子

78
00:03:59,280 --> 00:03:59,680
对吧

79
00:04:00,160 --> 00:04:02,080
但当实际上你看这个地方

80
00:04:02,080 --> 00:04:07,520
这个地方实际上告诉你说我这个GPU16G的内存

81
00:04:07,520 --> 00:04:11,560
你在里大概用了应该是他说你用了9G

82
00:04:11,560 --> 00:04:13,680
但实际上我没有对吧

83
00:04:13,680 --> 00:04:17,080
实际上这个

84
00:04:17,459 --> 00:04:22,740
我觉得应该是我还有一个账户

85
00:04:22,740 --> 00:04:24,779
在用了一点显存在这个地方

86
00:04:25,060 --> 00:04:32,500
此外是说这台机器我大概两年没关过机了

87
00:04:32,699 --> 00:04:34,500
一年或者两年没关过机了

88
00:04:34,500 --> 00:04:35,699
从来没重启过

89
00:04:35,699 --> 00:04:39,060
所以他好像可能有一点内存泄露在这个地方

90
00:04:39,060 --> 00:04:40,219
但我也没去看过

91
00:04:40,219 --> 00:04:42,539
就反正我也不跑大任务

92
00:04:42,539 --> 00:04:43,659
我就跑一点很小的任务

93
00:04:43,659 --> 00:04:45,699
所以我没有感觉到太多区别

94
00:04:45,699 --> 00:04:46,819
所以我也没去管他

95
00:04:47,099 --> 00:04:50,659
因为重启我就难得重启就没弄这个事情

96
00:04:50,979 --> 00:04:53,979
所以你会看到就是说我就算没有在用它

97
00:04:53,979 --> 00:04:56,659
你看到它也是显示有在用

98
00:04:56,659 --> 00:04:58,659
可能是我另外一个账号在用了它的

99
00:04:58,659 --> 00:05:01,740
就是我的整个这本书的CI的账号

100
00:05:01,740 --> 00:05:02,899
就每次你改这种东西

101
00:05:02,899 --> 00:05:04,180
我都重新要运行一遍

102
00:05:04,180 --> 00:05:05,180
他有可能在用

103
00:05:05,180 --> 00:05:06,860
但感觉上来说

104
00:05:06,860 --> 00:05:09,099
因为你的utilization是0

105
00:05:09,099 --> 00:05:11,299
所以我感觉这个更多可能是内存泄露

106
00:05:11,299 --> 00:05:14,259
或者是因为太久没关机了

107
00:05:14,259 --> 00:05:16,180
你可能关一次机就好了

108
00:05:17,300 --> 00:05:17,779
OK

109
00:05:21,459 --> 00:05:22,099
我们看看

110
00:05:24,939 --> 00:05:25,659
问题三

111
00:05:25,659 --> 00:05:28,539
请问PyTorch或M3的训练的模型

112
00:05:28,539 --> 00:05:31,459
一般采用什么上限的部署到生产环境

113
00:05:31,459 --> 00:05:34,060
从全部用C++重写吗

114
00:05:34,060 --> 00:05:37,019
还是否有类似TensorFlow Serving的解决方案

115
00:05:37,019 --> 00:05:40,259
就是说如果你想

116
00:05:40,259 --> 00:05:42,459
这是个上限问题

117
00:05:42,680 --> 00:05:44,479
就是说你是想用

118
00:05:46,079 --> 00:05:47,039
C++

119
00:05:47,039 --> 00:05:49,879
假设你是一定线上要用C++的话

120
00:05:49,899 --> 00:05:51,439
那么PyTorch的话

121
00:05:51,439 --> 00:05:54,319
你可以通过TorchScript

122
00:05:55,959 --> 00:06:00,159
转到C++那个通道

123
00:06:00,159 --> 00:06:01,799
但是那个东西用起来

124
00:06:01,799 --> 00:06:04,319
取决你模型是不是特别复杂

125
00:06:04,339 --> 00:06:06,560
如果你的模型比较复杂的话

126
00:06:06,579 --> 00:06:08,519
TorchScript不一定能够过去

127
00:06:10,479 --> 00:06:11,399
M3的话

128
00:06:11,420 --> 00:06:13,940
如果你是用的imperative的话

129
00:06:13,940 --> 00:06:18,260
你需要通过Hybridize这个操作来得到它的计算图

130
00:06:18,260 --> 00:06:19,540
得到计算图之后

131
00:06:19,560 --> 00:06:23,100
然后就可以通过C++的backend

132
00:06:23,100 --> 00:06:23,900
TensorFlow的话

133
00:06:23,900 --> 00:06:26,900
它本身上是一个symbolic的一个计算框架

134
00:06:26,900 --> 00:06:29,060
所以它总是可以拿到计算图的

135
00:06:29,060 --> 00:06:32,260
所以它也有个C++的一个部署的方案

136
00:06:32,280 --> 00:06:36,060
这就是你如果想用C++的做是这样子做的

137
00:06:36,060 --> 00:06:38,500
当然你或者你可以通过第三方的解决方案

138
00:06:38,500 --> 00:06:39,940
比如说ONNX

139
00:06:39,959 --> 00:06:41,560
就是转到ONNX的模型

140
00:06:41,560 --> 00:06:43,240
就等于是你把模型传下来

141
00:06:43,240 --> 00:06:45,600
然后转到ONNX对应的模型

142
00:06:45,620 --> 00:06:48,959
或者你用别的或TVM或者别的工具转过去

143
00:06:48,959 --> 00:06:50,240
也是到C++段

144
00:06:52,079 --> 00:06:54,759
第二个是说你说的TensorFlow Serving

145
00:06:54,759 --> 00:06:57,079
Serving和C++又是不一样的东西

146
00:06:57,100 --> 00:06:58,920
就是说我有一个模型

147
00:06:58,920 --> 00:06:59,600
对吧

148
00:06:59,620 --> 00:07:01,159
我要把它

149
00:07:01,180 --> 00:07:03,120
我要跑一个server出来

150
00:07:03,139 --> 00:07:06,959
然后使得我可以不断把图片把input给你

151
00:07:06,959 --> 00:07:08,759
你反而给我的结果

152
00:07:08,759 --> 00:07:10,740
这个Serving通常要处理是说

153
00:07:10,740 --> 00:07:12,579
假设我有N个GPU的话

154
00:07:12,599 --> 00:07:14,219
我拿一个输入

155
00:07:14,219 --> 00:07:18,139
我怎么把它Route到一个合适的GPU做运算

156
00:07:18,139 --> 00:07:21,379
以及说我尽量能够在各个地方都能跑

157
00:07:21,379 --> 00:07:23,259
能够帮你管理一下内存

158
00:07:23,279 --> 00:07:25,899
然后说允许让你这个模型下去

159
00:07:25,920 --> 00:07:27,819
那个模型上来就帮你做这个事情

160
00:07:27,819 --> 00:07:31,219
就等于是你Serve一个HTML page一样的东西

161
00:07:31,240 --> 00:07:32,980
所以TFServing的话

162
00:07:33,000 --> 00:07:36,019
就是说不管PyTorch还是M3

163
00:07:36,019 --> 00:07:37,539
它都有自己的Serving方案

164
00:07:37,540 --> 00:07:40,560
比如说你可以用NVIDIA做的那个叫Triton

165
00:07:40,560 --> 00:07:44,040
那个应该是支持upvm框架的

166
00:07:44,040 --> 00:07:47,240
而且有很多这样的Serving解决方案

167
00:07:47,260 --> 00:07:48,600
当然反过来讲

168
00:07:48,600 --> 00:07:50,480
如果你不用

169
00:07:50,500 --> 00:07:51,879
你可以说我可以用Python

170
00:07:51,879 --> 00:07:52,600
或用别的话

171
00:07:52,600 --> 00:07:53,240
更方便

172
00:07:53,260 --> 00:07:54,439
你就写个Python

173
00:07:54,460 --> 00:07:56,360
Python做Serving非常简单

174
00:07:56,379 --> 00:07:57,640
这你写个for loop就是了

175
00:07:57,640 --> 00:07:58,840
对吧

176
00:07:58,840 --> 00:07:59,120
OK

177
00:08:01,000 --> 00:08:01,640
问题四

178
00:08:01,640 --> 00:08:03,720
这里做分类不用softmax吗

179
00:08:03,740 --> 00:08:06,040
这里分类是要做softmax

180
00:08:06,040 --> 00:08:07,819
那softmax

181
00:08:07,840 --> 00:08:10,900
大家回忆一下前面我们怎么讲的

182
00:08:10,900 --> 00:08:12,100
这个问题其实是说

183
00:08:12,100 --> 00:08:16,180
我们为什么在最后使用了一个全局的平均值

184
00:08:16,180 --> 00:08:17,180
换成就没了

185
00:08:17,200 --> 00:08:20,379
然后就不需要做那个softmax

186
00:08:20,379 --> 00:08:21,740
我们要softmax

187
00:08:21,740 --> 00:08:25,420
softmax写在我们的training那个函数里面

188
00:08:25,420 --> 00:08:27,020
就写在loss里面的

189
00:08:27,040 --> 00:08:31,460
所以是不会softmax没有放在你那个网络的上面

190
00:08:31,480 --> 00:08:34,659
就是为什么我们你可以看到我们基本所有的

191
00:08:34,679 --> 00:08:36,719
全局甚至网络都没有softmax在里面

192
00:08:38,559 --> 00:08:39,240
就是

193
00:08:40,399 --> 00:08:40,959
所以啊

194
00:08:40,959 --> 00:08:42,879
问题五是跟前面是一样的

195
00:08:42,879 --> 00:08:46,600
就是adaptive average pooling相对softmax有什么优势吗

196
00:08:46,600 --> 00:08:49,439
它其实不是去替代softmax

197
00:08:49,459 --> 00:08:52,279
它是去替代最后那个全连接层的

198
00:08:53,439 --> 00:08:53,799
你解吗

199
00:08:53,799 --> 00:08:55,120
就是说

200
00:08:56,279 --> 00:08:59,159
比如说我们之前有讲

201
00:08:59,159 --> 00:09:00,360
NullNet会怎么样

202
00:09:00,860 --> 00:09:03,580
嗯

203
00:09:04,379 --> 00:09:06,379
就是要加就还是AlexNet吧

204
00:09:06,379 --> 00:09:11,300
AlexNet你在最后的最后你要有一个输出层

205
00:09:12,899 --> 00:09:13,300
对吧

206
00:09:13,300 --> 00:09:14,419
它也是没有softmax

207
00:09:14,419 --> 00:09:17,779
因为softmax是写在这个里面

208
00:09:17,779 --> 00:09:21,580
我们的softmax是放在这个函数里面

209
00:09:21,580 --> 00:09:23,139
就trainCH6

210
00:09:23,139 --> 00:09:25,379
大家可以回过去看一下这个函数怎么实现

211
00:09:26,379 --> 00:09:27,259
然后呢

212
00:09:27,439 --> 00:09:28,600
所以啊

213
00:09:28,620 --> 00:09:30,480
你如果你不管是用谁啊

214
00:09:30,500 --> 00:09:31,720
NullNet也好

215
00:09:31,740 --> 00:09:32,279
别人也好

216
00:09:32,279 --> 00:09:34,039
或者是之前的MLP也好

217
00:09:34,059 --> 00:09:36,279
我们最后有一个输出层

218
00:09:36,299 --> 00:09:38,360
就是你要前面你不管是多大

219
00:09:38,360 --> 00:09:42,360
最后我要通过一个全连接层输出到你等于类别的那个个数

220
00:09:43,600 --> 00:09:45,360
就是全局啊

221
00:09:45,379 --> 00:09:48,519
那个平均齿化层替代的是这个输出层

222
00:09:49,919 --> 00:09:50,279
OK

223
00:09:50,299 --> 00:09:53,200
就是说把你的高宽里面的东西全部压到一个

224
00:09:53,200 --> 00:09:56,240
然后你的通道数就等于你的输出的类别数

225
00:09:56,259 --> 00:09:58,180
所以这个是它按个层的干的作用

226
00:10:01,019 --> 00:10:05,740
问题六是说全局的平均齿化层在设计上是不是很关键

227
00:10:05,740 --> 00:10:08,379
是不是会带来影响有哪一些啊

228
00:10:08,379 --> 00:10:10,539
这个这是这是一个很好的问题啊

229
00:10:10,539 --> 00:10:12,820
我们可能会之后也会多次提到

230
00:10:12,820 --> 00:10:18,620
就是说这个设计思想给后面带来了非常大的那个影响

231
00:10:18,620 --> 00:10:21,980
后来发现大家发现这个东西挺好用的

232
00:10:21,980 --> 00:10:25,340
就是把你所有的就是说

233
00:10:25,340 --> 00:10:27,680
等着你等着你干个什么事情啊

234
00:10:27,680 --> 00:10:30,120
就是说我在啊

235
00:10:30,120 --> 00:10:32,639
我在这个地方也能够加平啊

236
00:10:32,660 --> 00:10:33,560
全局的齿化层

237
00:10:33,560 --> 00:10:33,879
对吧

238
00:10:38,720 --> 00:10:40,200
我在这个地方也是能加的

239
00:10:40,220 --> 00:10:40,720
对吧

240
00:10:40,720 --> 00:10:42,680
因为我就在啊

241
00:10:42,680 --> 00:10:46,240
这个地方我就在每次我可以在卷机后面加对

242
00:10:46,240 --> 00:10:49,960
所以他唯一的干的作用就是把我的通道数啊

243
00:10:49,960 --> 00:10:51,080
不通道数不改变

244
00:10:51,100 --> 00:10:55,100
就把我的那个卷机的那个feature那个输出

245
00:10:55,100 --> 00:10:56,460
他的高宽压成了1

246
00:10:56,480 --> 00:10:57,180
不管是多大

247
00:10:57,180 --> 00:10:59,740
7x7好5x5都变成1x1

248
00:10:59,740 --> 00:11:01,180
那他的主要效果是什么

249
00:11:01,180 --> 00:11:03,620
他把输入变小了

250
00:11:03,620 --> 00:11:05,820
而且他没有可学习的参数

251
00:11:05,840 --> 00:11:09,500
所以你会看到是说加入全局齿化层

252
00:11:09,500 --> 00:11:11,700
当然是说你可以让计算变简单了

253
00:11:11,720 --> 00:11:16,500
他的主要的好处是你可以认为他把模型复杂度降低了

254
00:11:16,519 --> 00:11:19,679
是一个很强的一个操作

255
00:11:19,679 --> 00:11:21,639
来把模型复杂度降低

256
00:11:21,659 --> 00:11:26,000
大家发现加入他之后会提升你的泛化性

257
00:11:26,000 --> 00:11:28,679
就基本上就是会让你的精度变得更好

258
00:11:28,679 --> 00:11:30,720
所以这个层会被大量的使用

259
00:11:33,480 --> 00:11:35,480
第二个是带他有一个坏处

260
00:11:35,480 --> 00:11:37,519
他的话就是让你的收敛变慢了

261
00:11:39,000 --> 00:11:40,519
就说

262
00:11:40,519 --> 00:11:42,879
就说你把这个后面东西这么变小吗

263
00:11:42,879 --> 00:11:45,639
使得后面的全连接层啊

264
00:11:45,659 --> 00:11:47,740
他这里他NNN没有全连接层

265
00:11:47,740 --> 00:11:52,179
假设我要在这个COMP2D后面插入一个全局的齿化层的话

266
00:11:52,179 --> 00:11:57,299
那么之后你的输入到你的DANCE输入到全连接层的那个东西

267
00:11:57,299 --> 00:11:58,539
当然会变小很多啊

268
00:11:58,539 --> 00:11:59,699
就变小25倍吧

269
00:11:59,699 --> 00:12:00,860
对他来说

270
00:12:00,879 --> 00:12:02,939
他当然是啊

271
00:12:02,939 --> 00:12:05,819
他会让你的收敛变得很慢

272
00:12:05,819 --> 00:12:12,620
是因为AlexNet和VGG之所以收敛快是因为这两个层实在是太强了

273
00:12:12,620 --> 00:12:15,299
太容易去fit这个数据了

274
00:12:15,300 --> 00:12:17,640
所以你现在比如说用AlexNet和VGG

275
00:12:17,640 --> 00:12:22,720
你能够训练个30个50个数据

276
00:12:22,720 --> 00:12:23,800
pass 50个数据

277
00:12:23,800 --> 00:12:25,360
你可能就训练好了

278
00:12:25,360 --> 00:12:29,080
加入全连接层的话和别的后面的操作的话

279
00:12:29,100 --> 00:12:31,400
现在大家都要训练个120了

280
00:12:31,400 --> 00:12:33,680
就是要扫120个数据

281
00:12:33,680 --> 00:12:35,160
就是说多扫两三倍数据

282
00:12:36,600 --> 00:12:36,920
OK

283
00:12:36,920 --> 00:12:40,000
所以是说这是他的好处和不好处

284
00:12:40,000 --> 00:12:41,440
对当然是说

285
00:12:41,460 --> 00:12:42,400
绝大部分情况来讲

286
00:12:42,400 --> 00:12:44,640
我们训练久一点不要紧

287
00:12:44,659 --> 00:12:45,980
就多训练几个小时不要紧

288
00:12:45,980 --> 00:12:47,139
精度好才是关键

289
00:12:49,699 --> 00:12:51,419
这问题其实说

290
00:12:51,419 --> 00:12:55,100
为什么NN块中需要两个一乘一的卷进算

291
00:12:55,120 --> 00:12:58,259
不是一个或者三个是参数数量的原因吗

292
00:12:58,259 --> 00:13:00,819
为什么选用两个这个东西

293
00:13:04,179 --> 00:13:06,139
其实我觉得是试出来的

294
00:13:06,139 --> 00:13:09,179
其实我没有感觉谁有在解释

295
00:13:09,179 --> 00:13:11,139
为什么用一个不用一个用三个

296
00:13:11,159 --> 00:13:13,580
我猜是当时候他去刷

297
00:13:14,480 --> 00:13:17,360
就是说他是从AlexNet那边卷积层拿过去

298
00:13:17,360 --> 00:13:19,280
再加两个一乘一卷积

299
00:13:19,280 --> 00:13:20,280
你可能换成别人

300
00:13:20,280 --> 00:13:21,759
可能就用一个混用三个

301
00:13:21,759 --> 00:13:22,920
说不定是有变化

302
00:13:22,920 --> 00:13:26,320
但是我觉得这个是我们是目前是没有做过实验

303
00:13:26,320 --> 00:13:26,800
不知道

304
00:13:30,800 --> 00:13:34,280
就是说问题8是说请再解释一下

305
00:13:34,280 --> 00:13:37,040
我们说到两个一乘一的卷积层

306
00:13:37,040 --> 00:13:39,759
对每个像素增加了非线信息是什么意思

307
00:13:40,000 --> 00:13:43,480
就是说我们知道一乘一的卷积层

308
00:13:43,680 --> 00:13:47,360
他我们有在讲过这样讲卷积层那一节的时候

309
00:13:47,360 --> 00:13:49,560
我们特地讲过一乘一卷积层是干嘛

310
00:13:49,560 --> 00:13:52,440
他说白了就是对每一个像素

311
00:13:53,280 --> 00:13:54,520
它的对应的通道

312
00:13:54,520 --> 00:13:55,280
那个向量

313
00:13:56,080 --> 00:13:57,480
做了一个全连接层对吧

314
00:13:57,920 --> 00:14:00,960
然后把这一个全连接层对每个像素分别做远一点

315
00:14:02,440 --> 00:14:04,840
那么现在你用了两个一乘一的卷积层

316
00:14:04,840 --> 00:14:05,639
意味着什么

317
00:14:05,639 --> 00:14:08,680
意味着说我对每个像素它的输入通道数

318
00:14:08,680 --> 00:14:13,080
做了一个有两个隐含层的一个MLP

319
00:14:14,560 --> 00:14:16,879
MLP中间有个redu这个函数

320
00:14:16,879 --> 00:14:20,120
就不管一层两层它都增加了非线信息

321
00:14:21,120 --> 00:14:21,400
对吧

322
00:14:21,400 --> 00:14:23,920
这就是这句话的意思

323
00:14:29,120 --> 00:14:32,360
问题九可以演示一下预测代码吗

324
00:14:32,360 --> 00:14:34,320
我弄了几次都有问题

325
00:14:34,320 --> 00:14:36,040
预测代码

326
00:14:36,560 --> 00:14:38,960
预测代码我建议大家去看一下

327
00:14:38,960 --> 00:14:40,160
Nernet的实现

328
00:14:40,160 --> 00:14:41,680
Nernet有

329
00:14:44,000 --> 00:14:47,840
Nernet我们有预测代码吗

330
00:14:47,840 --> 00:14:49,920
Nernet我们没有

331
00:14:49,920 --> 00:14:51,280
我们用的是MLP

332
00:14:53,280 --> 00:14:55,040
MLP是有的

333
00:15:04,000 --> 00:15:05,000
MLP也没有

334
00:15:05,000 --> 00:15:05,920
那就是

335
00:15:06,520 --> 00:15:08,000
我想想谁有

336
00:15:08,000 --> 00:15:10,400
那么他有吧

337
00:15:12,320 --> 00:15:13,360
他总有吧

338
00:15:13,580 --> 00:15:23,160
对

339
00:15:23,180 --> 00:15:24,040
就是说

340
00:15:24,560 --> 00:15:26,120
我们这里是有一个了

341
00:15:26,120 --> 00:15:29,600
你可以去用这个函数

342
00:15:29,600 --> 00:15:31,840
就是说我们在这个函数

343
00:15:31,840 --> 00:15:34,879
这个函数是有你可以去copy这个东西

344
00:15:34,879 --> 00:15:38,080
我们predict CH6好像可能还真没有

345
00:15:38,080 --> 00:15:39,480
这个函数大家可以去找一下

346
00:15:39,480 --> 00:15:42,280
我们有没有predict下滑线CH6这个函数

347
00:15:42,299 --> 00:15:44,459
我就这一步给大家搜了

348
00:15:44,459 --> 00:15:45,139
如果没有的话

349
00:15:45,159 --> 00:15:46,459
你就去用它

350
00:15:46,480 --> 00:15:50,500
然后你的唯一的问唯一的区别

351
00:15:50,500 --> 00:15:53,299
就是说你的X得copy到GPU上

352
00:15:54,659 --> 00:15:56,579
然后就是说你唯一的就是把那个X

353
00:15:56,579 --> 00:15:57,659
copy到GPU上

354
00:15:57,679 --> 00:16:00,179
然后你的predict要拿出来之后

355
00:16:00,199 --> 00:16:02,459
你要copy回CPU

356
00:16:02,480 --> 00:16:03,899
就是因为我们在GPU做预测

357
00:16:03,919 --> 00:16:06,059
所以你要要干一下这个事情

358
00:16:06,059 --> 00:16:06,379
OK

359
00:16:06,759 --> 00:16:12,779
那问题是PyTorch构建的网络权重是自动

360
00:16:12,799 --> 00:16:13,759
初始化的吗

361
00:16:13,759 --> 00:16:17,399
它自动是给你初始化的

362
00:16:17,419 --> 00:16:24,000
但我们这里在在下滑线CH6里面有去手动的

363
00:16:24,019 --> 00:16:25,000
初始化这个权重

364
00:16:25,019 --> 00:16:27,759
这是为了得到我们可控性

365
00:16:28,200 --> 00:16:30,879
所以它自动会帮你做初始化

366
00:16:30,879 --> 00:16:31,679
这是没错的

367
00:16:31,679 --> 00:16:33,679
我们没有它默认的选项

368
00:16:33,679 --> 00:16:34,799
还是用的手动的选项

369
00:16:34,800 --> 00:16:37,980
因为我们书里面有讲过权重大概是怎么样子

370
00:16:38,000 --> 00:16:40,700
所以我们想跟书里面的东西一一对应起来

