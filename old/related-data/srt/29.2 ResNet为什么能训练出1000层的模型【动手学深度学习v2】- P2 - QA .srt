1
00:00:00,000 --> 00:00:04,860
第一个问题是说学习率可不可以让靠近输入的输出的小一点

2
00:00:04,860 --> 00:00:06,540
靠近输入的大一点

3
00:00:06,540 --> 00:00:08,779
这样会不会可以缓解T2小射的问题

4
00:00:08,779 --> 00:00:09,939
可以的

5
00:00:09,939 --> 00:00:10,900
确实是可以的

6
00:00:10,900 --> 00:00:17,900
这个在有很多叫layer wise的这种东西

7
00:00:17,900 --> 00:00:22,420
它确实有很大的空间在的地方

8
00:00:22,420 --> 00:00:24,539
但是它的问题是什么样子

9
00:00:24,539 --> 00:00:26,660
你其实不是那么的好射

10
00:00:27,320 --> 00:00:31,679
你不知道到底靠近输入要射到多大一点

11
00:00:31,679 --> 00:00:33,320
靠近输出要射多大一点

12
00:00:33,320 --> 00:00:34,520
就是说你得去调

13
00:00:34,520 --> 00:00:38,799
就加入一个新的一个这样子的一个hyperparameter

14
00:00:38,799 --> 00:00:40,240
叫超参数

15
00:00:40,240 --> 00:00:42,079
就你不知道谁要大一点

16
00:00:42,079 --> 00:00:43,799
谁要大多少小多少

17
00:00:43,799 --> 00:00:46,000
所以就是说给你带来很大问题

18
00:00:46,000 --> 00:00:47,760
所以如果你能通过结构上

19
00:00:47,760 --> 00:00:49,760
就是说residual connection相对来说比较简单

20
00:00:49,760 --> 00:00:50,840
你不需要调太多东西

21
00:00:51,920 --> 00:00:53,000
这样的话

22
00:00:53,000 --> 00:00:56,120
通常大家会希望是更简单的方案

23
00:00:57,520 --> 00:00:59,440
而且另外一个是说

24
00:00:59,440 --> 00:01:02,200
你调血虚率还是有局限性的

25
00:01:02,200 --> 00:01:06,400
因为你的T2大到超过你的伏点32位的

26
00:01:06,400 --> 00:01:07,880
比较有效的值的时候

27
00:01:07,880 --> 00:01:08,719
你会出问题

28
00:01:08,719 --> 00:01:10,760
当你小到很小的时候会变成0

29
00:01:10,760 --> 00:01:12,359
就我们之前讲过

30
00:01:12,359 --> 00:01:13,960
特别是用FP16

31
00:01:13,960 --> 00:01:15,480
就是16位伏点数的话

32
00:01:15,480 --> 00:01:16,600
你这个问题更明显一点

33
00:01:18,879 --> 00:01:19,800
问题二是说

34
00:01:19,800 --> 00:01:22,400
为什么深的神级网络底层训练比较难

35
00:01:22,400 --> 00:01:24,760
是因为他拿到的T2比较小

36
00:01:24,760 --> 00:01:25,040
对的

37
00:01:25,040 --> 00:01:26,960
我们在T2那一章有讲过

38
00:01:26,960 --> 00:01:30,600
T2消失和T2爆炸那一节有讲过

39
00:01:30,600 --> 00:01:32,960
因为你这T2是雷层

40
00:01:32,960 --> 00:01:34,160
就一直到最后

41
00:01:34,160 --> 00:01:36,640
你就是T2会变得越来越小

42
00:01:36,640 --> 00:01:38,920
或者你从误差的角度来讲

43
00:01:38,920 --> 00:01:41,000
就是说你的T2回传的时候

44
00:01:41,000 --> 00:01:44,320
就是说你的误差对你的预测值的误差

45
00:01:44,320 --> 00:01:47,320
然后随着网络往下降

46
00:01:47,320 --> 00:01:51,400
就是你的T2值跟你的误差值是有一定关系的

47
00:01:51,400 --> 00:01:53,520
然后你在前面一些层

48
00:01:53,519 --> 00:01:55,679
会慢慢的吸收掉他的误差

49
00:01:55,679 --> 00:01:56,799
然后越到下面

50
00:01:56,799 --> 00:01:58,599
越到下面的误差变得越来越小

51
00:01:58,599 --> 00:02:00,239
所以你的T2就变小

52
00:02:01,199 --> 00:02:01,920
OK

53
00:02:01,920 --> 00:02:06,039
这就是说这个也是为什么在很多年之内

54
00:02:06,039 --> 00:02:07,439
就过去几十年

55
00:02:07,439 --> 00:02:10,039
我们做深度神级网络都不成成功

56
00:02:10,039 --> 00:02:11,960
就是因为T2这个比较麻烦

57
00:02:11,960 --> 00:02:13,719
就大家一直没有做特别好

58
00:02:13,719 --> 00:02:16,120
使得训练深的神级网络一直很难

