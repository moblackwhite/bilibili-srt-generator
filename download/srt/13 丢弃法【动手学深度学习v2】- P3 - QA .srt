1
00:00:00,000 --> 00:00:02,720
好我们来看一下我们的实现

2
00:00:03,839 --> 00:00:09,120
首先我们实现一个dropout layer的一个函数

3
00:00:10,400 --> 00:00:13,919
这个函数就是当你输入一个x的话

4
00:00:13,919 --> 00:00:15,000
就是一个张量

5
00:00:15,000 --> 00:00:17,280
我就以dropout的概率

6
00:00:17,280 --> 00:00:19,440
就是dropout就是这样的p的概率

7
00:00:19,440 --> 00:00:21,800
随即丢掉我们张量中的元素

8
00:00:23,160 --> 00:00:26,000
就是说我们就是一个这样子的函数

9
00:00:26,440 --> 00:00:27,760
这函数也挺简单的

10
00:00:27,760 --> 00:00:28,199
是不是

11
00:00:30,440 --> 00:00:31,760
输表在这个地方

12
00:00:32,520 --> 00:00:33,280
就是说

13
00:00:34,439 --> 00:00:37,240
当你dropout的p得大于等于0

14
00:00:37,240 --> 00:00:38,000
小于等于1

15
00:00:38,000 --> 00:00:38,320
对吧

16
00:00:38,320 --> 00:00:39,240
如果你不行的话

17
00:00:39,240 --> 00:00:41,399
那就是出问题了

18
00:00:43,519 --> 00:00:46,280
那么如果你的dropout等于1的话

19
00:00:46,760 --> 00:00:47,799
那我就给偷个懒

20
00:00:48,000 --> 00:00:49,400
我就是返回一个0了

21
00:00:49,400 --> 00:00:50,280
返回全0

22
00:00:50,480 --> 00:00:52,200
跟x一样shape的

23
00:00:52,240 --> 00:00:57,719
一样数据类型的一个全0的张量

24
00:00:58,719 --> 00:01:00,159
如果你是等于0的话

25
00:01:00,159 --> 00:01:01,079
就dropout的概率

26
00:01:01,079 --> 00:01:02,679
就丢弃的概率等于0的话

27
00:01:02,679 --> 00:01:03,920
那就表示不用丢

28
00:01:04,560 --> 00:01:06,879
那就是说直接返回x

29
00:01:09,640 --> 00:01:10,719
那么接下来

30
00:01:11,039 --> 00:01:12,239
接下来就是

31
00:01:13,280 --> 00:01:15,319
这里我们其实我之前改了

32
00:01:15,519 --> 00:01:16,519
这个是

33
00:01:16,840 --> 00:01:19,239
random

34
00:01:19,239 --> 00:01:20,319
其实你不用那么写

35
00:01:21,000 --> 00:01:21,759
就是

36
00:01:24,960 --> 00:01:26,039
其实你可以这么写

37
00:01:28,439 --> 00:01:33,599
我们之前是用的一个比较老一点的版本

38
00:01:34,159 --> 00:01:34,719
然后

39
00:01:35,120 --> 00:01:35,920
英文版我改了

40
00:01:35,920 --> 00:01:37,359
中文版可能还没同步过来

41
00:01:37,599 --> 00:01:38,280
就是说

42
00:01:38,560 --> 00:01:39,120
不然的话

43
00:01:39,280 --> 00:01:41,480
它其实这个函数

44
00:01:41,480 --> 00:01:45,319
就是生成一个0到1之间的一个均匀

45
00:01:45,319 --> 00:01:46,079
随机分布

46
00:01:46,079 --> 00:01:46,599
然后取

47
00:01:47,280 --> 00:01:49,560
如果它大于我的dropout的话

48
00:01:49,799 --> 00:01:51,159
我就把它选出来

49
00:01:51,159 --> 00:01:51,879
就等于1

50
00:01:52,879 --> 00:01:54,680
不然的话我就是等于0

51
00:01:55,560 --> 00:01:58,840
然后我就是我的mask乘以它

52
00:01:59,240 --> 00:01:59,800
它一乘

53
00:02:00,320 --> 00:02:02,600
就是说如果我的那就在我的

54
00:02:04,440 --> 00:02:05,960
一定概率它就变成0

55
00:02:05,960 --> 00:02:07,240
一定概率它就变成1

56
00:02:07,400 --> 00:02:09,840
然后要除以我的1减dropout这个

57
00:02:09,960 --> 00:02:10,319
率

58
00:02:10,319 --> 00:02:11,920
就是说之前我们是这么算的

59
00:02:12,080 --> 00:02:14,400
所以这个mask就是说来mask

60
00:02:14,439 --> 00:02:16,200
哪些东西是要

61
00:02:16,439 --> 00:02:17,680
被等于0的

62
00:02:17,879 --> 00:02:19,560
哪些东西是要等于1的

63
00:02:20,719 --> 00:02:23,760
所以这就是一个很简单的一个

64
00:02:24,960 --> 00:02:25,360
函数

65
00:02:26,240 --> 00:02:28,760
而且为什么这里我们不是通常用的

66
00:02:28,760 --> 00:02:29,680
non-pilot的写法

67
00:02:29,680 --> 00:02:31,159
就是说我在x中

68
00:02:31,879 --> 00:02:33,760
随机选一些值

69
00:02:33,760 --> 00:02:34,879
把它设成0

70
00:02:34,879 --> 00:02:36,319
我不是写成那种

71
00:02:36,319 --> 00:02:38,000
比如说我这里就不实现了

72
00:02:38,000 --> 00:02:39,159
就是说x等于

73
00:02:39,400 --> 00:02:41,200
mask等于0

74
00:02:41,200 --> 00:02:42,760
不这么写是因为

75
00:02:43,120 --> 00:02:44,120
这样子它

76
00:02:44,400 --> 00:02:47,280
对GPU和CPU它都不是很好

77
00:02:47,280 --> 00:02:48,960
就是说做乘法

78
00:02:49,080 --> 00:02:51,280
远远的比去选函数

79
00:02:51,280 --> 00:02:53,760
就选一个元素来得快

80
00:02:53,879 --> 00:02:55,719
所以就是说经常我们会看到说

81
00:02:55,719 --> 00:02:57,840
你感觉好像直觉上来说

82
00:02:57,840 --> 00:02:59,239
从CS角度来讲

83
00:02:59,239 --> 00:03:00,879
我就是选一些相比上0

84
00:03:01,239 --> 00:03:03,319
但是我们会用一个mask来实现

85
00:03:03,439 --> 00:03:04,479
这样子的话

86
00:03:04,519 --> 00:03:05,759
主要是

87
00:03:06,359 --> 00:03:07,719
之后我们GPU的话

88
00:03:07,719 --> 00:03:08,679
算起来会比较快

89
00:03:10,359 --> 00:03:10,879
OK

90
00:03:14,039 --> 00:03:14,680
我们可以看一下

91
00:03:14,879 --> 00:03:17,159
这个其实是一个很简单的函数

92
00:03:17,239 --> 00:03:18,359
我们来测一下

93
00:03:18,359 --> 00:03:19,519
x就是

94
00:03:19,560 --> 00:03:22,039
生成一个2和2到8的一个

95
00:03:22,039 --> 00:03:22,959
从0开始

96
00:03:22,960 --> 00:03:24,439
足以递增的一个函数

97
00:03:24,439 --> 00:03:25,560
而一个张量

98
00:03:25,599 --> 00:03:26,400
那就是

99
00:03:28,120 --> 00:03:29,320
x等于这个对吧

100
00:03:29,439 --> 00:03:30,200
01234

101
00:03:30,200 --> 00:03:32,120
一直到131415

102
00:03:33,800 --> 00:03:36,000
然后如果我的dropout

103
00:03:36,000 --> 00:03:37,719
ray等于0的话

104
00:03:39,120 --> 00:03:40,600
那么就不会变化

105
00:03:40,600 --> 00:03:40,840
对吧

106
00:03:40,840 --> 00:03:41,960
就返回自己

107
00:03:45,280 --> 00:03:47,439
就如果我是0.5的话

108
00:03:47,439 --> 00:03:49,040
那个看到是有一定概率

109
00:03:49,040 --> 00:03:50,319
有50%的概率

110
00:03:50,319 --> 00:03:51,920
它就会被重新

111
00:03:52,040 --> 00:03:53,680
被它变成0

112
00:03:54,400 --> 00:03:56,680
这里有5个变成0

113
00:03:56,720 --> 00:03:57,960
但你6个变成0

114
00:03:57,960 --> 00:03:59,800
但你重新运行一下的话

115
00:03:59,800 --> 00:04:01,320
你下次会变得会不一样

116
00:04:02,760 --> 00:04:04,320
当然你如果dropout等于1的话

117
00:04:04,320 --> 00:04:05,440
那就全部变成0了

118
00:04:05,440 --> 00:04:06,480
就是我们之前

119
00:04:06,560 --> 00:04:07,840
CS的逻辑

120
00:04:08,240 --> 00:04:08,520
OK

121
00:04:08,520 --> 00:04:09,440
这就是说

122
00:04:09,440 --> 00:04:10,800
而且它的主要是说

123
00:04:10,800 --> 00:04:11,320
可以记得

124
00:04:11,480 --> 00:04:13,960
我们每一次运行的时候

125
00:04:13,960 --> 00:04:16,200
我们会去调用随机生成数

126
00:04:16,240 --> 00:04:19,720
使得你每一次被制成0的是不一样的

127
00:04:20,000 --> 00:04:21,400
这就是它的精髓

128
00:04:21,399 --> 00:04:23,519
如果你每一次都随机的制成0

129
00:04:23,519 --> 00:04:25,239
固定的制成0的话

130
00:04:25,239 --> 00:04:27,159
等价于是我神经网络

131
00:04:27,159 --> 00:04:29,199
就是我就把隐藏层大小变小了

132
00:04:29,199 --> 00:04:30,079
对吧

133
00:04:30,079 --> 00:04:31,919
一定要随机才会有我们的效果

134
00:04:35,560 --> 00:04:35,919
好

135
00:04:35,919 --> 00:04:37,319
接下来是说

136
00:04:38,079 --> 00:04:39,719
我们定一下我们一个

137
00:04:39,719 --> 00:04:42,039
我们定一个有两个隐藏层的

138
00:04:42,039 --> 00:04:43,159
多层感知机

139
00:04:43,319 --> 00:04:45,759
然后每个隐藏层有256个单元

140
00:04:46,359 --> 00:04:48,279
就是说可以看到

141
00:04:48,439 --> 00:04:49,159
可以看一下

142
00:04:49,199 --> 00:04:50,479
这里我就不仔细讲

143
00:04:50,480 --> 00:04:51,759
就是说唯一的区别是说

144
00:04:51,759 --> 00:04:53,560
我们要区别是不是在训练

145
00:04:53,560 --> 00:04:54,400
is training

146
00:04:55,000 --> 00:04:56,360
因为我们是

147
00:04:56,720 --> 00:04:57,840
如果是一圈里的

148
00:04:57,840 --> 00:04:58,640
true的话

149
00:04:58,960 --> 00:05:01,000
我们要给进来

150
00:05:01,000 --> 00:05:02,080
就是说是不是在训练

151
00:05:02,080 --> 00:05:02,720
是在测试

152
00:05:02,720 --> 00:05:03,400
在训练

153
00:05:04,400 --> 00:05:07,040
然后当我们是有两个隐藏层

154
00:05:07,040 --> 00:05:08,680
那就是有三个线性层

155
00:05:08,920 --> 00:05:09,960
这一个是

156
00:05:10,120 --> 00:05:11,560
这是第一个隐藏层

157
00:05:11,840 --> 00:05:12,720
第二个隐藏层

158
00:05:12,720 --> 00:05:14,080
最后一个是输出层

159
00:05:14,319 --> 00:05:15,680
那relu我们要放在这里

160
00:05:16,160 --> 00:05:18,120
可以唯一的可以看到是说

161
00:05:18,160 --> 00:05:20,439
主要的是在这个地方

162
00:05:20,439 --> 00:05:21,279
我把这个

163
00:05:21,439 --> 00:05:22,120
搞

164
00:05:23,639 --> 00:05:25,000
控制好删掉一点

165
00:05:25,480 --> 00:05:26,759
可以看到是说

166
00:05:26,800 --> 00:05:28,079
主要就在这个地方

167
00:05:29,680 --> 00:05:32,399
这个是第一个隐藏层的输出

168
00:05:32,480 --> 00:05:33,240
就看到是说

169
00:05:33,240 --> 00:05:34,839
我把x reshape以后

170
00:05:34,839 --> 00:05:36,360
变到我第一个隐藏层

171
00:05:36,480 --> 00:05:38,800
然后做一下我们的relu

172
00:05:39,079 --> 00:05:40,720
然后拿到我们的hd

173
00:05:41,199 --> 00:05:42,639
如果是在训练

174
00:05:43,000 --> 00:05:45,159
那我们作用一下dropout

175
00:05:47,720 --> 00:05:48,600
如果不是在训练

176
00:05:48,600 --> 00:05:49,480
我们就不作用了

177
00:05:49,480 --> 00:05:51,280
就是测试的时候就不作用

178
00:05:53,520 --> 00:05:55,360
那么接下来把h1

179
00:05:55,360 --> 00:05:57,400
放到我的第二个隐藏层

180
00:05:57,920 --> 00:05:59,840
拿到我的计算我的h2

181
00:06:00,480 --> 00:06:01,000
同样的话

182
00:06:01,000 --> 00:06:02,400
我们知道我们的dropout

183
00:06:02,400 --> 00:06:04,720
一般就是作用在隐藏层的输出上

184
00:06:04,720 --> 00:06:06,439
全连接隐藏层的输出上

185
00:06:06,879 --> 00:06:07,240
所以

186
00:06:09,120 --> 00:06:11,840
我就对它又做一次dropout

187
00:06:13,160 --> 00:06:14,200
用第二个概率

188
00:06:14,600 --> 00:06:15,840
最后我的输出层

189
00:06:16,040 --> 00:06:17,319
是不作用dropout的

190
00:06:18,360 --> 00:06:19,200
输出

191
00:06:19,399 --> 00:06:20,680
输出你还作用dropout

192
00:06:20,680 --> 00:06:22,599
就是那些概率都变成0了

193
00:06:22,599 --> 00:06:25,000
那就太强了

194
00:06:25,000 --> 00:06:29,039
这个反而这个政策效果

195
00:06:30,439 --> 00:06:30,920
OK

196
00:06:31,800 --> 00:06:32,959
就剩下的我就

197
00:06:33,319 --> 00:06:34,719
就是说我们直接训练一下

198
00:06:34,719 --> 00:06:36,000
就是可以看到是

199
00:06:38,680 --> 00:06:39,480
就是说

200
00:06:40,480 --> 00:06:41,279
可以看到是说

201
00:06:41,279 --> 00:06:42,959
我们现在的训练效果

202
00:06:42,959 --> 00:06:45,120
大家如果还记得上一次

203
00:06:45,120 --> 00:06:46,480
我们讲过的话

204
00:06:46,599 --> 00:06:49,000
可以看到我们其实跟上一次讲的

205
00:06:49,000 --> 00:06:49,600
差不多

206
00:06:49,600 --> 00:06:50,920
就是说你看到我们的

207
00:06:50,959 --> 00:06:51,879
可能我的精度

208
00:06:51,879 --> 00:06:53,399
还稍微高那么一点点

209
00:06:53,600 --> 00:06:55,120
然后我的损失

210
00:06:55,120 --> 00:06:57,240
我的训练的损失

211
00:06:57,240 --> 00:06:58,680
也没有低太多

212
00:06:58,680 --> 00:06:59,920
上次我记得是

213
00:06:59,920 --> 00:07:01,759
也是0.4往下去一点点

214
00:07:02,920 --> 00:07:03,600
就是说

215
00:07:03,639 --> 00:07:05,120
但是不一样的地方在哪里

216
00:07:05,720 --> 00:07:07,639
不一样的地方在于说

217
00:07:07,680 --> 00:07:10,399
我们之前是一个单隐藏层的

218
00:07:10,399 --> 00:07:13,199
一个MLP

219
00:07:13,240 --> 00:07:14,920
现在我们用的双隐藏层

220
00:07:14,920 --> 00:07:16,439
而且是256

221
00:07:16,600 --> 00:07:19,120
就是说我的隐藏层大小是256

222
00:07:19,160 --> 00:07:21,240
其实是一个非常大的一个

223
00:07:21,240 --> 00:07:22,360
对我们小数据

224
00:07:22,560 --> 00:07:24,759
这个实际网络已经算比较大了

225
00:07:24,959 --> 00:07:27,360
所以他的dropout

226
00:07:27,519 --> 00:07:29,519
在这里是起了很多作用的

227
00:07:30,680 --> 00:07:31,480
就是说

228
00:07:32,120 --> 00:07:33,519
这dropout我们第一个

229
00:07:33,519 --> 00:07:34,360
我就随机选的

230
00:07:34,680 --> 00:07:36,959
第一个是取了个0.5

231
00:07:37,120 --> 00:07:38,279
取了个0.2

232
00:07:39,120 --> 00:07:40,159
就是这样子

233
00:07:40,360 --> 00:07:41,639
但我会给大家试一下

234
00:07:41,759 --> 00:07:43,720
如果我看看时间来不来得及

235
00:07:44,680 --> 00:07:45,800
来得及应该

236
00:07:46,160 --> 00:07:46,920
给大家试一下

237
00:07:46,960 --> 00:07:47,600
就是

238
00:07:48,000 --> 00:07:50,120
假设我取的是

239
00:07:50,319 --> 00:07:50,920
我取0

240
00:07:52,079 --> 00:07:53,560
假设不用dropout会怎么样

241
00:07:56,360 --> 00:07:57,759
记得我们的

242
00:07:58,199 --> 00:08:01,400
损失在0.4往下一点点

243
00:08:01,400 --> 00:08:02,680
我们的精度在这个地方

244
00:08:03,960 --> 00:08:05,360
我们假设来

245
00:08:05,720 --> 00:08:06,600
跑一跑

246
00:08:06,600 --> 00:08:16,000
我们这个跑起来比较慢

247
00:08:16,560 --> 00:08:18,000
因为我在一个

248
00:08:18,000 --> 00:08:21,400
我现在直播用的iMac上面

249
00:08:21,680 --> 00:08:24,080
我还得我的录屏还得录了

250
00:08:24,840 --> 00:08:26,640
所以他得大家抢CPU

251
00:08:29,439 --> 00:08:30,760
可以等一等看一下

252
00:08:30,760 --> 00:08:31,520
我们可以

253
00:08:31,760 --> 00:08:32,680
在等的时候

254
00:08:32,680 --> 00:08:34,000
我们可以往前走一走

255
00:08:34,600 --> 00:08:35,600
就干脆来讲一下

256
00:08:35,600 --> 00:08:36,720
我们的简洁实现

257
00:08:36,960 --> 00:08:38,279
简洁实现就很简单

258
00:08:39,519 --> 00:08:40,440
你这个怎么算

259
00:08:42,240 --> 00:08:43,639
我把它调回来一点

260
00:08:48,559 --> 00:08:49,639
就是说简洁实现

261
00:08:49,639 --> 00:08:50,200
我们知道

262
00:08:50,320 --> 00:08:53,320
我们先把我们的输入拉平

263
00:08:53,320 --> 00:08:54,679
拉成一个二维的东西

264
00:08:55,039 --> 00:08:57,240
然后第一个全连接层

265
00:08:57,279 --> 00:08:58,120
就784

266
00:08:58,120 --> 00:08:59,399
784是我的输入维度

267
00:08:59,399 --> 00:09:01,000
256是我的隐藏层大小

268
00:09:01,200 --> 00:09:02,039
加个软路

269
00:09:02,440 --> 00:09:03,320
区别在这个地方

270
00:09:03,320 --> 00:09:05,600
区别就是我加了一个dropout的layer

271
00:09:06,160 --> 00:09:08,120
当然你所有的框架

272
00:09:08,120 --> 00:09:09,440
都有dropout这个layer

273
00:09:10,200 --> 00:09:10,600
就是说

274
00:09:10,600 --> 00:09:12,520
然后给定的是dropout的概率

275
00:09:13,240 --> 00:09:15,240
然后把它放在reload后面

276
00:09:16,440 --> 00:09:17,240
你放在前面

277
00:09:17,240 --> 00:09:18,160
放在后面其实都一样

278
00:09:18,600 --> 00:09:19,680
大家可以想一下为什么

279
00:09:20,879 --> 00:09:21,800
接下来

280
00:09:24,000 --> 00:09:25,520
再来一个隐藏层

281
00:09:25,680 --> 00:09:26,720
再来一个reload

282
00:09:26,840 --> 00:09:27,960
再放一个dropout

283
00:09:28,360 --> 00:09:29,600
最后是我们的输出层

284
00:09:29,960 --> 00:09:30,760
别的都一样

285
00:09:30,760 --> 00:09:32,120
就唯一的区别就是

286
00:09:32,639 --> 00:09:33,720
多了这两个

287
00:09:34,879 --> 00:09:35,919
多了这两个东西

288
00:09:37,080 --> 00:09:37,480
对吧

289
00:09:37,480 --> 00:09:39,279
就跟之前所有东西都是一样

290
00:09:39,480 --> 00:09:40,399
所以实现的话

291
00:09:40,399 --> 00:09:42,600
就是别的东西都一样

292
00:09:42,600 --> 00:09:43,279
就是

293
00:09:48,759 --> 00:09:49,639
我们的

294
00:09:56,159 --> 00:09:57,080
可以看到是说

295
00:09:57,080 --> 00:09:59,799
我们的跟之前应该是结果是一样的

296
00:09:59,799 --> 00:10:01,360
就是实现理论上是一样的

297
00:10:01,360 --> 00:10:03,919
就是loss在0.4下一点点

298
00:10:03,919 --> 00:10:06,480
精度大概是0.8283

299
00:10:07,120 --> 00:10:07,720
这样子

300
00:10:08,200 --> 00:10:10,200
我们来看一下我们之前的训练完没有

301
00:10:11,639 --> 00:10:14,440
可以看到是之前我们就没有dropout

302
00:10:14,440 --> 00:10:15,519
就dropout等于0

303
00:10:15,960 --> 00:10:16,840
可以看到是

304
00:10:17,879 --> 00:10:19,519
其实也还行

305
00:10:19,519 --> 00:10:20,320
就是说

306
00:10:20,560 --> 00:10:22,200
没有想象那么差

307
00:10:22,200 --> 00:10:23,000
就比0.4

308
00:10:23,000 --> 00:10:24,440
就是这里会往下低一点点

309
00:10:25,080 --> 00:10:26,519
这个地方会往下

310
00:10:26,519 --> 00:10:27,680
我们之前大概在这个地方

311
00:10:27,680 --> 00:10:28,080
对吧

312
00:10:28,560 --> 00:10:30,200
现在是到

313
00:10:30,920 --> 00:10:31,640
到底了

314
00:10:31,640 --> 00:10:33,000
因为这个Y轴

315
00:10:33,000 --> 00:10:34,440
我记得我们是固定住了

316
00:10:34,440 --> 00:10:35,879
所以可以看一下

317
00:10:36,040 --> 00:10:37,720
对Y轴我们是固定住了

318
00:10:37,720 --> 00:10:39,000
所以不管你怎么变

319
00:10:39,000 --> 00:10:40,920
我的Y轴是那个趋值是固定的

320
00:10:40,920 --> 00:10:41,879
所以你可以看到

321
00:10:42,200 --> 00:10:44,120
如果你dropout的这个在这个地方

322
00:10:44,879 --> 00:10:48,320
如果精度大概是快到将近0了

323
00:10:48,600 --> 00:10:50,400
如果我dropout的全部设成0了

324
00:10:51,600 --> 00:10:52,680
我的测试精度

325
00:10:52,680 --> 00:10:55,080
我的训练精度其实是增加了

326
00:10:55,080 --> 00:10:55,960
那是很正常

327
00:10:55,960 --> 00:10:56,840
因为我的

328
00:10:57,040 --> 00:10:59,200
我fit的更好了

329
00:10:59,320 --> 00:11:01,480
那我的loss也是稍微低一点点了

330
00:11:01,800 --> 00:11:02,600
大家可以试一下

331
00:11:02,600 --> 00:11:03,920
因为你可以改成0.9

332
00:11:03,920 --> 00:11:04,960
0.9会怎么样

333
00:11:04,960 --> 00:11:08,160
或者是说你可以把它改得更

334
00:11:08,720 --> 00:11:09,800
更加的

335
00:11:09,800 --> 00:11:12,000
比如说你把隐藏层改成256

336
00:11:12,280 --> 00:11:12,759
对吧

337
00:11:13,000 --> 00:11:14,320
或者你改三个隐藏层

338
00:11:14,320 --> 00:11:15,720
大家都可以去尝试一下

339
00:11:15,840 --> 00:11:17,960
就dropout对你的影响会怎么样子

340
00:11:18,800 --> 00:11:22,560
而且这个是我建议真的建议大家

341
00:11:22,560 --> 00:11:23,560
真的去试一下

342
00:11:23,560 --> 00:11:25,120
这个确实是dropout的是

343
00:11:25,120 --> 00:11:26,560
我们现在最主流的

344
00:11:26,560 --> 00:11:28,200
对于多层杆子机的

345
00:11:28,200 --> 00:11:29,560
模型控制的一个方法

346
00:11:29,840 --> 00:11:32,560
所以你很有可能是会遇到

347
00:11:32,560 --> 00:11:35,760
dropout的技术的

348
00:11:35,760 --> 00:11:37,200
所以大家一定去试一下

349
00:11:38,240 --> 00:11:39,800
当然另外一个是说

350
00:11:39,840 --> 00:11:41,960
我们在讲

351
00:11:42,480 --> 00:11:44,400
之前讲模型控制的时候

352
00:11:44,400 --> 00:11:45,400
有讲过说

353
00:11:45,440 --> 00:11:47,080
对深度学习来讲

354
00:11:47,120 --> 00:11:49,680
你通常可以把模型弄得复杂一点点

355
00:11:49,880 --> 00:11:51,720
然后再通过我们的政策化

356
00:11:51,720 --> 00:11:53,080
来控制模型复杂度

357
00:11:53,200 --> 00:11:55,320
这意味着说你有dropout的情况下

358
00:11:55,360 --> 00:11:56,800
你可以把你的隐藏层

359
00:11:56,800 --> 00:11:58,440
稍微设大一点点

360
00:11:58,480 --> 00:12:01,000
然后再把dropout率也设大一点点

361
00:12:01,040 --> 00:12:03,160
可能比你不用dropout

362
00:12:03,160 --> 00:12:04,200
但是把你的number of

363
00:12:04,200 --> 00:12:04,760
hidden

364
00:12:04,800 --> 00:12:06,200
就是说隐藏层变得比较小

365
00:12:06,200 --> 00:12:07,400
可能效果还好一点点

366
00:12:07,560 --> 00:12:08,640
就大家可以试一下

367
00:12:08,640 --> 00:12:10,680
这个也是常用的一些trick

368
00:12:11,520 --> 00:12:11,879
OK

369
00:12:11,879 --> 00:12:13,879
这个就是我们的实现

370
00:12:13,879 --> 00:12:16,120
就讲到这个地方

