1
00:00:00,000 --> 00:00:02,879
问题,Attention在搜索的时候

2
00:00:02,879 --> 00:00:05,639
是在当前句子搜索还是所有的文本搜索

3
00:00:05,639 --> 00:00:08,679
是在当前的句子这个地方

4
00:00:08,679 --> 00:00:10,839
就是说具体来说

5
00:00:10,839 --> 00:00:12,320
具体我们应用是说

6
00:00:12,320 --> 00:00:16,280
在把一个英语句子翻译到法语的时候

7
00:00:16,320 --> 00:00:21,640
他在Attention是去搜法语英语句子

8
00:00:21,679 --> 00:00:23,199
它的每一个词它的输出

9
00:00:23,240 --> 00:00:26,160
他不会去看别的句子的输入

10
00:00:30,519 --> 00:00:32,240
Q是decode的输出

11
00:00:32,240 --> 00:00:34,320
那么第一次的Q是怎么来的

12
00:00:34,320 --> 00:00:35,719
第一次的Q

13
00:00:35,719 --> 00:00:41,920
是decoded的最后一个词的输入状态

14
00:00:42,799 --> 00:00:44,039
OK说完了

15
00:00:44,039 --> 00:00:48,159
可能要么就是你PAD的符号

16
00:00:48,159 --> 00:00:51,280
要么就是你句子结束的符号

17
00:00:54,519 --> 00:00:57,400
一般都是在decode加入注意力吗

18
00:00:57,400 --> 00:00:59,519
不可以在incode加入吗

19
00:00:59,760 --> 00:01:02,640
可以incode当然可以

20
00:01:02,640 --> 00:01:04,120
BERT就是incode加入的

21
00:01:04,120 --> 00:01:09,240
我们只是说讲了一个例子

22
00:01:09,240 --> 00:01:11,359
BERT就是在incode

23
00:01:11,359 --> 00:01:12,359
BERT只有incode

24
00:01:12,359 --> 00:01:13,359
BERT没有decode

25
00:01:13,359 --> 00:01:15,560
所以它是全注意力

26
00:01:15,560 --> 00:01:18,520
就是说我们只是讲今天讲一个注意力机制的应用

27
00:01:18,680 --> 00:01:20,359
但我们之后看Attention

28
00:01:20,359 --> 00:01:26,480
就transformer和就是transformer这个架构

29
00:01:26,480 --> 00:01:28,240
BERT就是纯Attention的

30
00:01:28,359 --> 00:01:29,559
就没有RN了

31
00:01:29,559 --> 00:01:30,039
对吧

32
00:01:33,119 --> 00:01:35,879
incode的valid length的值是怎么设置

33
00:01:35,879 --> 00:01:36,000
对

34
00:01:36,000 --> 00:01:39,199
这个我知道大家可能会觉得有点奇怪

35
00:01:39,199 --> 00:01:40,319
就这个词是什么

36
00:01:40,319 --> 00:01:44,159
这个词是一个句子的长度

37
00:01:44,159 --> 00:01:45,759
就是原英语句子的长度

38
00:01:45,759 --> 00:01:47,280
就是我说Hello world

39
00:01:47,919 --> 00:01:50,439
句号还加一个句子的修制符

40
00:01:50,439 --> 00:01:51,280
4个词

41
00:01:51,799 --> 00:01:54,599
在进入我的data set的时候

42
00:01:54,599 --> 00:01:57,239
我会把它搞成一个成为10的相当

43
00:01:57,240 --> 00:01:57,760
是为什么

44
00:01:57,760 --> 00:02:00,120
是因为要大家都统一长度

45
00:02:00,600 --> 00:02:02,280
那么为了变成10

46
00:02:02,280 --> 00:02:04,560
我会加6个pad的符号进去

47
00:02:05,920 --> 00:02:07,640
那valid length就记录说

48
00:02:07,640 --> 00:02:11,840
这个里面有多少个词是不是pad的

49
00:02:12,040 --> 00:02:12,840
就是valid

50
00:02:12,840 --> 00:02:14,360
就是说pad是不valid

51
00:02:14,920 --> 00:02:15,439
OK

52
00:02:15,439 --> 00:02:17,120
是说它是不是设的

53
00:02:17,120 --> 00:02:18,000
它是算出来的

54
00:02:18,000 --> 00:02:19,400
就是你的data是怎么

55
00:02:19,400 --> 00:02:20,680
原始data长度是多少

56
00:02:20,680 --> 00:02:21,439
它就是多少

57
00:02:21,800 --> 00:02:25,879
注意力机制是不是和昨天讲的

58
00:02:26,199 --> 00:02:28,039
数搜索有些类似

59
00:02:28,400 --> 00:02:29,520
不是那么类似

60
00:02:33,439 --> 00:02:35,479
跟数搜索不类似

61
00:02:35,479 --> 00:02:36,840
数就注意力机制

62
00:02:36,840 --> 00:02:39,079
我们根本就没有用到数搜索

63
00:02:39,079 --> 00:02:42,079
是说你最后的全列阶层预测层的

64
00:02:42,079 --> 00:02:44,319
输出里面怎么样做那个东西

65
00:02:44,960 --> 00:02:47,719
注意力机制只是在RNN的层面做的

66
00:02:47,919 --> 00:02:49,960
就是说在怎么样更有效的

67
00:02:49,960 --> 00:02:51,960
用encode的信息

68
00:02:52,360 --> 00:02:55,000
它应该这两个东西是不一样的

69
00:02:57,800 --> 00:02:59,280
课程快结束了

70
00:02:59,280 --> 00:03:02,879
能不能推荐一些其他的学习的资料

71
00:03:03,400 --> 00:03:04,640
我觉得你这个问题很好

72
00:03:05,840 --> 00:03:07,040
我得想一想

73
00:03:07,719 --> 00:03:08,680
就是说

74
00:03:10,120 --> 00:03:12,560
大家知道我们这个课也就讲了

75
00:03:12,560 --> 00:03:14,080
讲了我们这课算长的

76
00:03:14,080 --> 00:03:14,640
说真话

77
00:03:14,640 --> 00:03:16,960
斯大夫一本课18节课就讲完了

78
00:03:16,960 --> 00:03:19,520
我当然正准备材料

79
00:03:19,560 --> 00:03:20,520
18节课就讲完了

80
00:03:20,520 --> 00:03:21,960
我们这个东西讲了两倍长

81
00:03:22,120 --> 00:03:23,680
起码我们有36节课

82
00:03:23,680 --> 00:03:24,680
我们可能还真有

83
00:03:25,960 --> 00:03:27,320
就我们这么课真的不短了

84
00:03:27,320 --> 00:03:27,840
不算短

85
00:03:27,840 --> 00:03:30,040
像国内一个学习课也讲不了我们那么多

86
00:03:30,920 --> 00:03:32,040
但是不管怎么样

87
00:03:32,520 --> 00:03:34,840
我们讲的只是其中的很小一块

88
00:03:34,840 --> 00:03:36,879
我可能在最后一节课

89
00:03:36,879 --> 00:03:38,120
总结的时候给大家讲一下

90
00:03:38,120 --> 00:03:40,240
我们到底讲的是在整个圈

91
00:03:40,439 --> 00:03:41,719
在整个大的picture里面

92
00:03:41,719 --> 00:03:43,640
我们讲的中间的一块

93
00:03:44,000 --> 00:03:45,840
但是你可能要学的

94
00:03:45,840 --> 00:03:47,320
还有很多很多东西

95
00:03:47,879 --> 00:03:48,599
就是说

96
00:03:49,680 --> 00:03:51,840
不要觉得我们上了课就怎么样了

97
00:03:51,840 --> 00:03:52,840
其实不怎么样

98
00:03:52,840 --> 00:03:53,680
就是一个基础

99
00:03:53,680 --> 00:03:54,680
一个开始

100
00:03:55,199 --> 00:03:56,159
只是说

101
00:03:57,000 --> 00:03:57,840
开始了之后

102
00:03:57,840 --> 00:03:58,759
然后你有更多

103
00:03:58,759 --> 00:04:00,319
你可以去学更多的东西

104
00:04:00,319 --> 00:04:01,840
但是具体要怎么学

105
00:04:01,959 --> 00:04:03,759
这东西东西很多

106
00:04:04,240 --> 00:04:07,199
很多东西你没有必要一开始学

107
00:04:07,199 --> 00:04:09,479
就是很多东西是噪音

108
00:04:10,120 --> 00:04:11,719
就是说信息含量比不高

109
00:04:11,719 --> 00:04:13,919
所以我在可能这个问题很好

110
00:04:13,919 --> 00:04:15,240
我再想一想

111
00:04:16,199 --> 00:04:19,120
可能根据大家选择的路线不一样

112
00:04:19,120 --> 00:04:21,240
可能学习的东西会不一样一些

113
00:04:22,480 --> 00:04:23,160
OK

114
00:04:23,319 --> 00:04:25,800
所以看看大家还有没有别的问题

115
00:04:30,560 --> 00:04:31,400
我看一下问题

116
00:04:31,480 --> 00:04:33,400
我就看一下有没有比赛

117
00:04:33,400 --> 00:04:36,759
本来我确实有考虑过做一个文本比赛

118
00:04:36,840 --> 00:04:38,400
但我们这个课就两个

119
00:04:38,400 --> 00:04:39,920
差不多还有两个星期结束的话

120
00:04:39,920 --> 00:04:40,720
我做个文本比赛

121
00:04:40,720 --> 00:04:41,600
做一个一个月

122
00:04:41,600 --> 00:04:43,240
然后归大家没热情了

123
00:04:43,240 --> 00:04:44,280
所以我再想一下

124
00:04:44,280 --> 00:04:48,560
就是说文本比赛的时候

125
00:04:48,560 --> 00:04:50,960
我们就可能是放到下一次

126
00:04:51,240 --> 00:04:51,760
就下一次

127
00:04:51,760 --> 00:04:56,720
也许我再下一次可能把它放进去算了

128
00:04:56,720 --> 00:04:59,760
就这一次我们就时间上不一定来得及了

129
00:04:59,880 --> 00:05:00,840
因为做个比赛

130
00:05:00,840 --> 00:05:02,400
基本上要一个半月的样子

131
00:05:06,320 --> 00:05:07,480
图像的tension

132
00:05:07,600 --> 00:05:09,080
图像的tension你说的挺好的

133
00:05:09,160 --> 00:05:10,240
图像的tension

134
00:05:10,560 --> 00:05:11,680
就我有打算

135
00:05:11,680 --> 00:05:12,040
嗯

136
00:05:12,040 --> 00:05:13,760
我就没打算讲图像的tension

137
00:05:13,759 --> 00:05:14,319
图像的tension

138
00:05:14,360 --> 00:05:15,120
你可以认为

139
00:05:17,159 --> 00:05:17,959
图像的tension

140
00:05:17,959 --> 00:05:19,000
self attention

141
00:05:19,000 --> 00:05:21,680
就是说要下节课讲的技术里面

142
00:05:21,680 --> 00:05:23,879
就是自注意力机制

143
00:05:24,399 --> 00:05:24,920
就图像

144
00:05:24,920 --> 00:05:27,360
但是你可以整个认为说图像里面

145
00:05:27,360 --> 00:05:28,759
你要抽很多词出来

146
00:05:28,759 --> 00:05:29,560
词是什么

147
00:05:29,560 --> 00:05:30,839
词是一个patch

148
00:05:31,680 --> 00:05:32,920
就一个图片在那里

149
00:05:33,599 --> 00:05:35,159
我怎么把一个图片变成一个序列

150
00:05:35,159 --> 00:05:36,519
或者变成一些key value pair

151
00:05:36,839 --> 00:05:39,199
我觉得里面抠很多子图出来

152
00:05:39,519 --> 00:05:42,000
一个子图就是一个key value

153
00:05:42,000 --> 00:05:42,480
你可以认为

154
00:05:42,480 --> 00:05:43,439
或者是你可以

155
00:05:43,480 --> 00:05:44,040
就是说

156
00:05:44,080 --> 00:05:46,200
一个key就是一个子图

157
00:05:46,360 --> 00:05:47,200
或者你value

158
00:05:47,200 --> 00:05:49,040
其实在自学系里

159
00:05:49,200 --> 00:05:51,600
在自self的tension里面

160
00:05:51,960 --> 00:05:52,800
key value

161
00:05:52,800 --> 00:05:54,080
query都是一个东西

162
00:05:54,600 --> 00:05:55,200
在图片

163
00:05:55,200 --> 00:05:57,600
他对的图片就是一个子图

164
00:05:58,319 --> 00:05:59,960
在图片这一块进展比较快

165
00:06:00,120 --> 00:06:01,240
我们自节课

166
00:06:01,600 --> 00:06:03,160
我们这次就肯定cover不到

167
00:06:03,319 --> 00:06:04,920
但我们之后可能会讲

168
00:06:04,960 --> 00:06:06,319
因为这东西

169
00:06:07,400 --> 00:06:09,520
出了一些不错的工作

170
00:06:09,640 --> 00:06:10,360
但是相对来说

171
00:06:10,360 --> 00:06:11,920
还是才出来一两年

172
00:06:11,920 --> 00:06:12,879
所以我们就

173
00:06:13,160 --> 00:06:14,000
不那么自信说

174
00:06:14,000 --> 00:06:15,000
要不要写进教材

175
00:06:15,160 --> 00:06:17,319
可能在今年可能

176
00:06:18,040 --> 00:06:19,800
年底可能我们会

177
00:06:19,840 --> 00:06:21,160
加入几个模型进来

178
00:06:24,560 --> 00:06:25,640
有同学问说

179
00:06:25,640 --> 00:06:28,319
后期有没有方式可以交流

180
00:06:28,680 --> 00:06:29,680
你这个问题很好

181
00:06:29,680 --> 00:06:30,520
我还得想一想

182
00:06:31,360 --> 00:06:32,400
想一想怎么样

183
00:06:33,920 --> 00:06:35,680
要请给出噪音的名字

184
00:06:35,800 --> 00:06:36,520
噪音

185
00:06:37,680 --> 00:06:38,759
怎么说呢

186
00:06:39,879 --> 00:06:40,840
我读那么多乐本

187
00:06:41,000 --> 00:06:41,600
可能

188
00:06:43,280 --> 00:06:44,240
诺文里面都有东西

189
00:06:44,240 --> 00:06:45,160
但我都不记得

190
00:06:45,520 --> 00:06:46,360
不记得的东西

191
00:06:46,360 --> 00:06:47,760
我都把它叫做噪音

192
00:06:47,760 --> 00:06:48,240
对吧

193
00:06:48,480 --> 00:06:49,280
就是说噪音

194
00:06:49,280 --> 00:06:50,920
不是不见得是一个有害的东西

195
00:06:50,920 --> 00:06:51,800
这噪音是说

196
00:06:51,800 --> 00:06:52,880
噪音能帮助你学习

197
00:06:52,880 --> 00:06:53,440
但是

198
00:06:54,360 --> 00:06:56,800
但是你要读很多东西

199
00:06:56,840 --> 00:06:57,680
但是可以

200
00:06:57,760 --> 00:06:59,400
而且很多是错的

201
00:07:00,640 --> 00:07:01,560
现在叫

202
00:07:02,040 --> 00:07:03,760
诺文很多时候是不对的

