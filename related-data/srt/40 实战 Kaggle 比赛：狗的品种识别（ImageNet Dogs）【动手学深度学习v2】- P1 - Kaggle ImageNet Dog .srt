1
00:00:00,000 --> 00:00:08,199
好 我们接下来看一个比SIFAR-10稍微复杂一点的一个数据集

2
00:00:08,199 --> 00:00:10,400
叫做ImageNetDock

3
00:00:10,400 --> 00:00:13,800
就是说ImageNet这个数据集有很好玩

4
00:00:13,800 --> 00:00:15,320
就是说它有一千个类

5
00:00:15,320 --> 00:00:17,000
但实际上它一个很大的数据集

6
00:00:17,000 --> 00:00:17,800
它也很大

7
00:00:17,800 --> 00:00:20,800
它的具体是怎么做出来的呢

8
00:00:20,800 --> 00:00:22,519
它就是把一个WordNet

9
00:00:22,519 --> 00:00:24,480
就是一个词

10
00:00:24,480 --> 00:00:26,400
就是说它有个词根

11
00:00:26,400 --> 00:00:27,480
就是自然物体

12
00:00:27,480 --> 00:00:29,000
下面有多少多少多少多少

13
00:00:29,000 --> 00:00:30,519
要不人啊 动物啊

14
00:00:30,519 --> 00:00:31,719
动物下面有狗

15
00:00:31,719 --> 00:00:33,119
狗有很多种 狗

16
00:00:33,119 --> 00:00:35,240
它就把那一个树状的词

17
00:00:35,240 --> 00:00:39,719
它一个从一个NLP那边叫WordNet拿过来

18
00:00:39,719 --> 00:00:42,120
它里面大概有十万个词

19
00:00:42,120 --> 00:00:45,679
他们把每一个词拿去Google搜

20
00:00:45,679 --> 00:00:46,840
然后Google搜那么搜

21
00:00:46,840 --> 00:00:49,400
搜个爬个几千张图片回来

22
00:00:49,400 --> 00:00:51,920
然后再人去标一标

23
00:00:51,920 --> 00:00:54,519
它其实是这样子做出来的

24
00:00:54,519 --> 00:00:57,200
所以它的导致的它的问题是什么

25
00:00:57,200 --> 00:01:01,240
它导致它就是狗

26
00:01:01,240 --> 00:01:04,000
它里面有一百种狗

27
00:01:04,000 --> 00:01:05,320
就它整个一千类里面

28
00:01:05,320 --> 00:01:08,280
它也有一百个就是狗

29
00:01:08,280 --> 00:01:09,960
然后它有很多是猫

30
00:01:09,960 --> 00:01:11,760
就是狗猫这种图片

31
00:01:11,760 --> 00:01:14,359
就是说确实网上很多种图片

32
00:01:14,359 --> 00:01:20,600
然后所以就导致说里面对狗分的特别好

33
00:01:20,600 --> 00:01:22,760
但是在实际生活中

34
00:01:22,760 --> 00:01:23,520
大家care吗

35
00:01:23,520 --> 00:01:24,120
不care

36
00:01:24,120 --> 00:01:25,480
我为什么care那个狗

37
00:01:25,480 --> 00:01:31,240
我管你是什么哪种狗

38
00:01:31,240 --> 00:01:32,760
我其实不那么关心

39
00:01:32,760 --> 00:01:36,040
所以就是说ImageNet这个数据集

40
00:01:36,040 --> 00:01:38,840
它的那个label就那一千类

41
00:01:38,840 --> 00:01:42,040
它跟我们现实生活中看到的东西不一样

42
00:01:42,400 --> 00:01:44,120
就它里面净是一些奇奇怪怪的

43
00:01:44,120 --> 00:01:45,560
一些动物在里面

44
00:01:45,560 --> 00:01:48,840
各种谁都不认识的昆虫什么东西

45
00:01:48,840 --> 00:01:50,359
然后在现实生活中

46
00:01:50,359 --> 00:01:51,680
你可能根本就看不到它

47
00:01:51,680 --> 00:01:53,080
所以就导致说

48
00:01:53,079 --> 00:01:56,640
你对它做ImageNet的pre-trained model

49
00:01:56,640 --> 00:01:58,519
在好像在工业界直接用

50
00:01:58,519 --> 00:02:01,159
是因为它跟我们现实生活太不相关了

51
00:02:02,159 --> 00:02:03,599
所以通常来说

52
00:02:04,920 --> 00:02:06,039
对公司来讲

53
00:02:06,039 --> 00:02:08,719
它的大的pre-trained dataset

54
00:02:08,719 --> 00:02:11,800
一般是说根据我的公司的用户的喜好

55
00:02:12,919 --> 00:02:16,400
比如说人家在我们这里

56
00:02:16,400 --> 00:02:20,759
那些用户的图片跟哪一类物体相关

57
00:02:20,759 --> 00:02:21,319
或哪一类

58
00:02:21,319 --> 00:02:22,319
比如说是人造物体

59
00:02:22,639 --> 00:02:24,199
建筑类自然物体

60
00:02:24,319 --> 00:02:25,239
当然也OK了

61
00:02:25,239 --> 00:02:29,319
就是说我会在针对它们出现

62
00:02:29,319 --> 00:02:31,400
那个频率去采样我们的类别

63
00:02:32,639 --> 00:02:32,919
OK

64
00:02:32,919 --> 00:02:34,479
这就是学术数据集

65
00:02:34,479 --> 00:02:36,280
和真实场景的数据集的

66
00:02:36,280 --> 00:02:37,719
一个统计上的不一样

67
00:02:38,280 --> 00:02:39,560
当然反正学术数据集

68
00:02:39,719 --> 00:02:43,120
追求的是通用性

69
00:02:43,400 --> 00:02:45,439
所以ImageNet就是说

70
00:02:46,240 --> 00:02:47,039
猫猫狗狗

71
00:02:47,039 --> 00:02:49,560
然后你因为你网上找猫猫狗狗比较简单

72
00:02:49,560 --> 00:02:52,240
所以它里面就是待机种的挺好的

73
00:02:52,920 --> 00:02:54,879
所以这个数据集是什么

74
00:02:55,039 --> 00:02:57,120
这个数据集就是把ImageNet里面的狗

75
00:02:57,120 --> 00:02:58,000
全部拿出来

76
00:02:58,439 --> 00:02:59,840
这个点进去可以看一下

77
00:02:59,840 --> 00:03:02,520
就是说它就是各种狗在里面

78
00:03:03,439 --> 00:03:07,159
然后它的也大概就是这个数据也不小

79
00:03:07,599 --> 00:03:09,719
大概也是700兆的样子

80
00:03:09,719 --> 00:03:11,000
你可以看到里面就是

81
00:03:12,159 --> 00:03:14,120
ImageNet里面的各种狗

82
00:03:15,400 --> 00:03:16,000
OK

83
00:03:16,840 --> 00:03:18,719
所以就是说它比刚刚的数据集

84
00:03:18,719 --> 00:03:20,759
还是要复杂一点

85
00:03:20,759 --> 00:03:22,879
刚刚虽然都大概都是700兆

86
00:03:23,120 --> 00:03:25,560
但是这个图片类别更多

87
00:03:25,560 --> 00:03:26,919
应该是100类

88
00:03:27,360 --> 00:03:29,159
然后图片更大

89
00:03:30,400 --> 00:03:31,439
刚刚Sifa时

90
00:03:31,439 --> 00:03:33,560
它那个东西主要是测试级特别的

91
00:03:33,560 --> 00:03:36,159
所以训练级其实就100兆的样子

92
00:03:37,400 --> 00:03:37,800
OK

93
00:03:37,800 --> 00:03:39,599
所以跟import的东西

94
00:03:39,599 --> 00:03:41,039
跟我们刚刚是差不多的

95
00:03:42,000 --> 00:03:42,759
另外一样

96
00:03:42,959 --> 00:03:44,560
我们就没有去

97
00:03:45,239 --> 00:03:47,560
真正的去说去开口上

98
00:03:47,560 --> 00:03:48,519
把那个数据集下下来

99
00:03:48,519 --> 00:03:50,479
我们还是一样的采样了一个

100
00:03:50,479 --> 00:03:51,239
跟Sifa是一样

101
00:03:51,239 --> 00:03:52,239
采样了每个狗

102
00:03:52,239 --> 00:03:53,319
采样了几张图片

103
00:03:53,319 --> 00:03:54,560
来给大家做个demo

104
00:03:55,679 --> 00:03:56,239
同样的话

105
00:03:56,239 --> 00:03:58,399
你如果想要跑文字数据集的话

106
00:03:58,399 --> 00:04:00,000
你要把它demo改成force

107
00:04:00,000 --> 00:04:02,560
然后去把数据下在你的

108
00:04:02,919 --> 00:04:04,280
上一集目录data的

109
00:04:06,599 --> 00:04:07,599
文件夹下面

110
00:04:08,560 --> 00:04:10,199
然后跑一下也不容易

111
00:04:10,560 --> 00:04:12,359
但是我建议大家可以去试一试

112
00:04:13,760 --> 00:04:14,320
好

113
00:04:14,320 --> 00:04:15,240
跟之前一样

114
00:04:15,400 --> 00:04:17,800
我们这个跟我们前面是一样

115
00:04:17,800 --> 00:04:18,960
因为它的数据集

116
00:04:18,960 --> 00:04:21,000
都也就是一个train一个test

117
00:04:21,280 --> 00:04:22,759
然后一个CSV文件

118
00:04:22,759 --> 00:04:25,000
所以我们就一样的

119
00:04:25,000 --> 00:04:26,640
调用刚刚我们之前的

120
00:04:26,920 --> 00:04:28,240
把label都进来

121
00:04:28,240 --> 00:04:30,560
然后把train和valid的test

122
00:04:30,560 --> 00:04:31,960
重新reorg一下

123
00:04:31,960 --> 00:04:33,759
这跟之前是复用的关系

124
00:04:36,800 --> 00:04:37,120
好

125
00:04:37,120 --> 00:04:38,720
我们看一下图片增广

126
00:04:39,240 --> 00:04:39,520
好

127
00:04:39,520 --> 00:04:40,840
图片增广跟之前

128
00:04:41,120 --> 00:04:42,760
我们知道这个东西其实挺重要的

129
00:04:43,080 --> 00:04:44,520
我们跟之前是有点不一样的

130
00:04:44,520 --> 00:04:45,280
地方在哪里

131
00:04:45,280 --> 00:04:46,360
给大家看一看

132
00:04:47,800 --> 00:04:50,680
首先我们可以看到是

133
00:04:51,800 --> 00:04:53,640
狗的图片还是比较大的

134
00:04:53,640 --> 00:04:54,680
是img.net的图片

135
00:04:54,720 --> 00:04:56,320
大概是平均可能下来

136
00:04:56,320 --> 00:04:58,400
就256成256的样子

137
00:04:58,720 --> 00:05:02,200
所以我们的random resized crop

138
00:05:02,200 --> 00:05:02,960
就是从

139
00:05:04,520 --> 00:05:05,840
直接到一个224

140
00:05:05,840 --> 00:05:07,400
就是img.net的scale

141
00:05:08,040 --> 00:05:10,000
然后它的scale

142
00:05:10,000 --> 00:05:10,879
就是说

143
00:05:11,720 --> 00:05:15,600
从0.08到1.0

144
00:05:16,439 --> 00:05:18,000
意思就是说

145
00:05:18,000 --> 00:05:20,920
大概是你最小在每一个边

146
00:05:20,920 --> 00:05:23,000
给我一个大概是30%

147
00:05:23,000 --> 00:05:23,839
就高宽

148
00:05:24,040 --> 00:05:24,600
最小的时候

149
00:05:24,600 --> 00:05:26,800
你高宽可能给我保留一个

150
00:05:26,800 --> 00:05:29,199
30%的东西在里面

151
00:05:30,639 --> 00:05:31,839
然后当然最大的

152
00:05:31,839 --> 00:05:32,959
当然是你可以做很大

153
00:05:32,959 --> 00:05:34,680
所以这个的意思是说

154
00:05:34,680 --> 00:05:35,560
我确实

155
00:05:35,560 --> 00:05:36,480
因为我那个物体

156
00:05:36,480 --> 00:05:37,959
在图片里面可能是比较大

157
00:05:37,959 --> 00:05:39,279
我可以把你弄小一点

158
00:05:39,280 --> 00:05:41,400
就真的把一个狗的头

159
00:05:41,600 --> 00:05:42,720
狗的局部的东西

160
00:05:42,720 --> 00:05:43,600
给你弄出来

161
00:05:43,640 --> 00:05:44,920
所以刚刚我们看到是

162
00:05:44,920 --> 00:05:46,960
我们scale是调的比较大的

163
00:05:46,960 --> 00:05:48,640
是0.64

164
00:05:49,640 --> 00:05:51,080
因为刚刚图片比较小

165
00:05:51,120 --> 00:05:52,600
狗的话就是说

166
00:05:52,800 --> 00:05:53,720
你对一个狗

167
00:05:53,720 --> 00:05:54,920
就把一个狗腿拉过来

168
00:05:54,920 --> 00:05:56,040
你很有可能计算机

169
00:05:56,040 --> 00:05:57,680
是帮你能区别开来的

170
00:05:59,000 --> 00:06:01,880
Ratio反正也因为这个图片

171
00:06:02,120 --> 00:06:04,600
因为它是长宽高有点

172
00:06:04,640 --> 00:06:06,560
高宽比是不一样的

173
00:06:06,560 --> 00:06:07,400
就有的是方的

174
00:06:07,400 --> 00:06:08,080
有的是宽的

175
00:06:08,080 --> 00:06:09,040
有的是高的

176
00:06:09,040 --> 00:06:12,000
所以就是说你采样的时候

177
00:06:12,000 --> 00:06:13,960
你也是带有一定的Ratio的

178
00:06:13,960 --> 00:06:15,280
就是说你高宽

179
00:06:15,280 --> 00:06:16,240
要么就是

180
00:06:16,400 --> 00:06:17,640
最小是3比4

181
00:06:17,640 --> 00:06:19,240
要么最大是4比3

182
00:06:19,240 --> 00:06:20,960
就在之间可以随机取一个值

183
00:06:21,840 --> 00:06:25,200
另外一个是水平Flip

184
00:06:25,360 --> 00:06:26,439
这个东西就是万金油

185
00:06:26,439 --> 00:06:27,320
总是可以用的

186
00:06:29,040 --> 00:06:30,480
另外一个跟之前会不一样

187
00:06:30,480 --> 00:06:32,640
是说我们加了一点color在里面了

188
00:06:32,640 --> 00:06:34,520
因为相对来说数据比较复杂

189
00:06:34,759 --> 00:06:35,879
多样性比较大

190
00:06:35,959 --> 00:06:40,480
所以我们加的是在明亮度

191
00:06:40,480 --> 00:06:42,079
对比度和饱和度上面

192
00:06:42,079 --> 00:06:45,240
都可以在上下去40%

193
00:06:46,279 --> 00:06:47,120
最后normalize

194
00:06:47,480 --> 00:06:48,319
反正也是Meganet

195
00:06:48,319 --> 00:06:51,240
它copy过来的

196
00:06:52,360 --> 00:06:54,879
然后在test的话

197
00:06:55,920 --> 00:06:57,120
test是怎么做的

198
00:06:57,360 --> 00:06:58,519
我们有讲过

199
00:06:58,680 --> 00:06:59,719
大概是怎么回事

200
00:07:00,439 --> 00:07:02,519
先resize到256

201
00:07:03,479 --> 00:07:06,839
就应该是把最短边给你放到

202
00:07:06,839 --> 00:07:07,839
应该resize

203
00:07:07,839 --> 00:07:08,879
就看你具体实现

204
00:07:09,000 --> 00:07:11,560
有可能是把最短边放到256的高宽

205
00:07:11,799 --> 00:07:13,319
长边还是保留

206
00:07:13,319 --> 00:07:14,639
然后有可能是你

207
00:07:15,759 --> 00:07:18,319
真的就是高宽都到256

208
00:07:18,879 --> 00:07:21,000
然后在中心点

209
00:07:21,000 --> 00:07:23,439
Crop出一个24x24的东西出来

210
00:07:24,000 --> 00:07:26,279
OK最后是后面是一样的

211
00:07:26,680 --> 00:07:28,079
所以这个可以看到是跟

212
00:07:28,079 --> 00:07:29,439
因为图片更大了

213
00:07:29,479 --> 00:07:32,199
然后而且高宽比会不一样

214
00:07:32,199 --> 00:07:34,560
所以我们在train的时候

215
00:07:34,759 --> 00:07:36,279
这一块是有一点不一样的地方

216
00:07:36,279 --> 00:07:36,839
在里面

217
00:07:37,039 --> 00:07:39,399
就跟之前的我们的Sifa10相比

218
00:07:40,399 --> 00:07:40,599
好

219
00:07:40,639 --> 00:07:42,519
这个是一个主要的区别

220
00:07:44,439 --> 00:07:45,639
数据集我们就不过了

221
00:07:45,920 --> 00:07:47,519
基本上就跟前面一样的

222
00:07:47,519 --> 00:07:51,000
就是把那几个东西给弄出来

223
00:07:51,199 --> 00:07:52,919
然后都是一样的

224
00:07:53,879 --> 00:07:55,639
我们就跟前面重复的

225
00:07:55,639 --> 00:07:56,480
我们就不管了

226
00:07:57,159 --> 00:07:57,879
好

227
00:07:58,560 --> 00:08:00,240
这里会有一个不一样的地方

228
00:08:01,240 --> 00:08:03,560
就是我们这是不一样

229
00:08:03,560 --> 00:08:06,160
不代表说这个是最好的

230
00:08:07,319 --> 00:08:08,519
我们这里干了个什么事情

231
00:08:11,439 --> 00:08:12,480
就是说

232
00:08:13,680 --> 00:08:15,199
我们把

233
00:08:15,920 --> 00:08:17,439
我们做fine tuning

234
00:08:18,960 --> 00:08:21,439
因为我们说你

235
00:08:21,759 --> 00:08:22,519
你当然是说

236
00:08:22,519 --> 00:08:23,360
因为它是image

237
00:08:23,360 --> 00:08:24,000
那是自己

238
00:08:24,240 --> 00:08:25,600
你当然你就把image

239
00:08:25,600 --> 00:08:26,040
拿过来

240
00:08:26,040 --> 00:08:27,079
可能效果也挺好的

241
00:08:27,079 --> 00:08:28,639
就是说如果就拿过来就能做

242
00:08:28,639 --> 00:08:30,079
我们就不用给大家demo

243
00:08:30,399 --> 00:08:31,959
为什么要在干这个事情了

244
00:08:32,240 --> 00:08:34,720
所以我们给大家做了一点点变换

245
00:08:34,720 --> 00:08:36,159
就给大家一个思路

246
00:08:36,559 --> 00:08:37,600
是怎么做呢

247
00:08:39,159 --> 00:08:41,199
就是说因为我的

248
00:08:41,799 --> 00:08:44,000
狗的是imagine.net的一个子集

249
00:08:44,120 --> 00:08:45,919
或者是很相似的一个数据集

250
00:08:45,960 --> 00:08:47,679
所以imagine.net

251
00:08:47,679 --> 00:08:49,639
你券的那一些东西

252
00:08:50,399 --> 00:08:51,840
那些feature对我来说

253
00:08:51,840 --> 00:08:53,240
应该是非常好的

254
00:08:53,240 --> 00:08:55,439
因为我们两个数据长太像了

255
00:08:55,960 --> 00:08:57,639
所以这个地方干嘛

256
00:08:57,679 --> 00:08:58,799
我们就是把

257
00:08:59,039 --> 00:09:00,519
除了最后一层之外

258
00:09:00,519 --> 00:09:03,399
我们就固定住前面的参数不变

259
00:09:04,080 --> 00:09:05,559
让我给大家解释一下

260
00:09:06,039 --> 00:09:08,279
首先我们用一个resnet34

261
00:09:08,600 --> 00:09:10,639
4用一个稍微大一点点的模型

262
00:09:11,679 --> 00:09:13,399
要把pretrend等于

263
00:09:14,240 --> 00:09:15,199
true开开

264
00:09:15,199 --> 00:09:18,360
就是说把你的pretrend的model下下来

265
00:09:19,120 --> 00:09:20,279
然后output

266
00:09:21,879 --> 00:09:24,480
output就我们做了一个什么东西

267
00:09:25,319 --> 00:09:27,200
output是我们做了一个

268
00:09:27,200 --> 00:09:28,680
因为我们知道resnet出来

269
00:09:28,680 --> 00:09:32,440
是一个1000维的东西

270
00:09:32,480 --> 00:09:33,000
所以

271
00:09:36,200 --> 00:09:37,200
我看一下这里

272
00:09:39,240 --> 00:09:42,680
所以你就是在上面

273
00:09:42,960 --> 00:09:45,600
再加了几层

274
00:09:47,280 --> 00:09:50,320
就是说加了一个1000到256的

275
00:09:50,320 --> 00:09:53,600
一个就加了一个隐藏层到56

276
00:09:53,600 --> 00:09:55,720
然后再到后面是120

277
00:09:55,920 --> 00:09:57,279
你有一包是种狗

278
00:10:00,000 --> 00:10:01,080
然后

279
00:10:02,639 --> 00:10:05,279
然后就是说你当然copy到我的GPU上

280
00:10:05,720 --> 00:10:06,720
然后我干个什么事情

281
00:10:06,840 --> 00:10:07,879
然后就是说

282
00:10:08,000 --> 00:10:09,279
你所谓的features

283
00:10:09,279 --> 00:10:11,200
也就是说你的

284
00:10:12,279 --> 00:10:16,960
你的那些卷积层

285
00:10:17,120 --> 00:10:18,320
卷积层我的参数

286
00:10:18,320 --> 00:10:21,519
我全部把requiresgrad等于false

287
00:10:22,040 --> 00:10:23,440
我就说不更新它了

288
00:10:23,440 --> 00:10:25,680
因为你卷积层

289
00:10:25,920 --> 00:10:28,680
抽的feature对我来说应该是非常好的

290
00:10:28,720 --> 00:10:30,120
我可以不更新你

291
00:10:30,240 --> 00:10:31,560
我就train得就很快了

292
00:10:31,800 --> 00:10:34,000
就是说这个是给大家一个这样意思

293
00:10:34,000 --> 00:10:35,520
就是说我

294
00:10:35,960 --> 00:10:37,920
就是把你的pretrend模型拿过来

295
00:10:37,920 --> 00:10:39,480
做静态抽特征用

296
00:10:39,880 --> 00:10:42,000
然后把你的静态特征拿过来之后

297
00:10:42,000 --> 00:10:44,560
我在上面做一个单隐藏层的MLP

298
00:10:45,120 --> 00:10:48,280
也就是说这个是一个这样子的微调的方法

299
00:10:49,280 --> 00:10:49,920
OK

300
00:10:50,279 --> 00:10:52,079
这基本上可以看到是

301
00:10:52,439 --> 00:10:53,519
损失函数

302
00:10:53,639 --> 00:10:54,079
损失函数

303
00:10:54,079 --> 00:10:56,279
我们其实跟之前没什么太多区别

304
00:10:56,279 --> 00:10:57,799
我们就不一一讲了

305
00:10:58,559 --> 00:11:00,839
然后训练的话

306
00:11:00,839 --> 00:11:02,559
也就是跟之前是一样的

307
00:11:03,159 --> 00:11:06,799
我们有个这些train iterator

308
00:11:06,799 --> 00:11:07,599
valid iterator

309
00:11:07,959 --> 00:11:08,719
num of epoch

310
00:11:08,879 --> 00:11:09,439
lenger rate

311
00:11:09,559 --> 00:11:10,079
weight decay

312
00:11:10,079 --> 00:11:10,479
divide

313
00:11:10,479 --> 00:11:11,679
array decay

314
00:11:13,679 --> 00:11:14,959
然后momentum我们就直接去了

315
00:11:14,959 --> 00:11:16,919
我们会之后讲momentum

316
00:11:16,919 --> 00:11:17,679
所以没关系

317
00:11:18,080 --> 00:11:21,440
所以optimizer我们只要

318
00:11:21,480 --> 00:11:25,160
就是说我们只会把net的parameter里面

319
00:11:25,160 --> 00:11:27,800
就request grad是等于true的东西给他

320
00:11:27,800 --> 00:11:29,280
因为别的东西我就不更新了

321
00:11:31,000 --> 00:11:33,080
然后我的scheduler也就是说

322
00:11:34,440 --> 00:11:35,800
我的

323
00:11:36,760 --> 00:11:38,240
我这里其实有个LRP

324
00:11:38,240 --> 00:11:38,680
period

325
00:11:38,720 --> 00:11:41,200
就是说已经被藏在后面去了

326
00:11:44,400 --> 00:11:44,680
OK

327
00:11:44,680 --> 00:11:47,040
scheduler跟我们之前是一样的

328
00:11:47,319 --> 00:11:49,679
然后我们下面这些东西应该跟

329
00:11:49,679 --> 00:11:50,799
跟之前是一样的

330
00:11:50,919 --> 00:11:52,480
没有本质区别

331
00:11:53,079 --> 00:11:53,439
OK

332
00:11:53,439 --> 00:11:54,159
就是这一堆

333
00:11:54,159 --> 00:11:57,279
就是基本上等下刚刚还是copy过来

334
00:11:58,199 --> 00:11:58,719
OK

335
00:12:00,159 --> 00:12:00,519
好

336
00:12:00,519 --> 00:12:03,480
最后可以直接给大家看一下结果

337
00:12:03,480 --> 00:12:05,559
就是我们用的

338
00:12:06,240 --> 00:12:09,240
这两个就是我们说每隔两个epoch

339
00:12:09,279 --> 00:12:10,799
我们下降一点

340
00:12:11,079 --> 00:12:13,199
其实说这里的原理其实是

341
00:12:13,240 --> 00:12:15,039
就是说看你的数据有多大了

342
00:12:15,039 --> 00:12:16,240
就是你数据大一点的话

343
00:12:16,240 --> 00:12:19,080
你就是你可以

344
00:12:20,320 --> 00:12:22,320
就是说少几轮去下降

345
00:12:22,320 --> 00:12:24,560
你数据少一点的话

346
00:12:24,560 --> 00:12:26,440
你就多叠带一点再下降

347
00:12:27,440 --> 00:12:30,120
别的东西我都不再给大家一介绍了

348
00:12:30,120 --> 00:12:30,600
就device

349
00:12:30,600 --> 00:12:31,399
就所有的GPU

350
00:12:31,399 --> 00:12:32,840
number of epoch等于10

351
00:12:32,879 --> 00:12:34,480
numerator我们选的比较低

352
00:12:34,840 --> 00:12:37,120
因为反正fine tuning可以低一点

353
00:12:37,240 --> 00:12:38,840
而且我们就更新最后一点

354
00:12:39,480 --> 00:12:39,919
好

355
00:12:39,919 --> 00:12:42,000
我们就直接给大家看一下

356
00:12:42,560 --> 00:12:43,120
就可以看到

357
00:12:43,120 --> 00:12:45,840
所以说我们这里并没有变快太多

358
00:12:46,160 --> 00:12:48,360
就是刚刚是大概四五百的一个

359
00:12:48,360 --> 00:12:48,680
占卜

360
00:12:48,800 --> 00:12:49,879
这也是255

361
00:12:50,160 --> 00:12:51,519
因为我们其实

362
00:12:51,519 --> 00:12:53,879
因为你不需要去算你那些

363
00:12:53,879 --> 00:12:55,800
t就卷积层的t度

364
00:12:55,800 --> 00:12:58,240
它其实是计算量更低了

365
00:12:58,879 --> 00:13:01,240
但是因为我们这个图片变大了

366
00:13:01,600 --> 00:13:03,040
所以计算开架变高了

367
00:13:03,040 --> 00:13:06,000
而且我们是用的resnet34

368
00:13:06,000 --> 00:13:07,200
而不是resnet18

369
00:13:07,200 --> 00:13:08,440
也会更复杂一点

370
00:13:08,680 --> 00:13:09,680
所以就是说

371
00:13:09,879 --> 00:13:13,440
虽然我们t度计算成本更低

372
00:13:13,480 --> 00:13:14,800
但是因为图片更大

373
00:13:14,800 --> 00:13:16,840
所以而且网络更复杂

374
00:13:16,840 --> 00:13:17,560
所以相对来说

375
00:13:17,560 --> 00:13:20,800
我的速度其实是大概是减倍的样子

376
00:13:22,040 --> 00:13:23,480
而且可以看到是说

377
00:13:24,480 --> 00:13:25,920
基本上我的train loss

378
00:13:25,920 --> 00:13:27,480
value loss就很近了

379
00:13:27,480 --> 00:13:31,200
就是因为你就叠在后面小的mlp

380
00:13:31,640 --> 00:13:33,480
因为前面就是没有被

381
00:13:34,400 --> 00:13:35,120
没有动

382
00:13:35,120 --> 00:13:36,640
所以它的over fitting的概率

383
00:13:36,640 --> 00:13:38,120
其实是变得不高

384
00:13:38,120 --> 00:13:39,160
而且你的numerator

385
00:13:39,160 --> 00:13:40,080
因为不够大的缘故

386
00:13:40,400 --> 00:13:41,840
所以相对来说你可以

387
00:13:42,720 --> 00:13:43,880
fine tuning的时候

388
00:13:44,080 --> 00:13:45,600
就是我两个线是比较

389
00:13:46,399 --> 00:13:47,240
overlap的

390
00:13:47,240 --> 00:13:48,279
其实很正常的

391
00:13:49,799 --> 00:13:50,039
ok

392
00:13:50,039 --> 00:13:52,759
这基本上就是

393
00:13:53,080 --> 00:13:54,679
在一个狗的数学机上

394
00:13:54,679 --> 00:13:55,600
训练会怎么样子

395
00:13:56,799 --> 00:14:00,120
最后的我们也就不给大家再过一遍了

396
00:14:00,120 --> 00:14:02,000
那就是说怎么做预测

397
00:14:02,000 --> 00:14:02,919
这里唯一的不一样

398
00:14:02,919 --> 00:14:04,840
是说我做预测的时候

399
00:14:05,200 --> 00:14:07,240
我们之前是说给一个图片

400
00:14:07,240 --> 00:14:10,279
把最有可能类的名字告诉我

401
00:14:10,679 --> 00:14:11,440
这里会干嘛

402
00:14:11,880 --> 00:14:13,360
这里是把你的

403
00:14:16,720 --> 00:14:19,160
就是说我把net拿出去之后

404
00:14:19,480 --> 00:14:20,120
它拿

405
00:14:20,120 --> 00:14:22,640
就是说它是最后一层的输出

406
00:14:22,640 --> 00:14:24,040
全连接层的输出

407
00:14:24,320 --> 00:14:26,240
然后放进softmax

408
00:14:27,440 --> 00:14:29,920
就是用的n-functional softmax

409
00:14:30,360 --> 00:14:31,120
意思是说

410
00:14:31,120 --> 00:14:33,720
我把输出做成一个概率

411
00:14:33,880 --> 00:14:36,200
对每一个类的概率

412
00:14:36,240 --> 00:14:38,160
然后把它output都放在那里面

413
00:14:40,040 --> 00:14:41,160
那么就是说

414
00:14:41,440 --> 00:14:42,880
然后我们把output

415
00:14:43,040 --> 00:14:43,800
就predict

416
00:14:43,800 --> 00:14:44,520
就放在

417
00:14:44,560 --> 00:14:46,240
全部放拿出去

418
00:14:46,280 --> 00:14:48,720
所以我们这里的跟之前区别是

419
00:14:48,720 --> 00:14:50,840
我们提交的每一行

420
00:14:50,880 --> 00:14:53,240
是说某一个样本

421
00:14:53,320 --> 00:14:55,680
对每一个类的概率是多少

422
00:14:56,520 --> 00:14:58,840
就我们后面有120列

423
00:14:58,880 --> 00:14:59,920
而不是一列

424
00:15:00,520 --> 00:15:01,760
这是跟之前的不一样

425
00:15:01,800 --> 00:15:02,720
这也是说

426
00:15:02,840 --> 00:15:05,080
他也是提交要求你干这个事情

427
00:15:05,320 --> 00:15:06,400
你有了这个东西可以干嘛

428
00:15:06,400 --> 00:15:07,800
我可以算top1的

429
00:15:08,480 --> 00:15:11,320
进度和top5的进度

430
00:15:11,480 --> 00:15:12,640
所谓的top5的进度

431
00:15:12,640 --> 00:15:13,320
就是说

432
00:15:14,960 --> 00:15:16,800
我看一下你预测的

433
00:15:17,760 --> 00:15:18,560
前

434
00:15:19,560 --> 00:15:20,880
把你预测

435
00:15:21,200 --> 00:15:22,920
最知情度最高的

436
00:15:22,920 --> 00:15:24,160
全5个拿出来

437
00:15:24,360 --> 00:15:26,720
看看是不是有其中有一个

438
00:15:27,080 --> 00:15:28,480
是不是真实的样本

439
00:15:28,480 --> 00:15:29,360
真实的标号

440
00:15:29,360 --> 00:15:30,800
在这前5个里面

441
00:15:30,840 --> 00:15:31,640
如果是的话

442
00:15:31,640 --> 00:15:32,640
那你就答对了

443
00:15:33,160 --> 00:15:33,840
他就是说

444
00:15:33,840 --> 00:15:34,720
top5的概率

445
00:15:34,720 --> 00:15:36,440
很多时候可以到

446
00:15:36,440 --> 00:15:37,600
imagenet的数据

447
00:15:37,639 --> 00:15:39,879
top1大概是80%几的样子

448
00:15:39,920 --> 00:15:41,320
然后top5的话

449
00:15:41,320 --> 00:15:42,680
你可以到90%

450
00:15:42,680 --> 00:15:44,680
将近95以上的样子

451
00:15:45,800 --> 00:15:48,960
而且top5确实是对于图片来讲

452
00:15:48,960 --> 00:15:49,840
是比较

453
00:15:51,080 --> 00:15:53,080
很长的一个做法

454
00:15:53,080 --> 00:15:56,040
因为当你的标号变得很多的时候

455
00:15:56,080 --> 00:15:58,600
你有些标号真的就长得一样

456
00:15:58,639 --> 00:15:59,879
甚至是说

457
00:15:59,920 --> 00:16:00,639
如果你的标号

458
00:16:00,680 --> 00:16:02,720
可能几千类几万类的标号的话

459
00:16:02,720 --> 00:16:03,840
很有可能两个东西

460
00:16:03,840 --> 00:16:04,960
就是一个东西

461
00:16:05,560 --> 00:16:06,240
就是说

462
00:16:06,240 --> 00:16:07,600
假设我有个标号是狗

463
00:16:07,600 --> 00:16:09,000
有个标号是金毛

464
00:16:09,000 --> 00:16:10,600
然后给你一张金毛的图片

465
00:16:10,639 --> 00:16:11,639
我预测狗也是对

466
00:16:11,639 --> 00:16:12,040
对不对

467
00:16:12,040 --> 00:16:13,400
预测金毛也是对

468
00:16:14,160 --> 00:16:15,200
所以是说

469
00:16:15,200 --> 00:16:15,920
当你的标号

470
00:16:15,920 --> 00:16:17,080
真的很复杂的时候

471
00:16:17,080 --> 00:16:18,759
大家不那么去关心

472
00:16:18,759 --> 00:16:20,040
top1的进度

473
00:16:20,080 --> 00:16:22,680
而且top5的就不错了

474
00:16:23,120 --> 00:16:23,840
这就是为什么

475
00:16:23,840 --> 00:16:25,879
很多时候大家会直接预测说

476
00:16:25,879 --> 00:16:27,000
我在imagenet上

477
00:16:27,000 --> 00:16:28,440
top5的进度是什么样子

478
00:16:28,600 --> 00:16:29,680
而且跟人比的时候

479
00:16:29,680 --> 00:16:30,720
也先有top5

480
00:16:30,720 --> 00:16:33,080
OK

481
00:16:33,080 --> 00:16:37,240
这就是我们做狗分类的问题

482
00:16:37,279 --> 00:16:38,080
可以比较一下

483
00:16:38,080 --> 00:16:39,320
跟前面我们的cifar10

484
00:16:39,320 --> 00:16:39,920
有什么不一样

485
00:16:39,920 --> 00:16:41,480
有几点

486
00:16:41,480 --> 00:16:44,240
第一点是data augmentation不一样

487
00:16:44,879 --> 00:16:46,320
就是说我们图片更大

488
00:16:46,360 --> 00:16:48,080
让我们导弹的空间更大

489
00:16:48,399 --> 00:16:49,519
然后crop的时候

490
00:16:49,519 --> 00:16:51,080
因为你的长宽高

491
00:16:51,080 --> 00:16:53,000
没有被放成1比1

492
00:16:53,040 --> 00:16:54,440
所以我们确实在

493
00:16:54,879 --> 00:16:56,639
高宽比上可以做一点文章

494
00:16:56,840 --> 00:16:57,720
因为图片大

495
00:16:57,840 --> 00:17:00,080
而且你crop的时候

496
00:17:00,080 --> 00:17:01,360
你可以相对是crop

497
00:17:01,360 --> 00:17:02,800
比较小的一个区域出来

498
00:17:03,520 --> 00:17:04,680
第二个是说

499
00:17:04,720 --> 00:17:05,360
这个数据集

500
00:17:05,360 --> 00:17:07,000
相对来说比较复杂一点

501
00:17:07,039 --> 00:17:09,759
而且我们是做了颜色上的变化

502
00:17:10,600 --> 00:17:13,440
在network选择上面

503
00:17:13,480 --> 00:17:14,840
我们用的是34

504
00:17:15,480 --> 00:17:17,120
但是你可以用更多50

505
00:17:17,120 --> 00:17:19,319
可能50到甚至更高都可以

506
00:17:19,319 --> 00:17:20,480
因为它是相对来说

507
00:17:20,480 --> 00:17:21,680
比较复杂的数据集

508
00:17:21,720 --> 00:17:24,519
你用比较复杂一点的网络

509
00:17:24,680 --> 00:17:25,799
而且我们不一样的是

510
00:17:25,799 --> 00:17:26,720
我们偷了个懒

511
00:17:26,720 --> 00:17:28,319
我们做了一下fine tuning

512
00:17:28,519 --> 00:17:29,519
理论上数据集

513
00:17:29,920 --> 00:17:30,720
这个精彩的branch

514
00:17:30,720 --> 00:17:31,519
你做fine tuning

515
00:17:31,559 --> 00:17:33,200
因为你把imagine.net拿过来

516
00:17:33,319 --> 00:17:34,079
你还做个啥

517
00:17:34,440 --> 00:17:34,759
对吧

518
00:17:34,759 --> 00:17:36,079
你它是它的自己

519
00:17:36,879 --> 00:17:38,119
但是我们是给大家

520
00:17:38,119 --> 00:17:39,599
为了给大家演示说

521
00:17:39,639 --> 00:17:40,759
fine tuning可以怎么用

522
00:17:41,000 --> 00:17:42,400
尤其是说我们

523
00:17:43,039 --> 00:17:44,079
给了大家一个做法

524
00:17:44,079 --> 00:17:45,799
我们在做fine tuning的时候

525
00:17:45,799 --> 00:17:46,720
有讲过

526
00:17:47,039 --> 00:17:48,720
就是说你可以把前面那些

527
00:17:48,720 --> 00:17:49,839
下面的固定住

528
00:17:50,000 --> 00:17:51,839
作为一个很强的regulation

529
00:17:52,079 --> 00:17:54,559
我们这里就是把所谓的卷积层固定住

530
00:17:54,759 --> 00:17:56,200
所以是因为我觉得卷积层

531
00:17:56,200 --> 00:17:57,599
错的feature已经很好了

532
00:17:57,839 --> 00:17:58,639
然后后面层

533
00:17:58,840 --> 00:18:01,360
如果你担心它最后一层

534
00:18:01,360 --> 00:18:02,520
全连接圈的

535
00:18:03,840 --> 00:18:05,040
模型力不够的话

536
00:18:05,240 --> 00:18:06,360
就我们用了一个

537
00:18:06,480 --> 00:18:07,880
单影长程的MLP

538
00:18:08,440 --> 00:18:09,759
这是第二个不一样

539
00:18:10,240 --> 00:18:11,440
第三个不一样是说

540
00:18:11,440 --> 00:18:12,480
在预测的时候

541
00:18:12,480 --> 00:18:14,680
我们不再是预测单一个

542
00:18:14,680 --> 00:18:15,920
最大的概率类

543
00:18:16,120 --> 00:18:17,560
因为对于一些

544
00:18:17,720 --> 00:18:18,280
因为狗

545
00:18:18,720 --> 00:18:20,120
狗因为长得

546
00:18:21,040 --> 00:18:22,120
一样的太多了

547
00:18:22,120 --> 00:18:24,400
所以它是把每个狗的

548
00:18:24,400 --> 00:18:25,960
预测的概率值拿出来

549
00:18:26,200 --> 00:18:28,280
通常是说我在前

550
00:18:28,279 --> 00:18:29,879
top5的预测值里面

551
00:18:29,879 --> 00:18:31,599
能命中我的标号就不错了

552
00:18:31,599 --> 00:18:33,000
大家都觉得挺好的了

553
00:18:33,359 --> 00:18:34,759
OK这就是

554
00:18:34,879 --> 00:18:36,680
所以说这个数据集

555
00:18:36,680 --> 00:18:38,480
比前面一个数据更加复杂一点

556
00:18:39,839 --> 00:18:41,240
而且说更复杂数据集

557
00:18:41,240 --> 00:18:42,000
你可以怎么用

558
00:18:42,759 --> 00:18:43,200
OK

559
00:18:43,200 --> 00:18:44,519
所以这就是

560
00:18:44,559 --> 00:18:47,359
我们今天要讲的第二个

561
00:18:48,000 --> 00:18:48,839
计时本

562
00:18:49,240 --> 00:18:50,759
OK希望我们下个星期

563
00:18:50,759 --> 00:18:52,079
大家有踊跃

564
00:18:52,079 --> 00:18:53,399
提交一下大家的

565
00:18:53,399 --> 00:18:56,279
在数页分类上的一些结果

566
00:18:56,639 --> 00:18:58,519
给大家再看一看

567
00:18:59,359 --> 00:19:01,319
你这样赢比赛要怎么做

568
00:19:01,639 --> 00:19:02,879
用大量的injumbo

569
00:19:03,079 --> 00:19:04,720
大量的调的计数用

570
00:19:05,160 --> 00:19:06,920
更复杂的神经网络

571
00:19:07,399 --> 00:19:07,920
OK

