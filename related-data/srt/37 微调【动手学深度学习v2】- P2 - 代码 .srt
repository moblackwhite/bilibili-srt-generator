1
00:00:00,000 --> 00:00:05,919
好微调微调微调我们再导入一下我们的用的东西

2
00:00:05,919 --> 00:00:11,439
我们就直接我们用的是一个我们采用了一个数据集

3
00:00:11,439 --> 00:00:13,640
叫做一个热狗数据集

4
00:00:13,640 --> 00:00:15,160
就是两类问题

5
00:00:15,160 --> 00:00:17,719
我们自己在网上搜了一件图片搞进来的

6
00:00:17,719 --> 00:00:19,320
你可以直接从这下载

7
00:00:19,320 --> 00:00:22,120
我们就不去给大家解释一下这些东西了

8
00:00:22,120 --> 00:00:23,519
就反正就把它下下来

9
00:00:23,519 --> 00:00:24,359
然后load进来

10
00:00:24,359 --> 00:00:25,800
就是拿到train和test

11
00:00:25,800 --> 00:00:27,199
好我们看一下

12
00:00:27,199 --> 00:00:30,839
就是说就热狗就是里面就是热狗hotdog

13
00:00:30,839 --> 00:00:31,519
就是

14
00:00:32,119 --> 00:00:34,320
这类就是hotdog

15
00:00:34,320 --> 00:00:38,399
负类就是香蕉各种跟它长得很像的东西

16
00:00:38,399 --> 00:00:39,519
就是作为负类

17
00:00:41,879 --> 00:00:42,519
OK

18
00:00:42,759 --> 00:00:45,359
然后我们增广用的是可以看一下我们的增广

19
00:00:45,359 --> 00:00:47,640
我们做了normalization

20
00:00:47,640 --> 00:00:56,079
就是说把它的RGB的通道的均值和均值和方差拿出来

21
00:00:56,119 --> 00:00:59,199
对每一个通道做均值方差的normalization

22
00:00:59,199 --> 00:01:03,879
为什么做这个东西是因为你在img.net上训练的模型做了这个事情

23
00:01:03,879 --> 00:01:05,439
所以你要把这个东西搬过来

24
00:01:05,439 --> 00:01:07,560
另外一块就是说你可以看到是

25
00:01:07,560 --> 00:01:11,759
我们要用img.net上的模型做反应清理

26
00:01:11,759 --> 00:01:14,359
所以就是我们要搞成224的

27
00:01:14,719 --> 00:01:17,519
就是224x224img.net的东西

28
00:01:17,519 --> 00:01:21,560
然后我们也做了一个random的水平的反转

29
00:01:21,560 --> 00:01:23,719
在最后我们做了一个normalization

30
00:01:23,719 --> 00:01:25,599
这是因为img.net做这个事情

31
00:01:26,599 --> 00:01:28,120
同样道理的话

32
00:01:28,120 --> 00:01:32,719
就是我的测试机也是img.net常用的

33
00:01:32,719 --> 00:01:36,920
我先把它搞成一个256x256的图片

34
00:01:37,120 --> 00:01:42,159
然后应该是把短边弄成256

35
00:01:43,039 --> 00:01:45,840
长边跟着变就是长宽保持不变

36
00:01:45,840 --> 00:01:49,200
然后在center crop出了一个224x224的东西

37
00:01:49,319 --> 00:01:50,719
作为测试

38
00:01:50,719 --> 00:01:53,799
这是因为我们的图片的高宽比都不一样

39
00:01:53,799 --> 00:01:54,719
大小不一样

40
00:01:56,679 --> 00:01:57,399
好

41
00:01:57,640 --> 00:01:57,879
好

42
00:01:57,879 --> 00:01:58,920
这里是一个关键

43
00:01:58,920 --> 00:01:59,799
就是说

44
00:02:00,280 --> 00:02:02,280
我们去下模型的时候

45
00:02:02,280 --> 00:02:05,120
我们之前有用过resnet18这个模型

46
00:02:05,159 --> 00:02:08,000
但是我要把pretrend等于true

47
00:02:08,639 --> 00:02:09,719
设了这个东西之后

48
00:02:09,719 --> 00:02:12,639
就意味着说不仅把模型的定义弄下来

49
00:02:12,639 --> 00:02:16,919
还把在img.net上训练好的那些parameter

50
00:02:16,919 --> 00:02:17,639
给拿过来

51
00:02:19,120 --> 00:02:20,079
然后拿出来之后

52
00:02:20,079 --> 00:02:21,960
里面有一个就是FCFC

53
00:02:21,960 --> 00:02:23,280
就是一个fully connected

54
00:02:23,280 --> 00:02:24,599
就是最后那一层

55
00:02:24,879 --> 00:02:25,519
最后那一层

56
00:02:25,519 --> 00:02:26,400
因为是img.net上

57
00:02:26,400 --> 00:02:28,719
可以看到是说你的输出是1000

58
00:02:28,919 --> 00:02:30,000
输入是512

59
00:02:30,000 --> 00:02:31,519
就是你输入的dimension

60
00:02:33,240 --> 00:02:34,439
所以你要干的事情

61
00:02:34,439 --> 00:02:36,240
就是说我fine tune是什么

62
00:02:36,400 --> 00:02:38,319
我fine就是说这是pretrend

63
00:02:39,479 --> 00:02:40,840
就我fine tune的意思是什么

64
00:02:40,840 --> 00:02:41,519
fine tune的意思

65
00:02:41,519 --> 00:02:42,840
就是说我还是一样的

66
00:02:42,840 --> 00:02:46,120
把你模型中pretrend的下下来之后

67
00:02:46,159 --> 00:02:48,280
然后我把我的fully connected

68
00:02:48,280 --> 00:02:49,560
最后输出层

69
00:02:50,240 --> 00:02:55,400
改随机初始化成一个线性的一个层

70
00:02:55,520 --> 00:02:58,319
然后这是你的input feature是512

71
00:02:58,520 --> 00:03:00,759
但是你的类别数变成2了

72
00:03:01,199 --> 00:03:02,879
因为我们是个两分类问题

73
00:03:02,879 --> 00:03:04,439
我要把它标号改成2

74
00:03:04,960 --> 00:03:08,120
然后我们对它做xvr的一个随机初始化

75
00:03:08,120 --> 00:03:12,240
就只对最后一层的weight

76
00:03:12,240 --> 00:03:13,400
做随机初始化

77
00:03:13,439 --> 00:03:14,719
前面的话

78
00:03:14,759 --> 00:03:17,120
那些东西全部用的是pretrend的模型

79
00:03:17,439 --> 00:03:19,599
Ok这就是刚刚我们说的

80
00:03:19,599 --> 00:03:20,960
怎么样去随机

81
00:03:20,960 --> 00:03:22,719
怎么初始化我的模型

82
00:03:24,599 --> 00:03:24,960
好

83
00:03:24,960 --> 00:03:27,039
我们讲我们的fine tune

84
00:03:27,039 --> 00:03:28,920
fine tune就是这个东西都没什么区别

85
00:03:30,200 --> 00:03:31,800
这一块东西都没什么区别

86
00:03:33,240 --> 00:03:34,719
唯一不一样的是这个地方

87
00:03:35,840 --> 00:03:38,040
这个地方如果是parent group

88
00:03:38,040 --> 00:03:39,480
我们设了一个这样子的东西

89
00:03:40,040 --> 00:03:41,240
就给大家解释一下

90
00:03:41,240 --> 00:03:42,400
这个东西长什么样子

91
00:03:43,080 --> 00:03:43,879
就是说

92
00:03:44,240 --> 00:03:45,800
如果是等于true的话

93
00:03:45,960 --> 00:03:47,400
这是一个参数等于true的话

94
00:03:47,400 --> 00:03:48,200
我怎么办呢

95
00:03:49,640 --> 00:03:51,360
就是说我把

96
00:03:52,439 --> 00:03:54,360
不是最后一层的

97
00:03:54,360 --> 00:03:55,560
不是最后一层的

98
00:03:55,560 --> 00:03:57,080
所有的层都拿出来

99
00:03:57,560 --> 00:04:01,160
然后这一层用的是一个learning rate

100
00:04:01,160 --> 00:04:03,040
是一个默认的learning rate

101
00:04:05,080 --> 00:04:07,800
但是最后一层

102
00:04:08,680 --> 00:04:10,800
就是fc那个parameter

103
00:04:10,800 --> 00:04:12,760
它的learning rate用的是10倍的

104
00:04:12,760 --> 00:04:13,880
learning rate

105
00:04:14,120 --> 00:04:14,640
什么意思

106
00:04:14,640 --> 00:04:17,120
就是说我给你一个学习率

107
00:04:17,800 --> 00:04:18,720
除了最后一层

108
00:04:18,720 --> 00:04:19,840
因为是随机初始化的

109
00:04:19,840 --> 00:04:20,360
对吧

110
00:04:20,400 --> 00:04:21,800
别的已经初始化比较好的

111
00:04:21,800 --> 00:04:23,960
我用的是一个比较小的学习率

112
00:04:24,080 --> 00:04:26,080
最后一层用的是一个

113
00:04:26,400 --> 00:04:28,080
10倍更大的学习率

114
00:04:28,080 --> 00:04:30,080
是因为我们最后一层是随机初始化的

115
00:04:30,080 --> 00:04:31,920
我们希望他能够学得更快

116
00:04:31,920 --> 00:04:33,960
别的层我们不希望他改变太多

117
00:04:34,080 --> 00:04:37,120
这是一个小trick

118
00:04:38,199 --> 00:04:39,280
然后另外一个就是说

119
00:04:39,280 --> 00:04:40,920
你如果没有enable这个选项的话

120
00:04:41,000 --> 00:04:42,200
就是按正常来

121
00:04:42,319 --> 00:04:43,280
就跟之前一样

122
00:04:43,360 --> 00:04:44,160
然后我们可以看到

123
00:04:44,680 --> 00:04:45,680
跟之前train

124
00:04:45,720 --> 00:04:47,440
我们之前定义的train

125
00:04:47,440 --> 00:04:48,680
ch13

126
00:04:48,680 --> 00:04:51,000
ch3那个函数

127
00:04:51,000 --> 00:04:51,720
就直接调用了

128
00:04:51,880 --> 00:04:52,960
别的就是一样的

129
00:04:53,720 --> 00:04:54,520
所谓的区别

130
00:04:54,520 --> 00:04:55,240
就是说

131
00:04:55,320 --> 00:04:57,440
我们如果是enable这个选项的话

132
00:04:57,440 --> 00:04:58,360
那么就是

133
00:04:59,320 --> 00:05:01,000
最后一层用的比较大的学习率

134
00:05:01,000 --> 00:05:02,080
剩下的比较小

135
00:05:02,560 --> 00:05:03,120
OK

136
00:05:03,760 --> 00:05:05,160
那么可以直接看一下结果

137
00:05:06,600 --> 00:05:08,120
看结果就是说

138
00:05:08,480 --> 00:05:09,320
FindTuneNet

139
00:05:09,320 --> 00:05:10,920
FindTuneNet就是我们刚刚构造的

140
00:05:10,920 --> 00:05:11,640
那个东西

141
00:05:11,719 --> 00:05:12,319
构造的东西

142
00:05:12,319 --> 00:05:13,000
就是

143
00:05:13,399 --> 00:05:14,439
最后一层是随机的

144
00:05:14,439 --> 00:05:16,000
全面的全是

145
00:05:18,599 --> 00:05:19,639
copy来的

146
00:05:19,959 --> 00:05:21,560
然后我们用了多少个epoch看一下

147
00:05:21,560 --> 00:05:22,560
我们用了5个epoch

148
00:05:22,560 --> 00:05:22,959
对吧

149
00:05:23,079 --> 00:05:24,680
就我们没有用10个

150
00:05:25,439 --> 00:05:26,919
然后你看一下这个线

151
00:05:27,479 --> 00:05:28,959
你会发现说

152
00:05:29,240 --> 00:05:30,360
这两个accuracy

153
00:05:31,639 --> 00:05:32,360
就是说

154
00:05:32,399 --> 00:05:33,639
一开始就很高了

155
00:05:33,639 --> 00:05:34,360
你发现没有

156
00:05:34,360 --> 00:05:35,159
就是说

157
00:05:35,199 --> 00:05:37,319
一开始就到这个地方了

158
00:05:37,919 --> 00:05:39,680
然后基本上后面5轮

159
00:05:39,680 --> 00:05:41,399
你其实叠大概一两轮就行了

160
00:05:41,400 --> 00:05:43,000
根本不需要叠5个epoch

161
00:05:43,000 --> 00:05:43,800
就基本上到了

162
00:05:43,800 --> 00:05:45,840
第二个epoch的精度就很好了

163
00:05:46,320 --> 00:05:48,280
训练和测试都差不多

164
00:05:49,680 --> 00:05:50,680
而且

165
00:05:51,760 --> 00:05:53,240
test accuracy高一些

166
00:05:53,240 --> 00:05:54,040
看见没有

167
00:05:54,760 --> 00:05:56,720
测试精度比训练精度高

168
00:05:56,720 --> 00:06:00,240
这是因为我们确实没有太多overfilling

169
00:06:00,480 --> 00:06:02,560
因为我们在imginet上拿过来的东西

170
00:06:02,560 --> 00:06:03,760
在简单数据上

171
00:06:03,760 --> 00:06:05,520
确实overfilling不大

172
00:06:05,920 --> 00:06:06,680
因为我们学习率

173
00:06:06,680 --> 00:06:07,360
有特别小

174
00:06:07,360 --> 00:06:08,280
无一复

175
00:06:09,280 --> 00:06:11,360
而且是我们是无一复的

176
00:06:11,360 --> 00:06:12,160
意思是说

177
00:06:12,200 --> 00:06:13,600
所有的

178
00:06:13,880 --> 00:06:16,960
最后的全连接层用的是4一复

179
00:06:17,400 --> 00:06:18,280
5一复4

180
00:06:18,280 --> 00:06:19,320
这一个学习率

181
00:06:19,360 --> 00:06:20,120
还是很小

182
00:06:21,000 --> 00:06:23,120
别的我全部用的是5一复

183
00:06:23,120 --> 00:06:24,560
这一个很小的学习率

184
00:06:25,400 --> 00:06:26,920
可以看到基本上我的

185
00:06:26,960 --> 00:06:28,640
loss没有增加

186
00:06:28,640 --> 00:06:29,720
而且是有点点抖动

187
00:06:29,720 --> 00:06:31,200
就是很平的这个地方

188
00:06:31,600 --> 00:06:33,280
而且我的accuracy一开始

189
00:06:33,280 --> 00:06:34,480
就是比较平的一个地方

190
00:06:34,480 --> 00:06:35,240
就是说你这里

191
00:06:35,240 --> 00:06:36,400
其实这里有点点抖动

192
00:06:36,920 --> 00:06:37,720
但没关系

193
00:06:38,000 --> 00:06:38,680
可以看到

194
00:06:39,160 --> 00:06:41,280
用了fine tune的时候的效果

195
00:06:41,920 --> 00:06:42,880
对比一下

196
00:06:43,680 --> 00:06:45,120
对比一下不用fine tune怎么样

197
00:06:45,120 --> 00:06:46,040
不用fine tune很简单

198
00:06:46,040 --> 00:06:49,120
就是说你去构造network

199
00:06:49,120 --> 00:06:50,320
是你的pre-train

200
00:06:50,320 --> 00:06:51,280
不要等于true

201
00:06:52,040 --> 00:06:54,000
就pre-train你要等于

202
00:06:54,520 --> 00:06:55,720
就不要设pre-train等于true

203
00:06:55,840 --> 00:06:58,840
就是所有的全都是随机数值化的

204
00:06:58,880 --> 00:07:00,160
然后最后一层一样的

205
00:07:00,160 --> 00:07:01,400
你还得弄成一个

206
00:07:02,120 --> 00:07:02,520
等于2

207
00:07:02,680 --> 00:07:04,280
因为你的label数是2

208
00:07:04,520 --> 00:07:06,000
所以这个意味着说

209
00:07:06,000 --> 00:07:07,920
所有的东西都是从随机开始的

210
00:07:07,920 --> 00:07:09,399
但是网络架构是一样

211
00:07:10,360 --> 00:07:11,519
调完东西训练

212
00:07:11,879 --> 00:07:12,839
当然有点不公平

213
00:07:12,959 --> 00:07:13,519
有点不公平

214
00:07:13,560 --> 00:07:15,040
这个学习率对他来讲有点低

215
00:07:15,040 --> 00:07:16,160
你可以要调大一点

216
00:07:16,160 --> 00:07:17,560
就我们是调的5亿-4

217
00:07:17,560 --> 00:07:20,519
就是说跟之前的全年阶层的学习率

218
00:07:20,519 --> 00:07:21,120
是一样的

219
00:07:21,279 --> 00:07:22,920
但你可能还得调大一点点

220
00:07:22,959 --> 00:07:24,879
但主要是给大家看一下效果

221
00:07:24,879 --> 00:07:25,560
就是说

222
00:07:27,120 --> 00:07:28,399
基本上可以看到

223
00:07:28,959 --> 00:07:30,680
这基本上长得比较慢了

224
00:07:30,680 --> 00:07:32,240
他还在慢慢的长

225
00:07:32,240 --> 00:07:33,319
但是很慢了

226
00:07:33,439 --> 00:07:34,279
你可以看一下

227
00:07:34,319 --> 00:07:35,120
精度

228
00:07:35,439 --> 00:07:36,840
训练精度是

229
00:07:37,360 --> 00:07:38,399
82

230
00:07:38,399 --> 00:07:39,959
测试精度是84

231
00:07:40,920 --> 00:07:41,800
因为他高一点

232
00:07:41,800 --> 00:07:43,439
还是因为我们做了增强

233
00:07:43,600 --> 00:07:44,840
数据增强的原因

234
00:07:45,280 --> 00:07:46,240
但是你可以看一下

235
00:07:46,240 --> 00:07:48,800
84对比我们的94

236
00:07:48,840 --> 00:07:50,280
整整差了10个点

237
00:07:51,199 --> 00:07:51,720
OK

238
00:07:51,720 --> 00:07:52,959
这就是说唯一的区别

239
00:07:52,959 --> 00:07:53,600
就是说

240
00:07:54,480 --> 00:07:57,199
我们在前面用的是imagenet上

241
00:07:57,199 --> 00:07:58,800
拿过来的

242
00:07:59,000 --> 00:08:00,120
所有全年阶层

243
00:08:00,120 --> 00:08:00,920
都是imagenet上

244
00:08:00,920 --> 00:08:01,399
pre-trained的

245
00:08:01,399 --> 00:08:02,199
作为初始化

246
00:08:02,199 --> 00:08:03,639
这里是从随机开始

247
00:08:03,879 --> 00:08:05,959
所以基本上就一个那么改动

248
00:08:06,680 --> 00:08:09,519
使得我们差了10个点

249
00:08:09,719 --> 00:08:10,319
OK

250
00:08:10,319 --> 00:08:11,479
这就是

251
00:08:12,159 --> 00:08:12,800
fine tuning

252
00:08:12,800 --> 00:08:14,560
而且我的建议是说

253
00:08:15,360 --> 00:08:17,839
大家尽量从fine tuning开始

254
00:08:17,839 --> 00:08:18,680
从微调开始

255
00:08:18,680 --> 00:08:20,680
不要从真的从零开始

256
00:08:20,680 --> 00:08:22,199
对自己的数据进行训练

257
00:08:23,159 --> 00:08:23,719
所以

258
00:08:24,800 --> 00:08:27,639
这就是一般的计算机世界的做法

259
00:08:27,839 --> 00:08:31,479
而且未来我几乎可以认为

260
00:08:31,480 --> 00:08:34,200
所有用于深度学习的应用

261
00:08:34,200 --> 00:08:36,600
都会是主要是基于fine tuning

262
00:08:36,879 --> 00:08:39,159
就真的从原始数据上训练的

263
00:08:39,159 --> 00:08:40,600
会变得越来越少

264
00:08:40,920 --> 00:08:42,440
主要是学术界或者大公司

265
00:08:42,440 --> 00:08:43,240
在做一些

266
00:08:43,240 --> 00:08:44,519
在很大的数据上

267
00:08:44,519 --> 00:08:46,039
做一些重新开始训练

268
00:08:46,039 --> 00:08:47,759
但是对于个人来讲

269
00:08:47,759 --> 00:08:50,240
或者是你的应用来讲

270
00:08:50,240 --> 00:08:51,800
通常你是做fine tuning的

271
00:08:52,240 --> 00:08:52,800
OK

272
00:08:52,800 --> 00:08:54,320
这就是我们的要讲的

