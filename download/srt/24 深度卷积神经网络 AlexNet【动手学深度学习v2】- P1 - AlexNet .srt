1
00:00:00,000 --> 00:00:01,560
问题一

2
00:00:01,580 --> 00:00:03,399
ImageNet是不是已经成为历史了

3
00:00:03,419 --> 00:00:05,160
ImageNet还不是历史了

4
00:00:05,160 --> 00:00:12,240
ImageNet是一个当前当下使用最多的一个数据集

5
00:00:12,240 --> 00:00:16,719
它现在成为了基本上你的绝大部分的论文

6
00:00:16,740 --> 00:00:19,719
绝大部分论文说你要做剧情机神经网络

7
00:00:19,740 --> 00:00:22,760
想要比较好的证明你的效果比较好的话

8
00:00:22,760 --> 00:00:24,199
一般还是会用ImageNet

9
00:00:24,199 --> 00:00:26,400
ImageNet虽然是10年前

10
00:00:27,400 --> 00:00:30,680
它的具体的构造应该是他去Google上爬

11
00:00:30,680 --> 00:00:32,840
爬下来之后然后去mechanical Turk

12
00:00:32,840 --> 00:00:34,840
就是当年的Amazon的一个服务

13
00:00:35,600 --> 00:00:37,040
又让人给你来标

14
00:00:37,040 --> 00:00:38,560
用手动来标构建出来的

15
00:00:38,560 --> 00:00:39,880
当时应该花了100万美金

16
00:00:39,880 --> 00:00:41,520
现在花的也就很便宜了

17
00:00:41,520 --> 00:00:43,200
那时候比较贵

18
00:00:44,120 --> 00:00:48,800
所以这个数据集还是当下最为使用了最常见的数据集

19
00:00:48,800 --> 00:00:50,200
不是历史

20
00:00:52,640 --> 00:00:55,400
为什么2000年的时候神经网络会被何方替代

21
00:00:55,559 --> 00:00:56,799
这个问题我们回答过几次

22
00:00:56,799 --> 00:01:01,519
我觉得主要是何方法在

23
00:01:02,079 --> 00:01:06,079
神经网络一你确实算深的东西算不动

24
00:01:06,079 --> 00:01:09,159
第二块是何方法在理论上有更好

25
00:01:09,400 --> 00:01:10,599
学术界喜欢理论

26
00:01:14,920 --> 00:01:18,959
这个是第三个问题说NLP领域CNN是替代了人工特征工程吗

27
00:01:20,000 --> 00:01:23,039
如何看待NLP的什么东西和区别

28
00:01:23,200 --> 00:01:24,160
本质上没区别

29
00:01:24,160 --> 00:01:27,080
我们等会会讲NLP在就是说

30
00:01:27,080 --> 00:01:29,480
这种设计思路跟CNN是本质上没区别的

31
00:01:29,480 --> 00:01:31,680
我们之后讲到NLP会仔细来讲

32
00:01:34,320 --> 00:01:36,120
AlexNet让机器寻找特征

33
00:01:36,120 --> 00:01:38,040
如果找到特征符合人类逻辑吗

34
00:01:38,040 --> 00:01:39,760
如果不符合要怎么解释

35
00:01:41,160 --> 00:01:43,360
当然不符合

36
00:01:43,360 --> 00:01:48,280
我们昨天QA也去说有去给大家看过一个

37
00:01:48,560 --> 00:01:50,680
去看一下CNN到底在学些什么

38
00:01:50,680 --> 00:01:52,360
这一块是一个研究工作

39
00:01:52,480 --> 00:01:55,200
但是说我优化了什么

40
00:01:55,200 --> 00:01:57,640
我的优化目标就是要全球特征

41
00:01:57,640 --> 00:01:59,640
使得我的最后那一层能分类

42
00:02:00,239 --> 00:02:02,560
我的优化目标当然就是为了那个目标的话

43
00:02:02,560 --> 00:02:04,319
我没有保证你人能读懂

44
00:02:04,840 --> 00:02:11,079
所以如果人能够去理解那只碰巧

45
00:02:11,280 --> 00:02:13,879
不然的话他因为我的优化根本就没考虑人

46
00:02:13,879 --> 00:02:16,639
AlexNet怎么知道人存在这个事情

47
00:02:17,159 --> 00:02:21,680
所以说虽然神经网络在一开始去模拟

48
00:02:21,680 --> 00:02:22,800
人的大脑的工作

49
00:02:22,800 --> 00:02:24,760
但现在早就不提这一套了

50
00:02:24,760 --> 00:02:26,640
所以

51
00:02:26,640 --> 00:02:32,080
如果你想要解释的话会有

52
00:02:32,080 --> 00:02:34,640
就是说如果你想让模型可以解释

53
00:02:34,640 --> 00:02:35,879
你需要去修改模型

54
00:02:35,879 --> 00:02:37,240
修改你的训练的目标

55
00:02:37,240 --> 00:02:38,840
使得你的模型更加解释

56
00:02:38,840 --> 00:02:40,879
但是深度学习的科技的事情确实很差

57
00:02:44,439 --> 00:02:47,319
就是问题5把石化层放到不同的

58
00:02:47,319 --> 00:02:48,640
崛起层后面效果一样吗

59
00:02:48,640 --> 00:02:50,920
为什么放在第一个和最后一个崛起层后面

60
00:02:51,320 --> 00:02:58,120
就是说

61
00:03:01,320 --> 00:03:02,040
就这个问题

62
00:03:02,200 --> 00:03:05,120
这个问题就是说

63
00:03:05,480 --> 00:03:08,160
这道问题不是那么好回答

64
00:03:08,160 --> 00:03:08,920
就是说

65
00:03:10,400 --> 00:03:11,520
你得去试一下

66
00:03:11,520 --> 00:03:12,880
我没试过

67
00:03:12,880 --> 00:03:16,600
你这个问题我真的不好当场回答

68
00:03:16,600 --> 00:03:18,080
因为这个东西跑起来要跑三分钟

69
00:03:18,080 --> 00:03:19,640
我不能现场给你演示了

70
00:03:19,760 --> 00:03:21,000
你可以去试一下

71
00:03:21,000 --> 00:03:23,120
把它放在后面会怎么样

72
00:03:24,400 --> 00:03:24,760
对吧

73
00:03:24,760 --> 00:03:25,800
我就不回答了

74
00:03:29,280 --> 00:03:30,800
两个网络的设计

75
00:03:30,800 --> 00:03:31,680
除了看论文

76
00:03:31,680 --> 00:03:34,760
有没有其他方式了解到具体的设计过程

77
00:03:38,000 --> 00:03:38,680
这一块

78
00:03:39,880 --> 00:03:40,640
就是说

79
00:03:40,840 --> 00:03:44,120
确实你网络的设计的思想在什么地方

80
00:03:44,120 --> 00:03:45,360
以及它是怎么设计的

81
00:03:45,360 --> 00:03:47,200
当时他怎么想的

82
00:03:47,760 --> 00:03:51,320
这个东西确实大家不是那么的好说

83
00:03:53,280 --> 00:03:54,360
怎么说呢

84
00:03:56,680 --> 00:03:58,880
我还是我们之前有讲过

85
00:03:59,080 --> 00:04:02,680
就是说我们尽量的课给大家去尝试

86
00:04:02,680 --> 00:04:04,080
从我的角度去理解

87
00:04:04,080 --> 00:04:05,360
他怎么他们怎么想的

88
00:04:05,360 --> 00:04:06,280
怎么理解

89
00:04:06,280 --> 00:04:08,280
但实际上你说你人家怎么想的

90
00:04:08,280 --> 00:04:10,320
可能没想可能就调出来的

91
00:04:10,320 --> 00:04:11,680
谁知道

92
00:04:11,680 --> 00:04:14,520
他可能就是Alex小哥

93
00:04:14,560 --> 00:04:15,760
Alex叫做Alex

94
00:04:15,880 --> 00:04:18,319
是因为他第一座的人的名字叫Alex

95
00:04:18,319 --> 00:04:20,800
他其实是个硕士学生

96
00:04:20,800 --> 00:04:21,560
硕士生

97
00:04:21,560 --> 00:04:23,360
他就是做个毕业设计

98
00:04:23,360 --> 00:04:25,040
然后在那边调

99
00:04:25,040 --> 00:04:26,360
然后调出来了

100
00:04:26,480 --> 00:04:27,439
挺厉害的

101
00:04:27,600 --> 00:04:28,879
所以可能他也没想

102
00:04:28,879 --> 00:04:29,960
就调出来的

103
00:04:29,960 --> 00:04:30,319
对不对

104
00:04:33,640 --> 00:04:35,439
CNN玩爆MLP吗

105
00:04:35,439 --> 00:04:37,600
MLP是不是会成为主流

106
00:04:38,120 --> 00:04:40,080
最近有个模型叫MLP Mix

107
00:04:40,400 --> 00:04:43,200
就是说他说我能做赢CNN

108
00:04:43,839 --> 00:04:46,120
但是这一块我就不做预测了

109
00:04:46,120 --> 00:04:48,680
CNN就是一个卷迹是一个特殊的MLP

110
00:04:48,680 --> 00:04:50,120
我只能这么说

111
00:04:50,120 --> 00:04:53,120
MLP你可以做更多的结构化的信息

112
00:04:53,120 --> 00:04:53,599
能够

113
00:04:53,599 --> 00:04:56,560
Transformer你也可以认为是一个MLP

114
00:04:56,560 --> 00:04:58,039
加了一点别的东西在里面

115
00:04:58,079 --> 00:04:58,800
所以

116
00:04:59,360 --> 00:05:00,240
你好

117
00:05:00,240 --> 00:05:01,919
这个问题不是那么好回答

118
00:05:05,399 --> 00:05:07,039
问题8挺有意思的

119
00:05:07,039 --> 00:05:07,959
就是说AlexNet

120
00:05:07,959 --> 00:05:09,719
其实我们这里讲的AlexNet

121
00:05:09,719 --> 00:05:11,159
不是真正的AlexNet

122
00:05:11,200 --> 00:05:14,800
AlexNet他做了一个Local Response Normalization

123
00:05:15,000 --> 00:05:16,400
这个东西就是说

124
00:05:16,600 --> 00:05:17,920
大家没看懂

125
00:05:20,240 --> 00:05:21,640
就我们没讲这个东西

126
00:05:21,840 --> 00:05:23,600
就是说你像Nernet

127
00:05:23,600 --> 00:05:25,120
最后是一个Gauss Layer

128
00:05:25,120 --> 00:05:26,680
它不是一个softmax

129
00:05:27,480 --> 00:05:29,120
就是说这个东西

130
00:05:29,120 --> 00:05:31,240
LRN这个东西我们就不讲了

131
00:05:31,240 --> 00:05:34,640
他你可认为他是一些normalization在里面

132
00:05:34,680 --> 00:05:37,920
他后来证明说没什么太多用

133
00:05:37,960 --> 00:05:39,879
就是说其实这个东西

134
00:05:40,199 --> 00:05:41,920
就是说一个东西如果发现

135
00:05:41,920 --> 00:05:42,600
就是说

136
00:05:42,600 --> 00:05:44,680
基本上整个神经网络是大家去看

137
00:05:44,680 --> 00:05:46,120
里面到底是谁有用

138
00:05:46,120 --> 00:05:46,920
谁没用

139
00:05:46,959 --> 00:05:49,759
就一开始你发现者可能没有仔细想

140
00:05:49,759 --> 00:05:50,439
他就试了一下

141
00:05:50,639 --> 00:05:52,240
觉得好用就这样了

142
00:05:52,759 --> 00:05:54,120
后面的人会去

143
00:05:54,120 --> 00:05:56,199
每个人去看每一个东西到底有没有用

144
00:05:56,199 --> 00:05:57,879
后来发现这个LRN没什么用

145
00:05:57,920 --> 00:05:58,959
没什么大家就

146
00:05:59,519 --> 00:06:00,399
几乎没用过了

147
00:06:00,399 --> 00:06:02,279
我就好像也就AlexNet用过

148
00:06:02,279 --> 00:06:03,120
之后就没人用过了

149
00:06:03,120 --> 00:06:05,399
所以你也可以不用去理解没关系

150
00:06:05,439 --> 00:06:07,319
但之后我们有更多的normalization的

151
00:06:07,319 --> 00:06:07,639
技术

152
00:06:07,639 --> 00:06:09,120
让你真的是有用了

153
00:06:09,199 --> 00:06:10,680
比如说Batch Normalization

154
00:06:10,720 --> 00:06:13,000
虽然都叫normalization

155
00:06:13,000 --> 00:06:14,319
但当然是不一样的了

156
00:06:14,439 --> 00:06:15,759
所以

157
00:06:16,800 --> 00:06:18,759
所以可以就是说我们就可以不学

158
00:06:18,759 --> 00:06:19,399
没关系

159
00:06:22,959 --> 00:06:24,879
就为什么AlexNet最后有两个相同的

160
00:06:24,879 --> 00:06:26,360
全联接层都是4.96

161
00:06:26,480 --> 00:06:27,439
一个行吗

162
00:06:28,600 --> 00:06:30,360
一个还真不行

163
00:06:30,720 --> 00:06:31,560
你可以试一下

164
00:06:31,560 --> 00:06:35,079
我记得很早以前试过一个效果会差

165
00:06:35,439 --> 00:06:38,160
就是说两个巨大的全联接是非常

166
00:06:38,560 --> 00:06:41,160
非常的厉害的一个4.96

167
00:06:41,160 --> 00:06:43,640
乘4.96的全联接层是非常厉害的

168
00:06:43,640 --> 00:06:44,439
一个模型了

169
00:06:44,520 --> 00:06:46,880
所以你好像是因为前面的卷迹的

170
00:06:46,880 --> 00:06:48,840
特征抽的不够好不够深

171
00:06:48,840 --> 00:06:51,600
所以你后面得靠两个大的dense来补

172
00:06:51,640 --> 00:06:53,200
你砍掉一个效果会变差

173
00:06:53,200 --> 00:06:53,800
我记得

174
00:06:54,040 --> 00:06:55,280
所以你可以试一下

175
00:06:57,640 --> 00:07:00,240
我们现在讲的是不是只是MLP和CVE

176
00:07:00,240 --> 00:07:02,000
在推荐领域不适用

177
00:07:02,040 --> 00:07:04,400
而推荐领域最近在深度学习比较火

178
00:07:04,560 --> 00:07:05,680
最近几年都比较火

179
00:07:06,040 --> 00:07:07,040
国内同学可能

180
00:07:07,160 --> 00:07:08,360
如果是头条

181
00:07:08,600 --> 00:07:09,040
美团

182
00:07:11,560 --> 00:07:14,560
内容分销商公司或者是B站

183
00:07:14,920 --> 00:07:15,319
知乎

184
00:07:15,520 --> 00:07:16,720
你可能都得做推荐

185
00:07:16,760 --> 00:07:19,280
所以推荐在国内应该是比较火的一个话题

186
00:07:19,439 --> 00:07:21,080
我们有一张讲推荐

187
00:07:21,120 --> 00:07:23,480
但是现在讲的内容

188
00:07:23,520 --> 00:07:26,640
你可认为不能直接用在推荐上面

189
00:07:26,640 --> 00:07:27,480
推荐是一个文本

190
00:07:27,480 --> 00:07:30,400
或者很多是一些item的东西

191
00:07:30,480 --> 00:07:31,400
不能直接用

192
00:07:31,439 --> 00:07:34,040
但是他的思想其实是一路的

193
00:07:34,360 --> 00:07:37,240
模拟实验

194
00:07:37,439 --> 00:07:40,360
在识别细胞的程序里面做了颜色

195
00:07:40,360 --> 00:07:41,319
集合变化增强后

196
00:07:41,319 --> 00:07:44,760
想想反而比只比做集合变化差

197
00:07:44,760 --> 00:07:45,439
这个

198
00:07:48,920 --> 00:07:50,759
就是说你说我一个东西

199
00:07:50,759 --> 00:07:53,720
我一个具体的应用做了很多增强效果

200
00:07:53,720 --> 00:07:56,200
反而比少做几个差

201
00:07:56,200 --> 00:07:57,080
这个太正常了

202
00:07:57,080 --> 00:08:00,759
增强这个事情是一个

203
00:08:01,160 --> 00:08:02,720
我们之后会来仔细讲

204
00:08:02,920 --> 00:08:04,160
这个东西太正常了

205
00:08:04,160 --> 00:08:05,400
这是一个超参数

206
00:08:05,400 --> 00:08:07,120
调的挺辛苦的

207
00:08:07,120 --> 00:08:07,720
大家会

208
00:08:08,040 --> 00:08:08,800
这个很正常

209
00:08:08,800 --> 00:08:11,440
我觉得没有太多解

210
00:08:11,440 --> 00:08:12,720
你可以去尝试解释

211
00:08:12,720 --> 00:08:14,840
但是说加多变差

212
00:08:15,000 --> 00:08:16,320
很有可能是

213
00:08:17,240 --> 00:08:19,440
这不是一个很奇怪的事情

214
00:08:25,000 --> 00:08:27,120
你说我的PPT写错了

215
00:08:27,440 --> 00:08:29,320
46比0.6

216
00:08:29,720 --> 00:08:30,080
是的

217
00:08:30,080 --> 00:08:31,000
我是写错了

218
00:08:32,000 --> 00:08:33,360
我是写错了

219
00:08:35,559 --> 00:08:35,960
行

220
00:08:35,960 --> 00:08:37,200
这个其实不是我写的

221
00:08:37,240 --> 00:08:38,279
是我老板写的

222
00:08:38,480 --> 00:08:39,639
我再仔细研究一下

223
00:08:39,639 --> 00:08:41,559
看看是不是架错了

224
00:08:41,559 --> 00:08:46,200
其实我觉得XNet它不是46兆

225
00:08:46,200 --> 00:08:48,679
我记得它差不多是40兆

226
00:08:48,679 --> 00:08:49,200
应该没问题

227
00:08:49,200 --> 00:08:52,519
我记得它在内存里面大概占300兆内存

228
00:08:52,600 --> 00:08:55,000
乘个8

229
00:08:55,000 --> 00:08:57,360
所以那就是我写的

230
00:08:57,360 --> 00:08:58,879
那就是11倍是错了

231
00:08:59,879 --> 00:09:03,360
就为什么XNet不属于深度卷积网络

232
00:09:03,840 --> 00:09:04,279
这个东西

233
00:09:05,000 --> 00:09:06,439
这个也是个很好的问题

234
00:09:06,439 --> 00:09:08,399
就是说你为什么这个东西

235
00:09:08,399 --> 00:09:10,840
我们不把XNet的叫做深度卷积网络

236
00:09:11,919 --> 00:09:16,039
就是说整个那帮搞深度学习的人

237
00:09:16,360 --> 00:09:17,600
他们最厉害的

238
00:09:17,600 --> 00:09:18,799
其实不是调参

239
00:09:18,799 --> 00:09:19,320
不是什么

240
00:09:19,320 --> 00:09:21,080
他们最厉害的是包装

241
00:09:22,439 --> 00:09:23,200
就是说

242
00:09:24,519 --> 00:09:26,279
我说我要卖一个产品

243
00:09:26,279 --> 00:09:27,759
其实也没有太多变化

244
00:09:28,360 --> 00:09:30,000
就是我一个产品

245
00:09:30,000 --> 00:09:32,879
就是说我下一代比上一代多加了一点

246
00:09:32,879 --> 00:09:33,679
CPU更好了

247
00:09:33,679 --> 00:09:34,559
什么东西更好了

248
00:09:35,559 --> 00:09:39,480
我如果说这一代产品比上一代产品快了10%

249
00:09:40,279 --> 00:09:40,919
你会买吗

250
00:09:40,919 --> 00:09:41,879
你不会买

251
00:09:42,720 --> 00:09:43,439
我怎么让你买

252
00:09:43,559 --> 00:09:45,879
我说这一代是革命性的

253
00:09:46,559 --> 00:09:47,200
特别厉害

254
00:09:47,200 --> 00:09:47,759
就苹果

255
00:09:47,840 --> 00:09:49,519
你想想苹果是每次怎么宣传

256
00:09:49,519 --> 00:09:50,639
革命性的

257
00:09:50,960 --> 00:09:52,799
完全颠覆了所有的

258
00:09:52,799 --> 00:09:54,279
换一个特别好的名字

259
00:09:55,080 --> 00:09:57,240
所以深度神经网络Deep

260
00:09:57,759 --> 00:09:59,559
learning or deep neural network

261
00:09:59,799 --> 00:10:00,840
deep这个词

262
00:10:00,840 --> 00:10:04,519
就是他们想出来的一个很好的一个marketing的词

263
00:10:05,519 --> 00:10:07,240
就是他当然可以把learned叫做deep

264
00:10:07,399 --> 00:10:08,360
但你如果叫了的话

265
00:10:08,480 --> 00:10:11,039
我怎么显示我的工作厉害呢

266
00:10:11,039 --> 00:10:12,039
所以就说

267
00:10:12,039 --> 00:10:13,519
所以说relu

268
00:10:13,519 --> 00:10:14,279
就是一个max

269
00:10:14,279 --> 00:10:15,159
你叫什么relu

270
00:10:15,159 --> 00:10:16,399
你就max

271
00:10:16,399 --> 00:10:19,200
那个activation不行吗

272
00:10:19,200 --> 00:10:21,000
你叫一个relu干嘛

273
00:10:21,200 --> 00:10:22,439
而且你dropout

274
00:10:22,679 --> 00:10:23,720
dropout这个东西

275
00:10:23,720 --> 00:10:24,399
就是说

276
00:10:25,039 --> 00:10:26,559
神经网络没有什么东西

277
00:10:26,560 --> 00:10:27,600
太多东西是新的

278
00:10:27,720 --> 00:10:28,640
当然之后有新的

279
00:10:28,640 --> 00:10:29,320
我们先不讲

280
00:10:29,520 --> 00:10:30,720
到到

281
00:10:31,120 --> 00:10:32,520
到transformer之前

282
00:10:32,520 --> 00:10:33,640
就是tension之前

283
00:10:33,640 --> 00:10:34,920
没有东西是新的

284
00:10:35,240 --> 00:10:36,920
你那些东西都是

285
00:10:37,120 --> 00:10:38,120
几十年前的东西

286
00:10:38,120 --> 00:10:39,440
没什么变化

287
00:10:39,440 --> 00:10:40,680
你只把它做深了而已

288
00:10:40,720 --> 00:10:41,960
但是你说我就做深了

289
00:10:41,960 --> 00:10:42,240
不行

290
00:10:42,360 --> 00:10:42,760
迈不动

291
00:10:43,280 --> 00:10:44,400
大家不火

292
00:10:44,680 --> 00:10:45,240
那怎么办

293
00:10:45,440 --> 00:10:46,800
我得想词

294
00:10:46,800 --> 00:10:48,200
我叫deep你多厉害

295
00:10:48,280 --> 00:10:49,120
deep深度

296
00:10:49,120 --> 00:10:49,640
对吧

297
00:10:50,000 --> 00:10:53,640
然后我叫所有东西都要取一个好听的名字

298
00:10:53,760 --> 00:10:55,160
就他们那帮人特别厉害

299
00:10:55,160 --> 00:10:55,840
这个事情

300
00:10:55,840 --> 00:10:57,840
我没有讽刺

301
00:10:57,840 --> 00:10:58,879
也是我觉得

302
00:10:59,040 --> 00:11:00,280
我们应该也去想一下

303
00:11:00,280 --> 00:11:01,399
应该学这个事情

304
00:11:01,639 --> 00:11:02,399
这个东西

305
00:11:02,519 --> 00:11:03,519
你有个工作

306
00:11:03,519 --> 00:11:04,680
对一个研究者来说

307
00:11:04,680 --> 00:11:07,399
是怎么宣传你的工作非常重要

308
00:11:08,080 --> 00:11:09,240
你做一个东西

309
00:11:09,240 --> 00:11:10,480
同样的效果好

310
00:11:10,480 --> 00:11:12,080
我的宣传效果好

311
00:11:12,080 --> 00:11:13,240
我的名字取得好

312
00:11:13,280 --> 00:11:14,800
我出去讲的多

313
00:11:14,800 --> 00:11:15,960
那你宣传效果就好

314
00:11:15,960 --> 00:11:17,080
大家就会买你的

315
00:11:17,759 --> 00:11:19,440
根本就不是什么技术怎么样

316
00:11:19,440 --> 00:11:20,040
什么什么的

317
00:11:20,040 --> 00:11:21,040
这宣传特别重要

318
00:11:21,040 --> 00:11:21,639
你可以

319
00:11:22,120 --> 00:11:23,120
这一块你看一下

320
00:11:23,440 --> 00:11:25,840
国内有那么多产品

321
00:11:25,840 --> 00:11:26,039
对吧

322
00:11:26,039 --> 00:11:27,639
宣传套路一套一套的

323
00:11:27,639 --> 00:11:28,960
所以你不要意外

324
00:11:29,320 --> 00:11:30,399
在深度审量

325
00:11:30,399 --> 00:11:32,200
这个学术界没什么分支区别

326
00:11:32,879 --> 00:11:33,399
OK

327
00:11:36,279 --> 00:11:38,039
在设计的时候有什么讲究

328
00:11:38,399 --> 00:11:39,320
这层有什么讲究

329
00:11:39,320 --> 00:11:40,120
我们之后会讲

330
00:11:40,159 --> 00:11:41,200
我们今天就不讲了

331
00:11:41,200 --> 00:11:42,759
之后什么经典设计思路

332
00:11:42,759 --> 00:11:45,159
我们讲的都是经典设计思路

333
00:11:45,159 --> 00:11:47,600
我们这门课就是讲一点经典的

334
00:11:47,680 --> 00:11:48,320
这门课

335
00:11:48,680 --> 00:11:50,000
就不可能跟你讲最新的

336
00:11:50,000 --> 00:11:51,279
我都是跟你讲经典的

337
00:11:51,399 --> 00:11:52,600
就经典的就是说

338
00:11:52,600 --> 00:11:54,759
你至少大家用过都说好的

339
00:11:54,759 --> 00:11:55,360
就要经典

340
00:11:57,240 --> 00:11:58,159
Jobout为什么

341
00:11:58,159 --> 00:12:00,079
Jobout是用来做正则的

342
00:12:00,079 --> 00:12:00,879
大家可以这个问题

343
00:12:00,879 --> 00:12:02,319
大家的同学可以去看一下

344
00:12:02,319 --> 00:12:03,399
Jobout那一节

345
00:12:03,399 --> 00:12:04,559
他毕竟还有视频

346
00:12:04,759 --> 00:12:06,319
去看一下为什么Jobout

347
00:12:06,360 --> 00:12:08,000
Jobout就是说做正则项

348
00:12:11,279 --> 00:12:12,839
作为一个门外汉

349
00:12:12,839 --> 00:12:14,879
觉得CV的模型越来越少

350
00:12:14,879 --> 00:12:16,240
大家都在搞demo

351
00:12:16,240 --> 00:12:17,360
老师怎么看这件事情

352
00:12:17,480 --> 00:12:18,720
这个事情挺好玩的

353
00:12:18,720 --> 00:12:22,080
大家都在搞demo

354
00:12:22,080 --> 00:12:22,639
这是一件

355
00:12:22,639 --> 00:12:24,240
你觉得这是一件好事还是坏事

356
00:12:24,240 --> 00:12:25,680
我觉得是

357
00:12:26,080 --> 00:12:27,440
我觉得是一件

358
00:12:27,440 --> 00:12:29,040
首先这是一个历史必然性

359
00:12:29,080 --> 00:12:31,279
你的确实在卷积升级网络

360
00:12:31,279 --> 00:12:32,399
大家是有个瓶颈

361
00:12:32,639 --> 00:12:35,040
虽然大家的研究方向都做别的事

362
00:12:35,399 --> 00:12:38,320
大家不再去那么多做卷积升级网络

363
00:12:38,800 --> 00:12:40,279
应该是两三年前

364
00:12:40,279 --> 00:12:41,040
大家就不这么做了

365
00:12:41,399 --> 00:12:41,960
也还是有

366
00:12:41,960 --> 00:12:43,920
但是确实新工作出现的越来越少

367
00:12:43,920 --> 00:12:44,800
因为太难了

368
00:12:44,800 --> 00:12:46,200
就是基线太高

369
00:12:46,399 --> 00:12:47,800
所以大家就是说

370
00:12:47,800 --> 00:12:49,840
所以这一块已经进入了一个

371
00:12:49,840 --> 00:12:51,080
技术上已经成为

372
00:12:51,080 --> 00:12:52,080
我觉得是比较

373
00:12:52,080 --> 00:12:53,440
也还算成熟的领域

374
00:12:53,480 --> 00:12:55,160
所以成熟之后你要干嘛

375
00:12:56,360 --> 00:12:56,840
落地

376
00:12:58,320 --> 00:12:59,680
做应用

377
00:13:00,360 --> 00:13:01,000
做产业

378
00:13:01,120 --> 00:13:01,600
做demo

379
00:13:01,760 --> 00:13:02,360
做产品

380
00:13:02,640 --> 00:13:03,680
所以这是一件好事

381
00:13:03,680 --> 00:13:05,480
就说大家都在搞demo

382
00:13:05,480 --> 00:13:06,560
就是说都在搞产品

383
00:13:06,560 --> 00:13:08,600
是说你这个技术是能进入产品的

384
00:13:08,600 --> 00:13:09,600
比较成熟了

385
00:13:09,680 --> 00:13:11,240
所以确实这一块

386
00:13:11,240 --> 00:13:11,920
我觉得

387
00:13:13,720 --> 00:13:16,240
确实你现在如果还去做

388
00:13:16,440 --> 00:13:17,680
CN的设计的话

389
00:13:17,680 --> 00:13:18,680
会比较难一点

390
00:13:18,680 --> 00:13:19,360
我觉得

391
00:13:19,840 --> 00:13:21,520
但是反过来讲

392
00:13:21,520 --> 00:13:22,440
如果你就是

393
00:13:23,040 --> 00:13:23,880
都是在用了

394
00:13:23,880 --> 00:13:25,040
我觉得这是

395
00:13:25,600 --> 00:13:28,560
是一个历史的技术发展的一个趋势

396
00:13:29,000 --> 00:13:30,000
另外一块

397
00:13:30,720 --> 00:13:32,040
我觉得是一件好事情

398
00:13:32,040 --> 00:13:34,160
就说你要一个技术

399
00:13:34,160 --> 00:13:34,880
你需要落地

400
00:13:34,880 --> 00:13:36,320
你越落的地越多

401
00:13:36,320 --> 00:13:37,240
然后大家的

402
00:13:37,320 --> 00:13:38,640
就是说关注度越高

403
00:13:38,640 --> 00:13:40,080
所以你整个这一块的

404
00:13:40,080 --> 00:13:41,200
大家的兴趣越高

405
00:13:41,200 --> 00:13:42,800
所以这些人进来

406
00:13:42,800 --> 00:13:44,560
就公司进来投钱

407
00:13:44,680 --> 00:13:45,560
学校有钱

408
00:13:45,560 --> 00:13:47,640
然后支持大家做新的研究

409
00:13:48,160 --> 00:13:49,720
但如果发现大家

410
00:13:50,920 --> 00:13:51,920
不再那么关注了

411
00:13:51,920 --> 00:13:52,600
就没什么钱了

412
00:13:52,600 --> 00:13:53,040
没什么钱

413
00:13:53,040 --> 00:13:54,040
研究者就跑掉了

414
00:13:54,040 --> 00:13:54,800
那一块就

415
00:13:55,440 --> 00:13:56,960
你就是上课讲一讲了

416
00:13:57,160 --> 00:13:59,320
大家不会去真的做太多事情了

417
00:14:00,000 --> 00:14:00,560
OK

418
00:14:02,800 --> 00:14:04,200
网络是问题时期

419
00:14:04,200 --> 00:14:07,080
网络要求输入的大小是固定的

420
00:14:07,480 --> 00:14:09,480
实际图片不一定要求size

421
00:14:09,480 --> 00:14:10,560
如果强行resize

422
00:14:10,560 --> 00:14:12,280
会不会有效果差

423
00:14:16,000 --> 00:14:16,440
这个问题

424
00:14:18,360 --> 00:14:19,840
这个问题就是说

425
00:14:19,840 --> 00:14:20,840
我们之后会讲

426
00:14:20,840 --> 00:14:21,520
就是说

427
00:14:21,560 --> 00:14:22,680
你通常

428
00:14:22,800 --> 00:14:24,320
你比如说你这图片很大

429
00:14:24,320 --> 00:14:26,440
通常来说你的做法是说

430
00:14:27,680 --> 00:14:29,960
比如说我要224×224

431
00:14:29,960 --> 00:14:30,960
一般来说你图片

432
00:14:30,960 --> 00:14:33,480
当然你可能很随机了

433
00:14:33,520 --> 00:14:35,760
一般来说是你把短边

434
00:14:35,880 --> 00:14:36,720
给你

435
00:14:37,440 --> 00:14:39,240
resize到256

436
00:14:39,360 --> 00:14:40,520
长边我先不管

437
00:14:40,560 --> 00:14:42,440
然后再在里面

438
00:14:42,480 --> 00:14:43,920
我在中间抠出来

439
00:14:43,920 --> 00:14:45,120
224×224

440
00:14:45,120 --> 00:14:46,120
这一块做一侧

441
00:14:46,120 --> 00:14:47,440
这是一般的做法

442
00:14:47,520 --> 00:14:50,400
但你可以说我短边做到256之后

443
00:14:50,400 --> 00:14:52,320
我在里面随机的抠几个出来

444
00:14:52,320 --> 00:14:53,720
抠5个图出来

445
00:14:53,800 --> 00:14:54,880
同时去做预测

446
00:14:54,880 --> 00:14:56,160
然后来做一个

447
00:14:56,160 --> 00:14:57,400
最后做一个影响补

448
00:14:57,440 --> 00:14:59,640
所以强行resize

449
00:14:59,640 --> 00:15:01,520
就是说你不会强行resize

450
00:15:01,560 --> 00:15:02,960
就是说你会保持高宽比

451
00:15:02,960 --> 00:15:03,960
你不会成一个

452
00:15:03,960 --> 00:15:05,000
真的变成一个方的

453
00:15:05,000 --> 00:15:06,880
它会保证高宽比之后

454
00:15:06,880 --> 00:15:08,920
然后再在里面抠一块方的出来

455
00:15:09,240 --> 00:15:10,400
你可多抠两次

456
00:15:10,760 --> 00:15:12,600
所以这个效果其实不会那么变差

457
00:15:12,600 --> 00:15:13,560
没什么太多问题

458
00:15:13,560 --> 00:15:18,360
Alexander Nantai为什么要在全面

459
00:15:18,400 --> 00:15:20,160
最后面加全连接层

460
00:15:20,200 --> 00:15:23,560
这个是很好的问题

461
00:15:23,560 --> 00:15:24,520
我们之后会回答

462
00:15:25,040 --> 00:15:26,400
今天不说明天说

463
00:15:26,400 --> 00:15:31,200
Alexander为什么增加3×384的卷接层

464
00:15:31,200 --> 00:15:32,360
有什么道理吗

465
00:15:32,360 --> 00:15:33,080
为什么不增加

466
00:15:33,240 --> 00:15:37,160
这个东西这个问题也是模型设计

467
00:15:37,200 --> 00:15:38,120
就为什么这样

468
00:15:38,120 --> 00:15:39,080
为什么不那样

469
00:15:40,000 --> 00:15:44,280
大家可能试下来就是这样

470
00:15:44,280 --> 00:15:48,400
然后没有什么特别的原因

471
00:15:48,400 --> 00:15:52,160
就是说在我要为了数据调参调的好

472
00:15:52,160 --> 00:15:53,040
数据好

473
00:15:53,040 --> 00:15:53,960
我效果好

474
00:15:53,960 --> 00:15:54,960
我这么加了

475
00:15:55,000 --> 00:15:56,800
如果你觉得你有别的想法

476
00:15:56,800 --> 00:15:57,639
你可以去试一下

477
00:15:57,639 --> 00:15:58,520
就真的就这样子

478
00:15:58,520 --> 00:15:59,160
你让我解释

479
00:15:59,160 --> 00:16:00,840
可能我也解释不了

480
00:16:01,200 --> 00:16:02,680
我就算我是作者

481
00:16:02,680 --> 00:16:03,680
我可能也解释不了

482
00:16:03,680 --> 00:16:04,400
所以这个问题

483
00:16:04,520 --> 00:16:05,080
OK

