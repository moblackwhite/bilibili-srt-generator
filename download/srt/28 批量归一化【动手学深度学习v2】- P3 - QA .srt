1
00:00:00,000 --> 00:00:00,880
好

2
00:00:00,880 --> 00:00:05,440
接下来我们来看一下批量规划的从零开始实现

3
00:00:05,960 --> 00:00:09,240
我们就把这个层给你实现出来

4
00:00:10,679 --> 00:00:13,480
所以我们看到就是我们实现这个batch long

5
00:00:13,480 --> 00:00:15,240
它的名字叫batch normalization

6
00:00:15,960 --> 00:00:18,039
所以我们实现batch long这个函数

7
00:00:18,039 --> 00:00:19,679
现在它不是一个层

8
00:00:19,839 --> 00:00:21,839
我们先实现这个函数

9
00:00:21,839 --> 00:00:24,839
就跟我们定义从零开始实现卷积层一样的

10
00:00:24,839 --> 00:00:28,519
我们先实现相关2D的操作

11
00:00:28,719 --> 00:00:31,120
从而的话我们实现batch long的操作

12
00:00:31,120 --> 00:00:32,200
到底在干什么事情

13
00:00:33,679 --> 00:00:35,479
你可以看一下我们的输入

14
00:00:35,479 --> 00:00:39,159
输入是x就是我的这个层的输入

15
00:00:39,640 --> 00:00:41,719
它当然就输出了y就是我的输出了

16
00:00:42,399 --> 00:00:47,920
Gamma和Beta就是两个是可以学的一个parameter

17
00:00:48,799 --> 00:00:51,320
而moving mean和moving var

18
00:00:51,320 --> 00:00:54,399
这个东西其实说你可以认为它就是一个

19
00:00:54,399 --> 00:00:56,960
全局的一个均值和方差

20
00:00:57,960 --> 00:01:01,640
这个是在做inference做推理的时候用的

21
00:01:03,439 --> 00:01:04,239
你可以认为是在

22
00:01:04,239 --> 00:01:06,359
你可以近似认为是整个数据

23
00:01:06,359 --> 00:01:07,760
集上的一个均值和方差

24
00:01:07,760 --> 00:01:10,200
而不是小批量里面的均值和方差

25
00:01:11,120 --> 00:01:12,960
那么那个epsilon就是一个

26
00:01:12,960 --> 00:01:15,159
为了避免出零的一个东西了

27
00:01:15,920 --> 00:01:16,680
不要想它这个事情

28
00:01:16,799 --> 00:01:19,480
这个事情会给你带来很大很大的影响

29
00:01:19,480 --> 00:01:22,159
就是说如果你把这个东西换一下

30
00:01:22,159 --> 00:01:23,799
你会发现什么东西都不对了

31
00:01:24,280 --> 00:01:25,120
不是说不对了

32
00:01:25,120 --> 00:01:25,960
就不同了

33
00:01:26,920 --> 00:01:32,000
最后是个momentum

34
00:01:32,000 --> 00:01:36,280
momentum就是用来更新moving mean和moving var的一个东西

35
00:01:37,480 --> 00:01:41,600
通常这个东西是取0.9或者一个固定的数字

36
00:01:42,520 --> 00:01:44,200
epsilon也是通常有一个值的

37
00:01:44,440 --> 00:01:45,840
就是说大家不要去改这个值

38
00:01:48,160 --> 00:01:49,600
好 我们来看一下这个函数怎么干

39
00:01:51,360 --> 00:01:53,360
首先如果是做inference的情况

40
00:01:53,959 --> 00:01:56,959
就是torch is grad enabled

41
00:01:56,959 --> 00:01:57,920
就是说

42
00:01:58,439 --> 00:02:00,760
if not就是说你不要算t的

43
00:02:01,079 --> 00:02:02,519
不要算t的意思就是说

44
00:02:02,519 --> 00:02:04,239
你就是在做inference了

45
00:02:04,439 --> 00:02:06,039
就不要做这不是在做训练

46
00:02:06,760 --> 00:02:08,319
在做inference的情况下

47
00:02:08,319 --> 00:02:12,439
它就是用你的x减去你的整个数据

48
00:02:12,439 --> 00:02:14,280
集上那个全局的mean

49
00:02:15,199 --> 00:02:19,360
除以你这个全局的方差再加上epsilon

50
00:02:19,360 --> 00:02:25,280
这个就是做inference的时候

51
00:02:25,600 --> 00:02:26,400
就大家理解一下

52
00:02:26,400 --> 00:02:29,320
为什么你不能用批量的一个均值

53
00:02:29,640 --> 00:02:30,800
因为做inference的时候

54
00:02:30,800 --> 00:02:32,560
你可能你没有一个批量

55
00:02:32,560 --> 00:02:33,920
你可能就是一张图片进来

56
00:02:33,920 --> 00:02:34,600
那你怎么办

57
00:02:34,800 --> 00:02:35,680
图片还好一点

58
00:02:35,680 --> 00:02:36,960
如果你就一个样本进来

59
00:02:36,960 --> 00:02:38,120
你就算不出来一个东西

60
00:02:38,360 --> 00:02:38,760
对吧

61
00:02:39,000 --> 00:02:42,200
所以就是说这用一个全局的一个均值方差

62
00:02:43,880 --> 00:02:46,440
那么接下来就是来做训练了

63
00:02:47,439 --> 00:02:48,719
训练的话

64
00:02:48,719 --> 00:02:50,000
这里的话要看一下

65
00:02:50,000 --> 00:02:51,759
首先我的x的shape

66
00:02:52,319 --> 00:02:53,520
你要么是等于2

67
00:02:53,520 --> 00:02:55,439
2就是你是一个全联阶层

68
00:02:55,759 --> 00:02:57,599
等于4的话就是你的卷积层

69
00:02:58,039 --> 00:02:59,400
那如果你等于5等于6

70
00:02:59,400 --> 00:03:00,479
那就等于3的话

71
00:03:00,479 --> 00:03:01,800
那就是1D convolution

72
00:03:01,800 --> 00:03:03,000
3D convolution

73
00:03:03,000 --> 00:03:04,560
我们这就不支持了

74
00:03:04,560 --> 00:03:06,439
我们这是最简单的实现

75
00:03:06,439 --> 00:03:08,240
所以就要么就是一个全联阶层

76
00:03:08,240 --> 00:03:11,759
要么就是一个2D的卷积

77
00:03:13,719 --> 00:03:15,639
就如果你是一个二维的话

78
00:03:15,640 --> 00:03:17,560
那么你的第一位就是p量大小

79
00:03:17,560 --> 00:03:18,520
第二位就是你特征

80
00:03:18,960 --> 00:03:20,320
所以就是一个全联阶

81
00:03:20,760 --> 00:03:21,560
那么你的均值

82
00:03:21,560 --> 00:03:24,920
均值就是按照dimension等于0

83
00:03:24,920 --> 00:03:27,440
就是按照按行求均值

84
00:03:28,040 --> 00:03:30,160
就对每一列我算一个均值出来

85
00:03:30,240 --> 00:03:36,000
所以这个mean就是一个1乘1乘一个n的一个行向量

86
00:03:37,440 --> 00:03:38,240
同样道理

87
00:03:38,440 --> 00:03:40,360
就是我的x减去我的均值

88
00:03:40,360 --> 00:03:42,000
在平方在求命

89
00:03:42,200 --> 00:03:45,120
还是在按行的话

90
00:03:45,120 --> 00:03:48,280
那么就是我的方差也是一个行向量

91
00:03:49,560 --> 00:03:54,039
这就是所谓的按特征求均值和方差

92
00:03:56,159 --> 00:04:00,599
那么接下来就是我的2D卷积的情况

93
00:04:01,480 --> 00:04:02,520
可以给大家看一下

94
00:04:05,719 --> 00:04:09,159
就是说我们知道我们的第二个维度

95
00:04:09,280 --> 00:04:10,960
就是我的通道数

96
00:04:10,960 --> 00:04:12,400
就第一个维度

97
00:04:12,439 --> 00:04:13,319
或者定0位

98
00:04:13,319 --> 00:04:15,400
定等于0的时候

99
00:04:15,400 --> 00:04:17,079
就是我的p量大小

100
00:04:17,079 --> 00:04:19,720
1就是我的输出的通道

101
00:04:19,720 --> 00:04:20,800
输出通道

102
00:04:20,800 --> 00:04:23,199
2 3就是我的高和宽

103
00:04:24,280 --> 00:04:26,120
那么假设我是要做

104
00:04:26,160 --> 00:04:30,399
按照我的通道数来求均值的话

105
00:04:30,439 --> 00:04:32,160
就是说对每一个通道

106
00:04:32,240 --> 00:04:35,040
那么我要把里面所有的p量和所有的高宽

107
00:04:35,040 --> 00:04:37,040
里面的像素全部用来求均值

108
00:04:37,840 --> 00:04:38,360
OK

109
00:04:38,560 --> 00:04:40,800
所以它求出来就是一个

110
00:04:41,000 --> 00:04:45,000
1乘以n乘以1乘以1的一个4D的东西

111
00:04:45,040 --> 00:04:48,160
因为我们keep定等于true了

112
00:04:49,720 --> 00:04:50,639
同样道理

113
00:04:50,639 --> 00:04:53,319
我的方差也是一减一求命

114
00:04:53,319 --> 00:04:59,920
然后还是对我的p量维度和我的高和宽

115
00:04:59,920 --> 00:05:00,920
做的事情

116
00:05:00,960 --> 00:05:03,800
所以它一样的是一个1乘以n乘以1乘以1的

117
00:05:03,800 --> 00:05:05,240
一个4D的东西

118
00:05:06,720 --> 00:05:07,040
好

119
00:05:07,040 --> 00:05:08,319
接下来我们就是

120
00:05:08,640 --> 00:05:12,040
这个mean和这个var都是当前x里面求出来的

121
00:05:12,040 --> 00:05:13,000
均值和方差

122
00:05:13,360 --> 00:05:15,600
我们就把它一减

123
00:05:15,600 --> 00:05:17,280
就把它的x一减去mean

124
00:05:17,280 --> 00:05:21,000
然后除以它的方差

125
00:05:21,000 --> 00:05:22,000
它是方差平方

126
00:05:22,000 --> 00:05:23,200
因为我们没有开根号

127
00:05:23,240 --> 00:05:26,360
加上我的epsilon再开根号

128
00:05:27,120 --> 00:05:29,439
就是说这个是我训练时候干的事情

129
00:05:29,640 --> 00:05:32,240
这个是我的做推理的时候干的事情

130
00:05:32,240 --> 00:05:33,960
推理是用了全剧的mean

131
00:05:34,079 --> 00:05:37,599
训练是用当前的小p量里面的mean和方差

132
00:05:38,599 --> 00:05:42,399
然后我会更新一下我的moving mean和moving var

133
00:05:42,399 --> 00:05:43,399
它就是一个

134
00:05:44,560 --> 00:05:45,879
这个东西在

135
00:05:46,560 --> 00:05:50,679
我觉得在统计信号处里面常用的

136
00:05:50,679 --> 00:05:53,719
就是说我要算一个均值怎么算

137
00:05:53,719 --> 00:05:55,639
这个均值我又当前

138
00:05:55,679 --> 00:05:59,199
当前我又并不知道你到底有多少个

139
00:06:00,199 --> 00:06:04,120
那么我每一次就当前的均值

140
00:06:04,120 --> 00:06:05,920
全剧的均值乘以一个

141
00:06:05,920 --> 00:06:08,199
比如说0.9或者0.1

142
00:06:08,199 --> 00:06:09,120
我一般0.9

143
00:06:09,120 --> 00:06:12,360
再加上一个1.0减去0.9

144
00:06:12,360 --> 00:06:16,279
就是0.1乘以我的当前的算的均值

145
00:06:16,839 --> 00:06:19,159
就是一个moving的一个smooth的一个版本

146
00:06:20,800 --> 00:06:22,439
它就是一个这么滑动过去

147
00:06:22,439 --> 00:06:25,759
最后你的它会去无限的逼近你

148
00:06:26,159 --> 00:06:29,120
真实的在数据以上的一个均值和方差

149
00:06:29,120 --> 00:06:31,439
所以这个是一个全剧的用来做更新用的

150
00:06:32,039 --> 00:06:33,480
当在做inference的时候

151
00:06:33,480 --> 00:06:34,560
我就不会去更新它了

152
00:06:34,560 --> 00:06:36,159
只要在训练的时候会去更新它

153
00:06:37,360 --> 00:06:41,000
那么接下来就是说这一步还到目前为止

154
00:06:41,000 --> 00:06:42,920
我们唯一干的事情就是把你的

155
00:06:43,199 --> 00:06:45,199
给定的x减去你的均值

156
00:06:45,199 --> 00:06:46,240
除以你的方差

157
00:06:47,000 --> 00:06:47,959
那么最后的话

158
00:06:48,079 --> 00:06:50,639
我就是要把我的gamma一乘

159
00:06:50,680 --> 00:06:52,159
再加上我的beta

160
00:06:52,160 --> 00:06:55,680
我返回的就是我的

161
00:06:55,920 --> 00:06:58,240
对x做个变化的y

162
00:06:58,240 --> 00:06:59,640
所以它是一个线性变换

163
00:07:00,400 --> 00:07:03,200
和我的moving的min和moving的y

164
00:07:03,520 --> 00:07:06,480
那么如果它是一个torch的parameter的话

165
00:07:06,480 --> 00:07:07,880
那么我会返回它的data

166
00:07:07,880 --> 00:07:09,760
就是对应的tensor

167
00:07:09,760 --> 00:07:10,840
它的t度不重要

168
00:07:10,840 --> 00:07:12,280
因为我们不要对它算t度

169
00:07:13,120 --> 00:07:13,400
OK

170
00:07:13,400 --> 00:07:16,879
所以这个就是核心的batch long在干的事情

171
00:07:19,040 --> 00:07:20,360
就是说我们这里实现的

172
00:07:20,360 --> 00:07:22,480
就是我们刚刚所讲过的公式

173
00:07:24,160 --> 00:07:24,639
OK

174
00:07:25,720 --> 00:07:27,080
那么接下来就是说

175
00:07:27,120 --> 00:07:29,800
我们定一个batch long的一个层

176
00:07:30,319 --> 00:07:33,199
就是我们计成module来定一个batch long的层

177
00:07:33,199 --> 00:07:34,319
所以我们之后能用

178
00:07:35,319 --> 00:07:36,680
那么你要

179
00:07:37,120 --> 00:07:38,720
数的参数是说你的

180
00:07:39,080 --> 00:07:43,199
feature的个数和你的number of dimensions

181
00:07:44,160 --> 00:07:45,960
我们知道我们number of dimensions

182
00:07:45,960 --> 00:07:47,560
就是要么就2要么就4

183
00:07:48,120 --> 00:07:49,560
所以如果你是等于2的时候

184
00:07:49,759 --> 00:07:50,519
我的shape

185
00:07:50,800 --> 00:07:53,000
我所有这些东西的shape都是1乘以n

186
00:07:53,000 --> 00:07:54,280
就是number of features

187
00:07:55,160 --> 00:07:57,639
否则的话就是1乘以n乘以1乘以1

188
00:07:58,160 --> 00:07:59,240
为什么要定义这个东西

189
00:07:59,240 --> 00:08:00,959
是因为我的gamma也好

190
00:08:01,680 --> 00:08:02,720
我的beta也好

191
00:08:02,720 --> 00:08:03,879
我的moving min也好

192
00:08:03,879 --> 00:08:05,000
我的moving y也好

193
00:08:05,000 --> 00:08:06,600
它都是长成这个shape

194
00:08:07,720 --> 00:08:08,759
特别是我的gamma

195
00:08:08,759 --> 00:08:09,560
这也是等于1的

196
00:08:09,560 --> 00:08:10,160
全1的

197
00:08:10,439 --> 00:08:11,360
因为你不能等于0

198
00:08:11,519 --> 00:08:12,959
因为你如果等于0的话

199
00:08:12,959 --> 00:08:14,600
gamma一乘以那个东西

200
00:08:14,720 --> 00:08:15,720
就全部为0了

201
00:08:15,720 --> 00:08:16,800
就根本训练不动了

202
00:08:17,520 --> 00:08:20,240
所以gamma就是我要去拟合的方程

203
00:08:20,240 --> 00:08:22,120
我的beta是要去拟合的均值

204
00:08:22,120 --> 00:08:24,639
所以它的初始值是0

205
00:08:25,480 --> 00:08:26,240
同样的话

206
00:08:26,240 --> 00:08:27,720
moving min它也是0

207
00:08:27,800 --> 00:08:29,360
moving y也是1

208
00:08:29,920 --> 00:08:30,319
OK

209
00:08:30,319 --> 00:08:32,000
所以这个是我要

210
00:08:32,000 --> 00:08:34,519
整个层要维护的4个东西

211
00:08:35,000 --> 00:08:37,240
这里gamma和beta是要被迭代的

212
00:08:37,600 --> 00:08:40,600
moving min和moving y是不要被迭代的

213
00:08:42,000 --> 00:08:43,960
所以就是说gamma和beta

214
00:08:43,960 --> 00:08:45,840
放在一个n的parameter里面

215
00:08:45,879 --> 00:08:47,280
beta就没放在里面了

216
00:08:49,080 --> 00:08:49,519
好

217
00:08:49,519 --> 00:08:50,759
接下来就是说

218
00:08:52,360 --> 00:08:54,000
就是说因为我的moving min

219
00:08:54,000 --> 00:08:55,440
没有放在parameter里面

220
00:08:55,840 --> 00:08:57,240
所以导致说

221
00:08:57,440 --> 00:08:59,680
我得去自己去算你device

222
00:08:59,879 --> 00:09:00,360
不然的话

223
00:09:00,360 --> 00:09:01,840
你touch的时候

224
00:09:01,840 --> 00:09:02,600
你点2的时候

225
00:09:02,600 --> 00:09:03,800
它直接把你挪过去了

226
00:09:04,120 --> 00:09:04,960
所以这个地方就是说

227
00:09:04,960 --> 00:09:05,720
我得看一下

228
00:09:05,720 --> 00:09:07,000
如果我的device

229
00:09:07,200 --> 00:09:09,160
不是等于我的输入的device的话

230
00:09:09,320 --> 00:09:11,120
我就把moving min和moving y

231
00:09:11,120 --> 00:09:12,639
得挪到我的device上面

232
00:09:12,639 --> 00:09:13,879
就挪到我的GPU上面

233
00:09:13,879 --> 00:09:15,000
就这两行干的事情

234
00:09:16,600 --> 00:09:18,080
那么接下来就是说

235
00:09:18,440 --> 00:09:19,000
你可以看到

236
00:09:19,000 --> 00:09:20,840
我们调用刚刚那个函数

237
00:09:21,879 --> 00:09:23,240
就是调用我们的batch long

238
00:09:23,600 --> 00:09:25,680
我把我的输入x拿进来

239
00:09:26,000 --> 00:09:27,120
我把gamma拉进去

240
00:09:27,120 --> 00:09:27,720
beta拉进去

241
00:09:27,720 --> 00:09:29,480
moving min和moving y拉进去

242
00:09:29,759 --> 00:09:31,480
那epsilon我们是取了e-

243
00:09:31,720 --> 00:09:33,639
这也是一个常见的设定

244
00:09:34,200 --> 00:09:35,680
然后大家应该是

245
00:09:37,160 --> 00:09:38,879
就是说你如果要做

246
00:09:38,879 --> 00:09:40,360
在不同的框架之间

247
00:09:40,360 --> 00:09:41,080
做前移的时候

248
00:09:41,080 --> 00:09:42,240
大家一定要注意说

249
00:09:42,240 --> 00:09:43,080
这个epsilon

250
00:09:43,080 --> 00:09:44,840
大家每个框架设的是多少

251
00:09:45,240 --> 00:09:45,840
同样的话

252
00:09:45,840 --> 00:09:47,360
momentum我们设的是

253
00:09:47,920 --> 00:09:48,879
正常的0.9

254
00:09:48,879 --> 00:09:50,160
就大家一般使用0.9

255
00:09:50,440 --> 00:09:51,759
就在epsilon这个事情

256
00:09:52,320 --> 00:09:54,040
我记得是有人用1-

257
00:09:54,040 --> 00:09:55,680
1-6 1-7都有

258
00:09:56,000 --> 00:09:56,600
所以这个东西

259
00:09:56,600 --> 00:09:57,960
会给你造成一定的影响

260
00:09:59,000 --> 00:09:59,560
OK

261
00:09:59,600 --> 00:10:01,120
所以这个就是

262
00:10:01,160 --> 00:10:02,759
我的batch normalization

263
00:10:02,759 --> 00:10:03,920
这个layer怎么做了

264
00:10:04,560 --> 00:10:07,000
就是他整个实验就在这个地方了

265
00:10:08,360 --> 00:10:09,360
那么接下来我们看一下

266
00:10:09,360 --> 00:10:10,400
我们怎么用它

267
00:10:11,680 --> 00:10:12,759
我们怎么用它

268
00:10:12,759 --> 00:10:13,639
我们这个

269
00:10:14,159 --> 00:10:16,240
我就把这个

270
00:10:18,519 --> 00:10:20,000
这么好看一点点

271
00:10:21,840 --> 00:10:24,039
首先我们构造一个learnnet

272
00:10:24,039 --> 00:10:24,879
就是说我们看看

273
00:10:24,879 --> 00:10:26,600
它怎么用到learnnet上面

274
00:10:27,240 --> 00:10:27,919
这最简单

275
00:10:28,799 --> 00:10:29,879
首先learnnet就是说

276
00:10:29,879 --> 00:10:31,759
我们就把learnnet抄过来

277
00:10:31,879 --> 00:10:32,799
就唯一的不一样的

278
00:10:32,799 --> 00:10:33,480
是说你看一下

279
00:10:33,480 --> 00:10:34,639
我们加入了一些batch normalization

280
00:10:34,639 --> 00:10:35,120
在这里

281
00:10:35,399 --> 00:10:36,799
我们在第一个卷迹后面

282
00:10:36,799 --> 00:10:37,639
加了个batch normalization

283
00:10:38,679 --> 00:10:40,120
那就是我告诉你说

284
00:10:40,120 --> 00:10:44,120
你的输入的是通道数

285
00:10:44,120 --> 00:10:47,200
你的要给定

286
00:10:47,200 --> 00:10:49,320
就是说因为4D

287
00:10:49,320 --> 00:10:50,080
我是一个卷迹

288
00:10:50,240 --> 00:10:51,440
所以我dmg等于4

289
00:10:51,919 --> 00:10:53,799
我的feature数就是我的通道数

290
00:10:53,799 --> 00:10:55,679
就是他的输出通道是等于6

291
00:10:57,240 --> 00:10:57,960
同样道理说

292
00:10:57,960 --> 00:10:58,759
我第二个卷迹

293
00:10:58,759 --> 00:11:00,120
在后面也加一个batch normalization

294
00:11:00,120 --> 00:11:00,840
加在这个地方

295
00:11:02,080 --> 00:11:04,200
然后我在我的

296
00:11:04,200 --> 00:11:06,639
记得这里是加在我的sigmoid的前面

297
00:11:06,639 --> 00:11:08,360
不是加在我的sigmoid的后面

298
00:11:09,360 --> 00:11:11,919
接下来就是说我在我的线性层

299
00:11:13,680 --> 00:11:14,279
同样的话

300
00:11:14,279 --> 00:11:15,320
我加一个batch normalization

301
00:11:15,320 --> 00:11:16,159
就是说线性层

302
00:11:16,159 --> 00:11:17,159
它的输出是120

303
00:11:17,159 --> 00:11:19,000
所以它的输入的dmg是120

304
00:11:19,440 --> 00:11:20,560
dmg等于2了

305
00:11:20,560 --> 00:11:21,759
就number of dmg是2了

306
00:11:21,759 --> 00:11:22,960
因为它是一个2D的矩阵

307
00:11:23,480 --> 00:11:24,759
我们前面flatten过了

308
00:11:24,759 --> 00:11:27,240
最后我们还有一个线性层

309
00:11:27,519 --> 00:11:29,120
就也加了个batch normalization

310
00:11:29,120 --> 00:11:29,639
在这个地方

311
00:11:31,279 --> 00:11:33,000
注意我们最后一个层就不用加了

312
00:11:33,000 --> 00:11:35,920
最后一个层是我们的输出层

313
00:11:35,920 --> 00:11:37,680
输出层我们就不加东西了

314
00:11:37,680 --> 00:11:40,720
就不对输出做一个线性变换了

315
00:11:41,280 --> 00:11:44,200
因为你其实做不做都没什么太多关系

316
00:11:44,200 --> 00:11:45,400
最后做max一弄

317
00:11:45,400 --> 00:11:46,560
我就把你normalize掉了

318
00:11:46,680 --> 00:11:48,200
所以加不加都没问题

319
00:11:49,200 --> 00:11:49,480
好

320
00:11:49,480 --> 00:11:52,400
这就是我们如果用在learnlet上会怎么样

321
00:11:53,480 --> 00:11:54,920
然后当然我们可以训练一下了

322
00:11:54,920 --> 00:11:55,640
就训练一下

323
00:11:55,640 --> 00:11:58,000
就是说可以看到是说

324
00:11:58,440 --> 00:11:59,880
我们在了

325
00:12:00,040 --> 00:12:01,680
我们在看之前

326
00:12:01,680 --> 00:12:02,600
我们干脆就比一下

327
00:12:02,600 --> 00:12:03,960
我们之前learnlet是怎么做的

328
00:12:03,960 --> 00:12:07,560
我们干脆去比一下

329
00:12:07,560 --> 00:12:08,280
我们的

330
00:12:09,320 --> 00:12:12,720
之前我们有讲过我们的learnlet

331
00:12:13,240 --> 00:12:14,920
是怎么样做出来的

332
00:12:22,680 --> 00:12:23,920
我们跳到最后

333
00:12:25,840 --> 00:12:27,440
就之前我们用的是

334
00:12:27,440 --> 00:12:31,000
可以看到是0.8

335
00:12:31,519 --> 00:12:32,399
就是我们

336
00:12:32,919 --> 00:12:35,799
我们其实迭代的没有那么的好

337
00:12:35,799 --> 00:12:37,639
因为我们迭代和10轮

338
00:12:37,639 --> 00:12:39,559
其实learnlet都没怎么收敛

339
00:12:39,799 --> 00:12:41,440
所以你可以看到这learnlet的效果

340
00:12:41,799 --> 00:12:43,759
然后看一下我们现在的效果

341
00:12:43,759 --> 00:12:44,440
就是说

342
00:12:45,480 --> 00:12:47,720
可以看到是他的收敛

343
00:12:47,720 --> 00:12:49,279
可以认为感觉上更好一点

344
00:12:49,279 --> 00:12:49,559
对吧

345
00:12:49,559 --> 00:12:51,720
你看loss降的更低一点

346
00:12:51,720 --> 00:12:53,399
我们loss是0.245

347
00:12:53,600 --> 00:12:55,639
我们这里的区别其实区别不大

348
00:12:55,639 --> 00:12:57,120
我们就是number of epoch是一样的

349
00:12:57,120 --> 00:12:58,720
但是我们learning rate稍微大了一点点

350
00:12:58,720 --> 00:13:00,519
然后从0.9到1.0

351
00:13:00,679 --> 00:13:02,000
当然你可以试一下2.0

352
00:13:02,159 --> 00:13:04,000
2.0我猜也可能没问题

353
00:13:04,639 --> 00:13:06,840
然后我们的loss是0.245

354
00:13:06,840 --> 00:13:08,000
我们之前是多少

355
00:13:09,159 --> 00:13:10,960
之前是0.473

356
00:13:11,159 --> 00:13:13,159
可以看到我们的loss降的还是比较多的

357
00:13:13,799 --> 00:13:15,960
然后我的training accuracy之前是0.8

358
00:13:15,960 --> 00:13:18,679
现在我们变成了0.88

359
00:13:19,720 --> 00:13:20,840
就意味着说

360
00:13:21,519 --> 00:13:22,840
其实你learnlet

361
00:13:22,840 --> 00:13:25,799
你可能再多迭代个二三十轮

362
00:13:25,799 --> 00:13:27,279
可能一样的可以到0.88

363
00:13:27,279 --> 00:13:28,960
这是我们第一个发现

364
00:13:29,160 --> 00:13:30,560
第二个发现确实是说

365
00:13:30,560 --> 00:13:32,160
用了batch normalization之后

366
00:13:32,160 --> 00:13:34,040
你的收敛感觉是变快了

367
00:13:34,720 --> 00:13:36,600
但我们不是做严格的对比

368
00:13:36,600 --> 00:13:39,680
就是说很多时候你也没有那么多时间

369
00:13:39,680 --> 00:13:41,720
或者那么多钱去做严格的对比

370
00:13:41,720 --> 00:13:43,160
就是说很多时候你就觉得

371
00:13:43,720 --> 00:13:45,480
感觉还不可以就行了

372
00:13:45,879 --> 00:13:48,120
所以可以看到这是有效果的

373
00:13:48,120 --> 00:13:51,160
就是主要是降的更快了

374
00:13:53,120 --> 00:13:54,680
然后大家可以去试一下

375
00:13:54,680 --> 00:13:56,320
就是说两个模型

376
00:13:56,320 --> 00:13:58,000
我都给你足够多的data

377
00:13:58,000 --> 00:13:58,960
epoch的时候

378
00:13:58,960 --> 00:14:00,920
看看是不是大家的accuracy是不是一样

379
00:14:00,920 --> 00:14:03,160
应该是算下来应该是差不多的

380
00:14:03,160 --> 00:14:04,280
就不会有太多变化

381
00:14:05,360 --> 00:14:06,960
所以这个就是说我们从零开始

382
00:14:06,960 --> 00:14:07,800
实现会怎么样子

383
00:14:08,920 --> 00:14:09,879
接下来我们看一下

384
00:14:09,879 --> 00:14:14,080
就是说我们要从直接掉包的会怎么样

385
00:14:14,080 --> 00:14:15,320
但掉包之前我们看一下

386
00:14:15,320 --> 00:14:17,000
我们就看一下我们的

387
00:14:17,000 --> 00:14:18,120
gamma学出来什么样子

388
00:14:18,120 --> 00:14:22,360
我们把net E的gamma

389
00:14:22,360 --> 00:14:25,080
就是说就第一个batch normalization的

390
00:14:25,280 --> 00:14:26,879
gamma拿出来看一眼

391
00:14:26,879 --> 00:14:30,639
就是你可认为就是因为我们的就是6个通道

392
00:14:30,800 --> 00:14:34,840
就是说你学出来方差和学出来均值

393
00:14:34,840 --> 00:14:36,400
就是长成这个样子的

394
00:14:36,840 --> 00:14:37,879
但你看不出什么意义

395
00:14:37,879 --> 00:14:39,040
就是说就给大家看一下

396
00:14:39,040 --> 00:14:42,400
就是你实际上你也不知道他到底怎么回事

397
00:14:43,560 --> 00:14:44,560
但你可以去比较一下

398
00:14:44,560 --> 00:14:47,680
就是说可能特别很深的时候

399
00:14:48,200 --> 00:14:48,800
很深的时候

400
00:14:48,800 --> 00:14:50,960
你看一下最底的层和最上的层

401
00:14:50,960 --> 00:14:52,320
他的是不是长的一样

402
00:14:55,280 --> 00:14:56,000
然后我们看一下

403
00:14:56,000 --> 00:14:58,680
就是说如果你是想掉包实现的话

404
00:14:58,680 --> 00:15:00,200
其实也是挺简单的

405
00:15:03,080 --> 00:15:03,879
就掉包的话

406
00:15:03,879 --> 00:15:06,320
那就是要的是n module里面

407
00:15:06,320 --> 00:15:08,920
那个n到batch long 2d

408
00:15:09,720 --> 00:15:10,360
那么一样的

409
00:15:10,360 --> 00:15:13,560
你得给定你的输入的feature数

410
00:15:13,560 --> 00:15:16,280
就是说对他来对卷迹来讲

411
00:15:16,280 --> 00:15:17,360
就是输出通道数

412
00:15:17,360 --> 00:15:19,840
对于线性层来讲

413
00:15:19,840 --> 00:15:21,720
也就是这个东西是等于

414
00:15:21,720 --> 00:15:22,879
我回去回个车

415
00:15:23,200 --> 00:15:25,600
就等于他的输出的大小

416
00:15:26,919 --> 00:15:28,960
所以但这里跟我们之前不同

417
00:15:28,960 --> 00:15:31,240
你不需要去指定你的维度是多少了

418
00:15:31,720 --> 00:15:33,200
就是说说白了

419
00:15:33,200 --> 00:15:34,879
他其实他的

420
00:15:36,000 --> 00:15:38,480
他会之后会Pytorch内部

421
00:15:38,480 --> 00:15:40,080
会帮你去加合适的维度

422
00:15:40,080 --> 00:15:42,480
就是说我们比我们之前会少一个参数

423
00:15:42,960 --> 00:15:44,279
当然他不是很好的时候

424
00:15:44,279 --> 00:15:45,519
他还得指定这个参数

425
00:15:45,519 --> 00:15:47,960
也是挺不方便的

426
00:15:47,960 --> 00:15:48,919
但是没办法

427
00:15:48,919 --> 00:15:51,879
就是这种Pytorch

428
00:15:51,879 --> 00:15:54,639
就是得就享受别的好处的时候

429
00:15:54,639 --> 00:15:57,399
你这个不好处得承受一下

430
00:15:57,399 --> 00:15:58,240
OK

431
00:15:58,240 --> 00:15:59,320
所以就是说

432
00:16:00,200 --> 00:16:02,279
这里是说跟前面那个nullat

433
00:16:02,279 --> 00:16:04,120
我调用n module会怎么样

434
00:16:04,120 --> 00:16:05,159
跟之前没本事区别

435
00:16:05,159 --> 00:16:05,559
对吧

436
00:16:06,840 --> 00:16:08,080
让你重新训练一下

437
00:16:09,879 --> 00:16:10,840
训练一下

438
00:16:10,840 --> 00:16:13,439
你可以看到是loss也差不多

439
00:16:13,439 --> 00:16:15,000
最后到0.244

440
00:16:15,000 --> 00:16:16,159
跟我们之前的差不多

441
00:16:16,159 --> 00:16:18,799
精度这个精度会低一点

442
00:16:19,080 --> 00:16:20,120
而这个精度低一点

443
00:16:20,480 --> 00:16:24,919
我觉得可能重复几次实验

444
00:16:25,080 --> 00:16:25,960
就是不那么稳定

445
00:16:26,080 --> 00:16:30,200
这个小网络小数据可能不那么稳定

446
00:16:31,200 --> 00:16:32,360
但我觉得本质上来说

447
00:16:32,360 --> 00:16:34,679
应该是跟之前精度是差不多了

448
00:16:35,159 --> 00:16:35,679
OK

449
00:16:36,639 --> 00:16:42,919
所以这就是我们对于批量规划的层

450
00:16:42,919 --> 00:16:44,039
它的实现

451
00:16:44,519 --> 00:16:47,840
所以基本上就是根据我们前面所讲的

452
00:16:48,160 --> 00:16:51,240
说白了就是找到合适的维度

453
00:16:51,240 --> 00:16:52,480
做均值做方差

454
00:16:52,480 --> 00:16:54,240
然后减均值除方差

455
00:16:54,240 --> 00:16:56,200
再乘以你的beta加乘以你的

456
00:16:56,200 --> 00:16:57,480
伽玛加上你的beta

457
00:16:58,120 --> 00:16:59,879
然后你的伽玛和beta

458
00:16:59,920 --> 00:17:01,240
它是会被训练的

459
00:17:01,360 --> 00:17:03,800
就是在你给定的能力rate上面

460
00:17:03,800 --> 00:17:06,720
作为跟正常的群众一样参与训练

461
00:17:07,279 --> 00:17:07,759
OK

462
00:17:07,759 --> 00:17:10,079
这就是批量规划

463
00:17:10,079 --> 00:17:11,360
我们就讲到这里

