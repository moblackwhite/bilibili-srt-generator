1
00:00:00,000 --> 00:00:02,359
好我们来回答一下问题

2
00:00:03,120 --> 00:00:05,160
最近Keras从TF分离

3
00:00:05,200 --> 00:00:07,240
数据会不要重新整理

4
00:00:07,240 --> 00:00:08,199
然后要TensorFlow

5
00:00:11,560 --> 00:00:15,439
TensorFlow我们一开始有写1.0

6
00:00:15,439 --> 00:00:16,600
后来2.0出来了

7
00:00:16,600 --> 00:00:18,719
我就感觉整个都不一样了

8
00:00:18,960 --> 00:00:19,879
整个都不一样了

9
00:00:19,879 --> 00:00:21,839
感觉都不认识了

10
00:00:21,839 --> 00:00:23,480
但整体来讲我喜欢2.0

11
00:00:23,480 --> 00:00:25,560
我觉得2.0比1.0好用

12
00:00:25,600 --> 00:00:28,160
就一点几2.0比1.7好用很多

13
00:00:28,640 --> 00:00:31,600
进Priority那种一个Mode更好用

14
00:00:31,839 --> 00:00:34,640
所以我们就我就写信给Jeff Dean

15
00:00:35,000 --> 00:00:37,960
说你们改了我都不认得了

16
00:00:37,960 --> 00:00:39,960
你能不能派一个人来帮忙

17
00:00:39,960 --> 00:00:41,159
Jeff Dean说可以

18
00:00:41,159 --> 00:00:42,960
我派一个Engineer

19
00:00:42,960 --> 00:00:44,519
来帮你们看一下这个事情

20
00:00:44,920 --> 00:00:46,359
所以我们其实我们的翻译

21
00:00:46,359 --> 00:00:47,600
是有Google的员工

22
00:00:47,600 --> 00:00:49,079
帮我们做了很多事情

23
00:00:50,000 --> 00:00:51,480
但具体到这个问题来说

24
00:00:51,480 --> 00:00:53,840
我觉得Keras从TF分离

25
00:00:53,840 --> 00:00:56,480
并不说TF不再支持Keras

26
00:00:56,519 --> 00:00:57,679
我觉得也不是

27
00:00:57,679 --> 00:00:59,679
是说Keras说

28
00:01:00,000 --> 00:01:02,679
我们把Code搬出来

29
00:01:02,679 --> 00:01:04,000
还是搬回原来地方

30
00:01:04,000 --> 00:01:05,239
这样子开发方便

31
00:01:05,239 --> 00:01:06,319
你编译

32
00:01:06,319 --> 00:01:07,280
你每次改一个东西

33
00:01:07,280 --> 00:01:08,280
你不需要去编译

34
00:01:08,280 --> 00:01:09,760
触发编译整个TensorFlow

35
00:01:09,760 --> 00:01:11,280
因为Keras它就是一个Python

36
00:01:11,280 --> 00:01:11,680
Library

37
00:01:11,680 --> 00:01:12,960
它又不管下面东西

38
00:01:13,120 --> 00:01:14,640
下面东西都是基于别人的

39
00:01:14,640 --> 00:01:15,799
以前是基于Ciano

40
00:01:15,799 --> 00:01:16,879
现在基于TensorFlow

41
00:01:17,000 --> 00:01:18,319
所以对他来讲

42
00:01:18,319 --> 00:01:20,200
确实我就改一点Python的东西

43
00:01:20,239 --> 00:01:22,079
我要把整个TensorFlow编译一下

44
00:01:22,240 --> 00:01:23,799
怎么是太浪费了

45
00:01:24,480 --> 00:01:25,879
所以我觉得这个是

46
00:01:25,920 --> 00:01:28,519
很reasonable的一个东西

47
00:01:29,359 --> 00:01:29,959
另外一块

48
00:01:29,959 --> 00:01:32,200
但我觉得应该是不会影响到用户

49
00:01:32,200 --> 00:01:33,799
就是说等于是说

50
00:01:33,799 --> 00:01:35,759
Keras那边的新的东西

51
00:01:35,759 --> 00:01:37,759
会看

52
00:01:37,959 --> 00:01:38,920
就完美情况下

53
00:01:38,920 --> 00:01:40,319
是不会影响用户的

54
00:01:40,359 --> 00:01:41,719
但是这个东西不好说

55
00:01:41,759 --> 00:01:43,920
因为你搬出去之后

56
00:01:43,920 --> 00:01:45,560
很容易就是说

57
00:01:45,599 --> 00:01:46,679
Keras那边用户说

58
00:01:46,679 --> 00:01:47,759
我要做那样

59
00:01:47,759 --> 00:01:48,319
TensorFlow说

60
00:01:48,759 --> 00:01:50,400
TensorFlow这边觉得不行

61
00:01:50,439 --> 00:01:52,759
如果他们之间协调不够好的话

62
00:01:52,760 --> 00:01:55,359
那么可能TensorFlow的Keras版本

63
00:01:55,359 --> 00:01:57,120
和Keras那边自己的开发版本

64
00:01:57,120 --> 00:01:58,120
可能会有

65
00:01:59,359 --> 00:02:00,439
有diverge

66
00:02:00,439 --> 00:02:01,880
就是说不一样的情况

67
00:02:02,160 --> 00:02:04,240
这样子会给大家带来问题

68
00:02:04,280 --> 00:02:07,040
但我觉得这个事情是能解决的

69
00:02:07,480 --> 00:02:10,200
我相信他们应该是没问题的

70
00:02:11,640 --> 00:02:14,200
所以我们暂时是不会有影响的

71
00:02:16,159 --> 00:02:16,800
问题二

72
00:02:16,800 --> 00:02:20,360
能否用bias权等于0

73
00:02:20,480 --> 00:02:21,800
weight权大于0

74
00:02:21,800 --> 00:02:24,320
作为参数的约束条件

75
00:02:24,440 --> 00:02:26,360
训练权叫网络

76
00:02:27,440 --> 00:02:28,920
其实我不是特别理解

77
00:02:29,080 --> 00:02:30,240
bias权等于0

78
00:02:30,240 --> 00:02:31,200
weight权大于0

79
00:02:31,200 --> 00:02:33,200
作为约束条件

80
00:02:33,440 --> 00:02:34,400
就是说

81
00:02:34,600 --> 00:02:35,320
那么

82
00:02:35,760 --> 00:02:37,240
你的weight权大于0

83
00:02:37,280 --> 00:02:38,720
那就比较尴尬了

84
00:02:38,760 --> 00:02:39,600
你的

85
00:02:40,880 --> 00:02:42,120
假设你是神经网

86
00:02:42,160 --> 00:02:42,840
你是个

87
00:02:42,840 --> 00:02:44,320
你输入是个图片的话

88
00:02:44,320 --> 00:02:47,000
图片你输入就是个正的

89
00:02:47,280 --> 00:02:48,600
你bias等于0

90
00:02:48,640 --> 00:02:49,640
你weight权大于0

91
00:02:49,640 --> 00:02:52,240
那么你所谓的材质都是一个正的

92
00:02:53,560 --> 00:02:55,000
那么你就没有负值了

93
00:02:55,000 --> 00:02:56,280
我觉得可能会奇怪

94
00:02:57,640 --> 00:02:59,080
权链阶你也很奇怪的

95
00:02:59,240 --> 00:02:59,960
就是说

96
00:03:00,000 --> 00:03:01,120
但我就不是很理解

97
00:03:01,120 --> 00:03:02,640
我觉得可能你可以试试

98
00:03:02,640 --> 00:03:04,200
看听上去挺奇怪的

99
00:03:06,520 --> 00:03:08,080
在resnet的卷积层中

100
00:03:08,080 --> 00:03:11,200
能否替换成MLP来实现一个很深的

101
00:03:11,200 --> 00:03:12,360
神经网络层

102
00:03:12,920 --> 00:03:13,360
可以

103
00:03:14,120 --> 00:03:16,120
最近不是有paper吗

104
00:03:16,120 --> 00:03:16,880
最近paper说

105
00:03:16,879 --> 00:03:17,439
你用

106
00:03:18,079 --> 00:03:21,199
special构造的权链阶层来实现

107
00:03:21,199 --> 00:03:22,759
就是等价也是用一些

108
00:03:22,759 --> 00:03:24,079
一乘一的卷积层来实现

109
00:03:24,639 --> 00:03:25,639
就是说我们也讲过

110
00:03:25,639 --> 00:03:26,560
一乘一的卷积层

111
00:03:26,560 --> 00:03:27,960
等价一个权链阶层

112
00:03:28,319 --> 00:03:29,840
的特殊版本

113
00:03:31,079 --> 00:03:31,479
OK

114
00:03:31,479 --> 00:03:32,799
所以你是能够做的

115
00:03:32,799 --> 00:03:34,120
但如果你是想说

116
00:03:34,120 --> 00:03:36,319
我用把卷积层替换成

117
00:03:36,919 --> 00:03:38,400
最原始的

118
00:03:38,400 --> 00:03:41,199
最简单的权链阶层是不行的

119
00:03:41,199 --> 00:03:42,359
我们有讲过

120
00:03:42,359 --> 00:03:43,319
你可以回去看一下

121
00:03:43,319 --> 00:03:44,159
我们讲过

122
00:03:44,159 --> 00:03:46,800
从权链阶层到卷积层

123
00:03:47,800 --> 00:03:48,920
如果你要全权

124
00:03:48,920 --> 00:03:50,960
你的参数特别大

125
00:03:52,120 --> 00:03:53,120
计算量可能没变

126
00:03:53,120 --> 00:03:54,520
但是参数会特别大

127
00:03:54,760 --> 00:03:56,040
就基本就overfitting了

128
00:03:56,040 --> 00:03:57,120
你做不了很深的

129
00:03:58,840 --> 00:03:59,400
问题4

130
00:03:59,400 --> 00:04:01,760
既然XXLong是一种正则

131
00:04:01,760 --> 00:04:02,640
那么原则上

132
00:04:02,640 --> 00:04:03,760
它能像dropout那样

133
00:04:03,760 --> 00:04:05,360
加强模型的犯坏能力

134
00:04:05,400 --> 00:04:07,200
那么应该提升模型的精度

135
00:04:07,200 --> 00:04:09,800
为什么说batchlong只加速训练

136
00:04:09,800 --> 00:04:13,280
还对精度没有影响呢

137
00:04:17,480 --> 00:04:18,680
你这个问题很好

138
00:04:26,040 --> 00:04:26,439
这个问题

139
00:04:26,439 --> 00:04:27,120
我的结论是

140
00:04:27,120 --> 00:04:27,840
我不知道

141
00:04:27,840 --> 00:04:29,400
我可能回去看一下

142
00:04:29,960 --> 00:04:31,639
我觉得你这个问题问的挺好的

143
00:04:31,639 --> 00:04:33,639
我们之前有讲过

144
00:04:33,720 --> 00:04:36,680
我说batchlong不会给你的精度带来

145
00:04:37,360 --> 00:04:39,600
不会给你的精度带来提升

146
00:04:39,600 --> 00:04:41,160
它只会让你加速训练

147
00:04:41,160 --> 00:04:42,080
但我们又讲过

148
00:04:42,080 --> 00:04:45,040
batchlong可以几乎认为是一种正则

149
00:04:45,040 --> 00:04:46,400
正则理论上来说

150
00:04:46,400 --> 00:04:49,560
是能提升你的犯法能力的

151
00:04:50,160 --> 00:04:53,439
但为什么这个东西不能够过去

152
00:04:53,600 --> 00:04:54,439
这个我其实不知道

153
00:04:54,920 --> 00:04:55,439
不好意思

154
00:04:55,439 --> 00:04:56,960
所以我得回去看一下

155
00:04:57,200 --> 00:04:58,920
看一下论文怎么说的

156
00:05:01,560 --> 00:05:02,480
问题5

157
00:05:02,920 --> 00:05:04,600
我们的工作站是什么配置

158
00:05:04,600 --> 00:05:05,560
我们没有工作站

159
00:05:06,200 --> 00:05:07,400
我们一个云

160
00:05:07,920 --> 00:05:08,600
亚马逊

161
00:05:10,040 --> 00:05:10,640
亚马逊

162
00:05:10,680 --> 00:05:11,640
亚马逊我不知道

163
00:05:11,640 --> 00:05:13,080
我是在亚马逊云计算

164
00:05:13,079 --> 00:05:14,039
跟亚马逊

165
00:05:16,079 --> 00:05:17,000
跟亚马逊

166
00:05:17,359 --> 00:05:19,879
可能亚马逊好像也用的是云计算

167
00:05:20,039 --> 00:05:20,839
其实我还真不知道

168
00:05:20,839 --> 00:05:22,479
我没在亚马逊的部门工作过

169
00:05:22,479 --> 00:05:23,079
我在云计算

170
00:05:23,079 --> 00:05:24,199
云计算我们不需要工作站

171
00:05:24,199 --> 00:05:24,959
我们都用云

172
00:05:25,639 --> 00:05:27,479
所以我只需要我的

173
00:05:27,479 --> 00:05:28,639
我有笔记本

174
00:05:28,639 --> 00:05:30,439
笔记本唯一的干脆性是发邮件

175
00:05:30,439 --> 00:05:33,799
和连接到我的云上的机器就行了

176
00:05:37,120 --> 00:05:37,360
好

177
00:05:37,360 --> 00:05:38,000
问题6

178
00:05:38,000 --> 00:05:40,120
做建筑图纸的处理

179
00:05:40,120 --> 00:05:41,439
有没有相关的深度学习

180
00:05:41,480 --> 00:05:43,399
可以输实现输入一张图纸

181
00:05:43,399 --> 00:05:44,560
进行一定处理后

182
00:05:44,560 --> 00:05:46,079
输出一张图纸的操作

183
00:05:46,079 --> 00:05:47,920
还这个东西还真有

184
00:05:50,800 --> 00:05:51,560
就是说

185
00:05:51,759 --> 00:05:53,040
设计上还真

186
00:05:53,040 --> 00:05:54,519
我觉得这一块是有的

187
00:05:54,519 --> 00:05:55,639
但我没关注过

188
00:05:55,639 --> 00:05:57,079
我觉得你建议你去看一下

189
00:05:57,079 --> 00:05:58,639
相关的一些

190
00:05:58,639 --> 00:06:00,040
搜一搜相关的一些东西

191
00:06:00,040 --> 00:06:01,279
我记得是有的

192
00:06:01,319 --> 00:06:02,079
就是说

193
00:06:02,800 --> 00:06:03,639
建筑图纸

194
00:06:03,639 --> 00:06:04,480
你要两种做法

195
00:06:04,480 --> 00:06:05,120
一种是说

196
00:06:05,120 --> 00:06:07,439
我把它做成一个pixel

197
00:06:07,439 --> 00:06:08,800
来用干也好

198
00:06:08,800 --> 00:06:09,399
用什么也好

199
00:06:09,400 --> 00:06:10,920
来生成一个新的pixel

200
00:06:10,920 --> 00:06:11,920
based图纸

201
00:06:11,960 --> 00:06:12,960
需要我想的

202
00:06:12,960 --> 00:06:14,000
现在可以换换人脸

203
00:06:14,120 --> 00:06:14,960
换什么东西

204
00:06:15,360 --> 00:06:16,400
另外一种是说

205
00:06:16,400 --> 00:06:17,280
我建筑图纸

206
00:06:17,280 --> 00:06:18,800
我当然是一个实量图

207
00:06:19,000 --> 00:06:19,800
实量图的话

208
00:06:19,800 --> 00:06:23,400
我是可以去学习其中一些东西的

209
00:06:23,440 --> 00:06:23,960
对吧

210
00:06:23,960 --> 00:06:26,680
你把它认为做成一个文本来处理

211
00:06:27,240 --> 00:06:29,920
确实我记得是在建筑方面

212
00:06:29,920 --> 00:06:32,040
是有个AI辅助做图

213
00:06:32,320 --> 00:06:33,720
这样的东西确实有的

214
00:06:33,720 --> 00:06:34,640
但我没关注过

215
00:06:34,640 --> 00:06:36,320
建议你去搜一下

216
00:06:39,960 --> 00:06:41,080
就问你一期

217
00:06:41,080 --> 00:06:41,800
all reduce

218
00:06:41,800 --> 00:06:43,400
all gather的作用是什么

219
00:06:43,520 --> 00:06:44,880
实际用的时候发现

220
00:06:44,880 --> 00:06:46,360
PyTorch类似分布式OP

221
00:06:46,360 --> 00:06:47,400
不能传

222
00:06:48,440 --> 00:06:50,320
T柱不能会破坏计算图

223
00:06:50,320 --> 00:06:51,320
不能自动求教

224
00:06:51,320 --> 00:06:52,760
怎么办呢

225
00:06:54,600 --> 00:06:55,280
这个事情

226
00:06:56,560 --> 00:06:57,080
对的

227
00:06:57,080 --> 00:06:58,000
就是说

228
00:06:58,920 --> 00:06:59,720
取决于实现

229
00:07:00,480 --> 00:07:01,520
大家是说

230
00:07:01,560 --> 00:07:02,600
all reduce干嘛

231
00:07:02,680 --> 00:07:03,160
all reduce

232
00:07:03,160 --> 00:07:03,760
all gather

233
00:07:03,760 --> 00:07:05,080
就是说一些通讯的东西

234
00:07:05,080 --> 00:07:06,440
all reduce我们实现过了

235
00:07:07,120 --> 00:07:08,080
把东西

236
00:07:08,959 --> 00:07:10,359
把N个东西加在一起

237
00:07:10,399 --> 00:07:11,319
然后把所有的结果

238
00:07:11,319 --> 00:07:12,240
也复制回去

239
00:07:12,240 --> 00:07:13,079
all gather

240
00:07:13,199 --> 00:07:13,599
all gather

241
00:07:13,599 --> 00:07:14,719
就是不要加在一起

242
00:07:14,719 --> 00:07:15,719
就是把一些东西

243
00:07:15,719 --> 00:07:17,199
就是说一些东西

244
00:07:17,599 --> 00:07:20,159
scatter和gather是一个相反的

245
00:07:20,159 --> 00:07:21,159
scatter我们有讲过

246
00:07:21,159 --> 00:07:22,959
就把一个东西切成N份

247
00:07:22,959 --> 00:07:25,039
发给各个地方

248
00:07:25,519 --> 00:07:26,199
all gather

249
00:07:26,199 --> 00:07:29,000
就是把那些分别在不同地方

250
00:07:29,000 --> 00:07:30,399
东西全部合并起来

251
00:07:30,519 --> 00:07:31,560
然后再给告诉

252
00:07:31,560 --> 00:07:33,079
再传broadcast

253
00:07:33,079 --> 00:07:34,560
就是说传给所有人

254
00:07:35,240 --> 00:07:36,439
这都是一些

255
00:07:37,199 --> 00:07:38,040
最早企业

256
00:07:38,560 --> 00:07:40,600
就是说比如MPI这个东西

257
00:07:42,199 --> 00:07:44,120
HPC就是高性能计算里面的

258
00:07:44,120 --> 00:07:45,160
一些很常见的

259
00:07:45,160 --> 00:07:47,399
一些做分布式的一些

260
00:07:48,120 --> 00:07:49,079
一些操作

261
00:07:49,120 --> 00:07:50,519
但是分布式操作

262
00:07:50,519 --> 00:07:52,360
确实你放进去的时候

263
00:07:52,360 --> 00:07:56,079
会破坏一些你的自动求导

264
00:07:56,680 --> 00:07:58,399
因为自动求导

265
00:07:58,399 --> 00:08:01,159
跨GPU是不那么好做的

266
00:08:02,439 --> 00:08:03,319
Pytorch

267
00:08:03,959 --> 00:08:05,040
我不知道Pytorch

268
00:08:05,040 --> 00:08:05,959
能不能做这个事情

269
00:08:06,040 --> 00:08:07,240
我知道有些框架是能

270
00:08:07,240 --> 00:08:08,320
TensorFlow是能做的

271
00:08:09,480 --> 00:08:10,920
Pytorch现在不能做

272
00:08:10,920 --> 00:08:12,760
那就很遗憾了

273
00:08:12,760 --> 00:08:13,440
那就不要做

274
00:08:14,240 --> 00:08:15,000
那就手写

275
00:08:15,280 --> 00:08:16,160
就是自己

276
00:08:16,160 --> 00:08:17,360
其实你也没关系

277
00:08:17,360 --> 00:08:18,880
就是说你被破坏之后

278
00:08:18,880 --> 00:08:21,120
你就是几个小图

279
00:08:21,120 --> 00:08:21,800
自己搞一搞

280
00:08:22,160 --> 00:08:23,520
然后再手动把它弄回去

281
00:08:23,760 --> 00:08:24,800
这是你可以做的

282
00:08:25,120 --> 00:08:27,000
但未来说不定Pytorch可以做了

283
00:08:31,360 --> 00:08:31,880
问题8

284
00:08:31,880 --> 00:08:33,240
两个GPU训练时

285
00:08:33,240 --> 00:08:34,720
最后的T图是把两个GPU上的

286
00:08:34,720 --> 00:08:35,400
T图先加吗

287
00:08:35,400 --> 00:08:35,840
是的

288
00:08:36,759 --> 00:08:37,840
为什么能加T图

289
00:08:38,400 --> 00:08:39,120
就带回忆一下

290
00:08:39,120 --> 00:08:40,000
T图怎么算了

291
00:08:40,000 --> 00:08:41,960
所谓的mini-batch的T图

292
00:08:41,960 --> 00:08:44,639
就是每一个样本的T图求核

293
00:08:46,960 --> 00:08:49,040
所以你现在的两个GPU

294
00:08:49,040 --> 00:08:50,360
就是说每个GPU

295
00:08:50,360 --> 00:08:52,800
把自己的样本的那些T图求核

296
00:08:52,800 --> 00:08:54,280
然后完整的T图

297
00:08:54,280 --> 00:08:55,240
就是两个GPU的T图

298
00:08:55,240 --> 00:08:56,040
再求核就行了

299
00:08:56,040 --> 00:08:56,400
对吧

300
00:08:56,400 --> 00:08:57,600
所以T图是累加的

301
00:08:57,600 --> 00:08:59,240
所以是可以这样子相加的

302
00:09:03,639 --> 00:09:05,120
为什么参数大的模型

303
00:09:05,120 --> 00:09:06,080
不一定慢

304
00:09:07,279 --> 00:09:09,879
flop是为越多的模型性能越好

305
00:09:09,879 --> 00:09:10,879
这样是为什么

306
00:09:12,919 --> 00:09:14,399
我们有讲过一点点

307
00:09:14,399 --> 00:09:16,279
就是说你不是那么唯一的

308
00:09:16,279 --> 00:09:18,519
就是说你最后看的是什么

309
00:09:18,519 --> 00:09:20,600
最后我们在上一节有讲

310
00:09:20,600 --> 00:09:22,240
就是讲硬件有讲说

311
00:09:22,279 --> 00:09:23,919
你的性能取决于

312
00:09:24,000 --> 00:09:27,039
你每算一个乘法

313
00:09:27,840 --> 00:09:29,399
或每算一个东西

314
00:09:29,440 --> 00:09:31,279
你要访问多少个bit

315
00:09:32,200 --> 00:09:33,320
你的等于是

316
00:09:34,320 --> 00:09:35,960
你的flop是

317
00:09:35,960 --> 00:09:37,480
就你的计算量

318
00:09:37,879 --> 00:09:40,600
除以你的内存访问

319
00:09:40,840 --> 00:09:42,360
这个比例越高越好

320
00:09:43,560 --> 00:09:45,879
因为你的CPU

321
00:09:46,040 --> 00:09:47,240
不管是CPUGPU

322
00:09:47,320 --> 00:09:47,760
很容易

323
00:09:47,760 --> 00:09:49,920
你不是被卡在你的频率上面

324
00:09:50,040 --> 00:09:51,200
你大部分是被卡在

325
00:09:51,200 --> 00:09:52,440
你的访问数据上面

326
00:09:52,440 --> 00:09:54,120
访问你内存这个地方

327
00:09:54,120 --> 00:09:54,920
所以的话

328
00:09:54,920 --> 00:09:58,120
尽量模型参数比较小

329
00:09:58,120 --> 00:09:59,520
算力比较高的

330
00:09:59,560 --> 00:10:00,160
性能更好

331
00:10:00,160 --> 00:10:01,480
就是卷机性能还不错

332
00:10:01,480 --> 00:10:01,760
对吧

333
00:10:01,760 --> 00:10:03,120
矩阵乘法也还可以

334
00:10:03,320 --> 00:10:10,480
为什么分布到多GPU的测试

335
00:10:10,480 --> 00:10:12,000
进度会比单GPU抖动

336
00:10:12,000 --> 00:10:13,400
其实不是的

337
00:10:13,400 --> 00:10:14,760
其实是说抖动

338
00:10:14,760 --> 00:10:17,040
是因为我们的学习率变大了

339
00:10:17,280 --> 00:10:18,520
如果你学习率不变

340
00:10:18,520 --> 00:10:20,040
你的p单大小不变的话

341
00:10:20,080 --> 00:10:23,600
多GPU和单GPU是不会有任何区别的

342
00:10:23,680 --> 00:10:26,520
所以我们还是要分离开这两个事情

343
00:10:26,560 --> 00:10:29,800
就是说我们干了两个事情

344
00:10:29,920 --> 00:10:33,000
一个是就把一个GPU的任务

345
00:10:33,000 --> 00:10:34,399
放到了多个GPU上

346
00:10:34,440 --> 00:10:36,679
假设我别的参数不变

347
00:10:36,679 --> 00:10:37,519
Batch Size不变

348
00:10:37,519 --> 00:10:38,279
学习率不变

349
00:10:38,279 --> 00:10:39,639
理论上你的测试进度

350
00:10:39,639 --> 00:10:40,799
是不会发生变化的

351
00:10:40,840 --> 00:10:42,960
变化的唯一的变化是你的性能

352
00:10:42,960 --> 00:10:44,639
就是没处理一个

353
00:10:44,679 --> 00:10:46,399
扫一遍数据要多少秒钟

354
00:10:47,440 --> 00:10:52,639
但是为了得到更好的速度

355
00:10:52,639 --> 00:10:54,600
我们需要把Batch Size变大

356
00:10:54,720 --> 00:10:55,919
一旦Batch Size变大

357
00:10:55,919 --> 00:10:57,679
就带来你的参数

358
00:10:57,679 --> 00:10:59,200
你的收敛会发生变化

359
00:10:59,360 --> 00:11:00,679
这就是为什么Batch Size变大

360
00:11:00,679 --> 00:11:01,759
我们把能力Rate

361
00:11:01,759 --> 00:11:02,919
把学习率往上调

362
00:11:02,919 --> 00:11:04,559
翻进度更陡

363
00:11:05,079 --> 00:11:05,399
OK

364
00:11:05,399 --> 00:11:06,519
我们等会会来讲一讲

365
00:11:07,639 --> 00:11:09,199
在讲分布式的时候

366
00:11:09,199 --> 00:11:10,719
分布式一样的有这个问题

367
00:11:12,839 --> 00:11:13,919
能力Rate太大

368
00:11:13,919 --> 00:11:15,439
会导致训练不收敛吗

369
00:11:15,439 --> 00:11:16,679
会的能力Rate

370
00:11:16,679 --> 00:11:17,919
当然会不收敛了

371
00:11:18,120 --> 00:11:19,079
Batch Size太大

372
00:11:19,079 --> 00:11:20,120
会导致Loss

373
00:11:20,120 --> 00:11:22,039
Batch Size太大

374
00:11:22,039 --> 00:11:22,959
不会导致Loss

375
00:11:22,959 --> 00:11:24,199
会变成Loss Number

376
00:11:24,199 --> 00:11:26,559
通常是说你的Batch Size太大

377
00:11:26,559 --> 00:11:29,480
你的能力Rate可能你也调得不行

378
00:11:29,599 --> 00:11:31,120
就是说能力Rate太大

379
00:11:31,120 --> 00:11:32,600
会导致你的Loss Number

380
00:11:32,600 --> 00:11:33,919
是有数值稳定的问题

381
00:11:33,960 --> 00:11:34,919
在Batch Size变大

382
00:11:34,919 --> 00:11:35,759
理论上是不会的

383
00:11:35,759 --> 00:11:37,519
因为我们是对Batch Size

384
00:11:37,840 --> 00:11:39,200
就是Batch Size变大

385
00:11:39,200 --> 00:11:39,919
因为我们的T度

386
00:11:39,919 --> 00:11:40,919
是最后求均值了

387
00:11:40,919 --> 00:11:41,440
记得吗

388
00:11:42,279 --> 00:11:44,440
所以说Batch Size变大

389
00:11:44,440 --> 00:11:45,200
其实理论上

390
00:11:45,200 --> 00:11:46,759
你的数值稳定性会更好

391
00:11:49,240 --> 00:11:50,679
GPU的显存如何优化

392
00:11:50,919 --> 00:11:52,919
最近跑新老是out of memory

393
00:11:52,919 --> 00:11:54,960
我的显存是$14

394
00:11:57,919 --> 00:12:00,519
显存说句真话

395
00:12:00,679 --> 00:12:02,360
手动优化是很难的

396
00:12:02,360 --> 00:12:03,519
靠的是框架

397
00:12:04,079 --> 00:12:04,960
框架来讲

398
00:12:06,240 --> 00:12:07,159
我其实觉得

399
00:12:07,159 --> 00:12:09,159
PyTorch的显存优化还行

400
00:12:09,799 --> 00:12:10,639
它不是最好的

401
00:12:10,639 --> 00:12:11,559
但是也不错

402
00:12:11,559 --> 00:12:12,879
所以我觉得框架

403
00:12:12,879 --> 00:12:14,399
可能你能干的事情不多

404
00:12:14,399 --> 00:12:15,120
我建议

405
00:12:15,120 --> 00:12:16,480
除非你特别懂这一块

406
00:12:16,639 --> 00:12:17,079
不然的话

407
00:12:17,079 --> 00:12:18,840
你就是把Batch Size弄小一点

408
00:12:19,399 --> 00:12:20,840
或者是你的模型想一想

409
00:12:20,840 --> 00:12:22,759
怎么样把模型搞得简单一点

410
00:12:23,840 --> 00:12:26,720
如果我说如何优化显存

411
00:12:26,720 --> 00:12:29,159
可能我得讲个一两个小时

412
00:12:29,159 --> 00:12:29,879
可能还讲不完

413
00:12:29,879 --> 00:12:32,200
这一块里面还挺复杂的

414
00:12:34,480 --> 00:12:35,279
对京都来讲

415
00:12:35,279 --> 00:12:36,600
Batch Size等于1是最好的情况

416
00:12:36,720 --> 00:12:37,039
是的

417
00:12:37,360 --> 00:12:37,799
很遗憾

418
00:12:37,799 --> 00:12:39,320
Batch Size等于1的时候

419
00:12:39,320 --> 00:12:40,360
在收敛来讲

420
00:12:40,360 --> 00:12:41,200
可能是

421
00:12:44,120 --> 00:12:45,360
可能是最好的

422
00:12:49,720 --> 00:12:52,159
Primary Server和PyTorch可以结合吗

423
00:12:52,159 --> 00:12:53,159
具体如何实现

424
00:12:53,480 --> 00:12:55,399
PyTorch没有实现Primary Server

425
00:12:55,399 --> 00:12:57,679
M3的TensorFlow有

426
00:12:57,919 --> 00:12:59,039
因为PyTorch它不做

427
00:12:59,039 --> 00:13:00,399
Async那一套东西

428
00:13:00,559 --> 00:13:03,399
PyTorch做了一个最简单的Data Parallel

429
00:13:03,599 --> 00:13:04,319
所以就是说

430
00:13:04,319 --> 00:13:06,319
你不需要Primary Server是没关系的

431
00:13:10,399 --> 00:13:12,039
但是我觉得有第三方实现

432
00:13:12,199 --> 00:13:13,639
我觉得比如说

433
00:13:14,639 --> 00:13:16,399
头条他们不是实现了一个

434
00:13:16,399 --> 00:13:17,199
PS Lite

435
00:13:18,519 --> 00:13:20,319
一个BytePS

436
00:13:20,480 --> 00:13:22,159
它是应该支持PyTorch的

437
00:13:24,480 --> 00:13:25,519
问题是5

438
00:13:25,519 --> 00:13:27,199
用了NN.Data Parallel

439
00:13:27,240 --> 00:13:29,320
是不是数据也被自动分配到GPU上

440
00:13:29,320 --> 00:13:29,759
是的

441
00:13:30,280 --> 00:13:31,840
他就是说他理论上

442
00:13:31,840 --> 00:13:33,920
在算Net.4的时候

443
00:13:33,920 --> 00:13:36,040
他会把你这个东西跟我们之间切开

444
00:13:37,040 --> 00:13:37,840
问题16

445
00:13:38,120 --> 00:13:39,040
我们等会讲

446
00:13:39,240 --> 00:13:39,879
Batch Size调大

447
00:13:39,879 --> 00:13:41,120
为什么进度会变第一

448
00:13:42,560 --> 00:13:44,920
验证几项准确度震荡较大

449
00:13:44,920 --> 00:13:46,080
是哪个参数影响最大

450
00:13:46,960 --> 00:13:48,080
是能力Rate

451
00:13:50,759 --> 00:13:51,680
问题18

452
00:13:52,200 --> 00:13:54,040
让网络前级才能训练

453
00:13:54,040 --> 00:13:55,400
我们采用BN等操作

454
00:13:55,399 --> 00:13:57,000
为什么不采用不同

455
00:13:57,000 --> 00:13:58,720
为了不采用不同stage

456
00:13:58,720 --> 00:13:59,639
采用不同学习率

457
00:13:59,840 --> 00:14:00,959
比如说开始学习率

458
00:14:00,959 --> 00:14:02,480
这个问题我们之前有同学问过

459
00:14:02,480 --> 00:14:03,279
我觉得挺好的

460
00:14:04,519 --> 00:14:06,279
你可以这么做没问题

461
00:14:06,600 --> 00:14:08,079
等会我们可能

462
00:14:08,919 --> 00:14:10,959
我们会在Fine Tuning的时候讲一下

463
00:14:10,959 --> 00:14:12,480
我们也会用到类似的东西

464
00:14:12,480 --> 00:14:14,279
但是这东西用起来最大麻烦

465
00:14:14,279 --> 00:14:15,480
是比较麻烦

466
00:14:15,600 --> 00:14:16,720
你怎么调对吧

467
00:14:16,919 --> 00:14:19,039
你到底谁大谁小

468
00:14:19,039 --> 00:14:21,000
我知道到底大多少谁

469
00:14:21,120 --> 00:14:22,279
就不好调

470
00:14:22,840 --> 00:14:23,679
加一个Batch Long

471
00:14:23,679 --> 00:14:25,079
Batch Long也没什么参数可以调

472
00:14:25,080 --> 00:14:26,280
Batch Long有操参数

473
00:14:26,440 --> 00:14:27,759
但是大家不会调

474
00:14:27,800 --> 00:14:29,400
所以方便简单

475
00:14:31,440 --> 00:14:34,440
在用Torch的数据比拼中

476
00:14:34,440 --> 00:14:36,200
将input的label放在GPU0上

477
00:14:36,200 --> 00:14:37,480
会不会导致性的问题

478
00:14:37,720 --> 00:14:40,520
因为这些数据最初挪到GPU上

479
00:14:41,240 --> 00:14:41,720
对

480
00:14:41,720 --> 00:14:43,400
就是说你这个问题很好

481
00:14:43,400 --> 00:14:45,080
就是说你发现说

482
00:14:46,680 --> 00:14:47,320
你发现说

483
00:14:47,320 --> 00:14:48,680
我们做了一个额外的操作

484
00:14:48,680 --> 00:14:50,840
我们把数据挪到了GPU0上

485
00:14:51,280 --> 00:14:53,200
通常把数据挪到GPU0上

486
00:14:53,200 --> 00:14:54,080
不是太多问题

487
00:14:54,080 --> 00:14:55,400
因为是挪个数据

488
00:14:55,400 --> 00:14:56,720
相对来说比较小

489
00:14:56,960 --> 00:14:59,120
但是这个东西看上去是挺额外的

490
00:14:59,360 --> 00:15:01,040
我没有仔细看这个问题

491
00:15:01,040 --> 00:15:03,840
我觉得不应该需要去挪它

492
00:15:03,840 --> 00:15:04,759
但实际上来说

493
00:15:04,759 --> 00:15:06,040
你不挪它又会出错

494
00:15:06,040 --> 00:15:08,120
所以我其实没仔细看出错原因是什么

495
00:15:08,120 --> 00:15:09,280
你可去研究一下

496
00:15:09,320 --> 00:15:11,120
我觉得我是感觉

497
00:15:11,120 --> 00:15:12,520
但是一句多余的话

498
00:15:12,520 --> 00:15:14,280
就是我们在刚刚代码实现

499
00:15:14,639 --> 00:15:15,280
把

500
00:15:15,440 --> 00:15:15,960
又没必要

501
00:15:15,960 --> 00:15:16,920
我放在GPU上

502
00:15:16,920 --> 00:15:18,720
CPU上你自己给我挪去呗

503
00:15:20,640 --> 00:15:22,160
理论上这样子更直观

504
00:15:22,159 --> 00:15:24,600
但是这么做会出错

505
00:15:25,360 --> 00:15:27,480
反过来讲挪一下也问题不大

506
00:15:27,679 --> 00:15:29,039
为什么是你数据

507
00:15:29,519 --> 00:15:30,639
数据你

508
00:15:31,519 --> 00:15:33,120
你那点数据不算啥

509
00:15:33,240 --> 00:15:34,279
跟梯度

510
00:15:34,279 --> 00:15:37,240
发梯度收发梯度来讲不算什么

511
00:15:39,519 --> 00:15:41,000
为什么Batch Size调的比较小

512
00:15:41,000 --> 00:15:42,559
比如8进度会在0.1

513
00:15:42,559 --> 00:15:43,639
一直不变化

514
00:15:43,879 --> 00:15:45,159
那是你能力Rate太大了

515
00:15:45,360 --> 00:15:47,679
你Batch Size变小

516
00:15:47,679 --> 00:15:49,399
你能力Rate就不能太大

517
00:15:51,240 --> 00:15:51,439
好

518
00:15:51,440 --> 00:15:52,000
问题21

519
00:15:52,200 --> 00:15:53,000
这个问题很好

520
00:15:53,080 --> 00:15:53,840
这个问题我们就

521
00:15:53,840 --> 00:15:55,560
但是这里回答不了

522
00:15:55,600 --> 00:15:57,560
训练级和验证级不同分布怎么办

523
00:15:57,560 --> 00:15:59,200
这个就是著名的

524
00:16:00,880 --> 00:16:02,880
就我们就是说一句话

525
00:16:03,000 --> 00:16:04,040
就我们所有的东西

526
00:16:04,040 --> 00:16:05,360
都假设你是

527
00:16:06,280 --> 00:16:07,480
RID的

528
00:16:07,680 --> 00:16:09,000
独立同分布的

529
00:16:09,360 --> 00:16:11,200
实际上来说根本就不是

530
00:16:11,200 --> 00:16:13,080
在现实生活中没有人是

531
00:16:13,120 --> 00:16:13,840
没有

532
00:16:14,240 --> 00:16:15,200
昨天的

533
00:16:15,240 --> 00:16:16,760
今天的股票和明天的股票

534
00:16:16,760 --> 00:16:18,760
绝对不是一个独立同分布的东西

535
00:16:19,480 --> 00:16:20,360
所以说

536
00:16:21,360 --> 00:16:23,560
统计模型一开始就是错的

537
00:16:24,480 --> 00:16:25,960
所有统计模型都是错的

538
00:16:25,960 --> 00:16:27,960
只是有些错的少一点点

539
00:16:27,960 --> 00:16:28,360
对吧

540
00:16:29,000 --> 00:16:30,639
所以这个东西是一个很

541
00:16:31,639 --> 00:16:32,519
这个叫做

542
00:16:32,879 --> 00:16:33,720
Convance Shift

543
00:16:33,720 --> 00:16:34,039
也行

544
00:16:34,039 --> 00:16:34,639
Labor Shift

545
00:16:34,639 --> 00:16:34,840
也行

546
00:16:34,840 --> 00:16:35,519
Concept Shift

547
00:16:35,519 --> 00:16:36,560
也就是一堆Shift

548
00:16:36,600 --> 00:16:37,200
所以这个东西

549
00:16:37,200 --> 00:16:38,759
我们这堂课不会讲

550
00:16:38,759 --> 00:16:39,800
但我们会

551
00:16:40,720 --> 00:16:41,480
在

552
00:16:41,759 --> 00:16:43,200
接下来一堂课

553
00:16:43,200 --> 00:16:44,120
我们会

554
00:16:44,159 --> 00:16:46,039
我在考虑说

555
00:16:46,039 --> 00:16:47,519
今年秋天会在

556
00:16:47,519 --> 00:16:49,360
斯坦福开门课讲这些

557
00:16:49,360 --> 00:16:50,200
讲这一块

558
00:16:50,200 --> 00:16:51,480
就讲这一块

559
00:16:51,480 --> 00:16:52,560
模型以外的东西

560
00:16:52,560 --> 00:16:53,480
我们就不讲模型

561
00:16:53,480 --> 00:16:54,440
就讲各种

562
00:16:54,720 --> 00:16:55,920
你在实际的操作中

563
00:16:55,920 --> 00:16:56,759
你的验证解析

564
00:16:56,759 --> 00:16:58,240
那些根本就不是一个东西

565
00:16:58,560 --> 00:16:59,360
怎么办

566
00:17:00,040 --> 00:17:01,160
反正明天周六

567
00:17:01,240 --> 00:17:01,680
明天周六

568
00:17:01,680 --> 00:17:02,320
我去到斯坦福

569
00:17:02,320 --> 00:17:03,560
跟大家聊一聊

570
00:17:04,080 --> 00:17:06,920
反正这门课还在规划中

571
00:17:08,039 --> 00:17:09,039
我们会讲这一块

572
00:17:09,039 --> 00:17:10,120
但是这一块也

573
00:17:10,319 --> 00:17:11,400
这块不容易弄的

574
00:17:12,559 --> 00:17:14,480
不是我们QA环节

575
00:17:14,480 --> 00:17:15,600
讲几句能讲完的

576
00:17:16,680 --> 00:17:18,039
多差数结构

577
00:17:19,039 --> 00:17:20,920
这个dependence

578
00:17:20,960 --> 00:17:22,359
dependency path tree

579
00:17:22,359 --> 00:17:23,720
如果encode

580
00:17:24,440 --> 00:17:25,200
我们

581
00:17:25,279 --> 00:17:26,720
你可以用BERT来做一些东西

582
00:17:26,720 --> 00:17:27,920
或者你用Graph Neural Network

583
00:17:27,920 --> 00:17:28,480
来做东西

584
00:17:28,480 --> 00:17:30,519
或者你用什么TreeLSTM做东西

585
00:17:30,519 --> 00:17:31,119
我们

586
00:17:31,680 --> 00:17:32,399
很遗憾

587
00:17:32,399 --> 00:17:33,960
Graph Neural Network和TreeLSTM

588
00:17:33,960 --> 00:17:34,759
我们都不讲

589
00:17:35,680 --> 00:17:38,319
就是所以我们就跳过这个问题

590
00:17:41,159 --> 00:17:42,680
不同层之间的信息交换

591
00:17:42,680 --> 00:17:43,359
有没有好的思路

592
00:17:43,359 --> 00:17:45,079
比较layer block之间

593
00:17:45,079 --> 00:17:48,480
这个我就不是很理解

594
00:17:48,480 --> 00:17:49,480
这句话是什么意思

595
00:17:50,319 --> 00:17:50,759
时间关系

596
00:17:50,759 --> 00:17:51,559
我们先跳过了

597
00:17:51,919 --> 00:17:53,759
你可以把这个东西

598
00:17:53,759 --> 00:17:55,279
大家可以把这个问题

599
00:17:55,279 --> 00:17:57,439
再写得稍微详细一点

600
00:17:57,839 --> 00:18:00,279
层之间信息交换不就是

601
00:18:00,839 --> 00:18:03,079
一层是个顺序串行结构

602
00:18:03,199 --> 00:18:03,759
不是

603
00:18:04,159 --> 00:18:05,720
因为串串串串过去了

604
00:18:06,159 --> 00:18:07,839
当然是交互信息的

605
00:18:09,960 --> 00:18:12,480
我用RESTnet跑我们的数页训练集

606
00:18:12,480 --> 00:18:13,319
Batch Size为8

607
00:18:13,319 --> 00:18:14,519
会用到6G算的写程

608
00:18:14,519 --> 00:18:15,039
为什么

609
00:18:15,039 --> 00:18:15,639
RESTnet

610
00:18:18,720 --> 00:18:20,119
Batch Size8

611
00:18:20,119 --> 00:18:21,480
为什么会用到6G

612
00:18:21,680 --> 00:18:22,400
这个有点大

613
00:18:23,799 --> 00:18:25,639
就是说RESTnet是18

614
00:18:25,639 --> 00:18:26,319
就18兆了

615
00:18:26,319 --> 00:18:27,039
还不是几百兆

616
00:18:27,039 --> 00:18:28,119
它的显存

617
00:18:28,119 --> 00:18:29,319
它是要存

618
00:18:29,319 --> 00:18:32,119
它要存它的中间的结果

619
00:18:32,119 --> 00:18:33,119
中间变量结果

620
00:18:33,119 --> 00:18:34,599
就是每一个layer

621
00:18:34,599 --> 00:18:35,559
就我们有讲过

622
00:18:35,680 --> 00:18:37,359
我们在讲T度的时候

623
00:18:37,359 --> 00:18:37,920
有讲过说

624
00:18:37,920 --> 00:18:39,559
你为了算你的backward的T度

625
00:18:39,559 --> 00:18:41,160
你把前面那些算的结果

626
00:18:41,160 --> 00:18:43,119
存在了以后之后用

627
00:18:43,120 --> 00:18:45,280
所以存那个东西是很占空间的

628
00:18:45,360 --> 00:18:48,120
所以那个是跟你的Batch Size线性关系

629
00:18:52,720 --> 00:18:53,520
另外我觉得

630
00:18:54,240 --> 00:18:56,440
我觉得等于8

631
00:18:56,440 --> 00:18:57,760
你用到6G挺奇怪的

632
00:18:58,440 --> 00:18:59,120
你可以看一下

633
00:18:59,120 --> 00:19:04,600
你是不是在别的地方用了显存

634
00:19:07,240 --> 00:19:08,680
个人DIY主机上

635
00:19:08,680 --> 00:19:10,120
装两块不同的GPU

636
00:19:10,120 --> 00:19:11,440
影响性能吗

637
00:19:12,440 --> 00:19:14,680
如果你要在两块不同的GPU上

638
00:19:14,680 --> 00:19:16,759
做Data Pilot的话

639
00:19:17,200 --> 00:19:18,000
你的就是说

640
00:19:18,000 --> 00:19:19,039
你得去算好

641
00:19:19,039 --> 00:19:21,559
这两块GPU之间的

642
00:19:22,000 --> 00:19:24,480
两块GPU之间的性能差怎么样

643
00:19:24,480 --> 00:19:26,360
假设一块GPU是另外一块

644
00:19:26,360 --> 00:19:27,200
GPU的计算量

645
00:19:27,200 --> 00:19:28,759
是性能是两倍的话

646
00:19:28,759 --> 00:19:32,160
那么假设你有一个

647
00:19:32,160 --> 00:19:33,720
那么你在分的时候

648
00:19:33,720 --> 00:19:35,640
你性能是两倍的GPU

649
00:19:35,640 --> 00:19:37,440
应该分到的样本数是性能

650
00:19:37,440 --> 00:19:39,360
是一的GPU的两倍

651
00:19:39,960 --> 00:19:41,880
这样子保证你的每个GPU之间

652
00:19:41,880 --> 00:19:44,720
是在同样的时间内算完

653
00:19:44,720 --> 00:19:45,720
T2同样

654
00:19:46,240 --> 00:19:47,760
这样子并行度会好一点

655
00:19:51,040 --> 00:19:53,760
克雷竞赛有用VGG11的吗

656
00:19:57,680 --> 00:19:59,960
VGG11其实是一个不错的模型

657
00:20:00,200 --> 00:20:03,800
就是说大家别觉得那个模型特别老了

658
00:20:03,840 --> 00:20:05,400
其实那个模型还可以

659
00:20:05,400 --> 00:20:06,680
这模型在

660
00:20:06,920 --> 00:20:08,080
你用来赢比赛

661
00:20:08,080 --> 00:20:09,040
可能比较难了

662
00:20:10,080 --> 00:20:14,480
但是在做那些fine tuning

663
00:20:14,640 --> 00:20:15,360
在抽feature上

664
00:20:15,360 --> 00:20:16,600
VGG还是不错的

665
00:20:17,160 --> 00:20:18,440
所以我觉得可以

666
00:20:19,000 --> 00:20:20,880
大家不用太小看这个模型

667
00:20:20,880 --> 00:20:21,920
另外一个是说

668
00:20:21,920 --> 00:20:23,960
我觉得VGG说不会收敛

669
00:20:23,960 --> 00:20:26,280
可能是你的learning rate太大了

670
00:20:27,200 --> 00:20:28,920
VGG没有用batch normalization

671
00:20:29,600 --> 00:20:32,960
所以你可以把batch normalization加进去

672
00:20:32,960 --> 00:20:34,400
会可能学习率会

673
00:20:35,480 --> 00:20:37,600
你可以这样子收敛会好一点点

674
00:20:38,080 --> 00:20:38,720
OK

