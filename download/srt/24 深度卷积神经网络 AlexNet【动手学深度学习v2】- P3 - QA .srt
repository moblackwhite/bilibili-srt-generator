1
00:00:00,000 --> 00:00:01,360
好

2
00:00:01,360 --> 00:00:03,879
接下来我们来看一下AlexNet的实现

3
00:00:04,480 --> 00:00:09,439
首先我们就是直接这一这一页公式

4
00:00:09,439 --> 00:00:11,960
这一页代码就直接实现AlexNet

5
00:00:13,080 --> 00:00:14,880
我们来给大家讲一下

6
00:00:14,880 --> 00:00:18,600
首先它跟Nunet是一个没本质区别

7
00:00:18,600 --> 00:00:19,039
对吧

8
00:00:19,080 --> 00:00:21,120
就是你把一串东西给串起来

9
00:00:21,519 --> 00:00:24,519
所以这里用的是Nsequential这个构造器

10
00:00:25,080 --> 00:00:26,400
然后接下来就是说

11
00:00:26,720 --> 00:00:30,080
其实你跟实现Nunet没有太多本质区别

12
00:00:30,200 --> 00:00:34,200
就是对着它参数架构把东西给你放进来

13
00:00:34,480 --> 00:00:34,840
比如说

14
00:00:36,280 --> 00:00:37,079
Comp2D

15
00:00:37,280 --> 00:00:38,840
因为我们用的是FashionMList

16
00:00:38,960 --> 00:00:40,920
所以我们的输入channel数是1

17
00:00:40,960 --> 00:00:43,280
但是你在ImageNet上应该是等于3的

18
00:00:44,280 --> 00:00:45,880
然后你96你的输出

19
00:00:46,040 --> 00:00:48,320
那你的是11的一个宽

20
00:00:48,359 --> 00:00:50,640
然后你的stride等于4

21
00:00:50,760 --> 00:00:52,040
然后我们加了一个piling

22
00:00:52,280 --> 00:00:54,800
接下来是在后面加一个reload

23
00:00:54,800 --> 00:00:55,480
函数在里面

24
00:00:55,480 --> 00:00:56,560
就是你的激活函数

25
00:00:57,160 --> 00:01:00,160
那么接下来是你的max最大持划层

26
00:01:00,160 --> 00:01:00,760
2D的

27
00:01:01,160 --> 00:01:02,520
你的kernelSize等于3

28
00:01:02,520 --> 00:01:03,440
stride等于2

29
00:01:04,719 --> 00:01:06,600
那么接下来你再有一个卷积

30
00:01:06,920 --> 00:01:09,280
然后你的输入是96个通道

31
00:01:09,280 --> 00:01:10,480
而输出是256

32
00:01:11,159 --> 00:01:12,400
你的这些东西都是

33
00:01:12,400 --> 00:01:15,640
基本上照着前面抄一抄

34
00:01:16,400 --> 00:01:18,520
然后再加一个最大的持划层

35
00:01:19,000 --> 00:01:24,120
接下来就是直接用了三个卷积层

36
00:01:24,400 --> 00:01:26,040
这里我们做了一点点的

37
00:01:26,079 --> 00:01:28,200
其实这个地方我觉得应该是384

38
00:01:29,680 --> 00:01:31,439
这应该是改成384的

39
00:01:34,240 --> 00:01:34,760
OK

40
00:01:35,400 --> 00:01:38,480
然后在卷积层之后

41
00:01:38,480 --> 00:01:40,880
加入了一个最大的持划层

42
00:01:42,320 --> 00:01:45,840
这一块就是之前的卷积的单元

43
00:01:46,439 --> 00:01:48,760
我们这里跟net不一样

44
00:01:48,760 --> 00:01:50,080
那是两个卷积层

45
00:01:50,080 --> 00:01:51,400
现在我们是5个卷积层

46
00:01:51,920 --> 00:01:54,440
卷积大家知道一个4D的东西

47
00:01:54,440 --> 00:01:55,680
进去出了一个4D

48
00:01:56,000 --> 00:01:57,600
4D就是你的通道

49
00:01:57,600 --> 00:02:00,680
你的plan大小为通道为高和宽

50
00:02:01,240 --> 00:02:04,560
那么接下来就是你的全连接这一块了

51
00:02:04,560 --> 00:02:07,200
就是我用flat把它拉成一个

52
00:02:07,200 --> 00:02:08,319
RD的一个形状

53
00:02:08,319 --> 00:02:09,800
跟那弄的一样

54
00:02:10,080 --> 00:02:13,480
然后来算加入两个全连接层

55
00:02:14,120 --> 00:02:16,120
这里用的是redo这个既往函数

56
00:02:16,960 --> 00:02:19,599
另外一块就是说我们用的是dropout

57
00:02:19,919 --> 00:02:22,439
在我的全连接层后面

58
00:02:22,439 --> 00:02:23,960
跟了我的dropout层

59
00:02:24,560 --> 00:02:25,519
就是丢弃层

60
00:02:25,519 --> 00:02:27,400
我的丢弃的概率是0.5

61
00:02:27,639 --> 00:02:28,879
就是有50%的概率

62
00:02:28,879 --> 00:02:30,039
把我的输出置为0

63
00:02:30,919 --> 00:02:32,319
剩下的就是乘个2

64
00:02:33,359 --> 00:02:34,959
然后因为你的卷积

65
00:02:35,359 --> 00:02:36,240
因为你的输出

66
00:02:36,400 --> 00:02:39,799
这里大概就是640

67
00:02:40,159 --> 00:02:41,240
我这里其实不能调

68
00:02:41,240 --> 00:02:41,840
我调了之后

69
00:02:41,840 --> 00:02:42,919
我这里下面都得改

70
00:02:43,039 --> 00:02:45,000
这就是拍照就不是那么好的地方

71
00:02:46,319 --> 00:02:47,560
所以你这个地方

72
00:02:47,560 --> 00:02:49,319
就是你256之后

73
00:02:50,400 --> 00:02:52,919
他这里会出来的是640

74
00:02:53,159 --> 00:02:54,960
6400

75
00:02:55,199 --> 00:02:57,240
然后全连接层

76
00:02:57,240 --> 00:02:58,719
4096

77
00:02:59,159 --> 00:03:00,120
最后因为我们用的

78
00:03:00,120 --> 00:03:01,319
FreshM list这个数据

79
00:03:01,319 --> 00:03:03,199
所以我们的输出类是10

80
00:03:03,560 --> 00:03:04,159
这个地方

81
00:03:04,159 --> 00:03:05,680
在imageNet当然说1000

82
00:03:05,879 --> 00:03:07,000
所以我们的人是10

83
00:03:07,560 --> 00:03:08,680
所以相对来说

84
00:03:08,959 --> 00:03:09,959
我们这是

85
00:03:10,359 --> 00:03:11,479
我们是一个相对来说

86
00:03:11,479 --> 00:03:12,959
比较简单的数据机

87
00:03:13,680 --> 00:03:16,759
用AlexNet其实有一点点太大了

88
00:03:16,759 --> 00:03:18,240
就是这一块都太大了

89
00:03:18,360 --> 00:03:20,360
但是就是为了掩饰网络

90
00:03:20,360 --> 00:03:22,080
我们就用的一个小数据机

91
00:03:22,080 --> 00:03:24,960
因为imageNet训练比较难

92
00:03:24,960 --> 00:03:26,719
就每个几天

93
00:03:26,719 --> 00:03:28,680
可能每个几个小时是训练不下来

94
00:03:29,120 --> 00:03:30,520
所以为了掩饰方便

95
00:03:30,520 --> 00:03:34,960
我们就套用AlexNet架构

96
00:03:34,960 --> 00:03:36,480
但是用一个相对来说

97
00:03:36,480 --> 00:03:37,680
比较简单的数据机

98
00:03:37,680 --> 00:03:39,360
给大家演示一下它的性能

99
00:03:40,320 --> 00:03:40,760
OK

100
00:03:41,719 --> 00:03:42,719
这个就是说

101
00:03:43,040 --> 00:03:44,400
基本上那么几行代码

102
00:03:44,400 --> 00:03:46,480
就是AlexNet的核心的思想

103
00:03:46,760 --> 00:03:48,080
但是你要知道

104
00:03:48,160 --> 00:03:49,120
跟Nernet比

105
00:03:49,120 --> 00:03:50,480
确实你从代码上来看

106
00:03:50,480 --> 00:03:51,560
没有太多区别

107
00:03:51,560 --> 00:03:52,000
对吧

108
00:03:52,000 --> 00:03:52,880
架构上长一样

109
00:03:52,880 --> 00:03:53,719
就多一行

110
00:03:53,719 --> 00:03:54,280
改一改这个

111
00:03:54,280 --> 00:03:55,040
改改那个

112
00:03:55,200 --> 00:03:57,439
但是从Nernet到AlexNet

113
00:03:57,560 --> 00:03:59,120
就是那么一点点的区别

114
00:03:59,400 --> 00:04:00,240
大家花了

115
00:04:01,080 --> 00:04:02,720
将近20年

116
00:04:03,280 --> 00:04:04,320
这中间20年

117
00:04:04,320 --> 00:04:05,960
其实是没有任何进展的

118
00:04:06,120 --> 00:04:07,680
然后20年之后突然捡起来了

119
00:04:07,880 --> 00:04:08,600
把它改一改

120
00:04:08,600 --> 00:04:09,800
然后效果出来了

121
00:04:10,520 --> 00:04:13,760
所以这个也是历史上挺好玩的一些事情

122
00:04:15,400 --> 00:04:16,040
OK

123
00:04:16,120 --> 00:04:18,240
所以我们定好了

124
00:04:18,240 --> 00:04:18,960
定好了之后

125
00:04:18,960 --> 00:04:19,720
就是说

126
00:04:20,160 --> 00:04:21,160
我们可以看一下

127
00:04:21,160 --> 00:04:23,920
就是说每一个层的输出的形状

128
00:04:24,640 --> 00:04:26,480
我们之前也做过这个事情

129
00:04:26,760 --> 00:04:29,200
你看到每一层是怎么样变化的

130
00:04:29,640 --> 00:04:30,879
基本上看到是

131
00:04:31,280 --> 00:04:32,520
我们的

132
00:04:34,680 --> 00:04:37,160
就假设你是给个224×224

133
00:04:37,280 --> 00:04:38,480
我们通道数等于1

134
00:04:38,720 --> 00:04:39,720
的情况下

135
00:04:40,200 --> 00:04:42,120
那么首先你

136
00:04:42,560 --> 00:04:43,879
你会发现第一个卷积层

137
00:04:43,879 --> 00:04:46,040
你就把你的224变得一个很小的数字

138
00:04:46,079 --> 00:04:47,279
54×54

139
00:04:47,279 --> 00:04:49,120
因为你除以了4倍

140
00:04:49,120 --> 00:04:50,639
你的不复等于4

141
00:04:51,600 --> 00:04:54,360
那么接下来就是你的max pooling

142
00:04:54,360 --> 00:04:55,560
把它又减半了

143
00:04:55,600 --> 00:04:56,560
变成26了

144
00:04:56,839 --> 00:04:59,159
就你变成就16×26×26

145
00:04:59,719 --> 00:05:01,879
那么接下来就是说256×26×26

146
00:05:01,879 --> 00:05:03,680
然后到一直减少减少

147
00:05:03,920 --> 00:05:05,240
减少到最后

148
00:05:06,279 --> 00:05:09,560
就变成了跟前面的Nernet是一样

149
00:05:09,600 --> 00:05:12,120
在进入你的权力阶层的时候

150
00:05:12,120 --> 00:05:14,439
我的卷积层使得

151
00:05:14,959 --> 00:05:17,000
我的高宽都是5

152
00:05:17,519 --> 00:05:19,720
这跟Nernet其实是一样的

153
00:05:20,199 --> 00:05:22,120
那么最后我把它拉成一个

154
00:05:22,160 --> 00:05:24,160
下量进入我的权力阶层

155
00:05:24,399 --> 00:05:25,399
最后我的输出是10

156
00:05:25,399 --> 00:05:26,399
因为我有10个类

157
00:05:26,840 --> 00:05:27,199
好

158
00:05:27,199 --> 00:05:29,120
这就是Nernet

159
00:05:29,120 --> 00:05:30,079
Alexson的

160
00:05:30,079 --> 00:05:31,480
他对一个图片

161
00:05:31,480 --> 00:05:34,000
他如何把它的高宽压缩

162
00:05:34,000 --> 00:05:36,040
然后在通道上拉长

163
00:05:36,040 --> 00:05:37,439
最后再拉成一条线

164
00:05:37,480 --> 00:05:38,800
进入我的权力阶层

165
00:05:39,439 --> 00:05:41,000
的一个变化

166
00:05:43,120 --> 00:05:46,720
接下来我们就训练了

167
00:05:46,720 --> 00:05:48,480
训练跟前面是一样的

168
00:05:48,480 --> 00:05:49,600
我们就是用

169
00:05:50,280 --> 00:05:52,800
我们取一个batch size等于128

170
00:05:53,120 --> 00:05:54,639
然后因为我的Fashion Analyst

171
00:05:54,639 --> 00:05:56,519
是一个28×28的数据集

172
00:05:56,519 --> 00:05:58,639
所以我们这里就做了一个

173
00:05:59,639 --> 00:06:00,840
挺奇怪的事情

174
00:06:00,879 --> 00:06:02,280
就是说我们把这个数据集

175
00:06:02,959 --> 00:06:04,120
读取来之后

176
00:06:04,120 --> 00:06:08,040
然后用图片把它拉到224×224

177
00:06:08,639 --> 00:06:10,319
就是我们一个resize选项

178
00:06:10,719 --> 00:06:11,680
大家有没有记得

179
00:06:11,680 --> 00:06:13,800
我们之前讲它的实现的时候

180
00:06:13,800 --> 00:06:16,079
有说如果是要resize的话

181
00:06:16,079 --> 00:06:18,120
我就又把图片给你拉一下

182
00:06:18,680 --> 00:06:22,199
这是为了模拟我们的AlexNet

183
00:06:22,199 --> 00:06:23,240
对输入的需求

184
00:06:23,639 --> 00:06:26,040
并没有说在真实情况下

185
00:06:26,040 --> 00:06:27,039
你不会去干这个事情

186
00:06:27,039 --> 00:06:29,120
就是说你把图片拉大

187
00:06:29,120 --> 00:06:30,719
并没有增加它的信息量

188
00:06:31,800 --> 00:06:33,360
所以人眼看当然会好一点

189
00:06:33,360 --> 00:06:35,279
但机器看是没什么区别的

190
00:06:35,319 --> 00:06:36,680
而且可能是有损的

191
00:06:37,279 --> 00:06:37,879
OK

192
00:06:40,439 --> 00:06:41,399
那么接下来就训练

193
00:06:41,560 --> 00:06:43,000
训练跟之前没什么区别

194
00:06:43,480 --> 00:06:44,680
就是我们调

195
00:06:44,680 --> 00:06:46,319
我们还是训练10次

196
00:06:46,319 --> 00:06:47,439
迭代10次

197
00:06:47,439 --> 00:06:48,599
然后这里可以看到

198
00:06:48,599 --> 00:06:50,439
是我们的学习率调的很低了

199
00:06:50,639 --> 00:06:51,560
我们的Nunet

200
00:06:51,560 --> 00:06:53,439
我记得应该是调到0.9

201
00:06:54,199 --> 00:06:55,159
是很大的

202
00:06:55,199 --> 00:06:56,439
然后AlexNet

203
00:06:56,439 --> 00:06:57,560
我们的学习率调的

204
00:06:57,600 --> 00:06:59,680
相对来说比较低的0.01

205
00:07:00,079 --> 00:07:02,719
我们之后会去仔细去讲这个事情

206
00:07:03,279 --> 00:07:04,079
学习率低

207
00:07:04,079 --> 00:07:06,439
当然是导致你的训练会变慢了

208
00:07:06,480 --> 00:07:08,120
这不是一件特别好的事情

209
00:07:08,920 --> 00:07:10,360
之后我们大量的技术

210
00:07:10,360 --> 00:07:11,920
来把学习率给弄高一点

211
00:07:12,759 --> 00:07:14,240
当然可以调0.01

212
00:07:14,240 --> 00:07:15,519
我是随手调的

213
00:07:15,519 --> 00:07:16,319
你可以调高一点

214
00:07:16,319 --> 00:07:17,399
应该是调到0.05

215
00:07:17,399 --> 00:07:18,840
或者0.1可能都没问题

216
00:07:18,840 --> 00:07:19,680
大家可以试一下

217
00:07:20,360 --> 00:07:22,959
另外一个我们就没有直接演示是训练

218
00:07:22,959 --> 00:07:24,759
如果你来的比较早的话

219
00:07:24,759 --> 00:07:26,800
你看到我在直播开始前

220
00:07:26,800 --> 00:07:28,399
是把它重新跑了一遍

221
00:07:28,399 --> 00:07:30,079
因为它跑起来挺慢的

222
00:07:30,360 --> 00:07:31,840
跑一下大概几分钟

223
00:07:31,840 --> 00:07:33,399
所以我就没有重新跑一下

224
00:07:33,399 --> 00:07:34,000
大概三分钟

225
00:07:34,160 --> 00:07:35,519
这你看三分钟的样子

226
00:07:35,800 --> 00:07:37,759
所以我就没有现场跑

227
00:07:38,719 --> 00:07:40,360
直播前我重新跑了一下

228
00:07:41,719 --> 00:07:42,719
首先看一下

229
00:07:42,719 --> 00:07:43,480
就是说

230
00:07:44,240 --> 00:07:45,560
首先我的accuracy

231
00:07:45,800 --> 00:07:48,680
大家记不记得我们nernet是accuracy 0.82

232
00:07:48,800 --> 00:07:50,839
就测试的精度也是0.82

233
00:07:50,879 --> 00:07:53,240
所以现在AlexNet就直接

234
00:07:53,439 --> 00:07:56,079
把我的精度变到了0.88

235
00:07:56,759 --> 00:07:57,399
对吧

236
00:07:59,000 --> 00:07:59,879
而且它的好处

237
00:07:59,879 --> 00:08:02,039
你看到它没有太多overfitting在里面

238
00:08:02,879 --> 00:08:03,920
这是因为

239
00:08:05,480 --> 00:08:06,519
没有太多overfitting

240
00:08:06,519 --> 00:08:07,879
是因为有很多原因

241
00:08:08,039 --> 00:08:09,079
学习率一比较低

242
00:08:09,120 --> 00:08:12,680
而且我们就跑了10个数据的一个apoc

243
00:08:12,680 --> 00:08:14,399
其实你的loss还在往下降

244
00:08:14,560 --> 00:08:16,079
所以这一块AlexNet

245
00:08:16,120 --> 00:08:19,439
绝对是能够overfit到数据集的

246
00:08:19,560 --> 00:08:20,799
只是我们学习率比较低

247
00:08:20,799 --> 00:08:22,639
然后没有跑很多次数据

248
00:08:22,639 --> 00:08:25,519
所以overfitting还没有发生在这个地方

249
00:08:25,759 --> 00:08:29,319
大家可以去把number of epoch改到50

250
00:08:29,319 --> 00:08:30,399
把学习率改大一点

251
00:08:30,399 --> 00:08:32,079
应该是能看到overfitting的情况

252
00:08:33,439 --> 00:08:35,279
所以但是另外一块

253
00:08:35,799 --> 00:08:37,799
主要是说我的精度有提升了

254
00:08:38,279 --> 00:08:40,000
第二个是说你的代价是什么

255
00:08:40,000 --> 00:08:40,279
对

256
00:08:40,600 --> 00:08:41,919
你的代价是记得

257
00:08:41,919 --> 00:08:44,759
Nernet你的训练速度大概是9万的

258
00:08:45,039 --> 00:08:48,120
我记得现在你的训练速度变成了4000

259
00:08:49,799 --> 00:08:50,959
慢了20倍

260
00:08:52,679 --> 00:08:54,600
所以为什么是说你的Nernet

261
00:08:54,600 --> 00:08:57,480
它其实计算量比你少了200倍的样子

262
00:08:57,600 --> 00:08:59,519
但你为什么这里只慢了20倍

263
00:09:00,079 --> 00:09:03,240
是因为Nernet太小了

264
00:09:03,240 --> 00:09:05,680
它都无法使用我们的GPU的核

265
00:09:05,680 --> 00:09:07,000
我们GPU有上千个核

266
00:09:07,000 --> 00:09:08,399
我们之后会来讲硬件的

267
00:09:08,399 --> 00:09:09,600
讲一点点硬件的东西

268
00:09:09,840 --> 00:09:11,799
就是说Nernet这个

269
00:09:12,080 --> 00:09:13,600
PLAN大小又很小

270
00:09:13,600 --> 00:09:14,720
然后卷迹又很小

271
00:09:14,799 --> 00:09:17,200
它的并行度很差

272
00:09:17,440 --> 00:09:20,600
就是根本无法用上我们GPU上上千个核

273
00:09:21,240 --> 00:09:23,639
所以Alexand相对来说会好一点

274
00:09:23,720 --> 00:09:27,120
但后面的网络更加是适合GPU计算

275
00:09:27,960 --> 00:09:29,399
所以Alexand相对来说

276
00:09:29,399 --> 00:09:31,720
在GPU的使用率上应该我猜

277
00:09:31,759 --> 00:09:33,399
大家可以去运选室观察一下

278
00:09:33,399 --> 00:09:34,320
它的GPU使用率

279
00:09:34,320 --> 00:09:35,560
我猜大概是70%

280
00:09:35,560 --> 00:09:36,560
80%可能是有的

281
00:09:36,680 --> 00:09:38,080
甚至可能80%多

282
00:09:38,160 --> 00:09:39,279
90%有可能

283
00:09:40,240 --> 00:09:42,200
所以就是说意味着是说

284
00:09:42,200 --> 00:09:45,759
我虽然比你计算量多了200倍

285
00:09:45,759 --> 00:09:47,399
但实际上也就慢了20倍

286
00:09:48,080 --> 00:09:50,399
而且你看到其实不慢

287
00:09:50,840 --> 00:09:53,399
每秒就能够跑个4000个样本

288
00:09:53,600 --> 00:09:54,920
如果你是

289
00:09:56,000 --> 00:09:58,480
如果你是就算是ImageNet的话

290
00:09:58,519 --> 00:09:59,680
那么你也就

291
00:10:00,200 --> 00:10:02,400
你120万个

292
00:10:03,440 --> 00:10:05,040
120万个样本的话

293
00:10:05,080 --> 00:10:08,960
那么也就是3000秒能跑完

294
00:10:09,240 --> 00:10:11,040
将近一个小时能够跑

295
00:10:11,080 --> 00:10:12,240
迭代一次数据

296
00:10:13,000 --> 00:10:15,720
你在ImageNet上可能训练个

297
00:10:16,480 --> 00:10:17,800
比如说100轮的话

298
00:10:17,840 --> 00:10:19,360
那就是你100个小时

299
00:10:19,360 --> 00:10:20,920
在单卡上也能跑完

300
00:10:21,080 --> 00:10:21,880
100个小时

301
00:10:22,200 --> 00:10:23,040
八九天的样子

302
00:10:24,200 --> 00:10:27,680
所以AlexNet的一个性能

303
00:10:27,840 --> 00:10:28,880
所以看到是

304
00:10:29,080 --> 00:10:29,920
慢了20倍

305
00:10:29,920 --> 00:10:32,920
精度从0.8涨到了0.88

306
00:10:33,040 --> 00:10:33,320
好

307
00:10:33,320 --> 00:10:34,400
这就是我们的

308
00:10:34,680 --> 00:10:36,760
第一个深度神经网络

