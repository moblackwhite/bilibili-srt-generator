1
00:00:00,000 --> 00:00:03,320
我们在讲Transformer之前

2
00:00:03,320 --> 00:00:07,679
我们先给大家介绍一个比较重要的一个东西

3
00:00:07,679 --> 00:00:09,279
叫做自注意力

4
00:00:09,279 --> 00:00:10,919
叫做Self-Attention

5
00:00:10,919 --> 00:00:14,240
它其实没有别的特殊的地方

6
00:00:14,240 --> 00:00:18,920
主要是说你整个Key、Value、Query到底是怎么选

7
00:00:18,920 --> 00:00:22,440
自注意力机制它有它自己的一套选法

8
00:00:23,120 --> 00:00:23,440
好

9
00:00:23,440 --> 00:00:24,879
我们来看一下是怎么回事

10
00:00:27,280 --> 00:00:29,280
首先你给定一个序列

11
00:00:29,920 --> 00:00:32,079
可以看一下就是X1一直到Xn

12
00:00:32,079 --> 00:00:34,719
就是成为一个n的一个序列

13
00:00:35,320 --> 00:00:39,120
然后每一个Xi是一个常为D的一个向量

14
00:00:39,960 --> 00:00:42,039
这样子我们以前看过了

15
00:00:42,160 --> 00:00:44,840
就是你一个序列

16
00:00:44,840 --> 00:00:46,719
每个序列有一个常为D的特征

17
00:00:46,719 --> 00:00:50,400
这是我们最正常的一个假设

18
00:00:51,400 --> 00:00:53,560
然后自注意力是怎么办

19
00:00:53,719 --> 00:00:55,040
自注意力

20
00:00:55,280 --> 00:00:57,799
实话称就是Self-Attention

21
00:00:57,920 --> 00:01:04,800
它将Xi又当Key又当Value又当Query

22
00:01:06,439 --> 00:01:11,840
这样来对每个序列抽取特征得到Y1到Yn

23
00:01:12,439 --> 00:01:13,920
具体来看是这么回事

24
00:01:13,920 --> 00:01:15,920
可以看一下是说

25
00:01:15,920 --> 00:01:18,080
首先Yi是怎么来的

26
00:01:19,000 --> 00:01:22,960
Yi就是对应的Xi的抽的特征

27
00:01:23,440 --> 00:01:29,440
说白了就是把Xi丢进注意力实话层里面

28
00:01:30,280 --> 00:01:33,320
然后实话层就是Xi是它的Query

29
00:01:33,840 --> 00:01:35,040
然后它对应的Y对吧

30
00:01:35,040 --> 00:01:38,400
这个是很直觉的是这样子

31
00:01:38,680 --> 00:01:40,800
但是它的Key它的Value

32
00:01:41,320 --> 00:01:47,120
就是所有的X1就是X1又是Key又是Value

33
00:01:47,160 --> 00:01:49,000
然后一直重复到Xn

34
00:01:49,000 --> 00:01:51,680
Xn也是作为Key和Value

35
00:01:51,960 --> 00:01:55,040
其中也包括了Xi和Xi

36
00:01:55,960 --> 00:01:56,440
OK

37
00:01:56,640 --> 00:01:57,520
所以基本上就是说

38
00:01:57,520 --> 00:01:59,880
你可认为我给定一个序列

39
00:02:00,760 --> 00:02:02,120
我放进去之后

40
00:02:02,480 --> 00:02:05,720
我就会对每一个序列中的每个元素可以输出

41
00:02:06,320 --> 00:02:08,520
就你这么看有点像RN对吧

42
00:02:08,680 --> 00:02:10,520
就是RN其实也是这么回事

43
00:02:10,520 --> 00:02:12,480
就是说你给一个序列进去

44
00:02:12,520 --> 00:02:13,760
然后一个RN的层

45
00:02:13,920 --> 00:02:15,400
它给你每个输出出来

46
00:02:16,439 --> 00:02:19,080
所以你这么做的情况下

47
00:02:19,280 --> 00:02:21,400
我不需要额外的Key和Value

48
00:02:21,400 --> 00:02:21,640
Query

49
00:02:21,640 --> 00:02:23,360
它们都是一个东西的情况下

50
00:02:23,560 --> 00:02:26,080
那么我们就可以用自注意力

51
00:02:26,520 --> 00:02:27,280
这个东西

52
00:02:28,040 --> 00:02:29,720
来处理序列

53
00:02:30,600 --> 00:02:30,920
OK

54
00:02:30,920 --> 00:02:33,400
就不需要说我们之前一定要有Encode

55
00:02:33,400 --> 00:02:34,080
Decode

56
00:02:34,320 --> 00:02:35,320
它就Self

57
00:02:35,320 --> 00:02:36,960
就是说Key

58
00:02:37,439 --> 00:02:37,960
Query

59
00:02:39,520 --> 00:02:41,240
Value都是来自于自己

60
00:02:41,439 --> 00:02:42,640
所以它叫自注意力

61
00:02:42,640 --> 00:02:43,560
就是自的意思

62
00:02:45,320 --> 00:02:46,520
如果你画图的话

63
00:02:46,520 --> 00:02:47,840
就基本上是这样子

64
00:02:47,840 --> 00:02:51,280
就是说你T时刻的XI

65
00:02:51,719 --> 00:02:52,840
它又作为Key

66
00:02:52,840 --> 00:02:53,680
又作为Value

67
00:02:53,680 --> 00:02:54,280
又作为Query

68
00:02:54,280 --> 00:02:55,960
到一个Attention里面

69
00:02:56,240 --> 00:02:58,640
Tension跟之前的Attention是一个东西

70
00:02:59,520 --> 00:03:01,480
就是说Self的意思

71
00:03:01,480 --> 00:03:03,480
只是说我这个Key

72
00:03:03,480 --> 00:03:05,199
Value和Query是怎么取的

73
00:03:05,719 --> 00:03:06,199
OK

74
00:03:07,759 --> 00:03:09,080
然后我们可以比较一下

75
00:03:09,640 --> 00:03:12,960
就是说CN RN和自注意力

76
00:03:13,039 --> 00:03:15,120
都可以用来处理序列

77
00:03:15,400 --> 00:03:16,520
所以因为篇幅关系

78
00:03:16,520 --> 00:03:18,760
我没有讲用CN怎么来做了

79
00:03:18,920 --> 00:03:20,680
CN的做法其实也很简单

80
00:03:23,280 --> 00:03:25,600
你就把一个序列当做一个

81
00:03:25,600 --> 00:03:28,920
EV的一个输入

82
00:03:28,920 --> 00:03:31,680
就是说我们之前的图片有高和宽

83
00:03:32,680 --> 00:03:33,800
然后有每个像素

84
00:03:33,800 --> 00:03:34,640
它有个Channel

85
00:03:34,640 --> 00:03:35,680
就是它的特征数

86
00:03:36,400 --> 00:03:38,120
然后你要做CN做序列的话

87
00:03:38,240 --> 00:03:40,520
你就当做是一个EV的东西

88
00:03:40,520 --> 00:03:43,040
就One Dimensional Convolution

89
00:03:43,680 --> 00:03:46,439
然后你就只有一个宽没有高了

90
00:03:46,840 --> 00:03:48,680
然后你一滴一滴卷进来

91
00:03:48,680 --> 00:03:51,240
然后你的滴每个项量

92
00:03:51,240 --> 00:03:54,000
每个元素的特征

93
00:03:54,120 --> 00:03:55,840
就当做它的Channel放进来

94
00:03:57,120 --> 00:03:58,800
所以它就可以用来处理

95
00:03:58,800 --> 00:04:00,280
你的文本序列

96
00:04:01,240 --> 00:04:02,879
所以如果你是用CN的话

97
00:04:02,879 --> 00:04:03,879
我们可以看一下

98
00:04:03,879 --> 00:04:06,800
就是说它的计算复杂度

99
00:04:07,600 --> 00:04:08,680
它的计算复杂度什么意思

100
00:04:08,680 --> 00:04:12,000
K是你的窗口的大小

101
00:04:14,080 --> 00:04:15,560
然后就是说你每一次看一个

102
00:04:15,560 --> 00:04:16,560
长为K的东西

103
00:04:17,199 --> 00:04:19,480
然后你再乘以你的长度

104
00:04:19,560 --> 00:04:20,680
再乘以你的D

105
00:04:20,680 --> 00:04:23,240
就是Dimension的平方

106
00:04:24,600 --> 00:04:26,280
并行度是什么意思

107
00:04:26,280 --> 00:04:27,240
并行度是说

108
00:04:27,240 --> 00:04:29,079
你每一次做的kernel

109
00:04:29,160 --> 00:04:31,360
就是每一个你的东西的输出

110
00:04:31,439 --> 00:04:33,040
假设你输出N个东西的话

111
00:04:33,120 --> 00:04:35,120
每一个输出可以自己并行

112
00:04:35,120 --> 00:04:35,759
做一个

113
00:04:36,800 --> 00:04:37,079
OK

114
00:04:37,079 --> 00:04:40,759
就是说每一个YI都可以同时做

115
00:04:40,759 --> 00:04:42,079
然后这样子好处是说

116
00:04:42,079 --> 00:04:43,000
我们有讲过

117
00:04:43,000 --> 00:04:44,720
因为GPU有大量的

118
00:04:45,040 --> 00:04:46,079
可以并行的单元

119
00:04:46,079 --> 00:04:47,519
所以它并行度高

120
00:04:47,519 --> 00:04:49,079
就意味着我的算的比较快

121
00:04:50,720 --> 00:04:52,759
最长的路径是什么意思

122
00:04:53,079 --> 00:04:54,800
最长的路径是说

123
00:04:54,800 --> 00:04:56,560
假设有一个信息

124
00:04:56,959 --> 00:04:59,360
要传递到

125
00:04:59,360 --> 00:05:00,560
就是说一个很长的序列

126
00:05:00,959 --> 00:05:01,879
一个很长序列

127
00:05:01,879 --> 00:05:03,199
我要通过细心

128
00:05:03,240 --> 00:05:04,879
我要把这个序列的信息

129
00:05:04,879 --> 00:05:06,560
传递到一个很厚的地方

130
00:05:07,439 --> 00:05:08,839
你应该怎么传递

131
00:05:08,839 --> 00:05:10,199
是因为你每一次

132
00:05:10,199 --> 00:05:12,839
你那个kernel只看K个kernel

133
00:05:13,079 --> 00:05:14,439
所以你的传递是说

134
00:05:14,439 --> 00:05:15,519
你要这么上去

135
00:05:15,519 --> 00:05:16,040
这么上去

136
00:05:16,040 --> 00:05:16,680
再这么下来

137
00:05:16,680 --> 00:05:17,399
这么下来

138
00:05:18,399 --> 00:05:18,800
OK

139
00:05:18,800 --> 00:05:20,199
当你的很长的时候

140
00:05:20,199 --> 00:05:21,759
你的k也不那么长的时候

141
00:05:22,480 --> 00:05:24,480
你可认为你最长那个序列

142
00:05:24,480 --> 00:05:26,480
就是信息是怎么传过去的

143
00:05:26,480 --> 00:05:28,639
是N除以k

144
00:05:29,399 --> 00:05:30,920
当你的kernel越大的话

145
00:05:30,920 --> 00:05:32,079
你就是说一次

146
00:05:32,079 --> 00:05:33,600
你kernel看整个序列

147
00:05:33,600 --> 00:05:34,480
很长的序列

148
00:05:34,519 --> 00:05:36,560
但如果kernel比较小的话

149
00:05:36,560 --> 00:05:38,279
你就得要很多层

150
00:05:38,560 --> 00:05:40,279
才能看到足够长的序列

151
00:05:40,279 --> 00:05:41,920
就最后那一层的输出

152
00:05:42,199 --> 00:05:44,520
能看到它的整个序列

153
00:05:44,520 --> 00:05:46,160
这在计算机视觉里面

154
00:05:46,160 --> 00:05:47,280
也叫做field

155
00:05:47,439 --> 00:05:48,720
就是你的视野

156
00:05:48,720 --> 00:05:50,439
就是你每一个神奇元的输出

157
00:05:50,439 --> 00:05:52,240
它对图片里面的视野

158
00:05:53,480 --> 00:05:54,920
这是用CN的情况

159
00:05:55,840 --> 00:05:58,920
RN的计算复杂度

160
00:05:58,920 --> 00:06:01,759
是说它是N乘以d的平方

161
00:06:02,040 --> 00:06:03,000
d的平方来自于

162
00:06:03,000 --> 00:06:04,680
你每次要做一个d的平方

163
00:06:04,680 --> 00:06:05,639
那个矩阵的乘法

164
00:06:05,960 --> 00:06:07,200
然后你要做n次

165
00:06:08,200 --> 00:06:09,840
所以它的复杂度

166
00:06:09,920 --> 00:06:11,800
其实比CN低那么一点点

167
00:06:12,920 --> 00:06:15,040
它的并行度很糟糕

168
00:06:15,040 --> 00:06:16,400
就是说你是每一次

169
00:06:16,400 --> 00:06:18,000
你要算完前一个

170
00:06:18,520 --> 00:06:20,319
就是说你要算它的时候

171
00:06:20,439 --> 00:06:21,920
你得把它给算完

172
00:06:22,600 --> 00:06:25,080
就是说你每一个yi的输出

173
00:06:25,200 --> 00:06:28,080
你得等yi-1给你算完了

174
00:06:28,639 --> 00:06:30,240
所以它的并行度很差

175
00:06:30,280 --> 00:06:31,439
它是OE的

176
00:06:32,800 --> 00:06:33,639
这样子的话

177
00:06:33,639 --> 00:06:37,080
RN其实在GPU上面做

178
00:06:37,720 --> 00:06:39,040
也不是那么容易

179
00:06:39,400 --> 00:06:40,720
我们也看到说

180
00:06:41,000 --> 00:06:42,480
我们框架的实现

181
00:06:42,480 --> 00:06:43,840
还比我们手写的实现

182
00:06:44,280 --> 00:06:45,760
要快个几倍

183
00:06:46,120 --> 00:06:46,600
另外一块

184
00:06:46,600 --> 00:06:48,400
我们把一些东西搞复杂一点

185
00:06:48,400 --> 00:06:51,120
好像也速度也没有变慢

186
00:06:51,120 --> 00:06:53,160
就是说我们把整个RN的复杂度

187
00:06:53,160 --> 00:06:53,520
变高

188
00:06:53,520 --> 00:06:56,080
是因为虽然计算量增加了

189
00:06:56,080 --> 00:06:57,360
但是并行度也增加了

190
00:06:57,360 --> 00:06:59,640
所以它实际的花石

191
00:06:59,840 --> 00:07:01,120
并没有发生变化

192
00:07:02,800 --> 00:07:05,080
它的最长的长度是什么意思

193
00:07:05,400 --> 00:07:06,240
最长的长度

194
00:07:06,240 --> 00:07:08,040
就是说x1的信息

195
00:07:08,240 --> 00:07:10,480
要一直传递到xn

196
00:07:10,560 --> 00:07:12,759
它需要经过一个漫长的

197
00:07:12,759 --> 00:07:14,240
一个ON的序列

198
00:07:15,160 --> 00:07:18,160
这个其实即使好处也是坏处

199
00:07:18,319 --> 00:07:19,400
坏处是说

200
00:07:19,400 --> 00:07:22,160
你一个信一个比较远的

201
00:07:22,160 --> 00:07:23,319
一句说一个句子

202
00:07:23,319 --> 00:07:24,480
一开头那个字

203
00:07:24,840 --> 00:07:27,120
假设你是句子最后的输出的话

204
00:07:27,120 --> 00:07:28,879
当你句子很长的时候

205
00:07:28,879 --> 00:07:30,319
你一路一路过去

206
00:07:30,360 --> 00:07:31,000
放到最后

207
00:07:31,000 --> 00:07:31,879
你可能就忘掉了

208
00:07:31,879 --> 00:07:32,960
一开始怎么回事

209
00:07:33,920 --> 00:07:35,520
当然是你很多弥补方法

210
00:07:35,680 --> 00:07:38,560
你可以用双向的

211
00:07:38,680 --> 00:07:40,200
就是倒过来走一遍

212
00:07:40,879 --> 00:07:43,360
另外一块是我们在上节课看到

213
00:07:43,360 --> 00:07:44,960
是说你可以加入

214
00:07:45,840 --> 00:07:46,680
注意力机制

215
00:07:46,800 --> 00:07:48,240
就sequence to sequence的时候

216
00:07:48,240 --> 00:07:49,520
你加入注意力机制

217
00:07:49,520 --> 00:07:52,800
就是说至少说你在解码的时候

218
00:07:52,800 --> 00:07:54,600
再去看你编码器

219
00:07:55,280 --> 00:07:56,800
我可以基本上可以回溯到

220
00:07:56,800 --> 00:07:58,280
所谓的前面的信息

221
00:07:58,280 --> 00:07:59,520
就是可以一次抓过来

222
00:07:59,520 --> 00:08:01,800
而不要看最后时刻的输出

223
00:08:03,319 --> 00:08:04,680
这是但反过来讲

224
00:08:05,160 --> 00:08:07,920
就RNN这种强持续的模型

225
00:08:08,240 --> 00:08:10,160
导致它对一个序列的

226
00:08:10,160 --> 00:08:14,040
它的对整个序列的记忆是做比较好的

227
00:08:14,200 --> 00:08:15,520
就是说它有个非常强的

228
00:08:15,520 --> 00:08:16,680
持续性在里面

229
00:08:16,720 --> 00:08:20,080
所以它特别擅长记忆一些序列

230
00:08:20,360 --> 00:08:21,360
假设你的模型

231
00:08:21,360 --> 00:08:23,120
对序列性要求比较高的话

232
00:08:23,400 --> 00:08:25,280
RNN是一个不错的选择

233
00:08:26,400 --> 00:08:27,680
那么看一下自注意力

234
00:08:27,879 --> 00:08:30,480
我们这是今天主要关注的东西

235
00:08:30,480 --> 00:08:31,240
是这个东西

236
00:08:31,840 --> 00:08:33,639
它的计算复杂度是什么

237
00:08:33,639 --> 00:08:35,920
是n平方乘以d

238
00:08:36,920 --> 00:08:37,960
n平方是什么

239
00:08:37,960 --> 00:08:38,519
来自什么

240
00:08:38,519 --> 00:08:42,199
来自于说每一个算每个output的时候

241
00:08:42,600 --> 00:08:46,319
你那个query要跟所有的input去做乘法

242
00:08:46,919 --> 00:08:48,240
乘法的复杂度是d

243
00:08:48,240 --> 00:08:49,279
因为你要长度

244
00:08:49,279 --> 00:08:50,319
你要个维度是d

245
00:08:51,439 --> 00:08:52,439
当然你可以做复杂一点

246
00:08:52,559 --> 00:08:54,600
大概也是差不多是

247
00:08:54,919 --> 00:08:56,679
你可以把那个东西做得复杂一点的话

248
00:08:56,679 --> 00:08:58,679
再加一个全连接进来的话

249
00:08:58,679 --> 00:08:59,480
会复杂一点

250
00:08:59,600 --> 00:09:01,120
我们说一下简单情况

251
00:09:02,120 --> 00:09:06,279
然后它的这个地方是一个n平方

252
00:09:06,279 --> 00:09:08,240
所以你跟它比较来看

253
00:09:08,399 --> 00:09:10,600
就是说自注意力比较RNN的话

254
00:09:10,639 --> 00:09:13,120
当你的序列比较长的时候

255
00:09:14,039 --> 00:09:16,000
它的计算复杂度还是挺高的

256
00:09:17,000 --> 00:09:17,480
对吧

257
00:09:17,519 --> 00:09:20,120
就是RNN是说你不要一个维度特别长

258
00:09:20,120 --> 00:09:21,039
你长不要紧

259
00:09:21,039 --> 00:09:22,360
长是一个线性的关系

260
00:09:22,799 --> 00:09:24,879
自注意力是说你很长的时候

261
00:09:24,879 --> 00:09:25,720
我就很麻烦

262
00:09:26,240 --> 00:09:27,680
因为就是说你越长的句子

263
00:09:27,680 --> 00:09:29,240
我每次的回去看

264
00:09:29,279 --> 00:09:30,960
我得每次看整个句子

265
00:09:31,360 --> 00:09:34,039
所以它的是n平方关系

266
00:09:34,360 --> 00:09:36,360
所以当你是比较长的句子的时候

267
00:09:36,399 --> 00:09:39,279
自注意力是通常来说是计算比较大的

268
00:09:39,519 --> 00:09:40,720
Cn也不会有问题

269
00:09:40,720 --> 00:09:42,639
Cn对n也是个线性关系

270
00:09:44,039 --> 00:09:44,919
并行度

271
00:09:45,600 --> 00:09:46,759
并行度它是很好的

272
00:09:46,759 --> 00:09:48,039
它是个ON的并行度

273
00:09:48,039 --> 00:09:50,360
就是说你算每一个yi的时候

274
00:09:51,360 --> 00:09:52,680
它不需要管别的

275
00:09:52,919 --> 00:09:53,480
对吧

276
00:09:53,720 --> 00:09:56,200
因为每个yi就是说你的query是自己

277
00:09:56,240 --> 00:09:58,600
然后你的key和value是别人的输入

278
00:09:58,720 --> 00:10:00,320
你跟别人的输出无关

279
00:10:00,360 --> 00:10:01,840
所以我就直接可以算了

280
00:10:02,200 --> 00:10:04,639
所以self-attention这个东西

281
00:10:05,159 --> 00:10:06,639
它的并行度非常好

282
00:10:08,080 --> 00:10:08,639
OK

283
00:10:08,679 --> 00:10:11,840
所以为什么是说大家花了很多

284
00:10:11,840 --> 00:10:14,720
就是说比如说现在TPU上

285
00:10:14,960 --> 00:10:17,000
就是说发现在GPU上

286
00:10:17,000 --> 00:10:20,679
好像Cn self-attention性能都差不多

287
00:10:20,720 --> 00:10:21,960
但是在TPU上

288
00:10:21,960 --> 00:10:23,519
我们有讲过TPU这种架构

289
00:10:23,639 --> 00:10:25,120
就是那种ASIC架构

290
00:10:25,399 --> 00:10:27,120
里面是一个巨大的局限乘法

291
00:10:27,279 --> 00:10:30,559
它这种架构就非常的适合你的这种

292
00:10:30,559 --> 00:10:31,159
transformer

293
00:10:31,200 --> 00:10:32,039
就是说这种

294
00:10:32,519 --> 00:10:34,240
attention这种架构

295
00:10:34,320 --> 00:10:37,519
因为你通常是做比较长的那一些序列

296
00:10:37,840 --> 00:10:39,720
所以它是个N平方关系

297
00:10:39,759 --> 00:10:41,799
所以它导致说它是一个

298
00:10:41,799 --> 00:10:43,200
总是一个很大的局限乘法

299
00:10:43,200 --> 00:10:44,440
在里面做运算

300
00:10:44,480 --> 00:10:46,759
所以它导致说性能特别好

301
00:10:47,399 --> 00:10:50,919
所以为什么是说大家去

302
00:10:53,080 --> 00:10:55,279
就也一方面解释了说

303
00:10:55,279 --> 00:10:57,240
为什么我们可以用很大的数据

304
00:10:57,240 --> 00:10:59,279
去训练自主义的意志

305
00:11:00,639 --> 00:11:03,720
另外一个是说它的最长路径是ON

306
00:11:04,240 --> 00:11:05,399
最长路径ON什么意思

307
00:11:05,399 --> 00:11:08,360
就是说任何一个地方的信息

308
00:11:08,720 --> 00:11:10,440
我要到任何一个输出的话

309
00:11:10,440 --> 00:11:12,240
我是直接可以过去的

310
00:11:12,399 --> 00:11:15,759
我不需要经过像RNN一样的这么转过去

311
00:11:15,759 --> 00:11:17,000
或者像CN一样的

312
00:11:17,000 --> 00:11:19,279
这么从上面走过去

313
00:11:20,080 --> 00:11:22,919
所以自主义力机制在于

314
00:11:23,080 --> 00:11:24,799
比较长的序列的时候

315
00:11:24,799 --> 00:11:27,519
它能够很快的抓取跟你很远的信息

316
00:11:27,519 --> 00:11:28,639
它有attention

317
00:11:28,639 --> 00:11:29,879
真的去匹配你

318
00:11:30,560 --> 00:11:32,120
就算你出现在很远的地方

319
00:11:32,120 --> 00:11:33,799
我也能够很快抓取过来

320
00:11:34,560 --> 00:11:38,000
所以就是说你从简单的数字

321
00:11:38,200 --> 00:11:40,440
我们就是说稍微总结一下

322
00:11:40,440 --> 00:11:41,960
就是说自主义力

323
00:11:42,159 --> 00:11:46,759
它其实比较适合处理比较长的文本

324
00:11:47,320 --> 00:11:48,960
就一个序列长度比较长

325
00:11:49,720 --> 00:11:53,200
是因为它的设计

326
00:11:53,200 --> 00:11:56,919
使得可以看的东西比较宽

327
00:11:57,000 --> 00:11:58,720
就是说可以无视距离

328
00:11:58,919 --> 00:12:00,399
有一次就过去了

329
00:12:00,399 --> 00:12:04,000
所以就自主义力机制

330
00:12:04,159 --> 00:12:05,159
就是整个这一块

331
00:12:05,319 --> 00:12:06,759
就是到我们之后的

332
00:12:06,759 --> 00:12:07,679
transformer

333
00:12:07,840 --> 00:12:08,240
BERT

334
00:12:08,559 --> 00:12:10,519
或者是GPT这种算法

335
00:12:10,519 --> 00:12:13,799
就是说他们真的就喜欢给一个很长的序列

336
00:12:13,799 --> 00:12:14,120
进来

337
00:12:14,120 --> 00:12:16,199
就是几百还是几千的序列

338
00:12:16,879 --> 00:12:19,120
然后因为它能看的比较长

339
00:12:19,279 --> 00:12:21,799
所以相对它的模型复杂度也够高

340
00:12:21,799 --> 00:12:25,679
然后它能够真的做比较大的数据

341
00:12:26,279 --> 00:12:27,439
但反过来讲说

342
00:12:27,439 --> 00:12:29,120
你虽然看的比较长

343
00:12:29,120 --> 00:12:30,399
但是你付出了代价

344
00:12:30,399 --> 00:12:30,639
对吧

345
00:12:30,639 --> 00:12:32,399
你的计算复杂度很高

346
00:12:32,440 --> 00:12:34,279
因为你要做一成为1000的话

347
00:12:34,279 --> 00:12:35,320
那就是1000的平方

348
00:12:35,360 --> 00:12:36,159
就是100万

349
00:12:36,840 --> 00:12:39,200
所以就导致说transformer

350
00:12:39,440 --> 00:12:39,960
BERT

351
00:12:39,960 --> 00:12:41,840
然后GPT这种模型

352
00:12:41,919 --> 00:12:43,879
就计算量特别大

353
00:12:44,200 --> 00:12:47,120
所以现在你是动不动要几千个GPU

354
00:12:47,120 --> 00:12:48,840
来做运算才行

355
00:12:49,080 --> 00:12:50,320
你讲CN RN

356
00:12:50,440 --> 00:12:52,039
通常来说也不会做那么大

357
00:12:52,159 --> 00:12:53,639
CN最大CN模型

358
00:12:53,680 --> 00:12:55,120
我们也不会做那么大

359
00:12:55,480 --> 00:12:58,919
但是基于自主义力的这些模型

360
00:12:59,000 --> 00:13:01,800
真的可以做到特别大了

361
00:13:01,920 --> 00:13:04,760
也是来自于他特别擅长处理

362
00:13:04,760 --> 00:13:05,680
比较长的版本

363
00:13:05,720 --> 00:13:06,480
但反过来讲

364
00:13:06,600 --> 00:13:08,320
它又是算的比较贵

365
00:13:08,320 --> 00:13:09,480
越长你算的越贵

366
00:13:09,480 --> 00:13:10,280
平方关系

367
00:13:10,600 --> 00:13:13,000
所以导致说计算的特别大

368
00:13:13,040 --> 00:13:15,320
也就是说最近很多工作

369
00:13:15,360 --> 00:13:18,320
都是说怎么样把它的计算复杂度

370
00:13:18,320 --> 00:13:18,840
降低一点

371
00:13:18,840 --> 00:13:20,840
怎么样用层次化的结构降低一点

372
00:13:20,880 --> 00:13:22,920
以及从系统角度来讲

373
00:13:22,960 --> 00:13:25,720
怎么样多弄点GPU来

374
00:13:26,680 --> 00:13:27,080
Ok

375
00:13:27,120 --> 00:13:28,759
这就是说

376
00:13:29,240 --> 00:13:31,639
自注意力基于自注意力的模型

377
00:13:31,840 --> 00:13:33,879
跟基于CN RN比

378
00:13:33,879 --> 00:13:35,759
大概是一个整体上来讲

379
00:13:35,759 --> 00:13:36,360
是什么回事

380
00:13:37,080 --> 00:13:37,680
Ok

381
00:13:40,440 --> 00:13:42,040
然后我接下来我们看到一个事情

382
00:13:42,040 --> 00:13:42,840
是说

383
00:13:43,960 --> 00:13:45,639
跟CN和RN不同

384
00:13:45,920 --> 00:13:46,759
自注意力机制

385
00:13:46,759 --> 00:13:48,720
没有记录位置的信息

386
00:13:49,480 --> 00:13:50,840
就CN其实你还是有的

387
00:13:50,840 --> 00:13:52,639
因为你是每一个输出

388
00:13:52,800 --> 00:13:54,000
你有一个窗口关系

389
00:13:54,080 --> 00:13:55,559
所以我知道每一个点的输出

390
00:13:55,960 --> 00:13:58,840
它肯定是在对应的输入的是

391
00:13:58,840 --> 00:14:00,639
它的位置隔壁的部分

392
00:14:01,039 --> 00:14:03,679
所以window窗口大小

393
00:14:04,039 --> 00:14:05,720
是有一点位置信息在里面的

394
00:14:06,639 --> 00:14:08,480
但RN当然不用说了

395
00:14:08,519 --> 00:14:10,519
它就是一个序列的东西

396
00:14:11,039 --> 00:14:12,840
但自注意力没有

397
00:14:13,399 --> 00:14:14,600
就是你会发现说

398
00:14:14,600 --> 00:14:15,840
假设我什么不管的话

399
00:14:15,840 --> 00:14:19,399
我的我整个输入的位置

400
00:14:19,519 --> 00:14:20,639
就XI序列

401
00:14:20,639 --> 00:14:22,120
我随机打乱一下

402
00:14:23,120 --> 00:14:24,600
我对输出没影响

403
00:14:24,600 --> 00:14:26,000
我不是说对输出没影响

404
00:14:26,000 --> 00:14:28,080
对输出的位置发生了变化

405
00:14:28,080 --> 00:14:30,320
但是对输出每个项量的内容

406
00:14:30,320 --> 00:14:31,320
不会发生变化

407
00:14:31,960 --> 00:14:32,759
就大家回忆一下

408
00:14:32,759 --> 00:14:36,039
就是说做attention的时候

409
00:14:36,720 --> 00:14:39,840
反正你的query和你的key

410
00:14:39,840 --> 00:14:40,560
做一个点击

411
00:14:40,560 --> 00:14:42,840
或做一个卷击

412
00:14:43,159 --> 00:14:43,759
不是做一个卷击

413
00:14:43,759 --> 00:14:44,919
就是做一个全连接层

414
00:14:45,919 --> 00:14:47,159
然后做一个softmax

415
00:14:47,600 --> 00:14:49,240
然后再一加就没了

416
00:14:50,000 --> 00:14:52,200
所以里面是没有任何位置信息的

417
00:14:52,879 --> 00:14:56,639
所以如果你是想纯用自注意力机制

418
00:14:56,639 --> 00:14:58,039
来做序列模型的话

419
00:14:58,799 --> 00:15:00,279
那么你没有位置信息

420
00:15:00,440 --> 00:15:01,440
你可能就有问题

421
00:15:02,560 --> 00:15:05,440
所以一个加入位置信息的办法

422
00:15:05,440 --> 00:15:08,680
是一个叫做位置编码的东西

423
00:15:09,519 --> 00:15:11,039
就是position encoding

424
00:15:11,759 --> 00:15:14,799
它有点有意思的地方

425
00:15:14,799 --> 00:15:16,680
是说它不是把位置信息

426
00:15:16,680 --> 00:15:17,840
加到模型里面

427
00:15:18,840 --> 00:15:20,320
就是说你鱼鸡蛋位置信息

428
00:15:20,320 --> 00:15:21,240
加进模型里面

429
00:15:21,240 --> 00:15:22,280
总会有各种问题

430
00:15:22,360 --> 00:15:23,560
CN的话就导致说

431
00:15:23,560 --> 00:15:26,320
你的每次的看一个比较长的序列

432
00:15:26,639 --> 00:15:27,720
RN的话更麻烦

433
00:15:27,720 --> 00:15:29,240
就是说你的变形度就低了

434
00:15:30,120 --> 00:15:35,800
所以他说我不要改变注意力机制本身

435
00:15:35,920 --> 00:15:39,399
就是能够享受说每一次能看所有

436
00:15:39,639 --> 00:15:41,240
第二个是说能够变形

437
00:15:42,080 --> 00:15:44,800
然后他就把位置编码信息

438
00:15:44,960 --> 00:15:46,680
放到输入里面

439
00:15:46,680 --> 00:15:47,960
让输入这个东西

440
00:15:48,600 --> 00:15:49,680
它有位置信息

441
00:15:50,960 --> 00:15:52,280
具体来看是这么回事

442
00:15:52,440 --> 00:15:53,000
就是说

443
00:15:55,600 --> 00:15:58,480
假设你的输入一个序列

444
00:15:58,680 --> 00:15:59,800
是有N个序列

445
00:16:00,040 --> 00:16:01,600
然后你每一个是D

446
00:16:01,960 --> 00:16:04,440
那么就是说它每一行是一个样本

447
00:16:04,640 --> 00:16:06,080
每列是特征

448
00:16:06,080 --> 00:16:07,160
我们一直是这么做的

449
00:16:07,960 --> 00:16:09,520
但是这个地方是一个序列

450
00:16:10,800 --> 00:16:13,680
然后你的位置编码矩阵

451
00:16:13,840 --> 00:16:17,840
也是一个同样大小的一个P

452
00:16:18,720 --> 00:16:20,480
它里面含了很多位置信息

453
00:16:22,560 --> 00:16:23,520
他怎么用它

454
00:16:23,680 --> 00:16:29,240
就是说他就把P加到X上面

455
00:16:31,120 --> 00:16:33,680
然后再作为自编码的输入

456
00:16:33,800 --> 00:16:35,280
等于是说每个P

457
00:16:35,280 --> 00:16:36,760
它就根据你的位置不一样

458
00:16:36,760 --> 00:16:38,240
有一点值不一样

459
00:16:38,520 --> 00:16:40,560
然后你就把这样子的话

460
00:16:40,720 --> 00:16:41,840
等于是说X

461
00:16:42,360 --> 00:16:44,399
IJ根据你IJ的不同

462
00:16:44,519 --> 00:16:46,720
所以它加了一个不同的值在里面

463
00:16:47,480 --> 00:16:48,840
然后它就作为输入

464
00:16:49,440 --> 00:16:51,440
放进你的自编码

465
00:16:52,080 --> 00:16:52,680
OK

466
00:16:52,720 --> 00:16:54,680
所以这个就是位置编码

467
00:16:54,720 --> 00:16:55,680
具体的计算

468
00:16:55,680 --> 00:16:57,680
它是说你的

469
00:16:58,920 --> 00:16:59,600
就是说

470
00:17:01,080 --> 00:17:02,440
这个东西就是说具体计算

471
00:17:02,440 --> 00:17:04,840
就是说它其实是每一列

472
00:17:04,960 --> 00:17:05,759
P的每一列

473
00:17:05,759 --> 00:17:06,440
就是说

474
00:17:06,559 --> 00:17:08,120
比如说基数

475
00:17:08,799 --> 00:17:10,920
基数列是一个cos函数

476
00:17:11,080 --> 00:17:13,640
偶数列是一个sin函数

477
00:17:13,800 --> 00:17:15,360
但是它不同的列

478
00:17:15,480 --> 00:17:17,039
之间周期不一样

479
00:17:17,039 --> 00:17:18,240
就是说这个地方

480
00:17:18,840 --> 00:17:20,720
这个地方就是周期会不一样

481
00:17:21,600 --> 00:17:23,240
就这个公式当然是这么写的

482
00:17:23,240 --> 00:17:23,880
就是说

483
00:17:24,519 --> 00:17:25,519
接下来给大家看一下

484
00:17:25,519 --> 00:17:27,160
实际效果长什么样子

485
00:17:29,640 --> 00:17:30,759
就是说你可以看到是说

486
00:17:30,920 --> 00:17:31,600
这里有个图

487
00:17:32,600 --> 00:17:33,360
图是什么意思

488
00:17:33,360 --> 00:17:35,240
图是说你的每一个

489
00:17:36,360 --> 00:17:38,680
X坐标是它的一个

490
00:17:39,039 --> 00:17:39,920
行数

491
00:17:40,600 --> 00:17:43,039
就是说我对第i个序列

492
00:17:43,279 --> 00:17:44,840
加的值是什么

493
00:17:45,600 --> 00:17:48,640
然后我们这里画了4根曲线

494
00:17:48,800 --> 00:17:50,759
就对应的是第6个col

495
00:17:50,759 --> 00:17:51,320
第7

496
00:17:51,320 --> 00:17:51,640
第8

497
00:17:51,640 --> 00:17:52,000
第9

498
00:17:52,000 --> 00:17:54,000
就是第6位到第9位

499
00:17:55,920 --> 00:17:57,160
然后这个维度

500
00:17:57,160 --> 00:17:58,519
你可以看到是说

501
00:17:59,840 --> 00:18:01,360
基本上就是说

502
00:18:03,039 --> 00:18:04,120
你的

503
00:18:04,840 --> 00:18:06,600
你蓝线第6位

504
00:18:06,680 --> 00:18:08,759
它的就是说对每一个列

505
00:18:09,000 --> 00:18:12,039
所以它的第6个特征

506
00:18:12,160 --> 00:18:13,240
它加的值

507
00:18:13,440 --> 00:18:14,720
它是一个这样子的变化

508
00:18:15,920 --> 00:18:17,359
然后你换一个下一点

509
00:18:17,400 --> 00:18:19,359
下列就是sin和cos的变化

510
00:18:19,359 --> 00:18:20,240
它就没有

511
00:18:20,240 --> 00:18:21,759
它的频率没有变

512
00:18:21,759 --> 00:18:22,720
主要是位移了

513
00:18:22,720 --> 00:18:23,599
就是说下一点

514
00:18:23,599 --> 00:18:24,480
就是第7列

515
00:18:24,559 --> 00:18:25,480
就是看到是

516
00:18:25,599 --> 00:18:27,240
它就蓝线位移了一下

517
00:18:27,240 --> 00:18:28,599
就是sin和cos就是位移

518
00:18:28,759 --> 00:18:29,640
就是周期一样的时候

519
00:18:29,640 --> 00:18:30,519
它就是位移

520
00:18:31,359 --> 00:18:32,519
第8列

521
00:18:32,720 --> 00:18:34,079
第8列可以看一下

522
00:18:34,079 --> 00:18:37,640
就是说这个绿线是周期变长了

523
00:18:38,360 --> 00:18:39,040
你看到没有

524
00:18:39,040 --> 00:18:40,560
就是说这个周期变长

525
00:18:40,759 --> 00:18:41,600
然后第9列

526
00:18:41,800 --> 00:18:43,080
也就是周期变长

527
00:18:43,080 --> 00:18:44,360
然后再位移一下

528
00:18:44,759 --> 00:18:45,520
所以导致说

529
00:18:45,520 --> 00:18:46,040
你可以看到

530
00:18:46,040 --> 00:18:48,240
不管每对每一个样本

531
00:18:50,759 --> 00:18:52,080
它的维度

532
00:18:52,240 --> 00:18:53,440
就是D dimension

533
00:18:53,560 --> 00:18:55,000
每一个东西加的值

534
00:18:55,000 --> 00:18:56,200
是有点不一样的

535
00:18:56,880 --> 00:18:58,840
但样本之间也是不一样的

536
00:18:58,840 --> 00:19:00,040
就是说这个是

537
00:19:01,200 --> 00:19:02,440
这个是第10

538
00:19:02,440 --> 00:19:03,880
比如说15个样本

539
00:19:04,000 --> 00:19:04,720
这个是

540
00:19:05,680 --> 00:19:07,600
这个是第16个样本的话

541
00:19:08,480 --> 00:19:09,560
可以看到是说

542
00:19:09,600 --> 00:19:12,320
它们每个样本加的值也不一样

543
00:19:13,880 --> 00:19:15,680
所以就是说P这个东西

544
00:19:15,800 --> 00:19:17,600
就是对于等于是说

545
00:19:17,600 --> 00:19:18,920
你对每个样本

546
00:19:18,960 --> 00:19:20,400
然后对每一个

547
00:19:22,080 --> 00:19:22,640
维度

548
00:19:22,759 --> 00:19:23,680
就是特征维度

549
00:19:23,800 --> 00:19:25,880
然后加了一点点不一样的值

550
00:19:26,000 --> 00:19:27,680
然后使得让你的模型

551
00:19:27,800 --> 00:19:28,840
就是说下次

552
00:19:28,840 --> 00:19:29,800
你总是发现

553
00:19:29,800 --> 00:19:31,160
模型总是发现说

554
00:19:31,440 --> 00:19:33,640
虽然你的在特定的

555
00:19:34,880 --> 00:19:35,640
XI里面

556
00:19:35,640 --> 00:19:37,040
它的总是加了一点东西

557
00:19:37,039 --> 00:19:38,559
是使得让模型

558
00:19:38,559 --> 00:19:40,920
去分辨这个细微的东西

559
00:19:40,920 --> 00:19:42,159
让它来去

560
00:19:42,200 --> 00:19:43,759
作为未知的信息在里面

561
00:19:44,200 --> 00:19:44,519
OK

562
00:19:44,519 --> 00:19:45,680
这也是一个比较

563
00:19:46,720 --> 00:19:47,480
奇怪的

564
00:19:47,799 --> 00:19:48,680
就是说一个东西

565
00:19:48,680 --> 00:19:49,960
就是说不像我们之前的

566
00:19:49,960 --> 00:19:52,119
像把未知信息放进模型

567
00:19:52,159 --> 00:19:53,799
或者就是把未知信息

568
00:19:53,920 --> 00:19:55,000
跟你数据分开

569
00:19:55,000 --> 00:19:56,159
就是空开的起来

570
00:19:56,200 --> 00:19:57,639
它直接给你加进去

571
00:19:58,440 --> 00:19:59,440
这也是一个

572
00:20:00,359 --> 00:20:01,000
加进去的

573
00:20:01,000 --> 00:20:02,480
就是说你的好处是说

574
00:20:02,480 --> 00:20:03,440
你不改变模型

575
00:20:03,440 --> 00:20:05,359
然后你也不改变数据的大小

576
00:20:05,399 --> 00:20:06,720
但坏处是说

577
00:20:07,039 --> 00:20:08,000
要看你的模型

578
00:20:08,000 --> 00:20:09,480
能不能去认它了

579
00:20:10,079 --> 00:20:11,879
就你模型能不能把你加进去

580
00:20:11,879 --> 00:20:12,639
那些未知信息

581
00:20:12,759 --> 00:20:13,960
当做有用的feature

582
00:20:13,960 --> 00:20:14,920
给你用起来

583
00:20:15,279 --> 00:20:17,639
这个是看你模型的好坏了

584
00:20:18,440 --> 00:20:19,039
OK

585
00:20:21,399 --> 00:20:22,720
另外一块就是说

586
00:20:24,039 --> 00:20:26,000
我们就是说举个比方

587
00:20:26,000 --> 00:20:26,799
就是说

588
00:20:27,159 --> 00:20:27,759
计算机

589
00:20:28,079 --> 00:20:30,879
你可以认为它使用的是二进制变码

590
00:20:31,799 --> 00:20:32,359
对吧

591
00:20:32,399 --> 00:20:35,039
就是说你说0一直到7

592
00:20:35,039 --> 00:20:37,079
假设我用三位数来表示的话

593
00:20:37,519 --> 00:20:39,519
那么第一位就是一个

594
00:20:40,839 --> 00:20:41,839
00011

595
00:20:41,839 --> 00:20:44,879
就是变化周期比较长的一个变化

596
00:20:44,920 --> 00:20:46,279
然后接下来就是说

597
00:20:46,319 --> 00:20:47,960
你的频率变化是4的话

598
00:20:47,960 --> 00:20:48,879
那么第二位的话

599
00:20:49,000 --> 00:20:50,639
就是一个频率变化是

600
00:20:51,440 --> 00:20:54,000
2的一个001001

601
00:20:54,000 --> 00:20:56,119
最后一位就是01001

602
00:20:56,680 --> 00:20:57,599
这样子的话

603
00:20:57,599 --> 00:20:58,599
就是说你可以认为

604
00:20:58,599 --> 00:21:00,119
计算机把一个数字

605
00:21:00,200 --> 00:21:02,079
假设我就表示8个数字的话

606
00:21:02,079 --> 00:21:05,879
我可以用一个常为3的特征来表示

607
00:21:07,000 --> 00:21:10,159
而且特征的编码就是说有三位

608
00:21:10,159 --> 00:21:11,839
然后它其实就是一个0和1

609
00:21:11,839 --> 00:21:13,519
一个东西在跳

610
00:21:13,759 --> 00:21:14,240
跳的话

611
00:21:14,439 --> 00:21:16,399
就是说你每一个维度的

612
00:21:16,399 --> 00:21:17,839
它的频率不一样

613
00:21:17,839 --> 00:21:19,399
最后一位它的频率最快

614
00:21:19,399 --> 00:21:21,079
就是00000

615
00:21:21,359 --> 00:21:22,919
最前面一位它的频率最慢

616
00:21:22,919 --> 00:21:24,639
就是00001到111

617
00:21:25,480 --> 00:21:25,960
OK

618
00:21:25,960 --> 00:21:28,639
这就是计算机用的二进制编码

619
00:21:29,119 --> 00:21:31,439
所以位置矩阵编码

620
00:21:32,079 --> 00:21:33,079
就是说你可以认为

621
00:21:33,079 --> 00:21:35,039
也有一点点类似于这个东西

622
00:21:35,199 --> 00:21:37,559
但是它首先它是一个实数的东西

623
00:21:38,199 --> 00:21:41,599
它是在1和-1之间一个实数的变化

624
00:21:41,960 --> 00:21:43,759
所以它能编码的东西当然多一点

625
00:21:44,199 --> 00:21:45,359
第二个你可以看到是说

626
00:21:45,359 --> 00:21:47,319
它其实因为它用的是一个

627
00:21:47,720 --> 00:21:48,639
sin和sin函数

628
00:21:48,759 --> 00:21:50,759
所以它也是一个这么一个周期

629
00:21:50,759 --> 00:21:51,359
在里面的

630
00:21:52,279 --> 00:21:53,679
你可以看到首先

631
00:21:54,079 --> 00:21:56,000
这个是这个图是刚刚那个图

632
00:21:56,079 --> 00:21:57,439
但是我们换一个方式

633
00:21:57,439 --> 00:21:58,759
用的热度图来画它

634
00:21:59,759 --> 00:22:02,200
你的x轴我们转了一下

635
00:22:03,240 --> 00:22:04,720
x轴现在是你的特征

636
00:22:06,400 --> 00:22:08,440
然后你的y一行就是一个样本

637
00:22:09,400 --> 00:22:09,759
OK

638
00:22:09,759 --> 00:22:10,720
一行是一个样本

639
00:22:11,720 --> 00:22:13,599
所以基本上你看到对每一行

640
00:22:13,879 --> 00:22:15,759
就是说等于是我对这一行

641
00:22:15,759 --> 00:22:17,960
它的位置信息进行了编码

642
00:22:18,680 --> 00:22:19,839
就把第二个样本

643
00:22:20,440 --> 00:22:22,559
用一个常为d的一个向量

644
00:22:22,559 --> 00:22:24,000
来去进行编码

645
00:22:24,440 --> 00:22:25,640
它的编码你可以认为

646
00:22:25,640 --> 00:22:27,400
就是说跟这个是有反的

647
00:22:27,800 --> 00:22:29,200
就是说最前的为

648
00:22:29,360 --> 00:22:31,440
它是用一个频率比较高的编码

649
00:22:32,040 --> 00:22:33,640
你也看到就是说频率变化比较快

650
00:22:34,280 --> 00:22:37,080
越到后面它的变化就越来越慢

651
00:22:37,519 --> 00:22:41,440
但是它确实是很类似于是说

652
00:22:41,800 --> 00:22:43,680
计算机的一个二进制编码

653
00:22:43,960 --> 00:22:45,960
但是它比它的编码

654
00:22:45,960 --> 00:22:47,720
它就是相对来说

655
00:22:47,720 --> 00:22:50,040
它可以任意根据你的

656
00:22:50,480 --> 00:22:52,200
你可以有任意的长度

657
00:22:52,200 --> 00:22:53,200
d都没关系

658
00:22:53,560 --> 00:22:54,960
第二个说它用的是实数

659
00:22:55,160 --> 00:22:55,840
正义到负义

660
00:22:55,839 --> 00:22:58,319
因为我们输入本来就是实数的东西

661
00:22:58,599 --> 00:22:59,639
所以它里面的东西

662
00:22:59,839 --> 00:23:01,559
就是说更加稍微复杂一点点

663
00:23:01,919 --> 00:23:03,359
但是核心思想是一样的

664
00:23:03,359 --> 00:23:05,199
就是说我对每一个位置

665
00:23:05,199 --> 00:23:06,000
每一个样本

666
00:23:06,359 --> 00:23:08,039
就是序列中的第二个样本

667
00:23:08,599 --> 00:23:11,720
给你一个独一无二的

668
00:23:11,720 --> 00:23:13,639
常为d的一个位置信息

669
00:23:13,919 --> 00:23:16,079
然后你把它加到你的数据里面去

670
00:23:16,079 --> 00:23:21,799
然后作为我的注意力的输入

671
00:23:21,799 --> 00:23:24,559
这样子它就能看到位置信息了

672
00:23:25,359 --> 00:23:28,240
Ok这就是整个的idea

673
00:23:29,119 --> 00:23:30,319
另外一个是说

674
00:23:30,679 --> 00:23:32,960
它为什么要用这样子的东西来做

675
00:23:33,960 --> 00:23:37,319
为什么你要用一个sin cos的东西来做

676
00:23:39,319 --> 00:23:40,919
它的原因是说

677
00:23:40,919 --> 00:23:43,399
你用sin cos的好处

678
00:23:43,559 --> 00:23:45,480
它的一个主要的好处是说

679
00:23:45,480 --> 00:23:49,720
它编码的是一个相对的位置信息

680
00:23:51,559 --> 00:23:52,359
Ok就是说

681
00:23:52,600 --> 00:23:56,400
你位置位于i加上delta

682
00:23:56,400 --> 00:23:58,840
就是它的位置编码

683
00:24:00,280 --> 00:24:04,520
可以线性的投影到位置i处的线性编码

684
00:24:04,520 --> 00:24:05,000
表示

685
00:24:05,520 --> 00:24:07,960
这个东西是说你跟i无关

686
00:24:08,120 --> 00:24:10,360
就跟你相对的位置相关

687
00:24:10,960 --> 00:24:12,520
就是说你怎么投投引进去

688
00:24:12,640 --> 00:24:19,160
就是说这个是加的delta的位置

689
00:24:19,320 --> 00:24:20,640
这个是没有加的时候

690
00:24:20,800 --> 00:24:22,240
那么它这两个东西

691
00:24:22,360 --> 00:24:27,040
它是可以用一个这样子的矩阵来进行投影的

692
00:24:27,360 --> 00:24:29,680
这个矩阵小矩阵二乘二的投影矩阵

693
00:24:30,000 --> 00:24:32,360
跟你的i跟你的位置是无关的

694
00:24:33,240 --> 00:24:35,480
跟你序列中的位置i是无关的

695
00:24:35,480 --> 00:24:36,800
当然跟j是相关的

696
00:24:36,800 --> 00:24:38,440
就跟你的位置

697
00:24:38,440 --> 00:24:40,360
就是你的dimension的信息当然是相关的

698
00:24:40,800 --> 00:24:42,600
但是所以就说这样子

699
00:24:42,600 --> 00:24:44,920
意味着我在一个序列中

700
00:24:44,960 --> 00:24:48,200
假设一个词出现在另外一个词

701
00:24:48,200 --> 00:24:50,920
比如后面两个或者三个的位置的时候

702
00:24:51,240 --> 00:24:53,960
不管这一对出现在我序列的哪个地方

703
00:24:54,840 --> 00:24:56,160
对于位置信息来讲

704
00:24:56,200 --> 00:24:59,080
他们都是可以通过一个同样的线性变换

705
00:24:59,080 --> 00:25:04,360
就同样一个同样的w是能够来给你找出来的

706
00:25:04,720 --> 00:25:07,680
所以这样子就鼓励我们的

707
00:25:07,680 --> 00:25:08,960
因为我们都是学的w

708
00:25:09,080 --> 00:25:11,039
我们都是去学线性变换

709
00:25:11,080 --> 00:25:12,480
所以相对来说

710
00:25:12,480 --> 00:25:13,799
你这样子编码的话

711
00:25:13,799 --> 00:25:16,000
它用w来线我们的线性

712
00:25:16,039 --> 00:25:18,759
那些w来建模的时候

713
00:25:19,079 --> 00:25:23,240
能够比较好的去算这种相对的位置信息

714
00:25:23,519 --> 00:25:26,119
不要看我一个词到底出现在一个句子

715
00:25:26,119 --> 00:25:27,039
绝对的位置

716
00:25:27,279 --> 00:25:28,400
绝对位置总是有问题的

717
00:25:28,400 --> 00:25:30,240
因为我们一个很长的语言模型的时候

718
00:25:30,240 --> 00:25:31,799
我随便切都可以

719
00:25:31,799 --> 00:25:32,240
对吧

720
00:25:32,240 --> 00:25:35,519
所以我需要的关心的是一个一对子的相对位置

721
00:25:36,519 --> 00:25:39,920
所以这个也是说多多少少是说解释了

722
00:25:39,920 --> 00:25:41,359
为什么我们用一个sine

723
00:25:41,480 --> 00:25:42,000
cosine

724
00:25:42,000 --> 00:25:45,640
这个奇怪的一个东西来进行位置编码

725
00:25:46,160 --> 00:25:49,079
总结一下

726
00:25:49,079 --> 00:25:51,240
总结一下

727
00:25:51,240 --> 00:25:52,119
就是说

728
00:25:52,559 --> 00:25:53,400
自注意力

729
00:25:53,400 --> 00:25:54,680
持话程

730
00:25:54,839 --> 00:25:57,440
就是说将你的一个序列的xi

731
00:25:57,799 --> 00:26:00,400
它又当key又当value又当query

732
00:26:01,480 --> 00:26:05,519
然后它能抽取对应的yi作为它的特征

733
00:26:05,960 --> 00:26:06,920
这样子的话

734
00:26:07,079 --> 00:26:08,839
自注意力持话程

735
00:26:09,039 --> 00:26:11,119
它就能够给一个序列

736
00:26:11,200 --> 00:26:12,359
就不需要额外的什么东西

737
00:26:12,359 --> 00:26:13,240
它就能输出

738
00:26:13,240 --> 00:26:15,160
对每个序列输出它的元素

739
00:26:15,840 --> 00:26:17,720
它的好处就是说它可以完全闭型

740
00:26:17,720 --> 00:26:18,880
它跟cn是一样的

741
00:26:18,880 --> 00:26:20,519
所以计算效率比较高

742
00:26:21,480 --> 00:26:24,200
它对它的最长序列唯一

743
00:26:24,200 --> 00:26:25,480
就是说任何一个输出

744
00:26:25,600 --> 00:26:27,560
它能看到整个序列的信息

745
00:26:28,960 --> 00:26:30,279
所以这也就是为什么

746
00:26:30,279 --> 00:26:32,200
通常当你的文本比较

747
00:26:32,240 --> 00:26:33,880
处理比较大的文本的时候

748
00:26:34,080 --> 00:26:35,519
处理比较长的序列的时候

749
00:26:35,800 --> 00:26:37,240
通常我们会有attention

750
00:26:37,279 --> 00:26:38,560
用self-attention

751
00:26:39,680 --> 00:26:40,640
但是它的问题是

752
00:26:40,640 --> 00:26:42,680
它对长序列计算复杂度比较高

753
00:26:42,880 --> 00:26:44,759
所以到现在是我们的一道痛点

754
00:26:44,920 --> 00:26:45,920
训练一个模型

755
00:26:45,960 --> 00:26:47,839
你得花个几十万

756
00:26:47,839 --> 00:26:48,319
几百万

757
00:26:48,319 --> 00:26:48,839
几千万

758
00:26:49,039 --> 00:26:50,519
所以大家又变成一个

759
00:26:51,200 --> 00:26:52,440
军备竞赛了

760
00:26:52,799 --> 00:26:56,359
所以我们就比较

761
00:26:58,279 --> 00:26:59,920
最近一年

762
00:27:00,200 --> 00:27:01,480
或可能未来一两年

763
00:27:01,480 --> 00:27:03,960
大家都会去对这个东西比较苦恼

764
00:27:05,799 --> 00:27:07,039
另外一个就是说

765
00:27:07,039 --> 00:27:07,680
自注意力

766
00:27:07,839 --> 00:27:10,240
如果用来处理时序序列的话

767
00:27:10,240 --> 00:27:11,879
它没有位置信息

768
00:27:11,879 --> 00:27:14,119
所以一个做法是说

769
00:27:14,120 --> 00:27:15,200
在你的书里面

770
00:27:15,200 --> 00:27:16,800
加入一个位置编码

771
00:27:17,680 --> 00:27:18,240
位置编码

772
00:27:18,240 --> 00:27:20,800
使得自注意力能够去分辨

773
00:27:20,800 --> 00:27:21,600
说每个序列

774
00:27:21,600 --> 00:27:23,400
确实是有一点时序在里面的

775
00:27:23,880 --> 00:27:24,400
它那个东西

776
00:27:24,840 --> 00:27:27,080
有一点点像我们对计算机的数字编码

777
00:27:27,280 --> 00:27:28,600
就是对每个样本

778
00:27:28,600 --> 00:27:31,280
等于是给你一个常位低的一个编码

779
00:27:31,280 --> 00:27:32,640
再怎么加回去

780
00:27:33,000 --> 00:27:35,360
第二个说编码用的是sine和cosine函数

781
00:27:35,720 --> 00:27:39,960
使得它对于一个两个序列中的位置编码

782
00:27:40,360 --> 00:27:42,400
就是说固定常位

783
00:27:42,440 --> 00:27:44,120
它的距离是一个delta的时候

784
00:27:45,120 --> 00:27:46,600
不管你在序列中哪个位置

785
00:27:46,960 --> 00:27:48,680
它的编码信息都是一个

786
00:27:48,680 --> 00:27:50,560
可以通过一个线性变换来转换

